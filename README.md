# æ¬¢è¿æ¥åˆ°MindSpore Transformersï¼ˆMindFormersï¼‰

[![LICENSE](https://img.shields.io/github/license/mindspore-lab/mindformers.svg?style=flat-square)](https://github.com/mindspore-lab/mindformers/blob/master/LICENSE)
[![Downloads](https://static.pepy.tech/badge/mindformers)](https://pepy.tech/project/mindformers)
[![PyPI](https://badge.fury.io/py/mindformers.svg)](https://badge.fury.io/py/mindformers)
[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/mindformers.svg)](https://pypi.org/project/mindformers)

## ä¸€ã€ä»‹ç»

MindSpore Transformerså¥—ä»¶çš„ç›®æ ‡æ˜¯æ„å»ºä¸€ä¸ªå¤§æ¨¡å‹è®­ç»ƒã€å¾®è°ƒã€è¯„ä¼°ã€æ¨ç†ã€éƒ¨ç½²çš„å…¨æµç¨‹å¼€å‘å¥—ä»¶ï¼Œæä¾›ä¸šå†…ä¸»æµçš„Transformerç±»é¢„è®­ç»ƒæ¨¡å‹å’ŒSOTAä¸‹æ¸¸ä»»åŠ¡åº”ç”¨ï¼Œæ¶µç›–ä¸°å¯Œçš„å¹¶è¡Œç‰¹æ€§ã€‚æœŸæœ›å¸®åŠ©ç”¨æˆ·è½»æ¾çš„å®ç°å¤§æ¨¡å‹è®­ç»ƒå’Œåˆ›æ–°ç ”å‘ã€‚

MindSpore Transformerså¥—ä»¶åŸºäºMindSporeå†…ç½®çš„å¹¶è¡ŒæŠ€æœ¯å’Œç»„ä»¶åŒ–è®¾è®¡ï¼Œå…·å¤‡å¦‚ä¸‹ç‰¹ç‚¹ï¼š

- ä¸€è¡Œä»£ç å®ç°ä»å•å¡åˆ°å¤§è§„æ¨¡é›†ç¾¤è®­ç»ƒçš„æ— ç¼åˆ‡æ¢ï¼›
- æä¾›çµæ´»æ˜“ç”¨çš„ä¸ªæ€§åŒ–å¹¶è¡Œé…ç½®ï¼›
- èƒ½å¤Ÿè‡ªåŠ¨è¿›è¡Œæ‹“æ‰‘æ„ŸçŸ¥ï¼Œé«˜æ•ˆåœ°èåˆæ•°æ®å¹¶è¡Œå’Œæ¨¡å‹å¹¶è¡Œç­–ç•¥ï¼›
- ä¸€é”®å¯åŠ¨ä»»æ„ä»»åŠ¡çš„å•å¡/å¤šå¡è®­ç»ƒã€å¾®è°ƒã€è¯„ä¼°ã€æ¨ç†æµç¨‹ï¼›
- æ”¯æŒç”¨æˆ·è¿›è¡Œç»„ä»¶åŒ–é…ç½®ä»»æ„æ¨¡å—ï¼Œå¦‚ä¼˜åŒ–å™¨ã€å­¦ä¹ ç­–ç•¥ã€ç½‘ç»œç»„è£…ç­‰ï¼›
- æä¾›Trainerã€pipelineã€AutoClassç­‰é«˜é˜¶æ˜“ç”¨æ€§æ¥å£ï¼›
- æä¾›é¢„ç½®SOTAæƒé‡è‡ªåŠ¨ä¸‹è½½åŠåŠ è½½åŠŸèƒ½ï¼›
- æ”¯æŒäººå·¥æ™ºèƒ½è®¡ç®—ä¸­å¿ƒæ— ç¼è¿ç§»éƒ¨ç½²ï¼›

å¦‚æœæ‚¨å¯¹MindSpore Transformersæœ‰ä»»ä½•å»ºè®®ï¼Œè¯·é€šè¿‡issueä¸æˆ‘ä»¬è”ç³»ï¼Œæˆ‘ä»¬å°†åŠæ—¶å¤„ç†ã€‚

- ğŸ“ **[MindFormersæ•™ç¨‹æ–‡æ¡£](https://mindformers.readthedocs.io/zh_CN/latest)**
- ğŸ“ [å¤§æ¨¡å‹èƒ½åŠ›è¡¨ä¸€è§ˆ](https://mindformers.readthedocs.io/zh-cn/latest/docs/model_support_list.html#llm)
- ğŸ“ [MindPetæŒ‡å¯¼æ•™ç¨‹](docs/feature_cards/Pet_Tuners.md)
- ğŸ“ [AICCæŒ‡å¯¼æ•™ç¨‹](docs/readthedocs/source_zh_cn/docs/practice/AICC.md)

ç›®å‰æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨å¦‚ä¸‹ï¼š

|                         æ¨¡å‹                         | model name                                                         |
|:--------------------------------------------------:|:-------------------------------------------------------------------|
|        [LLama2](docs/model_cards/llama2.md)        | llama2_7b, llama2_13b, llama2_7b_lora, llama2_13b_lora, llama2_70b |
|        [LLama3](research/llama3/llama3.md)         | llama3_8b                                                          |
|          [GLM2](docs/model_cards/glm2.md)          | glm2_6b, glm2_6b_lora                                              |
|          [GLM3](docs/model_cards/glm3.md)          | glm3_6b, glm3_6b_lora                                              |
|          [GPT2](docs/model_cards/gpt2.md)          | gpt2, gpt2_13b                                                     |
|    [Baichuan2](research/baichuan2/baichuan2.md)    | baichuan2_7b, baichuan2_13b, baichuan2_7b_lora, baichuan2_13b_lora |
|           [Qwen](research/qwen/qwen.md)            | qwen_7b, qwen_14b, qwen_7b_lora, qwen_14b_lora                     |
|       [Qwen1.5](research/qwen1_5/qwen1_5.md)       | qwen1.5-14b, qwen1.5-72b                                           |
|     [CodeGeex2](docs/model_cards/codegeex2.md)     | codegeex2_6b                                                       |
|     [CodeLlama](docs/model_cards/codellama.md)     | codellama_34b                                                      |
|     [DeepSeek](research/deepseek/deepseek.md)      | deepseek-coder-33b-instruct                                        |
|     [Internlm](research/internlm/internlm.md)      | internlm_7b, internlm_20b, internlm_7b_lora                        |
|       [Mixtral](research/mixtral/mixtral.md)       | mixtral-8x7b                                                       |
| [Wizardcoder](research/wizardcoder/wizardcoder.md) | wizardcoder_15b                                                    |
|              [Yi](research/yi/yi.md)               | yi_6b, yi_34b                                                      |

## äºŒã€MindFormerså®‰è£…

### Linuxæºç ç¼–è¯‘æ–¹å¼å®‰è£…

æ”¯æŒæºç ç¼–è¯‘å®‰è£…ï¼Œç”¨æˆ·å¯ä»¥æ‰§è¡Œä¸‹è¿°çš„å‘½ä»¤è¿›è¡ŒåŒ…çš„å®‰è£…ã€‚

```bash
git clone -b r1.1.0 https://gitee.com/mindspore/mindformers.git
cd mindformers
bash build.sh
```

## ä¸‰ã€ç‰ˆæœ¬åŒ¹é…å…³ç³»

å½“å‰æ”¯æŒçš„ç¡¬ä»¶ä¸º[Atlas 800T A2](https://www.hiascend.com/hardware/ai-server?tag=900A2)è®­ç»ƒæœåŠ¡å™¨ã€‚

å½“å‰å¥—ä»¶å»ºè®®ä½¿ç”¨çš„Pythonç‰ˆæœ¬ä¸º3.9ã€‚

| MindFormers | MindPet |                  MindSpore                   |                                                                                                                                          CANN                                                                                                                                          |                                  é©±åŠ¨å›ºä»¶                                  |                                 é•œåƒé“¾æ¥                                  | å¤‡æ³¨   |
|:-----------:|:-------:|:--------------------------------------------:|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------:|:---------------------------------------------------------------------:|------|
|   r1.1.0    |  1.0.4  | [2.3.0rc2](https://www.mindspore.cn/install) | 8.0.RC1.beta1:<br> [aarch64](https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/CANN/CANN%208.0.RC1/Ascend-cann-toolkit_8.0.RC1_linux-aarch64.run)<br> [x86_64](https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/CANN/CANN%208.0.RC1/Ascend-cann-toolkit_8.0.RC1_linux-x86_64.run) | [driver](https://www.hiascend.com/hardware/firmware-drivers/community) | [image](http://mirrors.cn-central-221.ovaijisuan.com/detail/129.html) | ç‰ˆæœ¬åˆ†æ”¯ |

**å½“å‰MindFormersä»…æ”¯æŒå¦‚ä¸Šçš„è½¯ä»¶é…å¥—å…³ç³»**ã€‚å…¶ä¸­CANNå’Œå›ºä»¶é©±åŠ¨çš„å®‰è£…éœ€ä¸ä½¿ç”¨çš„æœºå™¨åŒ¹é…ï¼Œè¯·æ³¨æ„è¯†åˆ«æœºå™¨å‹å·ï¼Œé€‰æ‹©å¯¹åº”æ¶æ„çš„ç‰ˆæœ¬ã€‚

## å››ã€å¿«é€Ÿä½¿ç”¨

MindFormerså¥—ä»¶å¯¹å¤–æä¾›ä¸¤ç§ä½¿ç”¨å’Œå¼€å‘å½¢å¼ï¼Œä¸ºå¼€å‘è€…æä¾›çµæ´»ä¸”ç®€æ´çš„ä½¿ç”¨æ–¹å¼å’Œé«˜é˜¶å¼€å‘æ¥å£ã€‚

### æ–¹å¼ä¸€ï¼šä½¿ç”¨[msrunæ–¹å¼å¯åŠ¨](https://www.mindspore.cn/tutorials/experts/zh-CN/r2.3/parallel/msrun_launcher.html)ï¼ˆä»…é€‚ç”¨äºé…å¥—MindSpore2.3ä»¥ä¸Šç‰ˆæœ¬ï¼‰

ç”¨æˆ·å¯ä»¥ç›´æ¥cloneæ•´ä¸ªä»“åº“ï¼ŒæŒ‰ç…§ä»¥ä¸‹æ­¥éª¤å³å¯è¿è¡Œå¥—ä»¶ä¸­å·²æ”¯æŒçš„ä»»æ„`configs`æ¨¡å‹ä»»åŠ¡é…ç½®æ–‡ä»¶ï¼Œæ–¹ä¾¿ç”¨æˆ·å¿«é€Ÿè¿›è¡Œä½¿ç”¨å’Œå¼€å‘ï¼š

ç›®å‰msrunæ–¹å¼å¯åŠ¨ä¸æ”¯æŒæŒ‡å®šdevice_idå¯åŠ¨ï¼Œmsrunå‘½ä»¤ä¼šæŒ‰å½“å‰èŠ‚ç‚¹æ‰€æœ‰æ˜¾å¡é¡ºåºè®¾ç½®rank_idã€‚

- å‚æ•°è¯´æ˜

  | **å‚æ•°**           | **å•æœºæ˜¯å¦å¿…é€‰**  | **å¤šæœºæ˜¯å¦å¿…é€‰** |     **é»˜è®¤å€¼**      | **è¯´æ˜**           |
  |------------------|:-----------:|:----------:|:----------------:|------------------|
  | WORKER_NUM       |      âˆš      |     âˆš      |        8         | æ‰€æœ‰èŠ‚ç‚¹ä¸­ä½¿ç”¨è®¡ç®—å¡çš„æ€»æ•°    |
  | LOCAL_WORKER     |      Ã—      |     âˆš      |        8         | å½“å‰èŠ‚ç‚¹ä¸­ä½¿ç”¨è®¡ç®—å¡çš„æ•°é‡    |
  | MASTER_ADDR      |      Ã—      |     âˆš      |    127.0.0.1     | æŒ‡å®šåˆ†å¸ƒå¼å¯åŠ¨ä¸»èŠ‚ç‚¹çš„ip    |
  | MASTER_PORT      |      Ã—      |     âˆš      |       8118       | æŒ‡å®šåˆ†å¸ƒå¼å¯åŠ¨ç»‘å®šçš„ç«¯å£å·    |
  | NODE_RANK        |      Ã—      |     âˆš      |        0         | æŒ‡å®šå½“å‰èŠ‚ç‚¹çš„rank id   |
  | LOG_DIR          |      Ã—      |     âˆš      | output/msrun_log | æ—¥å¿—è¾“å‡ºè·¯å¾„ï¼Œè‹¥ä¸å­˜åœ¨åˆ™é€’å½’åˆ›å»º |
  | JOIN             |      Ã—      |     âˆš      |      False       | æ˜¯å¦ç­‰å¾…æ‰€æœ‰åˆ†å¸ƒå¼è¿›ç¨‹é€€å‡º    |
  | CLUSTER_TIME_OUT |      Ã—      |     âˆš      |       600        | åˆ†å¸ƒå¼å¯åŠ¨çš„ç­‰å¾…æ—¶é—´ï¼Œå•ä½ä¸ºç§’  |

#### å•æœºå¤šå¡

  ```shell
  # å•æœºå¤šå¡å¿«é€Ÿå¯åŠ¨æ–¹å¼ï¼Œé»˜è®¤8å¡å¯åŠ¨
  bash scripts/msrun_launcher.sh "run_mindformer.py \
   --config {CONFIG_PATH} \
   --run_mode {train/finetune/eval/predict}"

  # å•æœºå¤šå¡å¿«é€Ÿå¯åŠ¨æ–¹å¼ï¼Œä»…è®¾ç½®ä½¿ç”¨å¡æ•°å³å¯
  bash scripts/msrun_launcher.sh "run_mindformer.py \
   --config {CONFIG_PATH} \
   --run_mode {train/finetune/eval/predict}" WORKER_NUM

  # å•æœºå¤šå¡è‡ªå®šä¹‰å¯åŠ¨æ–¹å¼
  bash scripts/msrun_launcher.sh "run_mindformer.py \
   --config {CONFIG_PATH} \
   --run_mode {train/finetune/eval/predict}" \
   WORKER_NUM MASTER_PORT LOG_DIR JOIN CLUSTER_TIME_OUT
  ```

- ä½¿ç”¨ç¤ºä¾‹

  ```shell
  # å•æœºå¤šå¡å¿«é€Ÿå¯åŠ¨æ–¹å¼ï¼Œé»˜è®¤8å¡å¯åŠ¨
  bash scripts/msrun_launcher.sh "run_mindformer.py \
    --config path/to/xxx.yaml \
    --run_mode finetune"

  # å•æœºå¤šå¡å¿«é€Ÿå¯åŠ¨æ–¹å¼
  bash scripts/msrun_launcher.sh "run_mindformer.py \
    --config path/to/xxx.yaml \
    --run_mode finetune" 8

  # å•æœºå¤šå¡è‡ªå®šä¹‰å¯åŠ¨æ–¹å¼
  bash scripts/msrun_launcher.sh "run_mindformer.py \
    --config path/to/xxx.yaml \
    --run_mode finetune" \
    8 8118 output/msrun_log False 300
  ```

#### å¤šæœºå¤šå¡

  å¤šæœºå¤šå¡æ‰§è¡Œè„šæœ¬è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒéœ€è¦åˆ†åˆ«åœ¨ä¸åŒèŠ‚ç‚¹è¿è¡Œè„šæœ¬ï¼Œå¹¶å°†å‚æ•°MASTER_ADDRè®¾ç½®ä¸ºä¸»èŠ‚ç‚¹çš„ipåœ°å€ï¼Œ
  æ‰€æœ‰èŠ‚ç‚¹è®¾ç½®çš„ipåœ°å€ç›¸åŒï¼Œä¸åŒèŠ‚ç‚¹ä¹‹é—´ä»…å‚æ•°NODE_RANKä¸åŒã€‚

  ```shell
  # å¤šæœºå¤šå¡è‡ªå®šä¹‰å¯åŠ¨æ–¹å¼
  bash scripts/msrun_launcher.sh "run_mindformer.py \
   --config {CONFIG_PATH} \
   --run_mode {train/finetune/eval/predict}" \
   WORKER_NUM LOCAL_WORKER MASTER_ADDR MASTER_PORT NODE_RANK LOG_DIR JOIN CLUSTER_TIME_OUT
  ```

- ä½¿ç”¨ç¤ºä¾‹

  ```shell
  # èŠ‚ç‚¹0ï¼ŒèŠ‚ç‚¹ipä¸º192.168.1.1ï¼Œä½œä¸ºä¸»èŠ‚ç‚¹ï¼Œæ€»å…±8å¡ä¸”æ¯ä¸ªèŠ‚ç‚¹4å¡
  bash scripts/msrun_launcher.sh "run_mindformer.py \
    --config {CONFIG_PATH} \
    --run_mode {train/finetune/eval/predict}" \
    8 4 192.168.1.1 8118 0 output/msrun_log False 300

  # èŠ‚ç‚¹1ï¼ŒèŠ‚ç‚¹ipä¸º192.168.1.2ï¼ŒèŠ‚ç‚¹0ä¸èŠ‚ç‚¹1å¯åŠ¨å‘½ä»¤ä»…å‚æ•°NODE_RANKä¸åŒ
  bash scripts/msrun_launcher.sh "run_mindformer.py \
    --config {CONFIG_PATH} \
    --run_mode {train/finetune/eval/predict}" \
    8 4 192.168.1.1 8118 1 output/msrun_log False 300
  ```

#### å•å¡å¯åŠ¨

é€šè¿‡ç»Ÿä¸€æ¥å£å¯åŠ¨ï¼Œæ ¹æ®æ¨¡å‹çš„configé…ç½®ï¼Œå®Œæˆä»»æ„æ¨¡å‹çš„å•å¡è®­ç»ƒã€å¾®è°ƒã€è¯„ä¼°ã€æ¨ç†æµç¨‹ã€‚

  ```shell
  # è®­ç»ƒå¯åŠ¨ï¼Œrun_modeæ”¯æŒtrainã€finetuneã€evalã€predictå››ä¸ªå…³é”®å­—ï¼Œä»¥åˆ†åˆ«å®Œæˆæ¨¡å‹è®­ç»ƒã€è¯„ä¼°ã€æ¨ç†åŠŸèƒ½ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„run_mode
  python run_mindformer.py --config {CONFIG_PATH} --run_mode {train/finetune/eval/predict}
  ```

### æ–¹å¼äºŒï¼šè°ƒç”¨APIå¯åŠ¨

**è¯¦ç»†é«˜é˜¶APIä½¿ç”¨æ•™ç¨‹è¯·å‚è€ƒï¼š**[MindFormerså¤§æ¨¡å‹ä½¿ç”¨æ•™ç¨‹](docs/readthedocs/source_zh_cn/docs/practice/Develop_With_Api.md)

- å‡†å¤‡å·¥ä½œ

    - step 1ï¼šå®‰è£…mindformers

      å…·ä½“å®‰è£…è¯·å‚è€ƒ[ç¬¬äºŒç« ](https://gitee.com/mindspore/mindformers/blob/dev/README.md#%E4%BA%8Cmindformers%E5%AE%89%E8%A3%85)ã€‚

    - step2: å‡†å¤‡æ•°æ®

      å‡†å¤‡ç›¸åº”ä»»åŠ¡çš„æ•°æ®é›†ï¼Œè¯·å‚è€ƒ`docs`ç›®å½•ä¸‹å„æ¨¡å‹çš„README.mdæ–‡æ¡£å‡†å¤‡ç›¸åº”æ•°æ®é›†ã€‚

- Trainer å¿«é€Ÿå…¥é—¨

  ç”¨æˆ·å¯ä»¥é€šè¿‡ä»¥ä¸Šæ–¹å¼å®‰è£…mindformersåº“ï¼Œç„¶ååˆ©ç”¨Traineré«˜é˜¶æ¥å£æ‰§è¡Œæ¨¡å‹ä»»åŠ¡çš„è®­ç»ƒã€å¾®è°ƒã€è¯„ä¼°ã€æ¨ç†åŠŸèƒ½ã€‚

  ```python
  # ä»¥gpt2æ¨¡å‹ä¸ºä¾‹
  import mindspore; mindspore.set_context(mode=0, device_id=0)
  from mindformers import Trainer

  # åˆå§‹åŒ–é¢„è®­ç»ƒä»»åŠ¡
  trainer = Trainer(task='text_generation',
                    model='gpt2',
                    train_dataset='path/to/train_dataset',
                    eval_dataset='path/to/eval_dataset')
  # å¼€å¯é¢„è®­ç»ƒ
  trainer.train()

  # å¼€å¯å…¨é‡å¾®è°ƒ
  trainer.finetune()

  # å¼€å¯è¯„æµ‹
  trainer.evaluate()

  # å¼€å¯æ¨ç†
  predict_result = trainer.predict(input_data="An increasing sequence: one,", do_sample=False, max_length=20)
  print(predict_result)
  # output result is: [{'text_generation_text': ['An increasing sequence: one, two, three, four, five, six, seven, eight,']}]

  # Loraå¾®è°ƒ
  trainer = Trainer(task="text_generation", model="gpt2", pet_method="lora",
                    train_dataset="path/to/train_dataset")
  trainer.finetune(finetune_checkpoint="gpt2")
  ```

- pipeline å¿«é€Ÿå…¥é—¨

  MindFormerså¥—ä»¶ä¸ºç”¨æˆ·æä¾›äº†å·²é›†æˆæ¨¡å‹çš„pipelineæ¨ç†æ¥å£ï¼Œæ–¹ä¾¿ç”¨æˆ·ä½“éªŒå¤§æ¨¡å‹æ¨ç†æœåŠ¡ã€‚

  pipelineä½¿ç”¨æ ·ä¾‹å¦‚ä¸‹ï¼š

  ```python
  # ä»¥gpt2 smallä¸ºä¾‹
  import mindspore; mindspore.set_context(mode=0, device_id=0)
  from mindformers.pipeline import pipeline

  pipeline_task = pipeline(task="text_generation", model="gpt2")
  pipeline_result = pipeline_task("An increasing sequence: one,", do_sample=False, max_length=20)
  print(pipeline_result)
  ```

  ç»“æœæ‰“å°ç¤ºä¾‹(å·²é›†æˆçš„gpt2æ¨¡å‹æƒé‡æ¨ç†ç»“æœ)ï¼š

  ```text
  [{'text_generation_text': ['An increasing sequence: one, two, three, four, five, six, seven, eight,']}]
  ```

- AutoClass å¿«é€Ÿå…¥é—¨

  MindFormerså¥—ä»¶ä¸ºç”¨æˆ·æä¾›äº†é«˜é˜¶AutoClassç±»ï¼ŒåŒ…å«AutoConfigã€AutoModelã€AutoProcessorã€AutoTokenizerå››ç±»ï¼Œæ–¹ä¾¿å¼€å‘è€…è¿›è¡Œè°ƒç”¨ã€‚

    - AutoConfigè·å–å·²æ”¯æŒçš„ä»»æ„æ¨¡å‹é…ç½®

      ```python
      from mindformers import AutoConfig

      # è·å–gpt2çš„æ¨¡å‹é…ç½®
      gpt2_config = AutoConfig.from_pretrained('gpt2')
      # è·å–vit_base_p16çš„æ¨¡å‹é…ç½®
      vit_base_p16_config = AutoConfig.from_pretrained('vit_base_p16')
      ```

    - AutoModelè·å–å·²æ”¯æŒçš„ç½‘ç»œæ¨¡å‹

      ```python
      from mindformers import AutoModel

      # åˆ©ç”¨from_pretrainedåŠŸèƒ½å®ç°æ¨¡å‹çš„å®ä¾‹åŒ–ï¼ˆé»˜è®¤åŠ è½½å¯¹åº”æƒé‡ï¼‰
      gpt2 = AutoModel.from_pretrained('gpt2')
      # åˆ©ç”¨from_configåŠŸèƒ½å®ç°æ¨¡å‹çš„å®ä¾‹åŒ–ï¼ˆé»˜è®¤åŠ è½½å¯¹åº”æƒé‡ï¼‰
      gpt2_config = AutoConfig.from_pretrained('gpt2')
      gpt2 = AutoModel.from_config(gpt2_config)
      # åˆ©ç”¨save_pretrainedåŠŸèƒ½ä¿å­˜æ¨¡å‹å¯¹åº”é…ç½®
      gpt2.save_pretrained('./gpt2', save_name='gpt2')
      ```

    - AutoProcessorè·å–å·²æ”¯æŒçš„é¢„å¤„ç†æ–¹æ³•

      ```python
      from mindformers import AutoProcessor

      # é€šè¿‡æ¨¡å‹åå…³é”®å­—è·å–å¯¹åº”æ¨¡å‹é¢„å¤„ç†è¿‡ç¨‹ï¼ˆå®ä¾‹åŒ–gpt2çš„é¢„å¤„ç†è¿‡ç¨‹ï¼Œé€šå¸¸ç”¨äºTrainer/pipelineæ¨ç†å…¥å‚ï¼‰
      gpt2_processor_a = AutoProcessor.from_pretrained('gpt2')
      # é€šè¿‡yamlæ–‡ä»¶è·å–ç›¸åº”çš„é¢„å¤„ç†è¿‡ç¨‹
      gpt2_processor_b = AutoProcessor.from_pretrained('configs/gpt2/run_gpt2.yaml')
      ```

    - AutoTokenizerè·å–å·²æ”¯æŒçš„tokenizeræ–¹æ³•

      ```python
      from mindformers import AutoTokenizer
      # é€šè¿‡æ¨¡å‹åå…³é”®å­—è·å–å¯¹åº”æ¨¡å‹é¢„å¤„ç†è¿‡ç¨‹ï¼ˆå®ä¾‹åŒ–gpt2çš„tokenizerï¼Œé€šå¸¸ç”¨äºTrainer/pipelineæ¨ç†å…¥å‚ï¼‰
      gpt2_tokenizer = AutoTokenizer.from_pretrained('gpt2')
      ```

## äº”ã€è´¡çŒ®

æ¬¢è¿å‚ä¸ç¤¾åŒºè´¡çŒ®ï¼Œå¯å‚è€ƒMindSporeè´¡çŒ®è¦æ±‚[Contributor Wiki](https://gitee.com/mindspore/mindspore/blob/master/CONTRIBUTING_CN.md)ã€‚

## å…­ã€è®¸å¯è¯

[Apache 2.0è®¸å¯è¯](LICENSE)
