{
    "mindformers.auto_class.AutoConfig": {
        "signature": "()"
    },
    "mindformers.auto_class.AutoConfig.invalid_yaml_name": {
        "signature": "(yaml_name_or_path)"
    },
    "mindformers.auto_class.AutoConfig.from_pretrained": {
        "signature": "(yaml_name_or_path, **kwargs)"
    },
    "mindformers.auto_class.AutoConfig.show_support_list": {
        "signature": "()"
    },
    "mindformers.auto_class.AutoConfig.get_support_list": {
        "signature": "()"
    },
    "mindformers.auto_class.AutoModel": {
        "signature": "()"
    },
    "mindformers.auto_class.AutoModel.invalid_model_name": {
        "signature": "(pretrained_model_name_or_dir)"
    },
    "mindformers.auto_class.AutoModel.from_config": {
        "signature": "(config, **kwargs)"
    },
    "mindformers.auto_class.AutoModel._inverse_parse_config": {
        "signature": "(config)"
    },
    "mindformers.auto_class.AutoModel._wrap_config": {
        "signature": "(config)"
    },
    "mindformers.auto_class.AutoModel._get_config_args": {
        "signature": "(pretrained_model_name_or_dir, **kwargs)"
    },
    "mindformers.auto_class.AutoModel.from_pretrained": {
        "signature": "(pretrained_model_name_or_dir, **kwargs)"
    },
    "mindformers.auto_class.AutoModel.show_support_list": {
        "signature": "()"
    },
    "mindformers.auto_class.AutoModel.get_support_list": {
        "signature": "()"
    },
    "mindformers.auto_class.AutoProcessor": {
        "signature": "()"
    },
    "mindformers.auto_class.AutoProcessor.invalid_yaml_name": {
        "signature": "(yaml_name_or_path)"
    },
    "mindformers.auto_class.AutoProcessor.from_pretrained": {
        "signature": "(yaml_name_or_path, **kwargs)"
    },
    "mindformers.auto_class.AutoProcessor.show_support_list": {
        "signature": "()"
    },
    "mindformers.auto_class.AutoProcessor.get_support_list": {
        "signature": "()"
    },
    "mindformers.auto_class.AutoTokenizer": {
        "signature": "()"
    },
    "mindformers.auto_class.AutoTokenizer.invalid_yaml_name": {
        "signature": "(yaml_name_or_path)"
    },
    "mindformers.auto_class.AutoTokenizer._get_class_name_from_yaml": {
        "signature": "(yaml_name_or_path)"
    },
    "mindformers.auto_class.AutoTokenizer.from_pretrained": {
        "signature": "(yaml_name_or_path, **kwargs)"
    },
    "mindformers.auto_class.AutoTokenizer.show_support_list": {
        "signature": "()"
    },
    "mindformers.auto_class.AutoTokenizer.get_support_list": {
        "signature": "()"
    },
    "mindformers.core.AdamW": {
        "signature": "(params, learning_rate=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.0)"
    },
    "mindformers.core.AdamW.clone_state": {
        "signature": "(self, prefix, init)"
    },
    "mindformers.core.AdamW.construct": {
        "signature": "(self, gradients)"
    },
    "mindformers.core.CheckpointMonitor": {
        "signature": "(prefix='CKP', directory=None, config=None, save_checkpoint_steps=1, save_checkpoint_seconds=0, keep_checkpoint_max=5, keep_checkpoint_per_n_minutes=0, integrated_save=True, save_network_params=True, save_trainable_params=False, async_save=False, saved_network=None, append_info=None, enc_key=None, enc_mode='AES-GCM', exception_save=False, global_batch_size=None, checkpoint_format='ckpt', remove_redundancy=False)"
    },
    "mindformers.core.CheckpointMonitor.print_savetime": {
        "signature": "(self, record_step, batch_num)"
    },
    "mindformers.core.CheckpointMonitor._save_ckpt": {
        "signature": "(self, cb_params, force_to_save=False)"
    },
    "mindformers.core.CheckpointMonitor.save_checkpoint": {
        "signature": "(self, cb_params)"
    },
    "mindformers.core.CheckpointMonitor._get_cur_dp": {
        "signature": "(self, cur_rank, parameter_redundancy_dict)"
    },
    "mindformers.core.CheckpointMonitor._tft_save_ckpt": {
        "signature": "(self, param_layout_set, save_param_names, cur_file, append_dict, network)"
    },
    "mindformers.core.CheckpointMonitor._do_remove_redundancy_for_tft": {
        "signature": "(self, redundancy_info, cur_file, network, append_dict)"
    },
    "mindformers.core.CheckpointMonitor.remove_redundancy": {
        "signature": "(self, network, cur_file, append_dict, train_network)"
    },
    "mindformers.core.CheckpointMonitor.save_checkpoint_network": {
        "signature": "(self, cb_params)"
    },
    "mindformers.core.CheckpointMonitor.record_last_ckpt_to_json": {
        "signature": "(self, epoch, step, ckpt_file)"
    },
    "mindformers.core.ConstantWarmUpLR": {
        "signature": "(learning_rate: float, warmup_steps: int = None, warmup_lr_init: float = 0.0, warmup_ratio: float = None, total_steps: int = None, **kwargs)"
    },
    "mindformers.core.ConstantWarmUpLR.construct": {
        "signature": "(self, global_step)"
    },
    "mindformers.core.CosineAnnealingLR": {
        "signature": "(base_lr: float, t_max: int, eta_min: float = 0.0, **kwargs)"
    },
    "mindformers.core.CosineAnnealingLR.construct": {
        "signature": "(self, global_step)"
    },
    "mindformers.core.CosineAnnealingWarmRestarts": {
        "signature": "(base_lr: float, t_0: int, t_mult: int = 1, eta_min: float = 0.0, **kwargs)"
    },
    "mindformers.core.CosineAnnealingWarmRestarts.construct": {
        "signature": "(self, global_step)"
    },
    "mindformers.core.CosineWithRestartsAndWarmUpLR": {
        "signature": "(learning_rate: float, warmup_steps: int = None, total_steps: int = None, num_cycles: float = 1.0, lr_end: float = 0.0, warmup_lr_init: float = 0.0, warmup_ratio: float = None, decay_steps: int = None, **kwargs)"
    },
    "mindformers.core.CosineWithRestartsAndWarmUpLR.construct": {
        "signature": "(self, global_step)"
    },
    "mindformers.core.CosineWithWarmUpLR": {
        "signature": "(learning_rate: float, warmup_steps: int = 0, total_steps: int = None, num_cycles: float = 0.5, lr_end: float = 0.0, warmup_lr_init: float = 0.0, warmup_ratio: float = None, decay_steps: int = None, decay_ratio: float = None, **kwargs)"
    },
    "mindformers.core.CosineWithWarmUpLR.construct": {
        "signature": "(self, global_step)"
    },
    "mindformers.core.CrossEntropyLoss": {
        "signature": "(parallel_config=<mindformers.modules.transformer.op_parallel_config.OpParallelConfig object>, check_for_nan_in_loss_and_grad=False, calculate_per_token_loss=False, seq_split_num=1, **kwargs)"
    },
    "mindformers.core.CrossEntropyLoss._check_and_modify_sharding_context": {
        "signature": "(dp)"
    },
    "mindformers.core.CrossEntropyLoss.construct": {
        "signature": "(self, logits, label, input_mask)"
    },
    "mindformers.core.EmF1Metric": {
        "signature": "()"
    },
    "mindformers.core.EmF1Metric.clear": {
        "signature": "(self)"
    },
    "mindformers.core.EmF1Metric.update": {
        "signature": "(self, *inputs)"
    },
    "mindformers.core.EmF1Metric.eval": {
        "signature": "(self)"
    },
    "mindformers.core.EmF1Metric.mixed_segmentation": {
        "signature": "(self, in_str, rm_punc=False)"
    },
    "mindformers.core.EmF1Metric.remove_punctuation": {
        "signature": "(self, in_str)"
    },
    "mindformers.core.EmF1Metric.find_lcs": {
        "signature": "(self, s1, s2)"
    },
    "mindformers.core.EmF1Metric.calc_f1_score": {
        "signature": "(self, answers, prediction)"
    },
    "mindformers.core.EmF1Metric.calc_em_score": {
        "signature": "(self, answers, prediction)"
    },
    "mindformers.core.EmF1Metric.evaluate_pairs": {
        "signature": "(self, pred_, ans_)"
    },
    "mindformers.core.EntityScore": {
        "signature": "()"
    },
    "mindformers.core.EntityScore.clear": {
        "signature": "(self)"
    },
    "mindformers.core.EntityScore.update": {
        "signature": "(self, *inputs)"
    },
    "mindformers.core.EntityScore.eval": {
        "signature": "(self)"
    },
    "mindformers.core.EntityScore.compute": {
        "signature": "(self, origin, found, right)"
    },
    "mindformers.core.EntityScore.get_entities_bios": {
        "signature": "(self, seq)"
    },
    "mindformers.core.EvalCallBack": {
        "signature": "(eval_func: Callable, step_interval: int = 100, epoch_interval: int = -1)"
    },
    "mindformers.core.EvalCallBack.epoch_end": {
        "signature": "(self, run_context)"
    },
    "mindformers.core.EvalCallBack.step_end": {
        "signature": "(self, run_context)"
    },
    "mindformers.core.EvalCallBack._execute_eval": {
        "signature": "(self)"
    },
    "mindformers.core.LearningRateWiseLayer": {
        "signature": "(base_lr, lr_scale)"
    },
    "mindformers.core.LearningRateWiseLayer.construct": {
        "signature": "(self, global_step)"
    },
    "mindformers.core.LinearWithWarmUpLR": {
        "signature": "(learning_rate: float, total_steps: int, warmup_steps: int = None, warmup_lr_init: float = 0.0, warmup_ratio: float = None, **kwargs)"
    },
    "mindformers.core.LinearWithWarmUpLR.construct": {
        "signature": "(self, global_step)"
    },
    "mindformers.core.MFLossMonitor": {
        "signature": "(learning_rate: Union[NoneType, float, mindspore.nn.learning_rate_schedule.LearningRateSchedule] = None, per_print_times: int = 1, micro_batch_num: int = 1, micro_batch_interleave_num: int = 1, origin_epochs: int = None, dataset_size: int = None, initial_epoch: int = 0, initial_step: int = 0, global_batch_size: int = 0, gradient_accumulation_steps: int = 1, check_for_nan_in_loss_and_grad: bool = False, calculate_per_token_loss: bool = False)"
    },
    "mindformers.core.MFLossMonitor.epoch_begin": {
        "signature": "(self, run_context)"
    },
    "mindformers.core.MFLossMonitor.epoch_end": {
        "signature": "(self, run_context)"
    },
    "mindformers.core.MFLossMonitor.step_begin": {
        "signature": "(self, run_context)"
    },
    "mindformers.core.MFLossMonitor.step_end": {
        "signature": "(self, run_context)"
    },
    "mindformers.core.MFLossMonitor._fix_loss_for_parallel": {
        "signature": "(self, loss)"
    },
    "mindformers.core.MFLossMonitor._get_pipeline_group": {
        "signature": "(self)"
    },
    "mindformers.core.MFLossMonitor._can_calculate_model_flops": {
        "signature": "(self, cb_params)"
    },
    "mindformers.core.MFLossMonitor._calculate_model_flops": {
        "signature": "(self)"
    },
    "mindformers.core.MFLossMonitor.print_output_info": {
        "signature": "(self, cb_params, cur_epoch_num, origin_epochs, throughput, cur_step_num, steps_per_epoch, loss, per_step_seconds, overflow, scaling_sens, time_remain, percent, global_norm)"
    },
    "mindformers.core.MFLossMonitor.dump_info_to_modelarts": {
        "signature": "(self, ma_step_num, ma_loss)"
    },
    "mindformers.core.PerplexityMetric": {
        "signature": "()"
    },
    "mindformers.core.PerplexityMetric.clear": {
        "signature": "(self)"
    },
    "mindformers.core.PerplexityMetric.update": {
        "signature": "(self, *inputs)"
    },
    "mindformers.core.PerplexityMetric.eval": {
        "signature": "(self)"
    },
    "mindformers.core.PolynomialWithWarmUpLR": {
        "signature": "(learning_rate: float, total_steps: int, warmup_steps: int = None, lr_end: float = 1e-07, power: float = 1.0, warmup_lr_init: float = 0.0, warmup_ratio: float = None, decay_steps: int = None, **kwargs)"
    },
    "mindformers.core.PolynomialWithWarmUpLR.construct": {
        "signature": "(self, global_step)"
    },
    "mindformers.core.ProfileMonitor": {
        "signature": "(start_step=1, stop_step=10, output_path=None, start_profile=True, profile_rank_ids=None, profile_pipeline=False, profile_communication=False, profile_memory=False, config=None, profiler_level=0, with_stack=False, data_simplification=True, **kwargs)"
    },
    "mindformers.core.ProfileMonitor.step_begin": {
        "signature": "(self, run_context)"
    },
    "mindformers.core.ProfileMonitor.step_end": {
        "signature": "(self, run_context)"
    },
    "mindformers.core.ProfileMonitor._record_metadata": {
        "signature": "(self, config)"
    },
    "mindformers.core.ProfileMonitor._is_profile_required": {
        "signature": "(self, rank_id)"
    },
    "mindformers.core.ProfileMonitor._get_profiler_level": {
        "signature": "(level)"
    },
    "mindformers.core.PromptAccMetric": {
        "signature": "()"
    },
    "mindformers.core.PromptAccMetric.clear": {
        "signature": "(self)"
    },
    "mindformers.core.PromptAccMetric.calculate_circle": {
        "signature": "(self, *inputs)"
    },
    "mindformers.core.PromptAccMetric.update": {
        "signature": "(self, *inputs)"
    },
    "mindformers.core.PromptAccMetric.eval": {
        "signature": "(self)"
    },
    "mindformers.core.SQuADMetric": {
        "signature": "(dataset_dir, n_best_size=20, max_answer_len=30, do_lower_case=True, temp_file_dir='./squad_temp')"
    },
    "mindformers.core.SQuADMetric.clear": {
        "signature": "(self)"
    },
    "mindformers.core.SQuADMetric.update": {
        "signature": "(self, *inputs)"
    },
    "mindformers.core.SQuADMetric.eval": {
        "signature": "(self)"
    },
    "mindformers.core.SQuADMetric._remove_temp_data": {
        "signature": "(self)"
    },
    "mindformers.core.SQuADMetric._load_temp_data": {
        "signature": "(self, temp_file_path)"
    },
    "mindformers.core.SQuADMetric._normalize_answer": {
        "signature": "(self, s)"
    },
    "mindformers.core.SQuADMetric._f1_score": {
        "signature": "(self, prediction, ground_truth)"
    },
    "mindformers.core.SQuADMetric._exact_match_score": {
        "signature": "(self, prediction, ground_truth)"
    },
    "mindformers.core.SQuADMetric._metric_max_over_ground_truths": {
        "signature": "(self, metric_fn, prediction, ground_truths)"
    },
    "mindformers.core.SQuADMetric._get_predictions": {
        "signature": "(self)"
    },
    "mindformers.core.SQuADMetric._get_prelim_predictions": {
        "signature": "(self, features, unique_id_to_result)"
    },
    "mindformers.core.SQuADMetric._get_nbest": {
        "signature": "(self, prelim_predictions, features, example)"
    },
    "mindformers.core.SQuADMetric._compute_softmax": {
        "signature": "(self, scores)"
    },
    "mindformers.core.SQuADMetric._get_final_text": {
        "signature": "(self, pred_text, orig_text)"
    },
    "mindformers.core.SQuADMetric._get_best_indexes": {
        "signature": "(self, logits)"
    },
    "mindformers.core.SummaryMonitor": {
        "signature": "(summary_dir=None, collect_freq=10, collect_specified_data=None, keep_default_action=True, custom_lineage_data=None, collect_tensor_freq=None, max_file_size=None, export_options=None)"
    },
    "mindformers.core.TrainingStateMonitor": {
        "signature": "(origin_epochs: int, config: dict = None, per_print_times: int = 1, dataset_size: int = None, initial_epoch: int = 0, initial_step: int = 0, global_batch_size: int = 0)"
    },
    "mindformers.core.TrainingStateMonitor.epoch_begin": {
        "signature": "(self, run_context)"
    },
    "mindformers.core.TrainingStateMonitor.epoch_end": {
        "signature": "(self, run_context)"
    },
    "mindformers.core.TrainingStateMonitor.step_begin": {
        "signature": "(self, run_context)"
    },
    "mindformers.core.TrainingStateMonitor.step_end": {
        "signature": "(self, run_context)"
    },
    "mindformers.core.TrainingStateMonitor._init_config": {
        "signature": "(self, config)"
    },
    "mindformers.core.TrainingStateMonitor._check_attr_formats": {
        "signature": "(self, attr)"
    },
    "mindformers.core.TrainingStateMonitor._parse_step": {
        "signature": "(self)"
    },
    "mindformers.core.TrainingStateMonitor._dump_data_in_step": {
        "signature": "(self, global_step)"
    },
    "mindformers.core.TrainingStateMonitor._dump_optimizer_state": {
        "signature": "(self, cb_params)"
    },
    "mindformers.core.TrainingStateMonitor._check_param_name": {
        "signature": "(self, param_name)"
    },
    "mindformers.core.TrainingStateMonitor._to_tensorboard": {
        "signature": "(self, tag, data, global_step)"
    },
    "mindformers.core.TrainingStateMonitor._to_log": {
        "signature": "(self, tag, data, global_step)"
    },
    "mindformers.core.TrainingStateMonitor._output": {
        "signature": "(self, tag, data, global_step, formats)"
    },
    "mindformers.core.build_context": {
        "signature": "(config: Union[dict, mindformers.tools.register.config.MindFormerConfig, mindformers.trainer.training_args.TrainingArguments])"
    },
    "mindformers.core.get_context": {
        "signature": "(attr_key)"
    },
    "mindformers.core.init_context": {
        "signature": "(use_parallel=False, context_config=None, parallel_config=None)"
    },
    "mindformers.core.set_context": {
        "signature": "(run_mode=None, **kwargs)"
    },
    "mindformers.core.callback.CheckpointMonitor": {
        "signature": "(prefix='CKP', directory=None, config=None, save_checkpoint_steps=1, save_checkpoint_seconds=0, keep_checkpoint_max=5, keep_checkpoint_per_n_minutes=0, integrated_save=True, save_network_params=True, save_trainable_params=False, async_save=False, saved_network=None, append_info=None, enc_key=None, enc_mode='AES-GCM', exception_save=False, global_batch_size=None, checkpoint_format='ckpt', remove_redundancy=False)"
    },
    "mindformers.core.callback.CheckpointMonitor.print_savetime": {
        "signature": "(self, record_step, batch_num)"
    },
    "mindformers.core.callback.CheckpointMonitor._save_ckpt": {
        "signature": "(self, cb_params, force_to_save=False)"
    },
    "mindformers.core.callback.CheckpointMonitor.save_checkpoint": {
        "signature": "(self, cb_params)"
    },
    "mindformers.core.callback.CheckpointMonitor._get_cur_dp": {
        "signature": "(self, cur_rank, parameter_redundancy_dict)"
    },
    "mindformers.core.callback.CheckpointMonitor._tft_save_ckpt": {
        "signature": "(self, param_layout_set, save_param_names, cur_file, append_dict, network)"
    },
    "mindformers.core.callback.CheckpointMonitor._do_remove_redundancy_for_tft": {
        "signature": "(self, redundancy_info, cur_file, network, append_dict)"
    },
    "mindformers.core.callback.CheckpointMonitor.remove_redundancy": {
        "signature": "(self, network, cur_file, append_dict, train_network)"
    },
    "mindformers.core.callback.CheckpointMonitor.save_checkpoint_network": {
        "signature": "(self, cb_params)"
    },
    "mindformers.core.callback.CheckpointMonitor.record_last_ckpt_to_json": {
        "signature": "(self, epoch, step, ckpt_file)"
    },
    "mindformers.core.callback.EvalCallBack": {
        "signature": "(eval_func: Callable, step_interval: int = 100, epoch_interval: int = -1)"
    },
    "mindformers.core.callback.EvalCallBack.epoch_end": {
        "signature": "(self, run_context)"
    },
    "mindformers.core.callback.EvalCallBack.step_end": {
        "signature": "(self, run_context)"
    },
    "mindformers.core.callback.EvalCallBack._execute_eval": {
        "signature": "(self)"
    },
    "mindformers.core.callback.MFLossMonitor": {
        "signature": "(learning_rate: Union[NoneType, float, mindspore.nn.learning_rate_schedule.LearningRateSchedule] = None, per_print_times: int = 1, micro_batch_num: int = 1, micro_batch_interleave_num: int = 1, origin_epochs: int = None, dataset_size: int = None, initial_epoch: int = 0, initial_step: int = 0, global_batch_size: int = 0, gradient_accumulation_steps: int = 1, check_for_nan_in_loss_and_grad: bool = False, calculate_per_token_loss: bool = False)"
    },
    "mindformers.core.callback.MFLossMonitor.epoch_begin": {
        "signature": "(self, run_context)"
    },
    "mindformers.core.callback.MFLossMonitor.epoch_end": {
        "signature": "(self, run_context)"
    },
    "mindformers.core.callback.MFLossMonitor.step_begin": {
        "signature": "(self, run_context)"
    },
    "mindformers.core.callback.MFLossMonitor.step_end": {
        "signature": "(self, run_context)"
    },
    "mindformers.core.callback.MFLossMonitor._fix_loss_for_parallel": {
        "signature": "(self, loss)"
    },
    "mindformers.core.callback.MFLossMonitor._get_pipeline_group": {
        "signature": "(self)"
    },
    "mindformers.core.callback.MFLossMonitor._can_calculate_model_flops": {
        "signature": "(self, cb_params)"
    },
    "mindformers.core.callback.MFLossMonitor._calculate_model_flops": {
        "signature": "(self)"
    },
    "mindformers.core.callback.MFLossMonitor.print_output_info": {
        "signature": "(self, cb_params, cur_epoch_num, origin_epochs, throughput, cur_step_num, steps_per_epoch, loss, per_step_seconds, overflow, scaling_sens, time_remain, percent, global_norm)"
    },
    "mindformers.core.callback.MFLossMonitor.dump_info_to_modelarts": {
        "signature": "(self, ma_step_num, ma_loss)"
    },
    "mindformers.core.callback.ProfileMonitor": {
        "signature": "(start_step=1, stop_step=10, output_path=None, start_profile=True, profile_rank_ids=None, profile_pipeline=False, profile_communication=False, profile_memory=False, config=None, profiler_level=0, with_stack=False, data_simplification=True, **kwargs)"
    },
    "mindformers.core.callback.ProfileMonitor.step_begin": {
        "signature": "(self, run_context)"
    },
    "mindformers.core.callback.ProfileMonitor.step_end": {
        "signature": "(self, run_context)"
    },
    "mindformers.core.callback.ProfileMonitor._record_metadata": {
        "signature": "(self, config)"
    },
    "mindformers.core.callback.ProfileMonitor._is_profile_required": {
        "signature": "(self, rank_id)"
    },
    "mindformers.core.callback.ProfileMonitor._get_profiler_level": {
        "signature": "(level)"
    },
    "mindformers.core.callback.SummaryMonitor": {
        "signature": "(summary_dir=None, collect_freq=10, collect_specified_data=None, keep_default_action=True, custom_lineage_data=None, collect_tensor_freq=None, max_file_size=None, export_options=None)"
    },
    "mindformers.core.callback.TrainingStateMonitor": {
        "signature": "(origin_epochs: int, config: dict = None, per_print_times: int = 1, dataset_size: int = None, initial_epoch: int = 0, initial_step: int = 0, global_batch_size: int = 0)"
    },
    "mindformers.core.callback.TrainingStateMonitor.epoch_begin": {
        "signature": "(self, run_context)"
    },
    "mindformers.core.callback.TrainingStateMonitor.epoch_end": {
        "signature": "(self, run_context)"
    },
    "mindformers.core.callback.TrainingStateMonitor.step_begin": {
        "signature": "(self, run_context)"
    },
    "mindformers.core.callback.TrainingStateMonitor.step_end": {
        "signature": "(self, run_context)"
    },
    "mindformers.core.callback.TrainingStateMonitor._init_config": {
        "signature": "(self, config)"
    },
    "mindformers.core.callback.TrainingStateMonitor._check_attr_formats": {
        "signature": "(self, attr)"
    },
    "mindformers.core.callback.TrainingStateMonitor._parse_step": {
        "signature": "(self)"
    },
    "mindformers.core.callback.TrainingStateMonitor._dump_data_in_step": {
        "signature": "(self, global_step)"
    },
    "mindformers.core.callback.TrainingStateMonitor._dump_optimizer_state": {
        "signature": "(self, cb_params)"
    },
    "mindformers.core.callback.TrainingStateMonitor._check_param_name": {
        "signature": "(self, param_name)"
    },
    "mindformers.core.callback.TrainingStateMonitor._to_tensorboard": {
        "signature": "(self, tag, data, global_step)"
    },
    "mindformers.core.callback.TrainingStateMonitor._to_log": {
        "signature": "(self, tag, data, global_step)"
    },
    "mindformers.core.callback.TrainingStateMonitor._output": {
        "signature": "(self, tag, data, global_step, formats)"
    },
    "mindformers.core.callback.callback.CheckpointMonitor": {
        "signature": "(prefix='CKP', directory=None, config=None, save_checkpoint_steps=1, save_checkpoint_seconds=0, keep_checkpoint_max=5, keep_checkpoint_per_n_minutes=0, integrated_save=True, save_network_params=True, save_trainable_params=False, async_save=False, saved_network=None, append_info=None, enc_key=None, enc_mode='AES-GCM', exception_save=False, global_batch_size=None, checkpoint_format='ckpt', remove_redundancy=False)"
    },
    "mindformers.core.callback.callback.CheckpointMonitor.print_savetime": {
        "signature": "(self, record_step, batch_num)"
    },
    "mindformers.core.callback.callback.CheckpointMonitor._save_ckpt": {
        "signature": "(self, cb_params, force_to_save=False)"
    },
    "mindformers.core.callback.callback.CheckpointMonitor.save_checkpoint": {
        "signature": "(self, cb_params)"
    },
    "mindformers.core.callback.callback.CheckpointMonitor._get_cur_dp": {
        "signature": "(self, cur_rank, parameter_redundancy_dict)"
    },
    "mindformers.core.callback.callback.CheckpointMonitor._tft_save_ckpt": {
        "signature": "(self, param_layout_set, save_param_names, cur_file, append_dict, network)"
    },
    "mindformers.core.callback.callback.CheckpointMonitor._do_remove_redundancy_for_tft": {
        "signature": "(self, redundancy_info, cur_file, network, append_dict)"
    },
    "mindformers.core.callback.callback.CheckpointMonitor.remove_redundancy": {
        "signature": "(self, network, cur_file, append_dict, train_network)"
    },
    "mindformers.core.callback.callback.CheckpointMonitor.save_checkpoint_network": {
        "signature": "(self, cb_params)"
    },
    "mindformers.core.callback.callback.CheckpointMonitor.record_last_ckpt_to_json": {
        "signature": "(self, epoch, step, ckpt_file)"
    },
    "mindformers.core.callback.callback.EvalCallBack": {
        "signature": "(eval_func: Callable, step_interval: int = 100, epoch_interval: int = -1)"
    },
    "mindformers.core.callback.callback.EvalCallBack.epoch_end": {
        "signature": "(self, run_context)"
    },
    "mindformers.core.callback.callback.EvalCallBack.step_end": {
        "signature": "(self, run_context)"
    },
    "mindformers.core.callback.callback.EvalCallBack._execute_eval": {
        "signature": "(self)"
    },
    "mindformers.core.callback.callback.MFLossMonitor": {
        "signature": "(learning_rate: Union[NoneType, float, mindspore.nn.learning_rate_schedule.LearningRateSchedule] = None, per_print_times: int = 1, micro_batch_num: int = 1, micro_batch_interleave_num: int = 1, origin_epochs: int = None, dataset_size: int = None, initial_epoch: int = 0, initial_step: int = 0, global_batch_size: int = 0, gradient_accumulation_steps: int = 1, check_for_nan_in_loss_and_grad: bool = False, calculate_per_token_loss: bool = False)"
    },
    "mindformers.core.callback.callback.MFLossMonitor.epoch_begin": {
        "signature": "(self, run_context)"
    },
    "mindformers.core.callback.callback.MFLossMonitor.epoch_end": {
        "signature": "(self, run_context)"
    },
    "mindformers.core.callback.callback.MFLossMonitor.step_begin": {
        "signature": "(self, run_context)"
    },
    "mindformers.core.callback.callback.MFLossMonitor.step_end": {
        "signature": "(self, run_context)"
    },
    "mindformers.core.callback.callback.MFLossMonitor._fix_loss_for_parallel": {
        "signature": "(self, loss)"
    },
    "mindformers.core.callback.callback.MFLossMonitor._get_pipeline_group": {
        "signature": "(self)"
    },
    "mindformers.core.callback.callback.MFLossMonitor._can_calculate_model_flops": {
        "signature": "(self, cb_params)"
    },
    "mindformers.core.callback.callback.MFLossMonitor._calculate_model_flops": {
        "signature": "(self)"
    },
    "mindformers.core.callback.callback.MFLossMonitor.print_output_info": {
        "signature": "(self, cb_params, cur_epoch_num, origin_epochs, throughput, cur_step_num, steps_per_epoch, loss, per_step_seconds, overflow, scaling_sens, time_remain, percent, global_norm)"
    },
    "mindformers.core.callback.callback.MFLossMonitor.dump_info_to_modelarts": {
        "signature": "(self, ma_step_num, ma_loss)"
    },
    "mindformers.core.callback.callback.ObsMonitor": {
        "signature": "(src_dir: str = None, target_dir: str = None, step_upload_frequence: int = 100, epoch_upload_frequence: int = -1, keep_last: bool = True)"
    },
    "mindformers.core.callback.callback.ProfileMonitor": {
        "signature": "(start_step=1, stop_step=10, output_path=None, start_profile=True, profile_rank_ids=None, profile_pipeline=False, profile_communication=False, profile_memory=False, config=None, profiler_level=0, with_stack=False, data_simplification=True, **kwargs)"
    },
    "mindformers.core.callback.callback.ProfileMonitor.step_begin": {
        "signature": "(self, run_context)"
    },
    "mindformers.core.callback.callback.ProfileMonitor.step_end": {
        "signature": "(self, run_context)"
    },
    "mindformers.core.callback.callback.ProfileMonitor._record_metadata": {
        "signature": "(self, config)"
    },
    "mindformers.core.callback.callback.ProfileMonitor._is_profile_required": {
        "signature": "(self, rank_id)"
    },
    "mindformers.core.callback.callback.ProfileMonitor._get_profiler_level": {
        "signature": "(level)"
    },
    "mindformers.core.callback.callback.SummaryMonitor": {
        "signature": "(summary_dir=None, collect_freq=10, collect_specified_data=None, keep_default_action=True, custom_lineage_data=None, collect_tensor_freq=None, max_file_size=None, export_options=None)"
    },
    "mindformers.core.context.build_context": {
        "signature": "(config: Union[dict, mindformers.tools.register.config.MindFormerConfig, mindformers.trainer.training_args.TrainingArguments])"
    },
    "mindformers.core.context.get_context": {
        "signature": "(attr_key)"
    },
    "mindformers.core.context.init_context": {
        "signature": "(use_parallel=False, context_config=None, parallel_config=None)"
    },
    "mindformers.core.context.set_context": {
        "signature": "(run_mode=None, **kwargs)"
    },
    "mindformers.core.loss.CrossEntropyLoss": {
        "signature": "(parallel_config=<mindformers.modules.transformer.op_parallel_config.OpParallelConfig object>, check_for_nan_in_loss_and_grad=False, calculate_per_token_loss=False, seq_split_num=1, **kwargs)"
    },
    "mindformers.core.loss.CrossEntropyLoss._check_and_modify_sharding_context": {
        "signature": "(dp)"
    },
    "mindformers.core.loss.CrossEntropyLoss.construct": {
        "signature": "(self, logits, label, input_mask)"
    },
    "mindformers.core.loss.loss.CompareLoss": {
        "signature": "(config)"
    },
    "mindformers.core.loss.loss.CompareLoss.construct": {
        "signature": "(self, rewards, loss_mask, end_ind)"
    },
    "mindformers.core.loss.loss.CrossEntropyLoss": {
        "signature": "(parallel_config=<mindformers.modules.transformer.op_parallel_config.OpParallelConfig object>, check_for_nan_in_loss_and_grad=False, calculate_per_token_loss=False, seq_split_num=1, **kwargs)"
    },
    "mindformers.core.loss.loss.CrossEntropyLoss._check_and_modify_sharding_context": {
        "signature": "(dp)"
    },
    "mindformers.core.loss.loss.CrossEntropyLoss.construct": {
        "signature": "(self, logits, label, input_mask)"
    },
    "mindformers.core.loss.loss.L1Loss": {
        "signature": "(reduction='mean', parallel_config=<mindformers.modules.transformer.op_parallel_config.OpParallelConfig object>)"
    },
    "mindformers.core.loss.loss.L1Loss.get_loss": {
        "signature": "(self, x, weights=1.0)"
    },
    "mindformers.core.loss.loss.L1Loss.construct": {
        "signature": "(self, logits, labels)"
    },
    "mindformers.core.loss.loss.MSELoss": {
        "signature": "(norm_pixel_loss=True, parallel_config=<mindformers.modules.transformer.op_parallel_config.OpParallelConfig object>)"
    },
    "mindformers.core.loss.loss.MSELoss.construct": {
        "signature": "(self, pred, target, mask)"
    },
    "mindformers.core.loss.loss.MSELoss.variance": {
        "signature": "(self, x)"
    },
    "mindformers.core.loss.loss.SoftTargetCrossEntropy": {
        "signature": "(parallel_config=<mindformers.modules.transformer.op_parallel_config.OpParallelConfig object>)"
    },
    "mindformers.core.loss.loss.SoftTargetCrossEntropy.construct": {
        "signature": "(self, logit, label)"
    },
    "mindformers.core.lr.ConstantWarmUpLR": {
        "signature": "(learning_rate: float, warmup_steps: int = None, warmup_lr_init: float = 0.0, warmup_ratio: float = None, total_steps: int = None, **kwargs)"
    },
    "mindformers.core.lr.ConstantWarmUpLR.construct": {
        "signature": "(self, global_step)"
    },
    "mindformers.core.lr.CosineAnnealingLR": {
        "signature": "(base_lr: float, t_max: int, eta_min: float = 0.0, **kwargs)"
    },
    "mindformers.core.lr.CosineAnnealingLR.construct": {
        "signature": "(self, global_step)"
    },
    "mindformers.core.lr.CosineAnnealingWarmRestarts": {
        "signature": "(base_lr: float, t_0: int, t_mult: int = 1, eta_min: float = 0.0, **kwargs)"
    },
    "mindformers.core.lr.CosineAnnealingWarmRestarts.construct": {
        "signature": "(self, global_step)"
    },
    "mindformers.core.lr.CosineWithRestartsAndWarmUpLR": {
        "signature": "(learning_rate: float, warmup_steps: int = None, total_steps: int = None, num_cycles: float = 1.0, lr_end: float = 0.0, warmup_lr_init: float = 0.0, warmup_ratio: float = None, decay_steps: int = None, **kwargs)"
    },
    "mindformers.core.lr.CosineWithRestartsAndWarmUpLR.construct": {
        "signature": "(self, global_step)"
    },
    "mindformers.core.lr.CosineWithWarmUpLR": {
        "signature": "(learning_rate: float, warmup_steps: int = 0, total_steps: int = None, num_cycles: float = 0.5, lr_end: float = 0.0, warmup_lr_init: float = 0.0, warmup_ratio: float = None, decay_steps: int = None, decay_ratio: float = None, **kwargs)"
    },
    "mindformers.core.lr.CosineWithWarmUpLR.construct": {
        "signature": "(self, global_step)"
    },
    "mindformers.core.lr.LearningRateWiseLayer": {
        "signature": "(base_lr, lr_scale)"
    },
    "mindformers.core.lr.LearningRateWiseLayer.construct": {
        "signature": "(self, global_step)"
    },
    "mindformers.core.lr.LinearWithWarmUpLR": {
        "signature": "(learning_rate: float, total_steps: int, warmup_steps: int = None, warmup_lr_init: float = 0.0, warmup_ratio: float = None, **kwargs)"
    },
    "mindformers.core.lr.LinearWithWarmUpLR.construct": {
        "signature": "(self, global_step)"
    },
    "mindformers.core.lr.PolynomialWithWarmUpLR": {
        "signature": "(learning_rate: float, total_steps: int, warmup_steps: int = None, lr_end: float = 1e-07, power: float = 1.0, warmup_lr_init: float = 0.0, warmup_ratio: float = None, decay_steps: int = None, **kwargs)"
    },
    "mindformers.core.lr.PolynomialWithWarmUpLR.construct": {
        "signature": "(self, global_step)"
    },
    "mindformers.core.lr.lr_schedule.ConstantWarmUpLR": {
        "signature": "(learning_rate: float, warmup_steps: int = None, warmup_lr_init: float = 0.0, warmup_ratio: float = None, total_steps: int = None, **kwargs)"
    },
    "mindformers.core.lr.lr_schedule.ConstantWarmUpLR.construct": {
        "signature": "(self, global_step)"
    },
    "mindformers.core.lr.lr_schedule.CosineAnnealingLR": {
        "signature": "(base_lr: float, t_max: int, eta_min: float = 0.0, **kwargs)"
    },
    "mindformers.core.lr.lr_schedule.CosineAnnealingLR.construct": {
        "signature": "(self, global_step)"
    },
    "mindformers.core.lr.lr_schedule.CosineAnnealingWarmRestarts": {
        "signature": "(base_lr: float, t_0: int, t_mult: int = 1, eta_min: float = 0.0, **kwargs)"
    },
    "mindformers.core.lr.lr_schedule.CosineAnnealingWarmRestarts.construct": {
        "signature": "(self, global_step)"
    },
    "mindformers.core.lr.lr_schedule.CosineWithRestartsAndWarmUpLR": {
        "signature": "(learning_rate: float, warmup_steps: int = None, total_steps: int = None, num_cycles: float = 1.0, lr_end: float = 0.0, warmup_lr_init: float = 0.0, warmup_ratio: float = None, decay_steps: int = None, **kwargs)"
    },
    "mindformers.core.lr.lr_schedule.CosineWithRestartsAndWarmUpLR.construct": {
        "signature": "(self, global_step)"
    },
    "mindformers.core.lr.lr_schedule.CosineWithWarmUpLR": {
        "signature": "(learning_rate: float, warmup_steps: int = 0, total_steps: int = None, num_cycles: float = 0.5, lr_end: float = 0.0, warmup_lr_init: float = 0.0, warmup_ratio: float = None, decay_steps: int = None, decay_ratio: float = None, **kwargs)"
    },
    "mindformers.core.lr.lr_schedule.CosineWithWarmUpLR.construct": {
        "signature": "(self, global_step)"
    },
    "mindformers.core.lr.lr_schedule.LearningRateWiseLayer": {
        "signature": "(base_lr, lr_scale)"
    },
    "mindformers.core.lr.lr_schedule.LearningRateWiseLayer.construct": {
        "signature": "(self, global_step)"
    },
    "mindformers.core.lr.lr_schedule.LinearWithWarmUpLR": {
        "signature": "(learning_rate: float, total_steps: int, warmup_steps: int = None, warmup_lr_init: float = 0.0, warmup_ratio: float = None, **kwargs)"
    },
    "mindformers.core.lr.lr_schedule.LinearWithWarmUpLR.construct": {
        "signature": "(self, global_step)"
    },
    "mindformers.core.lr.lr_schedule.PolynomialWithWarmUpLR": {
        "signature": "(learning_rate: float, total_steps: int, warmup_steps: int = None, lr_end: float = 1e-07, power: float = 1.0, warmup_lr_init: float = 0.0, warmup_ratio: float = None, decay_steps: int = None, **kwargs)"
    },
    "mindformers.core.lr.lr_schedule.PolynomialWithWarmUpLR.construct": {
        "signature": "(self, global_step)"
    },
    "mindformers.core.metric.EmF1Metric": {
        "signature": "()"
    },
    "mindformers.core.metric.EmF1Metric.clear": {
        "signature": "(self)"
    },
    "mindformers.core.metric.EmF1Metric.update": {
        "signature": "(self, *inputs)"
    },
    "mindformers.core.metric.EmF1Metric.eval": {
        "signature": "(self)"
    },
    "mindformers.core.metric.EmF1Metric.mixed_segmentation": {
        "signature": "(self, in_str, rm_punc=False)"
    },
    "mindformers.core.metric.EmF1Metric.remove_punctuation": {
        "signature": "(self, in_str)"
    },
    "mindformers.core.metric.EmF1Metric.find_lcs": {
        "signature": "(self, s1, s2)"
    },
    "mindformers.core.metric.EmF1Metric.calc_f1_score": {
        "signature": "(self, answers, prediction)"
    },
    "mindformers.core.metric.EmF1Metric.calc_em_score": {
        "signature": "(self, answers, prediction)"
    },
    "mindformers.core.metric.EmF1Metric.evaluate_pairs": {
        "signature": "(self, pred_, ans_)"
    },
    "mindformers.core.metric.EntityScore": {
        "signature": "()"
    },
    "mindformers.core.metric.EntityScore.clear": {
        "signature": "(self)"
    },
    "mindformers.core.metric.EntityScore.update": {
        "signature": "(self, *inputs)"
    },
    "mindformers.core.metric.EntityScore.eval": {
        "signature": "(self)"
    },
    "mindformers.core.metric.EntityScore.compute": {
        "signature": "(self, origin, found, right)"
    },
    "mindformers.core.metric.EntityScore.get_entities_bios": {
        "signature": "(self, seq)"
    },
    "mindformers.core.metric.PerplexityMetric": {
        "signature": "()"
    },
    "mindformers.core.metric.PerplexityMetric.clear": {
        "signature": "(self)"
    },
    "mindformers.core.metric.PerplexityMetric.update": {
        "signature": "(self, *inputs)"
    },
    "mindformers.core.metric.PerplexityMetric.eval": {
        "signature": "(self)"
    },
    "mindformers.core.metric.PromptAccMetric": {
        "signature": "()"
    },
    "mindformers.core.metric.PromptAccMetric.clear": {
        "signature": "(self)"
    },
    "mindformers.core.metric.PromptAccMetric.calculate_circle": {
        "signature": "(self, *inputs)"
    },
    "mindformers.core.metric.PromptAccMetric.update": {
        "signature": "(self, *inputs)"
    },
    "mindformers.core.metric.PromptAccMetric.eval": {
        "signature": "(self)"
    },
    "mindformers.core.metric.SQuADMetric": {
        "signature": "(dataset_dir, n_best_size=20, max_answer_len=30, do_lower_case=True, temp_file_dir='./squad_temp')"
    },
    "mindformers.core.metric.SQuADMetric.clear": {
        "signature": "(self)"
    },
    "mindformers.core.metric.SQuADMetric.update": {
        "signature": "(self, *inputs)"
    },
    "mindformers.core.metric.SQuADMetric.eval": {
        "signature": "(self)"
    },
    "mindformers.core.metric.SQuADMetric._remove_temp_data": {
        "signature": "(self)"
    },
    "mindformers.core.metric.SQuADMetric._load_temp_data": {
        "signature": "(self, temp_file_path)"
    },
    "mindformers.core.metric.SQuADMetric._normalize_answer": {
        "signature": "(self, s)"
    },
    "mindformers.core.metric.SQuADMetric._f1_score": {
        "signature": "(self, prediction, ground_truth)"
    },
    "mindformers.core.metric.SQuADMetric._exact_match_score": {
        "signature": "(self, prediction, ground_truth)"
    },
    "mindformers.core.metric.SQuADMetric._metric_max_over_ground_truths": {
        "signature": "(self, metric_fn, prediction, ground_truths)"
    },
    "mindformers.core.metric.SQuADMetric._get_predictions": {
        "signature": "(self)"
    },
    "mindformers.core.metric.SQuADMetric._get_prelim_predictions": {
        "signature": "(self, features, unique_id_to_result)"
    },
    "mindformers.core.metric.SQuADMetric._get_nbest": {
        "signature": "(self, prelim_predictions, features, example)"
    },
    "mindformers.core.metric.SQuADMetric._compute_softmax": {
        "signature": "(self, scores)"
    },
    "mindformers.core.metric.SQuADMetric._get_final_text": {
        "signature": "(self, pred_text, orig_text)"
    },
    "mindformers.core.metric.SQuADMetric._get_best_indexes": {
        "signature": "(self, logits)"
    },
    "mindformers.core.metric.metric.ADGENMetric": {
        "signature": "()"
    },
    "mindformers.core.metric.metric.ADGENMetric.clear": {
        "signature": "(self)"
    },
    "mindformers.core.metric.metric.ADGENMetric.update": {
        "signature": "(self, *inputs)"
    },
    "mindformers.core.metric.metric.ADGENMetric.eval": {
        "signature": "(self)"
    },
    "mindformers.core.metric.metric.EmF1Metric": {
        "signature": "()"
    },
    "mindformers.core.metric.metric.EmF1Metric.clear": {
        "signature": "(self)"
    },
    "mindformers.core.metric.metric.EmF1Metric.update": {
        "signature": "(self, *inputs)"
    },
    "mindformers.core.metric.metric.EmF1Metric.eval": {
        "signature": "(self)"
    },
    "mindformers.core.metric.metric.EmF1Metric.mixed_segmentation": {
        "signature": "(self, in_str, rm_punc=False)"
    },
    "mindformers.core.metric.metric.EmF1Metric.remove_punctuation": {
        "signature": "(self, in_str)"
    },
    "mindformers.core.metric.metric.EmF1Metric.find_lcs": {
        "signature": "(self, s1, s2)"
    },
    "mindformers.core.metric.metric.EmF1Metric.calc_f1_score": {
        "signature": "(self, answers, prediction)"
    },
    "mindformers.core.metric.metric.EmF1Metric.calc_em_score": {
        "signature": "(self, answers, prediction)"
    },
    "mindformers.core.metric.metric.EmF1Metric.evaluate_pairs": {
        "signature": "(self, pred_, ans_)"
    },
    "mindformers.core.metric.metric.EntityScore": {
        "signature": "()"
    },
    "mindformers.core.metric.metric.EntityScore.clear": {
        "signature": "(self)"
    },
    "mindformers.core.metric.metric.EntityScore.update": {
        "signature": "(self, *inputs)"
    },
    "mindformers.core.metric.metric.EntityScore.eval": {
        "signature": "(self)"
    },
    "mindformers.core.metric.metric.EntityScore.compute": {
        "signature": "(self, origin, found, right)"
    },
    "mindformers.core.metric.metric.EntityScore.get_entities_bios": {
        "signature": "(self, seq)"
    },
    "mindformers.core.metric.metric.PerplexityMetric": {
        "signature": "()"
    },
    "mindformers.core.metric.metric.PerplexityMetric.clear": {
        "signature": "(self)"
    },
    "mindformers.core.metric.metric.PerplexityMetric.update": {
        "signature": "(self, *inputs)"
    },
    "mindformers.core.metric.metric.PerplexityMetric.eval": {
        "signature": "(self)"
    },
    "mindformers.core.metric.metric.PromptAccMetric": {
        "signature": "()"
    },
    "mindformers.core.metric.metric.PromptAccMetric.clear": {
        "signature": "(self)"
    },
    "mindformers.core.metric.metric.PromptAccMetric.calculate_circle": {
        "signature": "(self, *inputs)"
    },
    "mindformers.core.metric.metric.PromptAccMetric.update": {
        "signature": "(self, *inputs)"
    },
    "mindformers.core.metric.metric.PromptAccMetric.eval": {
        "signature": "(self)"
    },
    "mindformers.core.metric.metric.SQuADMetric": {
        "signature": "(dataset_dir, n_best_size=20, max_answer_len=30, do_lower_case=True, temp_file_dir='./squad_temp')"
    },
    "mindformers.core.metric.metric.SQuADMetric.clear": {
        "signature": "(self)"
    },
    "mindformers.core.metric.metric.SQuADMetric.update": {
        "signature": "(self, *inputs)"
    },
    "mindformers.core.metric.metric.SQuADMetric.eval": {
        "signature": "(self)"
    },
    "mindformers.core.metric.metric.SQuADMetric._remove_temp_data": {
        "signature": "(self)"
    },
    "mindformers.core.metric.metric.SQuADMetric._load_temp_data": {
        "signature": "(self, temp_file_path)"
    },
    "mindformers.core.metric.metric.SQuADMetric._normalize_answer": {
        "signature": "(self, s)"
    },
    "mindformers.core.metric.metric.SQuADMetric._f1_score": {
        "signature": "(self, prediction, ground_truth)"
    },
    "mindformers.core.metric.metric.SQuADMetric._exact_match_score": {
        "signature": "(self, prediction, ground_truth)"
    },
    "mindformers.core.metric.metric.SQuADMetric._metric_max_over_ground_truths": {
        "signature": "(self, metric_fn, prediction, ground_truths)"
    },
    "mindformers.core.metric.metric.SQuADMetric._get_predictions": {
        "signature": "(self)"
    },
    "mindformers.core.metric.metric.SQuADMetric._get_prelim_predictions": {
        "signature": "(self, features, unique_id_to_result)"
    },
    "mindformers.core.metric.metric.SQuADMetric._get_nbest": {
        "signature": "(self, prelim_predictions, features, example)"
    },
    "mindformers.core.metric.metric.SQuADMetric._compute_softmax": {
        "signature": "(self, scores)"
    },
    "mindformers.core.metric.metric.SQuADMetric._get_final_text": {
        "signature": "(self, pred_text, orig_text)"
    },
    "mindformers.core.metric.metric.SQuADMetric._get_best_indexes": {
        "signature": "(self, logits)"
    },
    "mindformers.core.optim.AdamW": {
        "signature": "(params, learning_rate=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.0)"
    },
    "mindformers.core.optim.AdamW.clone_state": {
        "signature": "(self, prefix, init)"
    },
    "mindformers.core.optim.AdamW.construct": {
        "signature": "(self, gradients)"
    },
    "mindformers.core.optim.adamw.AdamW": {
        "signature": "(params, learning_rate=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.0)"
    },
    "mindformers.core.optim.adamw.AdamW.clone_state": {
        "signature": "(self, prefix, init)"
    },
    "mindformers.core.optim.adamw.AdamW.construct": {
        "signature": "(self, gradients)"
    },
    "mindformers.core.optim.optim.FP32StateAdamWeightDecay": {
        "signature": "(params, learning_rate=0.001, beta1=0.9, beta2=0.999, eps=1e-06, weight_decay=0.0)"
    },
    "mindformers.core.optim.optim.FusedAdamWeightDecay": {
        "signature": "(params, learning_rate=0.001, beta1=0.9, beta2=0.999, eps=1e-06, weight_decay=0.0, offload=False)"
    },
    "mindformers.core.optim.optim.FusedAdamWeightDecay.construct": {
        "signature": "(self, gradients)"
    },
    "mindformers.core.optim.optim.FusedAdamWeightDecay.clone_state": {
        "signature": "(self, prefix, init, forced_dtype=mindspore.float32, is_follow=False)"
    },
    "mindformers.core.optim.optim.FusedCastAdamWeightDecay": {
        "signature": "(params, learning_rate=0.001, beta1=0.9, beta2=0.999, eps=1e-06, weight_decay=0.0, clip_norm=1.0)"
    },
    "mindformers.core.optim.optim.FusedCastAdamWeightDecay.construct": {
        "signature": "(self, gradients, clip_value)"
    },
    "mindformers.dataset.BlendedMegatronDatasetDataLoader": {
        "signature": "(datasets_type: str, sizes: List[int], config: dict, tokenizer: Union[Callable, dict] = None, column_names: list = None, shuffle: bool = False, num_shards: int = None, shard_id: int = None, phase: str = 'train', **kwargs)"
    },
    "mindformers.dataset.CausalLanguageModelDataset": {
        "signature": "(dataset_config: Optional[dict] = None, data_loader: Union[Callable, dict] = None, input_columns: List[str] = None, output_columns: List[str] = None, batch_size: int = 8, drop_remainder: bool = True, num_parallel_workers: int = 8, python_multiprocessing: bool = False, repeat: int = 1, seed: int = 0, prefetch_size: int = 1, numa_enable: bool = False, eod_reset: bool = False, eod_token_id: Optional[int] = None, auto_tune: bool = False, filepath_prefix: str = './autotune', autotune_per_step: int = 10, profile: bool = False, **kwargs)"
    },
    "mindformers.dataset.CausalLanguageModelDataset._process_raw_text_data": {
        "signature": "(dataset_config)"
    },
    "mindformers.dataset.CausalLanguageModelDataset._process_mindrecord_data": {
        "signature": "(dataset_config)"
    },
    "mindformers.dataset.CausalLanguageModelDataset._reset_num_samples": {
        "signature": "(dataset_config)"
    },
    "mindformers.dataset.KeyWordGenDataset": {
        "signature": "(dataset_config: Optional[dict] = None, data_loader: Union[Callable, dict] = None, tokenizer: Union[Callable, dict] = None, input_columns: List[str] = None, batch_size: int = 8, drop_remainder: bool = True, num_parallel_workers: int = 8, repeat: int = 1, ignore_pad_token_for_loss: bool = True, max_source_length: int = None, max_target_length: int = None, phase: str = 'train', version: int = 1, seed: int = 0, prefetch_size: int = 1, numa_enable: bool = False, auto_tune: bool = False, filepath_prefix: str = './autotune', autotune_per_step: int = 10, profile: bool = False, **kwargs)"
    },
    "mindformers.dataset.KeyWordGenDataset._tokenizer_map": {
        "signature": "(dataset, dataset_config)"
    },
    "mindformers.dataset.KeyWordGenDataset._process_raw_text_data": {
        "signature": "(dataset_config)"
    },
    "mindformers.dataset.KeyWordGenDataset._process_mindrecord_data": {
        "signature": "(dataset_config)"
    },
    "mindformers.dataset.KeyWordGenDataset._train_dataset_function": {
        "signature": "(prompt, answer, dataset_config, tokenizer)"
    },
    "mindformers.dataset.KeyWordGenDataset._train_dataset_functionv2": {
        "signature": "(prompt, answer, dataset_config, tokenizer)"
    },
    "mindformers.dataset.KeyWordGenDataset._train_dataset_functionv3": {
        "signature": "(prompt, answer, dataset_config, tokenizer)"
    },
    "mindformers.dataset.KeyWordGenDataset._eval_dataset_function": {
        "signature": "(prompt, answer, dataset_config, tokenizer)"
    },
    "mindformers.dataset.KeyWordGenDataset._eval_dataset_functionv2": {
        "signature": "(prompt, answer, dataset_config, tokenizer)"
    },
    "mindformers.dataset.KeyWordGenDataset._get_masks": {
        "signature": "(input_ids, bos_token_id=130004)"
    },
    "mindformers.dataset.KeyWordGenDataset._get_position_ids": {
        "signature": "(input_ids, mask_positions, use_gmasks=None, bos_token_id=130004, position_encoding_2d=True)"
    },
    "mindformers.dataset.KeyWordGenDataset._create_position_ids": {
        "signature": "(input_ids, gmask_token_id=130001)"
    },
    "mindformers.dataset.MultiTurnDataset": {
        "signature": "(dataset_config: dict)"
    },
    "mindformers.dataset.MultiTurnDataset._tokenizer_map": {
        "signature": "(dataset, dataset_config)"
    },
    "mindformers.dataset.MultiTurnDataset._format_function_call": {
        "signature": "(function_name: str, parameters: Dict[str, str])"
    },
    "mindformers.dataset.MultiTurnDataset._format_conversation": {
        "signature": "(item, tokenizer, conversation_key: str, tool_key: str)"
    },
    "mindformers.dataset.MultiTurnDataset._train_dataset_function": {
        "signature": "(data, dataset_config, tokenizer)"
    },
    "mindformers.dataset.dataloader.BlendedMegatronDatasetDataLoader": {
        "signature": "(datasets_type: str, sizes: List[int], config: dict, tokenizer: Union[Callable, dict] = None, column_names: list = None, shuffle: bool = False, num_shards: int = None, shard_id: int = None, phase: str = 'train', **kwargs)"
    },
    "mindformers.dataset.dpo_dataset.DPODataset": {
        "signature": "(dataset_config: Optional[dict] = None, data_loader: Union[Callable, dict] = None, input_columns: list = None, output_columns: list = None, batch_size: int = 8, drop_remainder: bool = True, num_parallel_workers: int = 8, python_multiprocessing: bool = False, repeat: int = 1, seed: int = 0, prefetch_size: int = 1, numa_enable: bool = False, eod_reset: bool = False, eod_token_id: Optional[int] = None, auto_tune: bool = False, filepath_prefix: str = './autotune', autotune_per_step: int = 10, profile: bool = False, **kwargs)"
    },
    "mindformers.dataset.dpo_dataset.DPODataset._process_raw_text_data": {
        "signature": "(dataset_config)"
    },
    "mindformers.dataset.dpo_dataset.DPODataset._process_mindrecord_data": {
        "signature": "(dataset_config)"
    },
    "mindformers.dataset.mask.vision_mask.MaeMask": {
        "signature": "(input_size=192, patch_size=4, mask_ratio=0.75)"
    },
    "mindformers.dataset.mask.vision_mask.SimMask": {
        "signature": "(input_size=192, mask_patch_size=32, model_patch_size=4, mask_ratio=0.6)"
    },
    "mindformers.dataset.transforms.text_transforms.CaptionTransform": {
        "signature": "(tokenizer, prompt='', max_words=50, max_length=32, padding='max_length', random_seed=2022, truncation=True, add_special_tokens=True)"
    },
    "mindformers.dataset.transforms.text_transforms.CaptionTransform.pre_caption": {
        "signature": "(self, caption)"
    },
    "mindformers.dataset.transforms.text_transforms.LabelPadding": {
        "signature": "(max_length, padding_value=0)"
    },
    "mindformers.dataset.transforms.text_transforms.RandomChoiceTokenizerForward": {
        "signature": "(tokenizer, max_length=77, padding='max_length', random_seed=2022)"
    },
    "mindformers.dataset.transforms.text_transforms.TokenizeWithLabel": {
        "signature": "(tokenizer, max_length=128, padding='max_length')"
    },
    "mindformers.dataset.transforms.text_transforms.TokenizerForward": {
        "signature": "(tokenizer, max_length=77, padding='max_length')"
    },
    "mindformers.dataset.transforms.vision_transforms.BCHW2BHWC": {
        "signature": "()"
    },
    "mindformers.dataset.transforms.vision_transforms.BatchCenterCrop": {
        "signature": "(image_resolution)"
    },
    "mindformers.dataset.transforms.vision_transforms.BatchNormalize": {
        "signature": "(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711), is_hwc=False)"
    },
    "mindformers.dataset.transforms.vision_transforms.BatchPILize": {
        "signature": "()"
    },
    "mindformers.dataset.transforms.vision_transforms.BatchResize": {
        "signature": "(image_resolution, interpolation='cubic')"
    },
    "mindformers.dataset.transforms.vision_transforms.BatchToTensor": {
        "signature": "()"
    },
    "mindformers.dataset.transforms.vision_transforms.RandomCropDecodeResize": {
        "signature": "(size, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation='cubic', max_attempts=10)"
    },
    "mindformers.dataset.transforms.vision_transforms.RandomHorizontalFlip": {
        "signature": "(prob=0.5)"
    },
    "mindformers.dataset.transforms.vision_transforms.RandomResizedCrop": {
        "signature": "(size, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation='cubic', max_attempts=10)"
    },
    "mindformers.dataset.transforms.vision_transforms.Resize": {
        "signature": "(size, interpolation='cubic')"
    },
    "mindformers.experimental.LlamaForCausalLM": {
        "signature": "(config: mindformers.models.llama.llama_config.LlamaConfig, num_tokentypes: int = 0, parallel_output: bool = True, pre_process: bool = True, post_process: bool = True, **kwargs)"
    },
    "mindformers.experimental.LlamaForCausalLM.construct": {
        "signature": "(self, input_ids: mindspore.common.tensor.Tensor, position_ids: mindspore.common.tensor.Tensor = None, attention_mask: mindspore.common.tensor.Tensor = None, retriever_input_ids: mindspore.common.tensor.Tensor = None, retriever_position_ids: mindspore.common.tensor.Tensor = None, retriever_attn_mask: mindspore.common.tensor.Tensor = None, labels: mindspore.common.tensor.Tensor = None, tokentype_ids=None, inference_params=None, prefix_keys_values=None, input_embeds: mindspore.common.tensor.Tensor = None, loss_mask: mindspore.common.tensor.Tensor = None) -> mindspore.common.tensor.Tensor"
    },
    "mindformers.experimental.LlamaForCausalLM.post_language_model_processing": {
        "signature": "(self, lm_output: mindspore.common.tensor.Tensor, labels: mindspore.common.tensor.Tensor, logit_weights: mindspore.common.tensor.Tensor, parallel_output: bool, fp16_lm_cross_entropy: bool, loss_mask: mindspore.common.tensor.Tensor) -> mindspore.common.tensor.Tensor"
    },
    "mindformers.experimental.LlamaForCausalLM.set_input_tensor": {
        "signature": "(self, input_tensor)"
    },
    "mindformers.experimental.LlamaForCausalLM.set_dynamic_inputs": {
        "signature": "(self, **kwargs)"
    },
    "mindformers.experimental.LlamaForCausalLM._preprocess_input_labels_and_masks": {
        "signature": "(self, input_ids: mindspore.common.tensor.Tensor, labels: mindspore.common.tensor.Tensor = None, attention_mask: mindspore.common.tensor.Tensor = None) -> (<class 'mindspore.common.tensor.Tensor'>, <class 'mindspore.common.tensor.Tensor'>, <class 'mindspore.common.tensor.Tensor'>, <class 'mindspore.common.tensor.Tensor'>)"
    },
    "mindformers.experimental.LlamaForCausalLM.shard": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig)"
    },
    "mindformers.experimental.LlamaForCausalLM.sharding_propagation": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig)"
    },
    "mindformers.experimental.graph.ApplyRotaryPosEmb": {
        "signature": "(config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig)"
    },
    "mindformers.experimental.graph.ApplyRotaryPosEmb.construct": {
        "signature": "(self, t: mindspore.common.tensor.Tensor, freqs: mindspore.common.tensor.Tensor, rotary_interleaved: bool = False) -> mindspore.common.tensor.Tensor"
    },
    "mindformers.experimental.graph.ApplyRotaryPosEmb._rotate_half": {
        "signature": "(self, t: mindspore.common.tensor.Tensor, rotary_interleaved: bool = False) -> mindspore.common.tensor.Tensor"
    },
    "mindformers.experimental.graph.ApplyRotaryPosEmb.shard": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig)"
    },
    "mindformers.experimental.graph.ApplyRotaryPosEmb.sharding_propagation": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig)"
    },
    "mindformers.experimental.graph.AttnMaskType": {
        "signature": "(value, names=None, *, module=None, qualname=None, type=None, start=1)"
    },
    "mindformers.experimental.graph.ColumnParallelLinear": {
        "signature": "(input_size: int, output_size: int, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig, init_method: Callable = None, bias: bool = True, gather_output: bool = False, stride: int = 1, keep_master_weight_for_test: bool = False, skip_bias_add: bool = False, skip_weight_param_allocation: bool = False, embedding_activation_buffer: Optional[List[mindspore.common.tensor.Tensor]] = None, grad_output_buffer: Optional[List[mindspore.common.tensor.Tensor]] = None, is_expert: bool = False, tp_comm_buffer_name: str = None, disable_grad_reduce: bool = False, transpose_b: bool = True, compute_dtype: mindspore.common.dtype = mindspore.float16, bias_init: Callable = None)"
    },
    "mindformers.experimental.graph.ColumnParallelLinear.construct": {
        "signature": "(self, input_: mindspore.common.tensor.Tensor, weight: mindspore.common.tensor.Tensor = None) -> tuple[mindspore.common.tensor.Tensor, mindspore.common.tensor.Tensor]"
    },
    "mindformers.experimental.graph.ColumnParallelLinear.shard": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig) -> None"
    },
    "mindformers.experimental.graph.ColumnParallelLinear.sharding_propagation": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig) -> None"
    },
    "mindformers.experimental.graph.Dropout": {
        "signature": "(drop_prob=0.5, dtype=mindspore.float32)"
    },
    "mindformers.experimental.graph.Dropout.construct": {
        "signature": "(self, x)"
    },
    "mindformers.experimental.graph.Dropout.extend_repr": {
        "signature": "(self)"
    },
    "mindformers.experimental.graph.Dropout.shard": {
        "signature": "(self, strategy)"
    },
    "mindformers.experimental.graph.FusedScaleMaskSoftmax": {
        "signature": "(input_in_fp16: bool = False, input_in_bf16: bool = False, attn_mask_type: mindformers.experimental.graph.transformer.enums.AttnMaskType = <AttnMaskType.causal: 2>, scaled_masked_softmax_fusion: bool = False, mask_func: Callable = None, softmax_in_fp32: bool = True, scale: float = None, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig = None, softmax_compute_dtype: mindspore.common.dtype = None)"
    },
    "mindformers.experimental.graph.FusedScaleMaskSoftmax.construct": {
        "signature": "(self, input_: mindspore.common.tensor.Tensor, mask: mindspore.common.tensor.Tensor = None) -> mindspore.common.tensor.Tensor"
    },
    "mindformers.experimental.graph.FusedScaleMaskSoftmax.shard": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig)"
    },
    "mindformers.experimental.graph.FusedScaleMaskSoftmax.sharding_propagation": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig)"
    },
    "mindformers.experimental.graph.LoRAColumnParallelLinear": {
        "signature": "(input_size: int, output_size: int, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig, init_method: Callable = None, bias: bool = True, gather_output: bool = False, stride: int = 1, keep_master_weight_for_test: bool = False, skip_bias_add: bool = False, skip_weight_param_allocation: bool = False, embedding_activation_buffer: Optional[List[mindspore.common.tensor.Tensor]] = None, grad_output_buffer: Optional[List[mindspore.common.tensor.Tensor]] = None, is_expert: bool = False, tp_comm_buffer_name: str = None, disable_grad_reduce: bool = False, transpose_b: bool = True, compute_dtype: mindspore.common.dtype = mindspore.float16, bias_init: Callable = None, lora_rank: int = 8, lora_alpha: int = 32, lora_dropout: float = 0.0, lora_a_init='normal', lora_b_init='zeros')"
    },
    "mindformers.experimental.graph.LoRAColumnParallelLinear.construct": {
        "signature": "(self, input_: mindspore.common.tensor.Tensor, weight: mindspore.common.tensor.Tensor = None) -> tuple[mindspore.common.tensor.Tensor, mindspore.common.tensor.Tensor]"
    },
    "mindformers.experimental.graph.LoRAColumnParallelLinear.shard": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig) -> None"
    },
    "mindformers.experimental.graph.LoRARowParallelLinear": {
        "signature": "(input_size: int, output_size: int, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig, init_method: Callable = None, bias: bool = True, input_is_parallel: bool = False, skip_bias_add: bool = False, stride: int = 1, keep_master_weight_for_test: bool = False, is_expert: bool = False, tp_comm_buffer_name: str = None, transpose_b: bool = True, compute_dtype: mindspore.common.dtype = mindspore.float16, bias_init: Callable = None, lora_rank: int = 8, lora_alpha: int = 32, lora_dropout: float = 0.0, lora_a_init='normal', lora_b_init='zeros')"
    },
    "mindformers.experimental.graph.LoRARowParallelLinear.construct": {
        "signature": "(self, input_: mindspore.common.tensor.Tensor) -> tuple[mindspore.common.tensor.Tensor, mindspore.common.tensor.Tensor]"
    },
    "mindformers.experimental.graph.LoRARowParallelLinear.shard": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig) -> None"
    },
    "mindformers.experimental.graph.RotaryEmbedding": {
        "signature": "(kv_channels: int, rotary_percent: float = 1.0, rotary_interleaved: bool = False, seq_len_interpolation_factor: float = None, rotary_base: int = 10000)"
    },
    "mindformers.experimental.graph.RotaryEmbedding.construct": {
        "signature": "(self, max_seq_len: int, offset: int = 0) -> mindspore.common.tensor.Tensor"
    },
    "mindformers.experimental.graph.RowParallelLinear": {
        "signature": "(input_size: int, output_size: int, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig, init_method: Callable = None, bias: bool = True, input_is_parallel: bool = False, skip_bias_add: bool = False, stride: int = 1, keep_master_weight_for_test: bool = False, is_expert: bool = False, tp_comm_buffer_name: str = None, transpose_b: bool = True, compute_dtype: mindspore.common.dtype = mindspore.float16, bias_init: Callable = None)"
    },
    "mindformers.experimental.graph.RowParallelLinear.construct": {
        "signature": "(self, input_: mindspore.common.tensor.Tensor) -> tuple[mindspore.common.tensor.Tensor, mindspore.common.tensor.Tensor]"
    },
    "mindformers.experimental.graph.RowParallelLinear.shard": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig) -> None"
    },
    "mindformers.experimental.graph.RowParallelLinear.sharding_propagation": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig) -> None"
    },
    "mindformers.experimental.graph.VocabParallelEmbedding": {
        "signature": "(num_embeddings, embedding_dim, parallel_config, init_method: Callable, init_type=mindspore.float32)"
    },
    "mindformers.experimental.graph.VocabParallelEmbedding.construct": {
        "signature": "(self, input_ids)"
    },
    "mindformers.experimental.graph.VocabParallelEmbedding.shard": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig)"
    },
    "mindformers.experimental.graph.apply_rotary_pos_emb": {
        "signature": "(t: mindspore.common.tensor.Tensor, freqs: mindspore.common.tensor.Tensor, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig, cu_seqlens: mindspore.common.tensor.Tensor = None) -> mindspore.common.tensor.Tensor"
    },
    "mindformers.experimental.graph.activation.GELU": {
        "signature": "(config: mindformers.experimental.graph.transformer.transformer_config.ModelParallelConfig = None, approximate: bool = True)"
    },
    "mindformers.experimental.graph.activation.GELU.construct": {
        "signature": "(self, x: mindspore.common.tensor.Tensor, bias: mindspore.common.tensor.Tensor = None) -> mindspore.common.tensor.Tensor"
    },
    "mindformers.experimental.graph.activation.GELU.shard": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.ModelParallelConfig)"
    },
    "mindformers.experimental.graph.activation.GELU.sharding_propagation": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.ModelParallelConfig)"
    },
    "mindformers.experimental.graph.activation.SiLU": {
        "signature": "(config: mindformers.experimental.graph.transformer.transformer_config.ModelParallelConfig = None)"
    },
    "mindformers.experimental.graph.activation.SiLU.construct": {
        "signature": "(self, x: mindspore.common.tensor.Tensor, bias: mindspore.common.tensor.Tensor = None) -> mindspore.common.tensor.Tensor"
    },
    "mindformers.experimental.graph.activation.SiLU.shard": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.ModelParallelConfig)"
    },
    "mindformers.experimental.graph.activation.SiLU.sharding_propagation": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.ModelParallelConfig)"
    },
    "mindformers.experimental.graph.activation.SwiGlu": {
        "signature": "(config: mindformers.experimental.graph.transformer.transformer_config.ModelParallelConfig = None)"
    },
    "mindformers.experimental.graph.activation.SwiGlu.construct": {
        "signature": "(self, x: mindspore.common.tensor.Tensor, bias: mindspore.common.tensor.Tensor = None) -> mindspore.common.tensor.Tensor"
    },
    "mindformers.experimental.graph.activation.SwiGlu.shard": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.ModelParallelConfig)"
    },
    "mindformers.experimental.graph.activation.SwiGlu.sharding_propagation": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.ModelParallelConfig)"
    },
    "mindformers.experimental.graph.activation.bias_gelu_impl": {
        "signature": "(x: mindspore.common.tensor.Tensor, bias: mindspore.common.tensor.Tensor = None, config: mindformers.experimental.graph.transformer.transformer_config.ModelParallelConfig = None) -> mindspore.common.tensor.Tensor"
    },
    "mindformers.experimental.graph.activation.bias_swiglu_impl": {
        "signature": "(x: mindspore.common.tensor.Tensor, bias: mindspore.common.tensor.Tensor = None, fp8_input_store: bool = False, config: mindformers.experimental.graph.transformer.transformer_config.ModelParallelConfig = None) -> mindspore.common.tensor.Tensor"
    },
    "mindformers.experimental.graph.activation.get_activation": {
        "signature": "(activation_name, *args, **kwargs)"
    },
    "mindformers.experimental.graph.optimizer.lr_scheduler.lr_scheduler.LearningRateScheduler": {
        "signature": "(learning_rate: float, warmup_steps: int = 0, total_steps: int = None, lr_end: float = 0.0, warmup_lr_init: float = 0.0, warmup_ratio: float = None, num_cycles: float = 0.5, power: float = 1.0, decay_steps: int = None, lr_decay_style: str = 'cosine', **kwargs)"
    },
    "mindformers.experimental.graph.optimizer.lr_scheduler.lr_scheduler.LearningRateScheduler.construct": {
        "signature": "(self, global_step)"
    },
    "mindformers.experimental.graph.tensor_parallel.ColumnParallelLinear": {
        "signature": "(input_size: int, output_size: int, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig, init_method: Callable = None, bias: bool = True, gather_output: bool = False, stride: int = 1, keep_master_weight_for_test: bool = False, skip_bias_add: bool = False, skip_weight_param_allocation: bool = False, embedding_activation_buffer: Optional[List[mindspore.common.tensor.Tensor]] = None, grad_output_buffer: Optional[List[mindspore.common.tensor.Tensor]] = None, is_expert: bool = False, tp_comm_buffer_name: str = None, disable_grad_reduce: bool = False, transpose_b: bool = True, compute_dtype: mindspore.common.dtype = mindspore.float16, bias_init: Callable = None)"
    },
    "mindformers.experimental.graph.tensor_parallel.ColumnParallelLinear.construct": {
        "signature": "(self, input_: mindspore.common.tensor.Tensor, weight: mindspore.common.tensor.Tensor = None) -> tuple[mindspore.common.tensor.Tensor, mindspore.common.tensor.Tensor]"
    },
    "mindformers.experimental.graph.tensor_parallel.ColumnParallelLinear.shard": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig) -> None"
    },
    "mindformers.experimental.graph.tensor_parallel.ColumnParallelLinear.sharding_propagation": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig) -> None"
    },
    "mindformers.experimental.graph.tensor_parallel.LoRAColumnParallelLinear": {
        "signature": "(input_size: int, output_size: int, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig, init_method: Callable = None, bias: bool = True, gather_output: bool = False, stride: int = 1, keep_master_weight_for_test: bool = False, skip_bias_add: bool = False, skip_weight_param_allocation: bool = False, embedding_activation_buffer: Optional[List[mindspore.common.tensor.Tensor]] = None, grad_output_buffer: Optional[List[mindspore.common.tensor.Tensor]] = None, is_expert: bool = False, tp_comm_buffer_name: str = None, disable_grad_reduce: bool = False, transpose_b: bool = True, compute_dtype: mindspore.common.dtype = mindspore.float16, bias_init: Callable = None, lora_rank: int = 8, lora_alpha: int = 32, lora_dropout: float = 0.0, lora_a_init='normal', lora_b_init='zeros')"
    },
    "mindformers.experimental.graph.tensor_parallel.LoRAColumnParallelLinear.construct": {
        "signature": "(self, input_: mindspore.common.tensor.Tensor, weight: mindspore.common.tensor.Tensor = None) -> tuple[mindspore.common.tensor.Tensor, mindspore.common.tensor.Tensor]"
    },
    "mindformers.experimental.graph.tensor_parallel.LoRAColumnParallelLinear.shard": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig) -> None"
    },
    "mindformers.experimental.graph.tensor_parallel.LoRARowParallelLinear": {
        "signature": "(input_size: int, output_size: int, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig, init_method: Callable = None, bias: bool = True, input_is_parallel: bool = False, skip_bias_add: bool = False, stride: int = 1, keep_master_weight_for_test: bool = False, is_expert: bool = False, tp_comm_buffer_name: str = None, transpose_b: bool = True, compute_dtype: mindspore.common.dtype = mindspore.float16, bias_init: Callable = None, lora_rank: int = 8, lora_alpha: int = 32, lora_dropout: float = 0.0, lora_a_init='normal', lora_b_init='zeros')"
    },
    "mindformers.experimental.graph.tensor_parallel.LoRARowParallelLinear.construct": {
        "signature": "(self, input_: mindspore.common.tensor.Tensor) -> tuple[mindspore.common.tensor.Tensor, mindspore.common.tensor.Tensor]"
    },
    "mindformers.experimental.graph.tensor_parallel.LoRARowParallelLinear.shard": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig) -> None"
    },
    "mindformers.experimental.graph.tensor_parallel.RowParallelLinear": {
        "signature": "(input_size: int, output_size: int, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig, init_method: Callable = None, bias: bool = True, input_is_parallel: bool = False, skip_bias_add: bool = False, stride: int = 1, keep_master_weight_for_test: bool = False, is_expert: bool = False, tp_comm_buffer_name: str = None, transpose_b: bool = True, compute_dtype: mindspore.common.dtype = mindspore.float16, bias_init: Callable = None)"
    },
    "mindformers.experimental.graph.tensor_parallel.RowParallelLinear.construct": {
        "signature": "(self, input_: mindspore.common.tensor.Tensor) -> tuple[mindspore.common.tensor.Tensor, mindspore.common.tensor.Tensor]"
    },
    "mindformers.experimental.graph.tensor_parallel.RowParallelLinear.shard": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig) -> None"
    },
    "mindformers.experimental.graph.tensor_parallel.RowParallelLinear.sharding_propagation": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig) -> None"
    },
    "mindformers.experimental.graph.tensor_parallel.VocabParallelEmbedding": {
        "signature": "(num_embeddings, embedding_dim, parallel_config, init_method: Callable, init_type=mindspore.float32)"
    },
    "mindformers.experimental.graph.tensor_parallel.VocabParallelEmbedding.construct": {
        "signature": "(self, input_ids)"
    },
    "mindformers.experimental.graph.tensor_parallel.VocabParallelEmbedding.shard": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig)"
    },
    "mindformers.experimental.graph.tensor_parallel.layers.ColumnParallelLinear": {
        "signature": "(input_size: int, output_size: int, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig, init_method: Callable = None, bias: bool = True, gather_output: bool = False, stride: int = 1, keep_master_weight_for_test: bool = False, skip_bias_add: bool = False, skip_weight_param_allocation: bool = False, embedding_activation_buffer: Optional[List[mindspore.common.tensor.Tensor]] = None, grad_output_buffer: Optional[List[mindspore.common.tensor.Tensor]] = None, is_expert: bool = False, tp_comm_buffer_name: str = None, disable_grad_reduce: bool = False, transpose_b: bool = True, compute_dtype: mindspore.common.dtype = mindspore.float16, bias_init: Callable = None)"
    },
    "mindformers.experimental.graph.tensor_parallel.layers.ColumnParallelLinear.construct": {
        "signature": "(self, input_: mindspore.common.tensor.Tensor, weight: mindspore.common.tensor.Tensor = None) -> tuple[mindspore.common.tensor.Tensor, mindspore.common.tensor.Tensor]"
    },
    "mindformers.experimental.graph.tensor_parallel.layers.ColumnParallelLinear.shard": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig) -> None"
    },
    "mindformers.experimental.graph.tensor_parallel.layers.ColumnParallelLinear.sharding_propagation": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig) -> None"
    },
    "mindformers.experimental.graph.tensor_parallel.layers.RowParallelLinear": {
        "signature": "(input_size: int, output_size: int, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig, init_method: Callable = None, bias: bool = True, input_is_parallel: bool = False, skip_bias_add: bool = False, stride: int = 1, keep_master_weight_for_test: bool = False, is_expert: bool = False, tp_comm_buffer_name: str = None, transpose_b: bool = True, compute_dtype: mindspore.common.dtype = mindspore.float16, bias_init: Callable = None)"
    },
    "mindformers.experimental.graph.tensor_parallel.layers.RowParallelLinear.construct": {
        "signature": "(self, input_: mindspore.common.tensor.Tensor) -> tuple[mindspore.common.tensor.Tensor, mindspore.common.tensor.Tensor]"
    },
    "mindformers.experimental.graph.tensor_parallel.layers.RowParallelLinear.shard": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig) -> None"
    },
    "mindformers.experimental.graph.tensor_parallel.layers.RowParallelLinear.sharding_propagation": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig) -> None"
    },
    "mindformers.experimental.graph.tensor_parallel.layers.VocabParallelEmbedding": {
        "signature": "(num_embeddings, embedding_dim, parallel_config, init_method: Callable, init_type=mindspore.float32)"
    },
    "mindformers.experimental.graph.tensor_parallel.layers.VocabParallelEmbedding.construct": {
        "signature": "(self, input_ids)"
    },
    "mindformers.experimental.graph.tensor_parallel.layers.VocabParallelEmbedding.shard": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig)"
    },
    "mindformers.experimental.graph.tensor_parallel.lora_layers.LoRAColumnParallelLinear": {
        "signature": "(input_size: int, output_size: int, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig, init_method: Callable = None, bias: bool = True, gather_output: bool = False, stride: int = 1, keep_master_weight_for_test: bool = False, skip_bias_add: bool = False, skip_weight_param_allocation: bool = False, embedding_activation_buffer: Optional[List[mindspore.common.tensor.Tensor]] = None, grad_output_buffer: Optional[List[mindspore.common.tensor.Tensor]] = None, is_expert: bool = False, tp_comm_buffer_name: str = None, disable_grad_reduce: bool = False, transpose_b: bool = True, compute_dtype: mindspore.common.dtype = mindspore.float16, bias_init: Callable = None, lora_rank: int = 8, lora_alpha: int = 32, lora_dropout: float = 0.0, lora_a_init='normal', lora_b_init='zeros')"
    },
    "mindformers.experimental.graph.tensor_parallel.lora_layers.LoRAColumnParallelLinear.construct": {
        "signature": "(self, input_: mindspore.common.tensor.Tensor, weight: mindspore.common.tensor.Tensor = None) -> tuple[mindspore.common.tensor.Tensor, mindspore.common.tensor.Tensor]"
    },
    "mindformers.experimental.graph.tensor_parallel.lora_layers.LoRAColumnParallelLinear.shard": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig) -> None"
    },
    "mindformers.experimental.graph.tensor_parallel.lora_layers.LoRARowParallelLinear": {
        "signature": "(input_size: int, output_size: int, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig, init_method: Callable = None, bias: bool = True, input_is_parallel: bool = False, skip_bias_add: bool = False, stride: int = 1, keep_master_weight_for_test: bool = False, is_expert: bool = False, tp_comm_buffer_name: str = None, transpose_b: bool = True, compute_dtype: mindspore.common.dtype = mindspore.float16, bias_init: Callable = None, lora_rank: int = 8, lora_alpha: int = 32, lora_dropout: float = 0.0, lora_a_init='normal', lora_b_init='zeros')"
    },
    "mindformers.experimental.graph.tensor_parallel.lora_layers.LoRARowParallelLinear.construct": {
        "signature": "(self, input_: mindspore.common.tensor.Tensor) -> tuple[mindspore.common.tensor.Tensor, mindspore.common.tensor.Tensor]"
    },
    "mindformers.experimental.graph.tensor_parallel.lora_layers.LoRARowParallelLinear.shard": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig) -> None"
    },
    "mindformers.experimental.graph.transformer.ApplyRotaryPosEmb": {
        "signature": "(config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig)"
    },
    "mindformers.experimental.graph.transformer.ApplyRotaryPosEmb.construct": {
        "signature": "(self, t: mindspore.common.tensor.Tensor, freqs: mindspore.common.tensor.Tensor, rotary_interleaved: bool = False) -> mindspore.common.tensor.Tensor"
    },
    "mindformers.experimental.graph.transformer.ApplyRotaryPosEmb._rotate_half": {
        "signature": "(self, t: mindspore.common.tensor.Tensor, rotary_interleaved: bool = False) -> mindspore.common.tensor.Tensor"
    },
    "mindformers.experimental.graph.transformer.ApplyRotaryPosEmb.shard": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig)"
    },
    "mindformers.experimental.graph.transformer.ApplyRotaryPosEmb.sharding_propagation": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig)"
    },
    "mindformers.experimental.graph.transformer.AttnMaskType": {
        "signature": "(value, names=None, *, module=None, qualname=None, type=None, start=1)"
    },
    "mindformers.experimental.graph.transformer.Dropout": {
        "signature": "(drop_prob=0.5, dtype=mindspore.float32)"
    },
    "mindformers.experimental.graph.transformer.Dropout.construct": {
        "signature": "(self, x)"
    },
    "mindformers.experimental.graph.transformer.Dropout.extend_repr": {
        "signature": "(self)"
    },
    "mindformers.experimental.graph.transformer.Dropout.shard": {
        "signature": "(self, strategy)"
    },
    "mindformers.experimental.graph.transformer.FusedScaleMaskSoftmax": {
        "signature": "(input_in_fp16: bool = False, input_in_bf16: bool = False, attn_mask_type: mindformers.experimental.graph.transformer.enums.AttnMaskType = <AttnMaskType.causal: 2>, scaled_masked_softmax_fusion: bool = False, mask_func: Callable = None, softmax_in_fp32: bool = True, scale: float = None, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig = None, softmax_compute_dtype: mindspore.common.dtype = None)"
    },
    "mindformers.experimental.graph.transformer.FusedScaleMaskSoftmax.construct": {
        "signature": "(self, input_: mindspore.common.tensor.Tensor, mask: mindspore.common.tensor.Tensor = None) -> mindspore.common.tensor.Tensor"
    },
    "mindformers.experimental.graph.transformer.FusedScaleMaskSoftmax.shard": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig)"
    },
    "mindformers.experimental.graph.transformer.FusedScaleMaskSoftmax.sharding_propagation": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig)"
    },
    "mindformers.experimental.graph.transformer.RotaryEmbedding": {
        "signature": "(kv_channels: int, rotary_percent: float = 1.0, rotary_interleaved: bool = False, seq_len_interpolation_factor: float = None, rotary_base: int = 10000)"
    },
    "mindformers.experimental.graph.transformer.RotaryEmbedding.construct": {
        "signature": "(self, max_seq_len: int, offset: int = 0) -> mindspore.common.tensor.Tensor"
    },
    "mindformers.experimental.graph.transformer.apply_rotary_pos_emb": {
        "signature": "(t: mindspore.common.tensor.Tensor, freqs: mindspore.common.tensor.Tensor, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig, cu_seqlens: mindspore.common.tensor.Tensor = None) -> mindspore.common.tensor.Tensor"
    },
    "mindformers.experimental.graph.transformer.dropout.Dropout": {
        "signature": "(drop_prob=0.5, dtype=mindspore.float32)"
    },
    "mindformers.experimental.graph.transformer.dropout.Dropout.construct": {
        "signature": "(self, x)"
    },
    "mindformers.experimental.graph.transformer.dropout.Dropout.extend_repr": {
        "signature": "(self)"
    },
    "mindformers.experimental.graph.transformer.dropout.Dropout.shard": {
        "signature": "(self, strategy)"
    },
    "mindformers.experimental.graph.transformer.enums.AttnMaskType": {
        "signature": "(value, names=None, *, module=None, qualname=None, type=None, start=1)"
    },
    "mindformers.experimental.graph.transformer.fused_softmax.FusedScaleMaskSoftmax": {
        "signature": "(input_in_fp16: bool = False, input_in_bf16: bool = False, attn_mask_type: mindformers.experimental.graph.transformer.enums.AttnMaskType = <AttnMaskType.causal: 2>, scaled_masked_softmax_fusion: bool = False, mask_func: Callable = None, softmax_in_fp32: bool = True, scale: float = None, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig = None, softmax_compute_dtype: mindspore.common.dtype = None)"
    },
    "mindformers.experimental.graph.transformer.fused_softmax.FusedScaleMaskSoftmax.construct": {
        "signature": "(self, input_: mindspore.common.tensor.Tensor, mask: mindspore.common.tensor.Tensor = None) -> mindspore.common.tensor.Tensor"
    },
    "mindformers.experimental.graph.transformer.fused_softmax.FusedScaleMaskSoftmax.shard": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig)"
    },
    "mindformers.experimental.graph.transformer.fused_softmax.FusedScaleMaskSoftmax.sharding_propagation": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig)"
    },
    "mindformers.experimental.graph.transformer.language_model.Embedding": {
        "signature": "(hidden_size, vocab_size, max_sequence_length, embedding_dropout_prob, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig, num_tokentypes=0)"
    },
    "mindformers.experimental.graph.transformer.language_model.Embedding.construct": {
        "signature": "(self, input_ids, position_ids, tokentype_ids=None)"
    },
    "mindformers.experimental.graph.transformer.language_model.Embedding.shard": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig)"
    },
    "mindformers.experimental.graph.transformer.language_model.Embedding.sharding_propagation": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig)"
    },
    "mindformers.experimental.graph.transformer.language_model.Pooler": {
        "signature": "(hidden_size, init_method, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig)"
    },
    "mindformers.experimental.graph.transformer.language_model.Pooler.construct": {
        "signature": "(self, hidden_states, sequence_index=0)"
    },
    "mindformers.experimental.graph.transformer.language_model.TransformerLanguageModel": {
        "signature": "(config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig, encoder_attn_mask_type, num_tokentypes=0, add_encoder=True, add_decoder=False, decoder_attn_mask_type=None, add_pooler=False, pre_process=True, post_process=True)"
    },
    "mindformers.experimental.graph.transformer.language_model.TransformerLanguageModel.construct": {
        "signature": "(self, enc_input_ids, enc_position_ids, enc_attn_mask, dec_input_ids=None, dec_position_ids=None, dec_attn_mask=None, retriever_input_ids=None, retriever_position_ids=None, retriever_attn_mask=None, enc_dec_attn_mask=None, tokentype_ids=None, inference_params=None, pooling_sequence_index=0, enc_hidden_states=None, output_enc_hidden=False, prefix_keys_values=None)"
    },
    "mindformers.experimental.graph.transformer.language_model.TransformerLanguageModel._check_inputs": {
        "signature": "(self, dec_input_ids, dec_position_ids, dec_attn_mask, retriever_input_ids, retriever_position_ids, retriever_attn_mask, enc_dec_attn_mask, inference_params, output_enc_hidden)"
    },
    "mindformers.experimental.graph.transformer.language_model.TransformerLanguageModel.shard": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig)"
    },
    "mindformers.experimental.graph.transformer.language_model.TransformerLanguageModel.sharding_propagation": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig)"
    },
    "mindformers.experimental.graph.transformer.language_model.get_language_model": {
        "signature": "(config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig, num_tokentypes: int, add_pooler: bool, encoder_attn_mask_type: Optional[mindformers.experimental.graph.transformer.enums.AttnMaskType], add_encoder: bool = True, add_decoder: bool = False, decoder_attn_mask_type: Optional[mindformers.experimental.graph.transformer.enums.AttnMaskType] = <AttnMaskType.causal: 2>, pre_process: bool = True, post_process: bool = True) -> (<class 'mindformers.experimental.graph.transformer.language_model.TransformerLanguageModel'>, <class 'str'>)"
    },
    "mindformers.experimental.graph.transformer.norm.get_norm": {
        "signature": "(config)"
    },
    "mindformers.experimental.graph.transformer.rotary_pos_embedding.ApplyRotaryPosEmb": {
        "signature": "(config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig)"
    },
    "mindformers.experimental.graph.transformer.rotary_pos_embedding.ApplyRotaryPosEmb.construct": {
        "signature": "(self, t: mindspore.common.tensor.Tensor, freqs: mindspore.common.tensor.Tensor, rotary_interleaved: bool = False) -> mindspore.common.tensor.Tensor"
    },
    "mindformers.experimental.graph.transformer.rotary_pos_embedding.ApplyRotaryPosEmb._rotate_half": {
        "signature": "(self, t: mindspore.common.tensor.Tensor, rotary_interleaved: bool = False) -> mindspore.common.tensor.Tensor"
    },
    "mindformers.experimental.graph.transformer.rotary_pos_embedding.ApplyRotaryPosEmb.shard": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig)"
    },
    "mindformers.experimental.graph.transformer.rotary_pos_embedding.ApplyRotaryPosEmb.sharding_propagation": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig)"
    },
    "mindformers.experimental.graph.transformer.rotary_pos_embedding.RotaryEmbedding": {
        "signature": "(kv_channels: int, rotary_percent: float = 1.0, rotary_interleaved: bool = False, seq_len_interpolation_factor: float = None, rotary_base: int = 10000)"
    },
    "mindformers.experimental.graph.transformer.rotary_pos_embedding.RotaryEmbedding.construct": {
        "signature": "(self, max_seq_len: int, offset: int = 0) -> mindspore.common.tensor.Tensor"
    },
    "mindformers.experimental.graph.transformer.rotary_pos_embedding.apply_rotary_pos_emb": {
        "signature": "(t: mindspore.common.tensor.Tensor, freqs: mindspore.common.tensor.Tensor, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig, cu_seqlens: mindspore.common.tensor.Tensor = None) -> mindspore.common.tensor.Tensor"
    },
    "mindformers.experimental.graph.transformer.transformer.CausalMaskGenerate": {
        "signature": "(seq_length: int, compute_type: mindspore.common.dtype = mindspore.float16, is_dynamic: bool = False, pad_token_id: int = 0, use_flash_attention: bool = False, use_prompt_flash_attention: bool = False, use_incre_flash_attention: bool = False, use_attn_mask_compression: bool = False, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig = None)"
    },
    "mindformers.experimental.graph.transformer.transformer.CausalMaskGenerate.construct": {
        "signature": "(self, tokens=None, masks=None)"
    },
    "mindformers.experimental.graph.transformer.transformer.CausalMaskGenerate.shard": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig)"
    },
    "mindformers.experimental.graph.transformer.transformer.CausalMaskGenerate.sharding_propagation": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig)"
    },
    "mindformers.experimental.graph.transformer.transformer.CoreAttention": {
        "signature": "(layer_number, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig, attn_mask_type=None)"
    },
    "mindformers.experimental.graph.transformer.transformer.CoreAttention.construct": {
        "signature": "(self, query_layer, key_layer, value_layer, attention_mask)"
    },
    "mindformers.experimental.graph.transformer.transformer.CoreAttention._merge_heads": {
        "signature": "(self, x)"
    },
    "mindformers.experimental.graph.transformer.transformer.CoreAttention.shard": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig)"
    },
    "mindformers.experimental.graph.transformer.transformer.CoreAttention.sharding_propagation": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig)"
    },
    "mindformers.experimental.graph.transformer.transformer.ParallelAttention": {
        "signature": "(config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig, layer_number, attention_type='self_attn', attn_mask_type=None)"
    },
    "mindformers.experimental.graph.transformer.transformer.ParallelAttention._self_attn": {
        "signature": "(self)"
    },
    "mindformers.experimental.graph.transformer.transformer.ParallelAttention._ulysses_initial": {
        "signature": "(self)"
    },
    "mindformers.experimental.graph.transformer.transformer.ParallelAttention._cross_attn_init": {
        "signature": "(self)"
    },
    "mindformers.experimental.graph.transformer.transformer.ParallelAttention.construct": {
        "signature": "(self, hidden_states, attention_mask, encoder_output=None, inference_params=None, rotary_pos_emb=None, prefix_keys_values=None)"
    },
    "mindformers.experimental.graph.transformer.transformer.ParallelAttention._cat_prefix": {
        "signature": "(self, key, value, prefix_keys_values)"
    },
    "mindformers.experimental.graph.transformer.transformer.ParallelAttention._merge_heads": {
        "signature": "(self, x)"
    },
    "mindformers.experimental.graph.transformer.transformer.ParallelAttention._repeat_kv": {
        "signature": "(self, x, rep)"
    },
    "mindformers.experimental.graph.transformer.transformer.ParallelAttention._ulysses_q_a2a": {
        "signature": "(self, qkv)"
    },
    "mindformers.experimental.graph.transformer.transformer.ParallelAttention._ulysses_kv_a2a": {
        "signature": "(self, qkv)"
    },
    "mindformers.experimental.graph.transformer.transformer.ParallelAttention._ulysses_context_layer_a2a": {
        "signature": "(self, context_layer)"
    },
    "mindformers.experimental.graph.transformer.transformer.ParallelAttention.shard": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig)"
    },
    "mindformers.experimental.graph.transformer.transformer.ParallelLMLogits": {
        "signature": "(config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig, bias: bool = True, compute_dtype: mindspore.common.dtype = None)"
    },
    "mindformers.experimental.graph.transformer.transformer.ParallelLMLogits.construct": {
        "signature": "(self, logits: mindspore.common.tensor.Tensor, word_embedding_weight: mindspore.common.tensor.Tensor, parallel_output: bool = True, bias: mindspore.common.tensor.Tensor = None)"
    },
    "mindformers.experimental.graph.transformer.transformer.ParallelLMLogits.shard": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig) -> None"
    },
    "mindformers.experimental.graph.transformer.transformer.ParallelMLP": {
        "signature": "(config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig, is_expert: bool = False)"
    },
    "mindformers.experimental.graph.transformer.transformer.ParallelMLP.construct": {
        "signature": "(self, hidden_states: mindspore.common.tensor.Tensor) -> tuple[mindspore.common.tensor.Tensor, mindspore.common.tensor.Tensor]"
    },
    "mindformers.experimental.graph.transformer.transformer.ParallelMLP.shard": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig)"
    },
    "mindformers.experimental.graph.transformer.transformer.ParallelMLP.sharding_propagation": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig)"
    },
    "mindformers.experimental.graph.transformer.transformer.ParallelTransformer": {
        "signature": "(config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig, model_type=None, layer_type=None, self_attn_mask_type=None, post_norm: bool = True, pre_process=False, post_process=False, drop_path_rate: float = 0.0)"
    },
    "mindformers.experimental.graph.transformer.transformer.ParallelTransformer._get_layer": {
        "signature": "(self, layer_number)"
    },
    "mindformers.experimental.graph.transformer.transformer.ParallelTransformer.construct": {
        "signature": "(self, hidden_states: mindspore.common.tensor.Tensor, attention_mask: mindspore.common.tensor.Tensor, rotary_pos_emb: mindspore.common.tensor.Tensor = None, prefix_keys_values=None)"
    },
    "mindformers.experimental.graph.transformer.transformer.ParallelTransformer.shard": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig)"
    },
    "mindformers.experimental.graph.transformer.transformer.ParallelTransformerLayer": {
        "signature": "(config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig, layer_number: int, self_attn_mask_type=None, drop_path_rate: float = 0.0, layer_type=None)"
    },
    "mindformers.experimental.graph.transformer.transformer.ParallelTransformerLayer.construct": {
        "signature": "(self, hidden_states: mindspore.common.tensor.Tensor, attention_mask: mindspore.common.tensor.Tensor, encoder_output=None, enc_dec_attn_mask=None, retriever_input=None, retriever_output=None, retriever_attn_mask=None, inference_params=None, rotary_pos_emb: mindspore.common.tensor.Tensor = None, prefix_keys_values: mindspore.common.tensor.Tensor = None)"
    },
    "mindformers.experimental.graph.transformer.transformer.ParallelTransformerLayer.shard": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig)"
    },
    "mindformers.experimental.graph.transformer.transformer.ParallelTransformerLayer.sharding_propagation": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig)"
    },
    "mindformers.experimental.graph.transformer.utils.get_attn_mask_func": {
        "signature": "(mask_func_type)"
    },
    "mindformers.experimental.infer.core.ColumnParallelLinear": {
        "signature": "(input_size, output_size, config, weight_init='normal', bias_init='zeros', bias=True, gather_output=False, stride=1, keep_master_weight_for_test=False, skip_bias_add=False, skip_weight_param_allocation=False, embedding_activation_buffer=None, grad_output_buffer=None, is_expert=False, tp_comm_buffer_name=None, disable_grad_reduce=False, transpose_b=True, param_init_type=mindspore.float32, compute_dtype=mindspore.float16, expert_num=1)"
    },
    "mindformers.experimental.infer.core.ColumnParallelLinear.construct": {
        "signature": "(self, input_parallel, weight=None, group_list=None)"
    },
    "mindformers.experimental.infer.core.ColumnParallelLinear.sharded_state_dict": {
        "signature": "(self)"
    },
    "mindformers.experimental.infer.core.ParallelAttention": {
        "signature": "(config, layer_number, attention_type='self_attn', attn_mask_type=None)"
    },
    "mindformers.experimental.infer.core.ParallelAttention.construct": {
        "signature": "(self, x, batch_valid_length, block_tables, slot_mapping, freqs_cis=None, attn_mask=None, alibi_mask=None, prefix_keys_values=None, encoder_output=None, key_cache=None, value_cache=None)"
    },
    "mindformers.experimental.infer.core.ParallelAttention._cat_prefix": {
        "signature": "(self, key, value, prefix_keys_values)"
    },
    "mindformers.experimental.infer.core.ParallelAttention._check_gqa_valid": {
        "signature": "(self)"
    },
    "mindformers.experimental.infer.core.ParallelAttention._init_self_attn": {
        "signature": "(self)"
    },
    "mindformers.experimental.infer.core.ParallelAttention._init_cross_attn": {
        "signature": "(self)"
    },
    "mindformers.experimental.infer.core.ParallelMLP": {
        "signature": "(config, is_expert=False)"
    },
    "mindformers.experimental.infer.core.ParallelMLP.construct": {
        "signature": "(self, x)"
    },
    "mindformers.experimental.infer.core.ParallelTransformer": {
        "signature": "(config, model_type=None, layer_type=None, self_attn_mask_type=None, post_norm: bool = True, pre_process=False, post_process=False, drop_path_rate: float = 0.0)"
    },
    "mindformers.experimental.infer.core.ParallelTransformer.construct": {
        "signature": "(self, tokens: mindspore.common.tensor.Tensor, batch_valid_length=None, batch_index=None, zactivate_len=None, block_tables=None, slot_mapping=None, prefix_keys_values=None, key_cache=None, value_cache=None)"
    },
    "mindformers.experimental.infer.core.ParallelTransformerLayer": {
        "signature": "(config, layer_number: int, layer_type=None, self_attn_mask_type=None, drop_path_rate: float = 0.0)"
    },
    "mindformers.experimental.infer.core.ParallelTransformerLayer.construct": {
        "signature": "(self, x, freqs_cis=None, mask=None, batch_valid_length=None, block_tables=None, slot_mapping=None, prefix_keys_values=None, key_cache=None, value_cache=None)"
    },
    "mindformers.experimental.infer.core.RowParallelLinear": {
        "signature": "(input_size, output_size, config, input_is_parallel, weight_init='normal', bias_init='zeros', bias=True, skip_bias_add=False, stride=1, keep_master_weight_for_test=False, is_expert=False, tp_comm_buffer_name=None, transpose_b=True, param_init_type=mindspore.float32, compute_dtype=mindspore.float16, expert_num=1, moe_delay_allreduce=False)"
    },
    "mindformers.experimental.infer.core.RowParallelLinear.construct": {
        "signature": "(self, input_, group_list=None)"
    },
    "mindformers.experimental.infer.core.RowParallelLinear.sharded_state_dict": {
        "signature": "(self)"
    },
    "mindformers.experimental.infer.core.VocabParallelEmbedding": {
        "signature": "(num_embeddings, embedding_dim, parallel_config, init_method='normal', init_type=mindspore.float32)"
    },
    "mindformers.experimental.infer.core.VocabParallelEmbedding.construct": {
        "signature": "(self, x)"
    },
    "mindformers.experimental.infer.core.VocabParallelEmbedding._vocab_range_from_global_vocab_size": {
        "signature": "(self, global_vocab_size, rank, world_size)"
    },
    "mindformers.experimental.infer.core.VocabParallelEmbedding.sharded_state_dict": {
        "signature": "(self)"
    },
    "mindformers.experimental.infer.core.get_act_func": {
        "signature": "(activation_type, *args, **kwargs)"
    },
    "mindformers.experimental.infer.core.get_attn_mask_func": {
        "signature": "(mask_func_type)"
    },
    "mindformers.experimental.infer.core.get_norm": {
        "signature": "(config)"
    },
    "mindformers.experimental.infer.core.activation.get_act_func": {
        "signature": "(activation_type, *args, **kwargs)"
    },
    "mindformers.experimental.infer.core.layers.ColumnParallelLinear": {
        "signature": "(input_size, output_size, config, weight_init='normal', bias_init='zeros', bias=True, gather_output=False, stride=1, keep_master_weight_for_test=False, skip_bias_add=False, skip_weight_param_allocation=False, embedding_activation_buffer=None, grad_output_buffer=None, is_expert=False, tp_comm_buffer_name=None, disable_grad_reduce=False, transpose_b=True, param_init_type=mindspore.float32, compute_dtype=mindspore.float16, expert_num=1)"
    },
    "mindformers.experimental.infer.core.layers.ColumnParallelLinear.construct": {
        "signature": "(self, input_parallel, weight=None, group_list=None)"
    },
    "mindformers.experimental.infer.core.layers.ColumnParallelLinear.sharded_state_dict": {
        "signature": "(self)"
    },
    "mindformers.experimental.infer.core.layers.RowParallelLinear": {
        "signature": "(input_size, output_size, config, input_is_parallel, weight_init='normal', bias_init='zeros', bias=True, skip_bias_add=False, stride=1, keep_master_weight_for_test=False, is_expert=False, tp_comm_buffer_name=None, transpose_b=True, param_init_type=mindspore.float32, compute_dtype=mindspore.float16, expert_num=1, moe_delay_allreduce=False)"
    },
    "mindformers.experimental.infer.core.layers.RowParallelLinear.construct": {
        "signature": "(self, input_, group_list=None)"
    },
    "mindformers.experimental.infer.core.layers.RowParallelLinear.sharded_state_dict": {
        "signature": "(self)"
    },
    "mindformers.experimental.infer.core.layers.VocabParallelEmbedding": {
        "signature": "(num_embeddings, embedding_dim, parallel_config, init_method='normal', init_type=mindspore.float32)"
    },
    "mindformers.experimental.infer.core.layers.VocabParallelEmbedding.construct": {
        "signature": "(self, x)"
    },
    "mindformers.experimental.infer.core.layers.VocabParallelEmbedding._vocab_range_from_global_vocab_size": {
        "signature": "(self, global_vocab_size, rank, world_size)"
    },
    "mindformers.experimental.infer.core.layers.VocabParallelEmbedding.sharded_state_dict": {
        "signature": "(self)"
    },
    "mindformers.experimental.infer.core.norm.get_norm": {
        "signature": "(config)"
    },
    "mindformers.experimental.infer.core.transformer.ParallelAttention": {
        "signature": "(config, layer_number, attention_type='self_attn', attn_mask_type=None)"
    },
    "mindformers.experimental.infer.core.transformer.ParallelAttention.construct": {
        "signature": "(self, x, batch_valid_length, block_tables, slot_mapping, freqs_cis=None, attn_mask=None, alibi_mask=None, prefix_keys_values=None, encoder_output=None, key_cache=None, value_cache=None)"
    },
    "mindformers.experimental.infer.core.transformer.ParallelAttention._cat_prefix": {
        "signature": "(self, key, value, prefix_keys_values)"
    },
    "mindformers.experimental.infer.core.transformer.ParallelAttention._check_gqa_valid": {
        "signature": "(self)"
    },
    "mindformers.experimental.infer.core.transformer.ParallelAttention._init_self_attn": {
        "signature": "(self)"
    },
    "mindformers.experimental.infer.core.transformer.ParallelAttention._init_cross_attn": {
        "signature": "(self)"
    },
    "mindformers.experimental.infer.core.transformer.ParallelMLP": {
        "signature": "(config, is_expert=False)"
    },
    "mindformers.experimental.infer.core.transformer.ParallelMLP.construct": {
        "signature": "(self, x)"
    },
    "mindformers.experimental.infer.core.transformer.ParallelTransformer": {
        "signature": "(config, model_type=None, layer_type=None, self_attn_mask_type=None, post_norm: bool = True, pre_process=False, post_process=False, drop_path_rate: float = 0.0)"
    },
    "mindformers.experimental.infer.core.transformer.ParallelTransformer.construct": {
        "signature": "(self, tokens: mindspore.common.tensor.Tensor, batch_valid_length=None, batch_index=None, zactivate_len=None, block_tables=None, slot_mapping=None, prefix_keys_values=None, key_cache=None, value_cache=None)"
    },
    "mindformers.experimental.infer.core.transformer.ParallelTransformerLayer": {
        "signature": "(config, layer_number: int, layer_type=None, self_attn_mask_type=None, drop_path_rate: float = 0.0)"
    },
    "mindformers.experimental.infer.core.transformer.ParallelTransformerLayer.construct": {
        "signature": "(self, x, freqs_cis=None, mask=None, batch_valid_length=None, block_tables=None, slot_mapping=None, prefix_keys_values=None, key_cache=None, value_cache=None)"
    },
    "mindformers.experimental.infer.core.utils.generate_state_dict": {
        "signature": "(network)"
    },
    "mindformers.experimental.infer.core.utils.get_attn_mask_func": {
        "signature": "(mask_func_type)"
    },
    "mindformers.experimental.infer.models.llama.ParallelLlamaForCausalLM": {
        "signature": "(config)"
    },
    "mindformers.experimental.infer.models.llama.ParallelLlamaForCausalLM.prepare_inputs_for_predict_layout": {
        "signature": "(self, input_ids, **kwargs)"
    },
    "mindformers.experimental.infer.models.llama.ParallelLlamaForCausalLM.set_dynamic_inputs": {
        "signature": "(self, **kwargs)"
    },
    "mindformers.experimental.infer.models.llama.ParallelLlamaForCausalLM.add_flags_custom": {
        "signature": "(self, is_first_iteration)"
    },
    "mindformers.experimental.infer.models.llama.ParallelLlamaForCausalLM.construct": {
        "signature": "(self, input_ids, labels=None, input_position=None, position_ids=None, attention_mask=None, input_embeds=None, init_reset=None, batch_valid_length=None, batch_index=None, zactivate_len=None, block_tables=None, slot_mapping=None, prefix_keys_values=None, llm_boost_inputs=None, key_cache=None, value_cache=None)"
    },
    "mindformers.experimental.infer.models.llama.ParallelLlamaForCausalLM.kvcache": {
        "signature": "(self, layer_idx)"
    },
    "mindformers.experimental.infer.models.llama.ParallelLlamaForCausalLM.convert_name": {
        "signature": "(weight_name)"
    },
    "mindformers.experimental.infer.models.llama.ParallelLlamaForCausalLM.convert_weight_dict": {
        "signature": "(source_dict, **kwargs)"
    },
    "mindformers.experimental.infer.models.llama.ParallelLlamaForCausalLM.convert_map_dict": {
        "signature": "(source_dict, **kwargs)"
    },
    "mindformers.experimental.infer.models.llama.ParallelLlamaForCausalLM.obtain_qkv_ffn_concat_keys": {
        "signature": "()"
    },
    "mindformers.experimental.infer.models.llama.ParallelLlamaForCausalLM.clear_kv_cache": {
        "signature": "(self)"
    },
    "mindformers.experimental.infer.models.llama.llama.ParallelLlamaForCausalLM": {
        "signature": "(config)"
    },
    "mindformers.experimental.infer.models.llama.llama.ParallelLlamaForCausalLM.prepare_inputs_for_predict_layout": {
        "signature": "(self, input_ids, **kwargs)"
    },
    "mindformers.experimental.infer.models.llama.llama.ParallelLlamaForCausalLM.set_dynamic_inputs": {
        "signature": "(self, **kwargs)"
    },
    "mindformers.experimental.infer.models.llama.llama.ParallelLlamaForCausalLM.add_flags_custom": {
        "signature": "(self, is_first_iteration)"
    },
    "mindformers.experimental.infer.models.llama.llama.ParallelLlamaForCausalLM.construct": {
        "signature": "(self, input_ids, labels=None, input_position=None, position_ids=None, attention_mask=None, input_embeds=None, init_reset=None, batch_valid_length=None, batch_index=None, zactivate_len=None, block_tables=None, slot_mapping=None, prefix_keys_values=None, llm_boost_inputs=None, key_cache=None, value_cache=None)"
    },
    "mindformers.experimental.infer.models.llama.llama.ParallelLlamaForCausalLM.kvcache": {
        "signature": "(self, layer_idx)"
    },
    "mindformers.experimental.infer.models.llama.llama.ParallelLlamaForCausalLM.convert_name": {
        "signature": "(weight_name)"
    },
    "mindformers.experimental.infer.models.llama.llama.ParallelLlamaForCausalLM.convert_weight_dict": {
        "signature": "(source_dict, **kwargs)"
    },
    "mindformers.experimental.infer.models.llama.llama.ParallelLlamaForCausalLM.convert_map_dict": {
        "signature": "(source_dict, **kwargs)"
    },
    "mindformers.experimental.infer.models.llama.llama.ParallelLlamaForCausalLM.obtain_qkv_ffn_concat_keys": {
        "signature": "()"
    },
    "mindformers.experimental.infer.models.llama.llama.ParallelLlamaForCausalLM.clear_kv_cache": {
        "signature": "(self)"
    },
    "mindformers.experimental.model.LlamaForCausalLM": {
        "signature": "(config: mindformers.models.llama.llama_config.LlamaConfig, num_tokentypes: int = 0, parallel_output: bool = True, pre_process: bool = True, post_process: bool = True, **kwargs)"
    },
    "mindformers.experimental.model.LlamaForCausalLM.construct": {
        "signature": "(self, input_ids: mindspore.common.tensor.Tensor, position_ids: mindspore.common.tensor.Tensor = None, attention_mask: mindspore.common.tensor.Tensor = None, retriever_input_ids: mindspore.common.tensor.Tensor = None, retriever_position_ids: mindspore.common.tensor.Tensor = None, retriever_attn_mask: mindspore.common.tensor.Tensor = None, labels: mindspore.common.tensor.Tensor = None, tokentype_ids=None, inference_params=None, prefix_keys_values=None, input_embeds: mindspore.common.tensor.Tensor = None, loss_mask: mindspore.common.tensor.Tensor = None) -> mindspore.common.tensor.Tensor"
    },
    "mindformers.experimental.model.LlamaForCausalLM.post_language_model_processing": {
        "signature": "(self, lm_output: mindspore.common.tensor.Tensor, labels: mindspore.common.tensor.Tensor, logit_weights: mindspore.common.tensor.Tensor, parallel_output: bool, fp16_lm_cross_entropy: bool, loss_mask: mindspore.common.tensor.Tensor) -> mindspore.common.tensor.Tensor"
    },
    "mindformers.experimental.model.LlamaForCausalLM.set_input_tensor": {
        "signature": "(self, input_tensor)"
    },
    "mindformers.experimental.model.LlamaForCausalLM.set_dynamic_inputs": {
        "signature": "(self, **kwargs)"
    },
    "mindformers.experimental.model.LlamaForCausalLM._preprocess_input_labels_and_masks": {
        "signature": "(self, input_ids: mindspore.common.tensor.Tensor, labels: mindspore.common.tensor.Tensor = None, attention_mask: mindspore.common.tensor.Tensor = None) -> (<class 'mindspore.common.tensor.Tensor'>, <class 'mindspore.common.tensor.Tensor'>, <class 'mindspore.common.tensor.Tensor'>, <class 'mindspore.common.tensor.Tensor'>)"
    },
    "mindformers.experimental.model.LlamaForCausalLM.shard": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig)"
    },
    "mindformers.experimental.model.LlamaForCausalLM.sharding_propagation": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig)"
    },
    "mindformers.experimental.model.llama.LlamaForCausalLM": {
        "signature": "(config: mindformers.models.llama.llama_config.LlamaConfig, num_tokentypes: int = 0, parallel_output: bool = True, pre_process: bool = True, post_process: bool = True, **kwargs)"
    },
    "mindformers.experimental.model.llama.LlamaForCausalLM.construct": {
        "signature": "(self, input_ids: mindspore.common.tensor.Tensor, position_ids: mindspore.common.tensor.Tensor = None, attention_mask: mindspore.common.tensor.Tensor = None, retriever_input_ids: mindspore.common.tensor.Tensor = None, retriever_position_ids: mindspore.common.tensor.Tensor = None, retriever_attn_mask: mindspore.common.tensor.Tensor = None, labels: mindspore.common.tensor.Tensor = None, tokentype_ids=None, inference_params=None, prefix_keys_values=None, input_embeds: mindspore.common.tensor.Tensor = None, loss_mask: mindspore.common.tensor.Tensor = None) -> mindspore.common.tensor.Tensor"
    },
    "mindformers.experimental.model.llama.LlamaForCausalLM.post_language_model_processing": {
        "signature": "(self, lm_output: mindspore.common.tensor.Tensor, labels: mindspore.common.tensor.Tensor, logit_weights: mindspore.common.tensor.Tensor, parallel_output: bool, fp16_lm_cross_entropy: bool, loss_mask: mindspore.common.tensor.Tensor) -> mindspore.common.tensor.Tensor"
    },
    "mindformers.experimental.model.llama.LlamaForCausalLM.set_input_tensor": {
        "signature": "(self, input_tensor)"
    },
    "mindformers.experimental.model.llama.LlamaForCausalLM.set_dynamic_inputs": {
        "signature": "(self, **kwargs)"
    },
    "mindformers.experimental.model.llama.LlamaForCausalLM._preprocess_input_labels_and_masks": {
        "signature": "(self, input_ids: mindspore.common.tensor.Tensor, labels: mindspore.common.tensor.Tensor = None, attention_mask: mindspore.common.tensor.Tensor = None) -> (<class 'mindspore.common.tensor.Tensor'>, <class 'mindspore.common.tensor.Tensor'>, <class 'mindspore.common.tensor.Tensor'>, <class 'mindspore.common.tensor.Tensor'>)"
    },
    "mindformers.experimental.model.llama.LlamaForCausalLM.shard": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig)"
    },
    "mindformers.experimental.model.llama.LlamaForCausalLM.sharding_propagation": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig)"
    },
    "mindformers.experimental.model.llama.LlamaPretrainedModel": {
        "signature": "(config: mindformers.models.configuration_utils.PretrainedConfig, *inputs, **kwargs)"
    },
    "mindformers.experimental.model.llama.LlamaPretrainedModel.config_class": {
        "signature": "(batch_size: int = 1, seq_length: int = 2048, hidden_size: int = 4096, num_layers: int = 32, num_heads: int = 32, n_kv_heads: Optional[int] = None, max_position_embedding: Optional[int] = None, intermediate_size: Optional[int] = None, vocab_size: int = 32000, multiple_of: int = 256, ffn_dim_multiplier: Optional[int] = None, rms_norm_eps: float = 1e-05, bos_token_id: int = 1, eos_token_id: int = 2, pad_token_id: int = 0, ignore_token_id: int = -100, theta: float = 10000.0, compute_dtype: str = 'float16', layernorm_compute_type: str = 'float32', softmax_compute_type: str = 'float32', rotary_dtype: str = 'float32', param_init_type: str = 'float16', residual_dtype: str = None, embedding_init_type=None, qkv_has_bias: bool = False, qkv_concat: bool = False, attn_proj_has_bias: bool = False, parallel_config: Union[dict, mindformers.modules.transformer.transformer.TransformerOpParallelConfig] = <mindformers.modules.transformer.transformer.TransformerOpParallelConfig object>, moe_config: Union[dict, mindformers.modules.transformer.moe.MoEConfig] = <mindformers.modules.transformer.moe.MoEConfig object>, use_past: bool = False, extend_method: str = 'None', scaling_factor: float = 1.0, is_dynamic: bool = False, use_rope_slice: bool = False, use_flash_attention: bool = False, use_ring_attention: bool = False, use_attn_mask_compression: bool = False, use_eod_attn_mask_compression: bool = False, parallel_optimizer: bool = False, fine_grain_interleave: int = 1, pp_interleave_num: int = 1, offset: int = 0, init_method_std: float = 0.01, checkpoint_name_or_path: str = '', repetition_penalty: float = 1.0, max_decode_length: int = 1024, block_size: int = 16, num_blocks: int = 512, top_k: int = 5, top_p: float = 1.0, do_sample: bool = True, quant_config: dict = None, tie_word_embeddings: bool = False, llm_backend: str = '', fused_rms_norm: bool = True, input_sliced_sig: bool = False, rmsnorm_compute_2d: bool = False, chunk_prefill: bool = False, calculate_per_token_loss: bool = False, pipeline_stage: dict = None, **kwargs)"
    },
    "mindformers.experimental.model.llama.llama.LlamaForCausalLM": {
        "signature": "(config: mindformers.models.llama.llama_config.LlamaConfig, num_tokentypes: int = 0, parallel_output: bool = True, pre_process: bool = True, post_process: bool = True, **kwargs)"
    },
    "mindformers.experimental.model.llama.llama.LlamaForCausalLM.construct": {
        "signature": "(self, input_ids: mindspore.common.tensor.Tensor, position_ids: mindspore.common.tensor.Tensor = None, attention_mask: mindspore.common.tensor.Tensor = None, retriever_input_ids: mindspore.common.tensor.Tensor = None, retriever_position_ids: mindspore.common.tensor.Tensor = None, retriever_attn_mask: mindspore.common.tensor.Tensor = None, labels: mindspore.common.tensor.Tensor = None, tokentype_ids=None, inference_params=None, prefix_keys_values=None, input_embeds: mindspore.common.tensor.Tensor = None, loss_mask: mindspore.common.tensor.Tensor = None) -> mindspore.common.tensor.Tensor"
    },
    "mindformers.experimental.model.llama.llama.LlamaForCausalLM.post_language_model_processing": {
        "signature": "(self, lm_output: mindspore.common.tensor.Tensor, labels: mindspore.common.tensor.Tensor, logit_weights: mindspore.common.tensor.Tensor, parallel_output: bool, fp16_lm_cross_entropy: bool, loss_mask: mindspore.common.tensor.Tensor) -> mindspore.common.tensor.Tensor"
    },
    "mindformers.experimental.model.llama.llama.LlamaForCausalLM.set_input_tensor": {
        "signature": "(self, input_tensor)"
    },
    "mindformers.experimental.model.llama.llama.LlamaForCausalLM.set_dynamic_inputs": {
        "signature": "(self, **kwargs)"
    },
    "mindformers.experimental.model.llama.llama.LlamaForCausalLM._preprocess_input_labels_and_masks": {
        "signature": "(self, input_ids: mindspore.common.tensor.Tensor, labels: mindspore.common.tensor.Tensor = None, attention_mask: mindspore.common.tensor.Tensor = None) -> (<class 'mindspore.common.tensor.Tensor'>, <class 'mindspore.common.tensor.Tensor'>, <class 'mindspore.common.tensor.Tensor'>, <class 'mindspore.common.tensor.Tensor'>)"
    },
    "mindformers.experimental.model.llama.llama.LlamaForCausalLM.shard": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig)"
    },
    "mindformers.experimental.model.llama.llama.LlamaForCausalLM.sharding_propagation": {
        "signature": "(self, config: mindformers.experimental.graph.transformer.transformer_config.TransformerConfig)"
    },
    "mindformers.experimental.model.llama.llama.LlamaPretrainedModel": {
        "signature": "(config: mindformers.models.configuration_utils.PretrainedConfig, *inputs, **kwargs)"
    },
    "mindformers.experimental.model.llama.llama.LlamaPretrainedModel.config_class": {
        "signature": "(batch_size: int = 1, seq_length: int = 2048, hidden_size: int = 4096, num_layers: int = 32, num_heads: int = 32, n_kv_heads: Optional[int] = None, max_position_embedding: Optional[int] = None, intermediate_size: Optional[int] = None, vocab_size: int = 32000, multiple_of: int = 256, ffn_dim_multiplier: Optional[int] = None, rms_norm_eps: float = 1e-05, bos_token_id: int = 1, eos_token_id: int = 2, pad_token_id: int = 0, ignore_token_id: int = -100, theta: float = 10000.0, compute_dtype: str = 'float16', layernorm_compute_type: str = 'float32', softmax_compute_type: str = 'float32', rotary_dtype: str = 'float32', param_init_type: str = 'float16', residual_dtype: str = None, embedding_init_type=None, qkv_has_bias: bool = False, qkv_concat: bool = False, attn_proj_has_bias: bool = False, parallel_config: Union[dict, mindformers.modules.transformer.transformer.TransformerOpParallelConfig] = <mindformers.modules.transformer.transformer.TransformerOpParallelConfig object>, moe_config: Union[dict, mindformers.modules.transformer.moe.MoEConfig] = <mindformers.modules.transformer.moe.MoEConfig object>, use_past: bool = False, extend_method: str = 'None', scaling_factor: float = 1.0, is_dynamic: bool = False, use_rope_slice: bool = False, use_flash_attention: bool = False, use_ring_attention: bool = False, use_attn_mask_compression: bool = False, use_eod_attn_mask_compression: bool = False, parallel_optimizer: bool = False, fine_grain_interleave: int = 1, pp_interleave_num: int = 1, offset: int = 0, init_method_std: float = 0.01, checkpoint_name_or_path: str = '', repetition_penalty: float = 1.0, max_decode_length: int = 1024, block_size: int = 16, num_blocks: int = 512, top_k: int = 5, top_p: float = 1.0, do_sample: bool = True, quant_config: dict = None, tie_word_embeddings: bool = False, llm_backend: str = '', fused_rms_norm: bool = True, input_sliced_sig: bool = False, rmsnorm_compute_2d: bool = False, chunk_prefill: bool = False, calculate_per_token_loss: bool = False, pipeline_stage: dict = None, **kwargs)"
    },
    "mindformers.experimental.parallel_core.AdamW": {
        "signature": "(*args, **kwargs)"
    },
    "mindformers.experimental.parallel_core.ColumnParallelLinear": {
        "signature": "(*args, **kwargs)"
    },
    "mindformers.experimental.parallel_core.Embedding": {
        "signature": "(*args, **kwargs)"
    },
    "mindformers.experimental.parallel_core.ParallelAttention": {
        "signature": "(*args, **kwargs)"
    },
    "mindformers.experimental.parallel_core.ParallelLMLogits": {
        "signature": "(*args, **kwargs)"
    },
    "mindformers.experimental.parallel_core.ParallelMLP": {
        "signature": "(*args, **kwargs)"
    },
    "mindformers.experimental.parallel_core.ParallelTransformer": {
        "signature": "(*args, **kwargs)"
    },
    "mindformers.experimental.parallel_core.ParallelTransformerLayer": {
        "signature": "(*args, **kwargs)"
    },
    "mindformers.experimental.parallel_core.RotaryEmbedding": {
        "signature": "(*args, **kwargs)"
    },
    "mindformers.experimental.parallel_core.RowParallelLinear": {
        "signature": "(*args, **kwargs)"
    },
    "mindformers.experimental.parallel_core.VocabParallelEmbedding": {
        "signature": "(*args, **kwargs)"
    },
    "mindformers.experimental.parallel_core.apply_rotary_pos_emb": {
        "signature": "(*args, **kwargs)"
    },
    "mindformers.experimental.parallel_core.get_language_model": {
        "signature": "(*args, **kwargs)"
    },
    "mindformers.experimental.parallel_core.pynative.context_parallel.FlashSP": {
        "signature": "(head_num, keep_prob=1.0, scale_value=1.0, pre_tokens=2147483647, next_tokens=2147483647, input_layout='BSH', sparse_mode=0, use_attention_mask=False, use_alibi_mask=False, use_mqa=False, dp=1, mp=1, sp=1)"
    },
    "mindformers.experimental.parallel_core.pynative.context_parallel.FlashSP.p2p_send_communicate": {
        "signature": "(self, send_tensor, send_dst, sp_group, stream, send_ops, sr_tag)"
    },
    "mindformers.experimental.parallel_core.pynative.context_parallel.FlashSP.p2p_recv_communicate": {
        "signature": "(self, recv_tensor, recv_src, sp_group, stream, recv_ops, sr_tag)"
    },
    "mindformers.experimental.parallel_core.pynative.context_parallel.FlashSP.forward_update": {
        "signature": "(self, prev_attn_out, prev_softmax_max, prev_softmax_sum, cur_attn_out, cur_softmax_max, cur_softmax_sum)"
    },
    "mindformers.experimental.parallel_core.pynative.context_parallel.FlashSP.check_parameter": {
        "signature": "(self, q, k, v, attn_mask, alibi_mask, prefix, padding_mask, attn_mask_type)"
    },
    "mindformers.experimental.parallel_core.pynative.context_parallel.FlashSP.get_loop_steps": {
        "signature": "(self, sp_size)"
    },
    "mindformers.experimental.parallel_core.pynative.context_parallel.FlashSP.get_send_qkv_dst_rank": {
        "signature": "(self, rank, step, sp_size)"
    },
    "mindformers.experimental.parallel_core.pynative.context_parallel.FlashSP.get_recv_qkv_src_rank": {
        "signature": "(self, rank, step, sp_size)"
    },
    "mindformers.experimental.parallel_core.pynative.context_parallel.FlashSP.get_send_oml_dst_rank": {
        "signature": "(self, rank, step, sp_size)"
    },
    "mindformers.experimental.parallel_core.pynative.context_parallel.FlashSP.get_recv_oml_src_rank": {
        "signature": "(self, rank, step, sp_size)"
    },
    "mindformers.experimental.parallel_core.pynative.context_parallel.FlashSP.get_rank_index": {
        "signature": "(self, rank, step, sp_size)"
    },
    "mindformers.experimental.parallel_core.pynative.context_parallel.FlashSP.construct_send_oml_tensor": {
        "signature": "(self, softmax_max_, softmax_sum_, attn_out_)"
    },
    "mindformers.experimental.parallel_core.pynative.context_parallel.FlashSP.dismantle_recv_oml_tensor": {
        "signature": "(self, recv_oml_tensor, hidden_dim)"
    },
    "mindformers.experimental.parallel_core.pynative.context_parallel.FlashSP.get_recv_oml_tensor_shape": {
        "signature": "(self, q)"
    },
    "mindformers.experimental.parallel_core.pynative.context_parallel.FlashSP.prepare_comm": {
        "signature": "(self, q)"
    },
    "mindformers.experimental.parallel_core.pynative.context_parallel.FlashSP.prepare_qkv": {
        "signature": "(self, step, step_, inner_loop_steps, loop_steps, attn_mask_start, attn_mask_end, q, k, v, kv, rank, sp_size, len_k)"
    },
    "mindformers.experimental.parallel_core.pynative.context_parallel.FlashSP.construct_qkv_comm": {
        "signature": "(self, step, step_, inner_loop_steps, loop_steps, q, send_kv, rank, sp_size, sp_group)"
    },
    "mindformers.experimental.parallel_core.pynative.context_parallel.FlashSP.construct_oml_comm": {
        "signature": "(self, step, step_, inner_loop_steps, loop_steps, q, rank, sp_size, sp_group)"
    },
    "mindformers.experimental.parallel_core.pynative.context_parallel.FlashSP.update_out": {
        "signature": "(self, step, step_, inner_loop_steps, loop_steps, cur_attn_out, cur_softmax_max, cur_softmax_sum, rank, sp_size)"
    },
    "mindformers.experimental.parallel_core.pynative.context_parallel.FlashSP.construct": {
        "signature": "(self, q, k, v, attn_mask=None, alibi_mask=None, prefix=None, padding_mask=None, attn_mask_type='causal')"
    },
    "mindformers.experimental.parallel_core.pynative.context_parallel.RingAttention": {
        "signature": "(head_num, keep_prob=1.0, scale_value=1.0, pre_tokens=2147483647, next_tokens=2147483647, input_layout='BSH', sparse_mode=0, use_attention_mask=False, use_alibi_mask=False, use_mqa=False)"
    },
    "mindformers.experimental.parallel_core.pynative.context_parallel.RingAttention.p2p_communicate": {
        "signature": "(self, rank, send_tensor, send_dst, recv_src, sp_group)"
    },
    "mindformers.experimental.parallel_core.pynative.context_parallel.RingAttention.forward_update": {
        "signature": "(self, prev_attn_out, prev_softmax_max, prev_softmax_sum, cur_attn_out, cur_softmax_max, cur_softmax_sum)"
    },
    "mindformers.experimental.parallel_core.pynative.context_parallel.RingAttention.check_parameter": {
        "signature": "(self, q, k, v, alibi_mask, prefix, padding_mask, attn_mask_type)"
    },
    "mindformers.experimental.parallel_core.pynative.context_parallel.RingAttention.prepare_qkv": {
        "signature": "(self, q, k, v, attn_mask, send_kv, i, rank, attn_mask_type, cp_size)"
    },
    "mindformers.experimental.parallel_core.pynative.context_parallel.RingAttention.prepare_flash_attention_grad_input": {
        "signature": "(self, softmax_max, softmax_sum, q, attn_out, dout, attn_mask, cur_k, cur_v, rank, i, cp_size, attn_mask_type)"
    },
    "mindformers.experimental.parallel_core.pynative.context_parallel.RingAttention.backward_update": {
        "signature": "(self, cur_dq, cur_dk, cur_dv, dq, dk, dv, recv_dkv, recv_kv_dkv, i, rank, cp_size, attn_mask_type)"
    },
    "mindformers.experimental.parallel_core.pynative.context_parallel.RingAttention.construct": {
        "signature": "(self, q, k, v, attn_mask=None, alibi_mask=None, prefix=None, padding_mask=None, attn_mask_type='causal')"
    },
    "mindformers.experimental.parallel_core.pynative.context_parallel.RingAttention.bprop": {
        "signature": "(self, q, k, v, attn_mask, attn_out, dout, attn_mask_type='causal')"
    },
    "mindformers.experimental.parallel_core.pynative.dist_checkpointing.get_checkpoint_name": {
        "signature": "(ckpt_path, format='ckpt', get_name_from_file=False, prefix: str = 'network', epoch_num: int = None, step_num: int = None)"
    },
    "mindformers.experimental.parallel_core.pynative.dist_checkpointing.get_last_checkpoint": {
        "signature": "(ckpt_path: str, format: str = 'ckpt')"
    },
    "mindformers.experimental.parallel_core.pynative.dist_checkpointing.load_checkpoint": {
        "signature": "(config, model, optimizer=None, opt_param_scheduler=None, ckpt_path='./', format='ckpt', crc_check=False, **kwargs)"
    },
    "mindformers.experimental.parallel_core.pynative.dist_checkpointing.save_checkpoint": {
        "signature": "(config, model, optimizer=None, opt_param_scheduler=None, ckpt_path='./', format='ckpt', only_save_strategy=False, prefix: str = 'network', epoch_num: int = 0, step_num: int = 0, crc_check: bool = False, keep_checkpoint_max: int = 5, **kwargs)"
    },
    "mindformers.experimental.parallel_core.pynative.dist_checkpointing.checkpointing.get_checkpoint_name": {
        "signature": "(ckpt_path, format='ckpt', get_name_from_file=False, prefix: str = 'network', epoch_num: int = None, step_num: int = None)"
    },
    "mindformers.experimental.parallel_core.pynative.dist_checkpointing.checkpointing.load_checkpoint": {
        "signature": "(config, model, optimizer=None, opt_param_scheduler=None, ckpt_path='./', format='ckpt', crc_check=False, **kwargs)"
    },
    "mindformers.experimental.parallel_core.pynative.dist_checkpointing.checkpointing.load_post_process": {
        "signature": "(config, params_dict, optimizer=None)"
    },
    "mindformers.experimental.parallel_core.pynative.dist_checkpointing.checkpointing.load_rng_state": {
        "signature": "(param_dict)"
    },
    "mindformers.experimental.parallel_core.pynative.dist_checkpointing.checkpointing.save_checkpoint": {
        "signature": "(config, model, optimizer=None, opt_param_scheduler=None, ckpt_path='./', format='ckpt', only_save_strategy=False, prefix: str = 'network', epoch_num: int = 0, step_num: int = 0, crc_check: bool = False, keep_checkpoint_max: int = 5, **kwargs)"
    },
    "mindformers.experimental.parallel_core.pynative.dist_checkpointing.checkpointing.save_pre_process": {
        "signature": "(shard_info, model, optimizer, config)"
    },
    "mindformers.experimental.parallel_core.pynative.dist_checkpointing.checkpointing.save_rng_state": {
        "signature": "()"
    },
    "mindformers.experimental.parallel_core.pynative.distributed.Bucket": {
        "signature": "(ddp_config, params, param_data, grad_data, offset, numel_unpadded, data_parallel_group, data_parallel_world_size, gradient_scaling_factor)"
    },
    "mindformers.experimental.parallel_core.pynative.distributed.Bucket.inplace_reduce_dp": {
        "signature": "(self, src)"
    },
    "mindformers.experimental.parallel_core.pynative.distributed.Bucket.reset": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.distributed.Bucket.issue_grad_reduce": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.distributed.Bucket.final_grad_reduce": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.distributed.Bucket.register_grad_ready": {
        "signature": "(self, param)"
    },
    "mindformers.experimental.parallel_core.pynative.distributed.DistributedDataParallel": {
        "signature": "(config, ddp_config, module, disable_bucketing=False)"
    },
    "mindformers.experimental.parallel_core.pynative.distributed.DistributedDataParallel.allocate_buffers_for_parameters": {
        "signature": "(self, input_params, group, gradient_scaling_factor)"
    },
    "mindformers.experimental.parallel_core.pynative.distributed.DistributedDataParallel.issue_grad_reduce": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.distributed.DistributedDataParallel.final_grad_reduce": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.distributed.DistributedDataParallel.register_hook_for_params": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.distributed.DistributedDataParallel.set_input_tensor": {
        "signature": "(self, input_tensor)"
    },
    "mindformers.experimental.parallel_core.pynative.distributed.DistributedDataParallel.construct": {
        "signature": "(self, *inputs, **inputs_dict)"
    },
    "mindformers.experimental.parallel_core.pynative.distributed.DistributedDataParallel.zero_grad_buffer": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.distributed.DistributedDataParallel.enable_sync": {
        "signature": "(self, enable)"
    },
    "mindformers.experimental.parallel_core.pynative.distributed.DistributedDataParallel.no_sync": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.distributed.DistributedDataParallel._make_param_hook": {
        "signature": "(self, param, param_to_buffer)"
    },
    "mindformers.experimental.parallel_core.pynative.distributed.DistributedDataParallelConfig": {
        "signature": "(grad_reduce_in_fp32: bool = False, overlap_grad_reduce: bool = False, use_distributed_optimizer: bool = False, bucket_size: Optional[int] = None, average_in_collective: bool = False, check_for_nan_in_grad: bool = False, enable_mem_align: bool = True, use_zero3: bool = False) -> None"
    },
    "mindformers.experimental.parallel_core.pynative.distributed.ParamAndGradBuffer": {
        "signature": "(ddp_config, param_dtype, grad_dtype, params, data_parallel_group, bucket_size, param_to_name, gradient_scaling_factor)"
    },
    "mindformers.experimental.parallel_core.pynative.distributed.ParamAndGradBuffer._get_buffer_slice": {
        "signature": "(self, shape, start_index, buffer_type)"
    },
    "mindformers.experimental.parallel_core.pynative.distributed.ParamAndGradBuffer.reset": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.distributed.ParamAndGradBuffer.issue_grad_reduce": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.distributed.ParamAndGradBuffer.final_grad_reduce": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.distributed.ParamAndGradBuffer.register_grad_ready": {
        "signature": "(self, param)"
    },
    "mindformers.experimental.parallel_core.pynative.distributed.distributed_data_parallel.DistributedDataParallel": {
        "signature": "(config, ddp_config, module, disable_bucketing=False)"
    },
    "mindformers.experimental.parallel_core.pynative.distributed.distributed_data_parallel.DistributedDataParallel.allocate_buffers_for_parameters": {
        "signature": "(self, input_params, group, gradient_scaling_factor)"
    },
    "mindformers.experimental.parallel_core.pynative.distributed.distributed_data_parallel.DistributedDataParallel.issue_grad_reduce": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.distributed.distributed_data_parallel.DistributedDataParallel.final_grad_reduce": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.distributed.distributed_data_parallel.DistributedDataParallel.register_hook_for_params": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.distributed.distributed_data_parallel.DistributedDataParallel.set_input_tensor": {
        "signature": "(self, input_tensor)"
    },
    "mindformers.experimental.parallel_core.pynative.distributed.distributed_data_parallel.DistributedDataParallel.construct": {
        "signature": "(self, *inputs, **inputs_dict)"
    },
    "mindformers.experimental.parallel_core.pynative.distributed.distributed_data_parallel.DistributedDataParallel.zero_grad_buffer": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.distributed.distributed_data_parallel.DistributedDataParallel.enable_sync": {
        "signature": "(self, enable)"
    },
    "mindformers.experimental.parallel_core.pynative.distributed.distributed_data_parallel.DistributedDataParallel.no_sync": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.distributed.distributed_data_parallel.DistributedDataParallel._make_param_hook": {
        "signature": "(self, param, param_to_buffer)"
    },
    "mindformers.experimental.parallel_core.pynative.distributed.param_and_grad_buffer.Bucket": {
        "signature": "(ddp_config, params, param_data, grad_data, offset, numel_unpadded, data_parallel_group, data_parallel_world_size, gradient_scaling_factor)"
    },
    "mindformers.experimental.parallel_core.pynative.distributed.param_and_grad_buffer.Bucket.inplace_reduce_dp": {
        "signature": "(self, src)"
    },
    "mindformers.experimental.parallel_core.pynative.distributed.param_and_grad_buffer.Bucket.reset": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.distributed.param_and_grad_buffer.Bucket.issue_grad_reduce": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.distributed.param_and_grad_buffer.Bucket.final_grad_reduce": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.distributed.param_and_grad_buffer.Bucket.register_grad_ready": {
        "signature": "(self, param)"
    },
    "mindformers.experimental.parallel_core.pynative.distributed.param_and_grad_buffer.ParamAndGradBuffer": {
        "signature": "(ddp_config, param_dtype, grad_dtype, params, data_parallel_group, bucket_size, param_to_name, gradient_scaling_factor)"
    },
    "mindformers.experimental.parallel_core.pynative.distributed.param_and_grad_buffer.ParamAndGradBuffer._get_buffer_slice": {
        "signature": "(self, shape, start_index, buffer_type)"
    },
    "mindformers.experimental.parallel_core.pynative.distributed.param_and_grad_buffer.ParamAndGradBuffer.reset": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.distributed.param_and_grad_buffer.ParamAndGradBuffer.issue_grad_reduce": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.distributed.param_and_grad_buffer.ParamAndGradBuffer.final_grad_reduce": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.distributed.param_and_grad_buffer.ParamAndGradBuffer.register_grad_ready": {
        "signature": "(self, param)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.DistributedOptimizer": {
        "signature": "(optimizer, config, grad_scaler, init_state_fn, per_model_buffers, data_parallel_group)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.DistributedOptimizer._build_param_ranges_map": {
        "signature": "(param_index_map, bucket_world_range, bucket_offset)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.DistributedOptimizer._build_bucket_ranges_map": {
        "signature": "(param_and_grad_buffer, bucket_index)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.DistributedOptimizer._build_buffer_ranges_map": {
        "signature": "(param_and_grad_buffer)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.DistributedOptimizer._get_optimizer_group_ranges": {
        "signature": "(param_groups, param_ranges_map)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.DistributedOptimizer._build_sharded_params_and_grads": {
        "signature": "(param_ranges_map, param_to_bucket_map, sharded_param_groups, buffers)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.DistributedOptimizer.zero_grad": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.DistributedOptimizer.get_model_parallel_group": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.DistributedOptimizer.reload_main_params": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.DistributedOptimizer.step_with_ready_grads": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.DistributedOptimizer._load_state_from_fs_model_space": {
        "signature": "(self, state_dict)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.DistributedOptimizer._load_state_dict_from_dp_zero": {
        "signature": "(self, state_dict)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.DistributedOptimizer.load_state_dict": {
        "signature": "(self, state_dict)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.DistributedOptimizer._update_optimizer_attr": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.DistributedOptimizer._collect_main_grad_data": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.DistributedOptimizer._get_model_param_buffer_dp_views": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.DistributedOptimizer._copy_model_grads_to_main_grads": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.DistributedOptimizer._copy_model_params_to_main_params": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.DistributedOptimizer._copy_main_params_to_model_params": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.DistributedOptimizer.sync_gather_all_model_params": {
        "signature": "(self, force_sync=False)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.DistributedOptimizer._dispatch_gather_model_params": {
        "signature": "(self, all_gather_handle_index)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.DistributedOptimizer._pre_forward_cell_hook": {
        "signature": "(self, cell, input)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.DistributedOptimizer.enable_pre_hook": {
        "signature": "(self, module)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.DistributedOptimizer.disable_pre_hook": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.DistributedOptimizer._finish_param_sync_helper": {
        "signature": "(self, all_gather_handle_index)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.DistributedOptimizer.finish_param_sync": {
        "signature": "(self, model_index)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.DistributedOptimizer.save_opt_shard_strategy": {
        "signature": "(self, file)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.DistributedOptimizer.state_dict": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.Float16OptimizerWithFloat16Params": {
        "signature": "(optimizer, config, grad_scaler, init_state_fn, wrap_with_ddp=False)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.Float16OptimizerWithFloat16Params.zero_grad": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.Float16OptimizerWithFloat16Params.reload_main_params": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.Float16OptimizerWithFloat16Params.load_state_dict": {
        "signature": "(self, state_dict)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.Float16OptimizerWithFloat16Params.state_dict": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.Float16OptimizerWithFloat16Params._collect_main_grad_data": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.Float16OptimizerWithFloat16Params._get_model_and_main_params_data": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.Float16OptimizerWithFloat16Params._copy_model_grads_to_main_grads": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.Float16OptimizerWithFloat16Params._copy_model_params_to_main_params": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.Float16OptimizerWithFloat16Params._copy_main_params_to_model_params": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.Float16OptimizerWithFloat16Params._make_param_hook": {
        "signature": "(self, param)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.MixedPrecisionOptimizer": {
        "signature": "(optimizer, config, grad_scaler, init_state_fn)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.MixedPrecisionOptimizer._get_lrs": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.MixedPrecisionOptimizer._set_lrs": {
        "signature": "(self, value)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.MixedPrecisionOptimizer.zero_grad": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.MixedPrecisionOptimizer.get_loss_scale": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.MixedPrecisionOptimizer.reload_model_params": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.MixedPrecisionOptimizer.get_parameters_": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.MixedPrecisionOptimizer.get_lr": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.MixedPrecisionOptimizer.get_model_parallel_group": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.MixedPrecisionOptimizer.get_main_grads_for_grad_norm": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.MixedPrecisionOptimizer.clip_grad_norm": {
        "signature": "(self, clip_grad)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.MixedPrecisionOptimizer._unscale_main_grads_and_check_for_nan": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.MixedPrecisionOptimizer.prepare_grads": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.MixedPrecisionOptimizer.step_with_ready_grads": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.MixedPrecisionOptimizer.construct": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.MixedPrecisionOptimizer._get_param_groups": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.MixedPrecisionOptimizer._set_param_groups": {
        "signature": "(self, value)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.get_learning_rate_scheduler": {
        "signature": "(optimizer_config, return_instance: bool = True)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.get_optimizer": {
        "signature": "(optimizer_config, training_config, params=None, network=None, return_instance: bool = True, **kwargs)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.get_optimizer_param_scheduler": {
        "signature": "(optimizer, optimizer_config, dataset_config, training_config)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.lr_scheduler.LearningRateScheduler": {
        "signature": "(learning_rate, end_learning_rate, warmup_steps, decay_steps, power=1.0, use_cosine=True)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.lr_scheduler.LearningRateScheduler.construct": {
        "signature": "(self, global_step)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.lr_scheduler.get_learning_rate_scheduler": {
        "signature": "(optimizer_config, return_instance: bool = True)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.zero.AdamW": {
        "signature": "(network, params, learning_rate=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.0, zero_level='z1', param_resident=False, param_resident_rate=1.0, allreduce_after_grad_accumulation=False, grad_allreduce_op='sum', opt_parallel_group=None, cpu_offload=False, with_context_parallel=False)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.zero.AdamW._init_optimizer_shard_info": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.zero.AdamW._init_all_gather_ops": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.zero.AdamW._param_resident_flag": {
        "signature": "(self, zero3_param_numel, param_resident_rate)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.zero.AdamW._regist_hook_for_cells": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.zero.AdamW._parameter_status_split": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.zero.AdamW._regist_hook_for_params": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.zero.AdamW._init_momentum": {
        "signature": "(self, params, prefix, init='zeros')"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.zero.AdamW._offload_optimizer_params": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.zero.AdamW.construct": {
        "signature": "(self, grads)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.zero.AdamW.sharded_state_dict": {
        "signature": "(self, model_sharded_state_dict)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.zero.adamw_zero.AdamW": {
        "signature": "(network, params, learning_rate=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.0, zero_level='z1', param_resident=False, param_resident_rate=1.0, allreduce_after_grad_accumulation=False, grad_allreduce_op='sum', opt_parallel_group=None, cpu_offload=False, with_context_parallel=False)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.zero.adamw_zero.AdamW._init_optimizer_shard_info": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.zero.adamw_zero.AdamW._init_all_gather_ops": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.zero.adamw_zero.AdamW._param_resident_flag": {
        "signature": "(self, zero3_param_numel, param_resident_rate)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.zero.adamw_zero.AdamW._regist_hook_for_cells": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.zero.adamw_zero.AdamW._parameter_status_split": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.zero.adamw_zero.AdamW._regist_hook_for_params": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.zero.adamw_zero.AdamW._init_momentum": {
        "signature": "(self, params, prefix, init='zeros')"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.zero.adamw_zero.AdamW._offload_optimizer_params": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.zero.adamw_zero.AdamW.construct": {
        "signature": "(self, grads)"
    },
    "mindformers.experimental.parallel_core.pynative.optimizer.zero.adamw_zero.AdamW.sharded_state_dict": {
        "signature": "(self, model_sharded_state_dict)"
    },
    "mindformers.experimental.parallel_core.pynative.pipeline_parallel.P2PPrimitive": {
        "signature": "(config)"
    },
    "mindformers.experimental.parallel_core.pynative.pipeline_parallel.P2PPrimitive.send_forward": {
        "signature": "(self, output_tensor)"
    },
    "mindformers.experimental.parallel_core.pynative.pipeline_parallel.P2PPrimitive.recv_forward": {
        "signature": "(self, tensor_shape)"
    },
    "mindformers.experimental.parallel_core.pynative.pipeline_parallel.P2PPrimitive.send_backward": {
        "signature": "(self, input_tensor_grad)"
    },
    "mindformers.experimental.parallel_core.pynative.pipeline_parallel.P2PPrimitive.recv_backward": {
        "signature": "(self, tensor_shape)"
    },
    "mindformers.experimental.parallel_core.pynative.pipeline_parallel.P2PPrimitive.send_forward_recv_backward": {
        "signature": "(self, output_tensor, tensor_shape)"
    },
    "mindformers.experimental.parallel_core.pynative.pipeline_parallel.P2PPrimitive.send_backward_recv_forward": {
        "signature": "(self, input_tensor_grad, tensor_shape)"
    },
    "mindformers.experimental.parallel_core.pynative.pipeline_parallel.P2PPrimitive.send_forward_recv_forward": {
        "signature": "(self, output_tensor, recv_prev, tensor_shape, overlap_p2p_comm=False)"
    },
    "mindformers.experimental.parallel_core.pynative.pipeline_parallel.P2PPrimitive.send_backward_recv_backward": {
        "signature": "(self, input_tensor_grad, recv_next, tensor_shape, overlap_p2p_comm=False)"
    },
    "mindformers.experimental.parallel_core.pynative.pipeline_parallel.P2PPrimitive.send_forward_backward_recv_forward_backward": {
        "signature": "(self, output_tensor, input_tensor_grad, recv_prev, recv_next, tensor_shape)"
    },
    "mindformers.experimental.parallel_core.pynative.pipeline_parallel.P2PPrimitive.node_p2p_comm": {
        "signature": "(self, tensor_send_next, tensor_send_prev, recv_prev, recv_next, tensor_shape, wait_on_reqs=True)"
    },
    "mindformers.experimental.parallel_core.pynative.pipeline_parallel.P2PPrimitive._isend_and_irecv": {
        "signature": "(tensor_send_prev, tensor_info_recv_prev, tensor_send_next, tensor_info_recv_next, group)"
    },
    "mindformers.experimental.parallel_core.pynative.pipeline_parallel.forward_backward_pipelining_with_interleaving": {
        "signature": "(model, num_microbatches, seq_length, micro_batch_size, *input_data_tuple, decoder_seq_length=None, forward_only=False, collect_non_loss_data=False, first_val_step=None, config=None, scale_sense=1.0, total_tokens_nums=None, **input_data_dict)"
    },
    "mindformers.experimental.parallel_core.pynative.pipeline_parallel.forward_backward_pipelining_without_interleaving": {
        "signature": "(model, num_microbatches, seq_length, micro_batch_size, *input_data_tuple, decoder_seq_length=None, forward_only=False, collect_non_loss_data=False, first_val_step=None, config=None, scale_sense=1.0, total_tokens_nums=None, **input_data_dict)"
    },
    "mindformers.experimental.parallel_core.pynative.profiler.PynativeProfiler": {
        "signature": "(training_config)"
    },
    "mindformers.experimental.parallel_core.pynative.profiler.PynativeProfiler.step_begin": {
        "signature": "(self, current_step)"
    },
    "mindformers.experimental.parallel_core.pynative.profiler.PynativeProfiler.step_end": {
        "signature": "(self, current_step)"
    },
    "mindformers.experimental.parallel_core.pynative.recompute.CheckpointedRecomputeOrientedCell": {
        "signature": "(layers)"
    },
    "mindformers.experimental.parallel_core.pynative.recompute.CheckpointedRecomputeOrientedCell.construct": {
        "signature": "(self, hidden_states, *args, **kwargs)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.AllGatherFromTensorParallelRegion": {
        "signature": "()"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.AllGatherFromTensorParallelRegion.construct": {
        "signature": "(self, input_)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.AllGatherFromTensorParallelRegion.bprop": {
        "signature": "(self, x, out, dout)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.AllToAll": {
        "signature": "(group, output_shape, input_shape, output_splits, input_splits, use_self_defined_alltoall=False)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.AllToAll.construct": {
        "signature": "(self, input_)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.AllToAll.bprop": {
        "signature": "(self, input_, output, dout)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.AllToAllSP2HP": {
        "signature": "()"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.AllToAllSP2HP.construct": {
        "signature": "(self, input_)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.ColumnParallelLinear": {
        "signature": "(input_size, output_size, *, config, init_method, bias=True, gather_output=False, stride=1, keep_master_weight_for_test=False, skip_bias_add=False, skip_weight_param_allocation=False, embedding_activation_buffer=None, grad_output_buffer=None, is_expert=False, tp_comm_buffer_name=None, disable_grad_reduce=False, bias_init=<mindspore.common.initializer.Zero object>, param_init_dtype=None, compute_dtype=None, transpose_b=True)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.ColumnParallelLinear.construct": {
        "signature": "(self, input_, weight=None)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.ColumnParallelLinear.sharded_state_dict": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.CopyToModelParallelRegion": {
        "signature": "()"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.CopyToModelParallelRegion.construct": {
        "signature": "(self, input_)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.CopyToModelParallelRegion.bprop": {
        "signature": "(self, x, out, dout)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.GatherFromModelParallelRegion": {
        "signature": "()"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.GatherFromModelParallelRegion.construct": {
        "signature": "(self, input_)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.GatherFromModelParallelRegion.bprop": {
        "signature": "(self, x, out, dout)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.GatherFromSequenceParallelRegion": {
        "signature": "(need_to_swapaxes, tensor_parallel_output_grad=True)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.GatherFromSequenceParallelRegion.construct": {
        "signature": "(self, input_)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.GatherFromSequenceParallelRegion.bprop": {
        "signature": "(self, x, out, dout)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.GatherFromTensorAndExpertParallelRegion": {
        "signature": "()"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.GatherFromTensorAndExpertParallelRegion.construct": {
        "signature": "(self, input_)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.GatherFromTensorAndExpertParallelRegion.bprop": {
        "signature": "(self, x, out, dout)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.LinearWithGradAccumulationAndAsyncCommunication": {
        "signature": "(bias, gradient_accumulation_fusion, sequence_parallel, allreduce_dgrad, grad_output_buffer=None, wgrad_deferral_limit=0, transpose_b=True, data_layout='BSH', recompute_comm=False, need_gather_param_in_bw=False)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.LinearWithGradAccumulationAndAsyncCommunication.construct": {
        "signature": "(self, x, weight, bias, weight_param=None)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.LinearWithGradAccumulationAndAsyncCommunication.prepare_input_tensors_for_wgrad_compute": {
        "signature": "(self, dout, x)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.LinearWithGradAccumulationAndAsyncCommunication.bprop": {
        "signature": "(self, *args)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.RNGStateTracer": {
        "signature": "()"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.RNGStateTracer.reset": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.RNGStateTracer.set_state": {
        "signature": "(self, states)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.RNGStateTracer.get_state": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.RNGStateTracer.init_mode": {
        "signature": "(self, mode, seed)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.RNGStateTracer.rng_fork": {
        "signature": "(self, mode='tp_rng_generator')"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.ReduceFromModelParallelRegion": {
        "signature": "()"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.ReduceFromModelParallelRegion.construct": {
        "signature": "(self, input_)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.ReduceFromModelParallelRegion.bprop": {
        "signature": "(self, x, out, dout)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.ReduceScatterToSequenceParallelRegion": {
        "signature": "(need_to_swapaxes)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.ReduceScatterToSequenceParallelRegion.construct": {
        "signature": "(self, input_)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.ReduceScatterToSequenceParallelRegion.bprop": {
        "signature": "(self, *args)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.ReduceScatterToTensorParallelRegion": {
        "signature": "()"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.ReduceScatterToTensorParallelRegion.construct": {
        "signature": "(self, input_)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.ReduceScatterToTensorParallelRegion.bprop": {
        "signature": "(self, x, out, dout)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.RowParallelLinear": {
        "signature": "(input_size, output_size, *, config, init_method, bias, input_is_parallel, skip_bias_add=True, stride=1, keep_master_weight_for_test=False, is_expert=False, tp_comm_buffer_name=None, bias_init=<mindspore.common.initializer.Zero object>, param_init_dtype=None, compute_dtype=None, transpose_b=True)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.RowParallelLinear.construct": {
        "signature": "(self, input_)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.RowParallelLinear.sharded_state_dict": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.ScatterToModelParallelRegion": {
        "signature": "()"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.ScatterToModelParallelRegion.construct": {
        "signature": "(self, input_)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.ScatterToModelParallelRegion.bprop": {
        "signature": "(self, x, out, dout)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.ScatterToSequenceParallelRegion": {
        "signature": "(need_to_swapaxes)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.ScatterToSequenceParallelRegion.construct": {
        "signature": "(self, input_)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.ScatterToSequenceParallelRegion.bprop": {
        "signature": "(self, x, out, dout)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.VocabParallelCrossEntropy": {
        "signature": "(*args, **kwargs)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.VocabParallelCrossEntropy._calculate_logits_max": {
        "signature": "(self, vocab_parallel_logits)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.VocabParallelCrossEntropy._get_vocab_range": {
        "signature": "(self, partition_vocab_size, rank)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.VocabParallelCrossEntropy._calculate_predicted_logits": {
        "signature": "(self, vocab_parallel_logits, target, logits_max, vocab_start_index, vocab_end_index, partition_vocab_size)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.VocabParallelCrossEntropy._calculate_cross_entropy_loss": {
        "signature": "(self, exp_logits, predicted_logits, sum_exp_logits)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.VocabParallelCrossEntropy._prepare_gradient_calculation_operands": {
        "signature": "(self, softmax, target_mask)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.VocabParallelCrossEntropy._calculate_gradients": {
        "signature": "(self, grad_2d, arange_1d, masked_target_1d, softmax_update, grad_input, grad_output, softmax)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.VocabParallelCrossEntropy.construct": {
        "signature": "(self, vocab_parallel_logits, target, label_smoothing=0.0)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.VocabParallelCrossEntropy.bprop": {
        "signature": "(self, *args)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.VocabParallelEmbedding": {
        "signature": "(num_embeddings, embedding_dim, *, init_method, reduce_scatter_embeddings=False, config, param_init_dtype=None)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.VocabParallelEmbedding.construct": {
        "signature": "(self, input_)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.VocabParallelEmbedding._vocab_range_from_global_vocab_size": {
        "signature": "(self, global_vocab_size, rank, world_size)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.VocabParallelEmbedding.sharded_state_dict": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.all_to_all_hp2sp": {
        "signature": "(input)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.all_to_all_sp2hp": {
        "signature": "(input)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.get_rng_tracer": {
        "signature": "()"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.set_rng_seed": {
        "signature": "(seed, dp_random_init=False)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.cross_entropy.VocabParallelCrossEntropy": {
        "signature": "(*args, **kwargs)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.cross_entropy.VocabParallelCrossEntropy._calculate_logits_max": {
        "signature": "(self, vocab_parallel_logits)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.cross_entropy.VocabParallelCrossEntropy._get_vocab_range": {
        "signature": "(self, partition_vocab_size, rank)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.cross_entropy.VocabParallelCrossEntropy._calculate_predicted_logits": {
        "signature": "(self, vocab_parallel_logits, target, logits_max, vocab_start_index, vocab_end_index, partition_vocab_size)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.cross_entropy.VocabParallelCrossEntropy._calculate_cross_entropy_loss": {
        "signature": "(self, exp_logits, predicted_logits, sum_exp_logits)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.cross_entropy.VocabParallelCrossEntropy._prepare_gradient_calculation_operands": {
        "signature": "(self, softmax, target_mask)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.cross_entropy.VocabParallelCrossEntropy._calculate_gradients": {
        "signature": "(self, grad_2d, arange_1d, masked_target_1d, softmax_update, grad_input, grad_output, softmax)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.cross_entropy.VocabParallelCrossEntropy.construct": {
        "signature": "(self, vocab_parallel_logits, target, label_smoothing=0.0)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.cross_entropy.VocabParallelCrossEntropy.bprop": {
        "signature": "(self, *args)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.layers.ColumnParallelLinear": {
        "signature": "(input_size, output_size, *, config, init_method, bias=True, gather_output=False, stride=1, keep_master_weight_for_test=False, skip_bias_add=False, skip_weight_param_allocation=False, embedding_activation_buffer=None, grad_output_buffer=None, is_expert=False, tp_comm_buffer_name=None, disable_grad_reduce=False, bias_init=<mindspore.common.initializer.Zero object>, param_init_dtype=None, compute_dtype=None, transpose_b=True)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.layers.ColumnParallelLinear.construct": {
        "signature": "(self, input_, weight=None)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.layers.ColumnParallelLinear.sharded_state_dict": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.layers.LinearWithGradAccumulationAndAsyncCommunication": {
        "signature": "(bias, gradient_accumulation_fusion, sequence_parallel, allreduce_dgrad, grad_output_buffer=None, wgrad_deferral_limit=0, transpose_b=True, data_layout='BSH', recompute_comm=False, need_gather_param_in_bw=False)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.layers.LinearWithGradAccumulationAndAsyncCommunication.construct": {
        "signature": "(self, x, weight, bias, weight_param=None)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.layers.LinearWithGradAccumulationAndAsyncCommunication.prepare_input_tensors_for_wgrad_compute": {
        "signature": "(self, dout, x)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.layers.LinearWithGradAccumulationAndAsyncCommunication.bprop": {
        "signature": "(self, *args)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.layers.RowParallelLinear": {
        "signature": "(input_size, output_size, *, config, init_method, bias, input_is_parallel, skip_bias_add=True, stride=1, keep_master_weight_for_test=False, is_expert=False, tp_comm_buffer_name=None, bias_init=<mindspore.common.initializer.Zero object>, param_init_dtype=None, compute_dtype=None, transpose_b=True)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.layers.RowParallelLinear.construct": {
        "signature": "(self, input_)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.layers.RowParallelLinear.sharded_state_dict": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.layers.VocabParallelEmbedding": {
        "signature": "(num_embeddings, embedding_dim, *, init_method, reduce_scatter_embeddings=False, config, param_init_dtype=None)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.layers.VocabParallelEmbedding.construct": {
        "signature": "(self, input_)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.layers.VocabParallelEmbedding._vocab_range_from_global_vocab_size": {
        "signature": "(self, global_vocab_size, rank, world_size)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.layers.VocabParallelEmbedding.sharded_state_dict": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.random.RNGStateTracer": {
        "signature": "()"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.random.RNGStateTracer.reset": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.random.RNGStateTracer.set_state": {
        "signature": "(self, states)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.random.RNGStateTracer.get_state": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.random.RNGStateTracer.init_mode": {
        "signature": "(self, mode, seed)"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.random.RNGStateTracer.rng_fork": {
        "signature": "(self, mode='tp_rng_generator')"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.random.get_rng_tracer": {
        "signature": "()"
    },
    "mindformers.experimental.parallel_core.pynative.tensor_parallel.random.set_rng_seed": {
        "signature": "(seed, dp_random_init=False)"
    },
    "mindformers.experimental.parallel_core.pynative.training.ParallelTrainingReducer": {
        "signature": "(params, training_config)"
    },
    "mindformers.experimental.parallel_core.pynative.training.ParallelTrainingReducer.get_reduce_group": {
        "signature": "(self, idx)"
    },
    "mindformers.experimental.parallel_core.pynative.training.ParallelTrainingReducer.inplace_reduce_dp_grad": {
        "signature": "(self, grads, params=None)"
    },
    "mindformers.experimental.parallel_core.pynative.training.ParallelTrainingReducer.inplace_reduce_sp_grad": {
        "signature": "(self, grads, params=None)"
    },
    "mindformers.experimental.parallel_core.pynative.training.ParallelTrainingReducer.inplace_reduce_grad": {
        "signature": "(self, grads, params=None)"
    },
    "mindformers.experimental.parallel_core.pynative.training.ParallelTrainingReducer.reduce_dp_loss": {
        "signature": "(self, loss)"
    },
    "mindformers.experimental.parallel_core.pynative.training.ParallelTrainingReducer.reduce_overflow": {
        "signature": "(self, overflow)"
    },
    "mindformers.experimental.parallel_core.pynative.training.ParallelTrainingReducer.reduce_is_finite": {
        "signature": "(self, is_finite)"
    },
    "mindformers.experimental.parallel_core.pynative.training.TrainOneStepCell": {
        "signature": "(network_with_loss, optimizer, opt_param_scheduler, training_config, model_config, **kwargs)"
    },
    "mindformers.experimental.parallel_core.pynative.training.TrainOneStepCell.unscale_and_clip_grads": {
        "signature": "(self, grads, loss_scale=None)"
    },
    "mindformers.experimental.parallel_core.pynative.training.TrainOneStepCell.construct": {
        "signature": "(self, *inputs_tuple, **inputs_dict)"
    },
    "mindformers.experimental.parallel_core.pynative.training.get_loss_func": {
        "signature": "(training_config, return_instance: bool = True, **kwargs)"
    },
    "mindformers.experimental.parallel_core.pynative.training.get_model": {
        "signature": "(model_provider_func, training_config)"
    },
    "mindformers.experimental.parallel_core.pynative.training.pretrain": {
        "signature": "(train_valid_test_datasets_provider, model_provider_func, model_type, forward_step_func=None, process_non_loss_data_func=None, **kwargs)"
    },
    "mindformers.experimental.parallel_core.pynative.training.train": {
        "signature": "(train_one_step_cell, train_dataloader, training_config, val_dataloader=None, metrics=None, evaluation_func=None, resume_dict=None, get_batch_func=None, **kwargs)"
    },
    "mindformers.experimental.parallel_core.pynative.training.loss_func.get_loss_func": {
        "signature": "(training_config, return_instance: bool = True, **kwargs)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.ParallelAttention": {
        "signature": "(config, layer_number, attention_type=<AttnType.self_attn: 1>, attn_mask_type=<AttnMaskType.padding: 1>)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.ParallelAttention._init_qkv_proj": {
        "signature": "(self, input_size, output_size, cell_name)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.ParallelAttention._init_out_proj": {
        "signature": "(self, input_size, output_size, cell_name, input_is_parallel=True)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.ParallelAttention.construct": {
        "signature": "(self, hidden_states, attention_mask, encoder_output=None, inference_params=None, rotary_pos_emb=None)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.ParallelLMLogits": {
        "signature": "(config, bias=False, compute_dtype=None)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.ParallelLMLogits.construct": {
        "signature": "(self, input_, word_embeddings_weight, parallel_output=True, bias=None)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.ParallelMLP": {
        "signature": "(config, is_expert=False)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.ParallelMLP.construct": {
        "signature": "(self, hidden_states)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.ParallelTransformer": {
        "signature": "(config, model_type, layer_type=<LayerType.encoder: 1>, self_attn_mask_type=<AttnMaskType.padding: 1>, post_norm=True, pre_process=False, post_process=False, drop_path_rate=0.0)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.ParallelTransformer._get_recompute_nums": {
        "signature": "(self, recompute_config)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.ParallelTransformer._set_checkpointed_recompute": {
        "signature": "(self, recompute_method, recompute_num_layers)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.ParallelTransformer.set_input_tensor": {
        "signature": "(self, input_tensor)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.ParallelTransformer.construct": {
        "signature": "(self, hidden_states, attention_mask, encoder_output=None, enc_dec_attn_mask=None, retriever_input=None, retriever_output=None, retriever_attn_mask=None, inference_params=None, rotary_pos_emb=None)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.ParallelTransformerLayer": {
        "signature": "(config, layer_number, layer_type=<LayerType.encoder: 1>, self_attn_mask_type=<AttnMaskType.padding: 1>, drop_path_rate=0.0)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.ParallelTransformerLayer._set_selective_recompute": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.ParallelTransformerLayer.construct": {
        "signature": "(self, hidden_states, attention_mask, encoder_output=None, enc_dec_attn_mask=None, retriever_input=None, retriever_output=None, retriever_attn_mask=None, inference_params=None, rotary_pos_emb=None)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.RotaryEmbedding": {
        "signature": "(kv_channels, rotary_percent=1.0, rotary_interleaved=False, seq_len_interpolation_factor=None, rotary_base=10000)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.RotaryEmbedding.construct": {
        "signature": "(self, max_seq_len, offset=0)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.TransformerLanguageModel": {
        "signature": "(config, encoder_attn_mask_type, num_tokentypes=0, add_encoder=True, add_decoder=False, decoder_attn_mask_type=<AttnMaskType.causal: 2>, add_pooler=False, pre_process=True, post_process=True, visual_encoder=None, **kwargs)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.TransformerLanguageModel.set_input_tensor": {
        "signature": "(self, input_tensor)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.TransformerLanguageModel.visual_forward": {
        "signature": "(self, input_image)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.TransformerLanguageModel.mixed_embedding": {
        "signature": "(self, text_embedding, image_embedding, delimiter_position)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.TransformerLanguageModel.construct": {
        "signature": "(self, enc_input_ids, enc_position_ids, enc_attn_mask, dec_input_ids=None, dec_position_ids=None, dec_attn_mask=None, retriever_input_ids=None, retriever_position_ids=None, retriever_attn_mask=None, enc_dec_attn_mask=None, tokentype_ids=None, inference_params=None, pooling_sequence_index=0, enc_hidden_states=None, output_enc_hidden=False, input_image=None, delimiter_position=None, image_embedding=None, prefix_keys_values=None)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.VocabParallelEmbedding": {
        "signature": "(num_embeddings, embedding_dim, *, init_method, reduce_scatter_embeddings=False, config, param_init_dtype=None)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.VocabParallelEmbedding.construct": {
        "signature": "(self, input_)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.VocabParallelEmbedding._vocab_range_from_global_vocab_size": {
        "signature": "(self, global_vocab_size, rank, world_size)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.VocabParallelEmbedding.sharded_state_dict": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.apply_rotary_pos_emb": {
        "signature": "(t, freqs, config, cu_seqlens=None) -> mindspore.common.tensor.Tensor"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.get_attention_mask": {
        "signature": "(input_mask, data_layout='BSH')"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.activation.get_act_func": {
        "signature": "(activation_type, **kwargs)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.activation.get_act_func_gated_version": {
        "signature": "(activation_type)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.mlp.ParallelMLP": {
        "signature": "(config, is_expert=False)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.mlp.ParallelMLP.construct": {
        "signature": "(self, hidden_states)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.moe.MoELayer": {
        "signature": "(config: mindformers.experimental.parallel_core.pynative.config.TransformerConfig, submodules=None, layer_number: int = None)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.moe.MoELayer.construct": {
        "signature": "(self, hidden_states: mindspore.common.tensor.Tensor)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.moe.SequentialMLP": {
        "signature": "(num_local_experts: int, config: mindformers.experimental.parallel_core.pynative.config.TransformerConfig, submodules=None)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.moe.SequentialMLP.construct": {
        "signature": "(self, permuted_local_hidden_states, tokens_per_expert)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.moe.TopKRouter": {
        "signature": "(config: mindformers.experimental.parallel_core.pynative.config.TransformerConfig)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.moe.TopKRouter.construct": {
        "signature": "(self, input: mindspore.common.tensor.Tensor)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.moe.TopKRouter.aux_loss_load_balancing": {
        "signature": "(self, logits)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.moe.TopKRouter.apply_load_balancing_loss": {
        "signature": "(self, probs, num_local_tokens_per_expert, activation)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.moe.TopKRouter.apply_z_loss": {
        "signature": "(self, logits)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.norm.get_norm": {
        "signature": "(config, scale=1.0)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.rotary_pos_embedding.RotaryEmbedding": {
        "signature": "(kv_channels, rotary_percent=1.0, rotary_interleaved=False, seq_len_interpolation_factor=None, rotary_base=10000)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.rotary_pos_embedding.RotaryEmbedding.construct": {
        "signature": "(self, max_seq_len, offset=0)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.rotary_pos_embedding.apply_rotary_pos_emb": {
        "signature": "(t, freqs, config, cu_seqlens=None) -> mindspore.common.tensor.Tensor"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.transformer.ParallelAttention": {
        "signature": "(config, layer_number, attention_type=<AttnType.self_attn: 1>, attn_mask_type=<AttnMaskType.padding: 1>)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.transformer.ParallelAttention._init_qkv_proj": {
        "signature": "(self, input_size, output_size, cell_name)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.transformer.ParallelAttention._init_out_proj": {
        "signature": "(self, input_size, output_size, cell_name, input_is_parallel=True)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.transformer.ParallelAttention.construct": {
        "signature": "(self, hidden_states, attention_mask, encoder_output=None, inference_params=None, rotary_pos_emb=None)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.transformer.ParallelLMLogits": {
        "signature": "(config, bias=False, compute_dtype=None)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.transformer.ParallelLMLogits.construct": {
        "signature": "(self, input_, word_embeddings_weight, parallel_output=True, bias=None)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.transformer.ParallelMLP": {
        "signature": "(config, is_expert=False)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.transformer.ParallelMLP.construct": {
        "signature": "(self, hidden_states)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.transformer.ParallelTransformer": {
        "signature": "(config, model_type, layer_type=<LayerType.encoder: 1>, self_attn_mask_type=<AttnMaskType.padding: 1>, post_norm=True, pre_process=False, post_process=False, drop_path_rate=0.0)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.transformer.ParallelTransformer._get_recompute_nums": {
        "signature": "(self, recompute_config)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.transformer.ParallelTransformer._set_checkpointed_recompute": {
        "signature": "(self, recompute_method, recompute_num_layers)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.transformer.ParallelTransformer.set_input_tensor": {
        "signature": "(self, input_tensor)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.transformer.ParallelTransformer.construct": {
        "signature": "(self, hidden_states, attention_mask, encoder_output=None, enc_dec_attn_mask=None, retriever_input=None, retriever_output=None, retriever_attn_mask=None, inference_params=None, rotary_pos_emb=None)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.transformer.ParallelTransformerLayer": {
        "signature": "(config, layer_number, layer_type=<LayerType.encoder: 1>, self_attn_mask_type=<AttnMaskType.padding: 1>, drop_path_rate=0.0)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.transformer.ParallelTransformerLayer._set_selective_recompute": {
        "signature": "(self)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.transformer.ParallelTransformerLayer.construct": {
        "signature": "(self, hidden_states, attention_mask, encoder_output=None, enc_dec_attn_mask=None, retriever_input=None, retriever_output=None, retriever_attn_mask=None, inference_params=None, rotary_pos_emb=None)"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.transformer.get_attention_mask": {
        "signature": "(input_mask, data_layout='BSH')"
    },
    "mindformers.experimental.parallel_core.pynative.transformer.utils.get_attn_mask_func": {
        "signature": "(mask_func_type)"
    },
    "mindformers.experimental.utils.init_method_normal": {
        "signature": "(sigma: float = 0.01, param_init_dtype: mindspore.common.dtype = mindspore.float32)"
    },
    "mindformers.experimental.utils.init_method_zero": {
        "signature": "(param_init_dtype: mindspore.common.dtype = mindspore.float32)"
    },
    "mindformers.experimental.utils.scaled_init_method_normal": {
        "signature": "(sigma: float, num_layers: int, param_init_dtype: mindspore.common.dtype = mindspore.float32)"
    },
    "mindformers.generation.GenerationConfig": {
        "signature": "(**kwargs)"
    },
    "mindformers.generation.GenerationConfig.from_dict": {
        "signature": "(config_dict: Dict[str, Any], **kwargs)"
    },
    "mindformers.generation.GenerationConfig.from_model_config": {
        "signature": "(model_config: mindformers.models.configuration_utils.PretrainedConfig) -> 'GenerationConfig'"
    },
    "mindformers.generation.GenerationConfig.update": {
        "signature": "(self, **kwargs)"
    },
    "mindformers.generation.GenerationConfig.to_dict": {
        "signature": "(self) -> Dict[str, Any]"
    },
    "mindformers.generation.GenerationMixin": {
        "signature": "()"
    },
    "mindformers.generation.GenerationMixin._set_network_phase": {
        "signature": "(self, phase)"
    },
    "mindformers.generation.GenerationMixin._set_block_mgr": {
        "signature": "(self, batch_size, seq_length)"
    },
    "mindformers.generation.GenerationMixin._prepare_inputs_for_prefill_flatten": {
        "signature": "(self, input_ids, batch_valid_length, slot_mapping, model_inputs)"
    },
    "mindformers.generation.GenerationMixin.prepare_inputs_for_generation": {
        "signature": "(self, input_ids, **kwargs)"
    },
    "mindformers.generation.GenerationMixin.add_flags_custom": {
        "signature": "(self, is_first_iteration)"
    },
    "mindformers.generation.GenerationMixin.update_model_kwargs_before_generate": {
        "signature": "(self, input_ids, model_kwargs: dict)"
    },
    "mindformers.generation.GenerationMixin.slice_incremental_inputs": {
        "signature": "(self, model_inputs: dict, current_index)"
    },
    "mindformers.generation.GenerationMixin.process_logits": {
        "signature": "(self, logits, current_index=None, keep_all=False)"
    },
    "mindformers.generation.GenerationMixin.get_logits_processor": {
        "signature": "(self, generation_config: mindformers.generation.generation_config.GenerationConfig, input_ids_seq_length: int, logits_processor: Optional[mindformers.generation.logits_process.LogitsProcessorList])"
    },
    "mindformers.generation.GenerationMixin._merge_processor_list": {
        "signature": "(self, default_list: mindformers.generation.logits_process.LogitsProcessorList, custom_list: mindformers.generation.logits_process.LogitsProcessorList)"
    },
    "mindformers.generation.GenerationMixin.get_logits_warper": {
        "signature": "(self, generation_config: mindformers.generation.generation_config.GenerationConfig)"
    },
    "mindformers.generation.GenerationMixin._get_generation_mode": {
        "signature": "(self, generation_config: mindformers.generation.generation_config.GenerationConfig)"
    },
    "mindformers.generation.GenerationMixin._prepare_model_inputs_for_decoder": {
        "signature": "(self, input_ids, input_mask)"
    },
    "mindformers.generation.GenerationMixin._pad_inputs_using_max_length": {
        "signature": "(self, origin_inputs, pad_token_id=0)"
    },
    "mindformers.generation.GenerationMixin._incremental_infer": {
        "signature": "(self, model_inputs: dict, prefill, current_index, key_cache=None, value_cache=None)"
    },
    "mindformers.generation.GenerationMixin._beam_search": {
        "signature": "(self, origin_inputs, beam_scorer: mindformers.generation.beam_search.BeamSearchScorer, generation_config: mindformers.generation.generation_config.GenerationConfig, logits_processor: Optional[mindformers.generation.logits_process.LogitsProcessorList] = None, streamer: mindformers.generation.streamers.BaseStreamer = None, **model_kwargs)"
    },
    "mindformers.generation.GenerationMixin.generate": {
        "signature": "(self, input_ids: Union[List[List[int]], List[int], NoneType], generation_config: Optional[mindformers.generation.generation_config.GenerationConfig] = None, logits_processor: Optional[mindformers.generation.logits_process.LogitsProcessorList] = None, streamer: Optional[mindformers.generation.streamers.BaseStreamer] = None, seed: Optional[int] = None, **kwargs)"
    },
    "mindformers.generation.GenerationMixin.infer": {
        "signature": "(self, input_ids: Union[List[List[int]], List[int]], valid_length_each_example: numpy.ndarray, generation_config: mindformers.generation.generation_config.GenerationConfig = None, logits_processor: Optional[mindformers.generation.logits_process.LogitsProcessorList] = None, logits_warper: Optional[mindformers.generation.logits_process.LogitsProcessorList] = None, block_tables: Optional[mindspore.common.tensor.Tensor] = None, slot_mapping: Optional[mindspore.common.tensor.Tensor] = None, prefill: bool = True, is_finished: List[bool] = None, encoder_mask: Optional[mindspore.common.tensor.Tensor] = None, encoder_output: Optional[mindspore.common.tensor.Tensor] = None, target_mask: Optional[mindspore.common.tensor.Tensor] = None, **model_kwargs)"
    },
    "mindformers.generation.GenerationMixin.forward": {
        "signature": "(self, input_ids: [typing.Union[typing.List[int], typing.List[typing.List[int]]]], valid_length_each_example: numpy.ndarray, block_tables: Optional[mindspore.common.tensor.Tensor] = None, slot_mapping: Optional[mindspore.common.tensor.Tensor] = None, prefill: bool = None, use_past: bool = False, encoder_mask: Optional[mindspore.common.tensor.Tensor] = None, encoder_output: Optional[mindspore.common.tensor.Tensor] = None, target_mask: Optional[mindspore.common.tensor.Tensor] = None, key_cache: Optional[List[mindspore.common.tensor.Tensor]] = None, value_cache: Optional[List[mindspore.common.tensor.Tensor]] = None, **model_kwargs)"
    },
    "mindformers.generation.GenerationMixin.chunk_prefill_infer": {
        "signature": "(self, input_ids: [typing.Union[typing.List[int], typing.List[typing.List[int]]]], batch_valid_length: numpy.ndarray, block_tables: numpy.ndarray, slot_mapping: numpy.ndarray, attention_mask: Optional[numpy.ndarray] = None, **model_kwargs)"
    },
    "mindformers.generation.GenerationMixin.postprocess": {
        "signature": "(self, input_ids, is_finished, res, generation_config: mindformers.generation.generation_config.GenerationConfig, valid_length_each_example, current_index: Union[List[List[int]], List[int], NoneType], logits_processor: Optional[mindformers.generation.logits_process.LogitsProcessorList] = None, logits_warper: Optional[mindformers.generation.logits_process.LogitsProcessorList] = None, need_gather_logits: bool = True)"
    },
    "mindformers.generation.GenerationMixin.chat": {
        "signature": "(self, tokenizer: mindformers.models.tokenization_utils.PreTrainedTokenizer, query: str, history: Optional[List[Dict[str, str]]] = None, system_role_name: Optional[str] = 'system', user_role_name: Optional[str] = 'user', assistant_role_name: Optional[str] = 'assistant', instruction: Optional[str] = '', max_length: Optional[int] = 512, max_new_tokens: Optional[int] = None, min_length: Optional[int] = 0, min_new_tokens: Optional[int] = None, do_sample: Optional[bool] = True, temperature: Optional[float] = 1.0, top_k: Optional[int] = 50, top_p: Optional[float] = 1.0, repetition_penalty: Optional[float] = 1.0)"
    },
    "mindformers.generation.generation_config.GenerationConfig": {
        "signature": "(**kwargs)"
    },
    "mindformers.generation.generation_config.GenerationConfig.from_dict": {
        "signature": "(config_dict: Dict[str, Any], **kwargs)"
    },
    "mindformers.generation.generation_config.GenerationConfig.from_model_config": {
        "signature": "(model_config: mindformers.models.configuration_utils.PretrainedConfig) -> 'GenerationConfig'"
    },
    "mindformers.generation.generation_config.GenerationConfig.update": {
        "signature": "(self, **kwargs)"
    },
    "mindformers.generation.generation_config.GenerationConfig.to_dict": {
        "signature": "(self) -> Dict[str, Any]"
    },
    "mindformers.generation.logits_process.FrequencyPenaltyLogitsProcessor": {
        "signature": "(frequency_penalty: float = None, output_tokens_counts=None)"
    },
    "mindformers.generation.logits_process.FrequencyPenaltyLogitsProcessor.process_ms": {
        "signature": "(self, logits, sequence_ids, **kwargs)"
    },
    "mindformers.generation.logits_process.FrequencyPenaltyLogitsProcessor.process_np": {
        "signature": "(self, logits, sequence_ids)"
    },
    "mindformers.generation.logits_process.GreedySearchLogitsProcessor": {
        "signature": "()"
    },
    "mindformers.generation.logits_process.GreedySearchLogitsProcessor.process_ms": {
        "signature": "(self, logits, sequence_ids, **kwargs)"
    },
    "mindformers.generation.logits_process.GreedySearchLogitsProcessor.process_np": {
        "signature": "(self, logits, sequence_ids)"
    },
    "mindformers.generation.logits_process.LogitNormalization": {
        "signature": "()"
    },
    "mindformers.generation.logits_process.LogitNormalization.process_ms": {
        "signature": "(self, logits, sequence_ids, **kwargs)"
    },
    "mindformers.generation.logits_process.LogitNormalization.process_np": {
        "signature": "(self, logits, sequence_ids)"
    },
    "mindformers.generation.logits_process.LogitsProcessor": {
        "signature": "()"
    },
    "mindformers.generation.logits_process.LogitsProcessor.selected_scatter": {
        "signature": "(self, input_tensor, dim, index, src)"
    },
    "mindformers.generation.logits_process.LogitsProcessor.process_ms": {
        "signature": "(self, logits, sequence_ids, **kwargs)"
    },
    "mindformers.generation.logits_process.LogitsProcessor.process_np": {
        "signature": "(self, logits, sequence_ids)"
    },
    "mindformers.generation.logits_process.LogitsProcessor.check_params": {
        "signature": "(params, params_name, force_float=False, force_int=False, force_int_tuple=False, low_threshold=None, high_threshold=None)"
    },
    "mindformers.generation.logits_process.LogitsProcessorList": {
        "signature": "()"
    },
    "mindformers.generation.logits_process.LogitsProcessorList.process_ms": {
        "signature": "(self, input_ids, scores, **kwargs)"
    },
    "mindformers.generation.logits_process.LogitsProcessorList.process": {
        "signature": "(self, i, input_ids, scores)"
    },
    "mindformers.generation.logits_process.MinLengthLogitsProcessor": {
        "signature": "(min_length: int = None, eos_token_id: Union[List[int], int] = None, pad_token_id: int = None)"
    },
    "mindformers.generation.logits_process.MinLengthLogitsProcessor.process_ms": {
        "signature": "(self, logits, sequence_ids, **kwargs)"
    },
    "mindformers.generation.logits_process.MinLengthLogitsProcessor.process_np": {
        "signature": "(self, logits, sequence_ids)"
    },
    "mindformers.generation.logits_process.MinNewTokensLengthLogitsProcessor": {
        "signature": "(prompt_length_to_skip: int = None, min_new_tokens: int = None, eos_token_id: Union[List[int], int] = None, pad_token_id: int = None)"
    },
    "mindformers.generation.logits_process.MinNewTokensLengthLogitsProcessor.process_ms": {
        "signature": "(self, logits, sequence_ids, **kwargs)"
    },
    "mindformers.generation.logits_process.MinNewTokensLengthLogitsProcessor.process_np": {
        "signature": "(self, logits, sequence_ids)"
    },
    "mindformers.generation.logits_process.PresencePenaltyLogitsProcessor": {
        "signature": "(presence_penalty: float = None, output_tokens_mask=None)"
    },
    "mindformers.generation.logits_process.PresencePenaltyLogitsProcessor.process_ms": {
        "signature": "(self, logits, sequence_ids, **kwargs)"
    },
    "mindformers.generation.logits_process.PresencePenaltyLogitsProcessor.process_np": {
        "signature": "(self, logits, sequence_ids)"
    },
    "mindformers.generation.logits_process.RepetitionPenaltyLogitsProcessor": {
        "signature": "(repetition_penalty: float = None)"
    },
    "mindformers.generation.logits_process.RepetitionPenaltyLogitsProcessor.process_ms": {
        "signature": "(self, logits, sequence_ids, **kwargs)"
    },
    "mindformers.generation.logits_process.RepetitionPenaltyLogitsProcessor.process_np": {
        "signature": "(self, logits, sequence_ids)"
    },
    "mindformers.generation.logits_process.SamplingLogitsProcessor": {
        "signature": "(do_sample=None, seed_array=None)"
    },
    "mindformers.generation.logits_process.SamplingLogitsProcessor.process_ms": {
        "signature": "(self, logits, sequence_ids, **kwargs)"
    },
    "mindformers.generation.logits_process.SamplingLogitsProcessor.multinomial_ms": {
        "signature": "(prob_matrix, num_samples, seeds)"
    },
    "mindformers.generation.logits_process.SamplingLogitsProcessor.process_np": {
        "signature": "(self, logits, sequence_ids)"
    },
    "mindformers.generation.logits_process.SamplingLogitsProcessor.multinomial_np": {
        "signature": "(prob_matrix, num_samples, seeds)"
    },
    "mindformers.generation.logits_process.TemperatureLogitsWarper": {
        "signature": "(temperature: float = None)"
    },
    "mindformers.generation.logits_process.TemperatureLogitsWarper.process_ms": {
        "signature": "(self, logits, sequence_ids, **kwargs)"
    },
    "mindformers.generation.logits_process.TemperatureLogitsWarper.process_np": {
        "signature": "(self, logits, sequence_ids)"
    },
    "mindformers.generation.logits_process.TopKLogitsWarper": {
        "signature": "(top_k: int = None, filter_value: float = -50000, min_tokens_to_keep: int = 1)"
    },
    "mindformers.generation.logits_process.TopKLogitsWarper.process_ms": {
        "signature": "(self, logits, sequence_ids, **kwargs)"
    },
    "mindformers.generation.logits_process.TopKLogitsWarper.process_np": {
        "signature": "(self, logits, sequence_ids)"
    },
    "mindformers.generation.logits_process.TopPLogitsWarper": {
        "signature": "(top_p: float = None, filter_value: float = -50000, min_tokens_to_keep: int = 1)"
    },
    "mindformers.generation.logits_process.TopPLogitsWarper.process_ms": {
        "signature": "(self, logits, sequence_ids, **kwargs)"
    },
    "mindformers.generation.logits_process.TopPLogitsWarper.process_np": {
        "signature": "(self, logits, sequence_ids)"
    },
    "mindformers.generation.streamers.BaseStreamer": {
        "signature": "()"
    },
    "mindformers.generation.streamers.BaseStreamer.put": {
        "signature": "(self, value)"
    },
    "mindformers.generation.streamers.BaseStreamer.end": {
        "signature": "(self)"
    },
    "mindformers.generation.streamers.TextIteratorStreamer": {
        "signature": "(tokenizer: Optional[mindformers.models.tokenization_utils_base.PreTrainedTokenizerBase] = None, skip_prompt: bool = False, timeout: Optional[float] = None, **decode_kwargs)"
    },
    "mindformers.generation.streamers.TextIteratorStreamer.on_finalized_text": {
        "signature": "(self, text: str, stream_end: bool = False)"
    },
    "mindformers.generation.streamers.TextIteratorStreamer.clear": {
        "signature": "(self)"
    },
    "mindformers.generation.streamers.TextStreamer": {
        "signature": "(tokenizer: Optional[mindformers.models.tokenization_utils_base.PreTrainedTokenizerBase] = None, skip_prompt: bool = False, skip_special_tokens: bool = True, **decode_kwargs)"
    },
    "mindformers.generation.streamers.TextStreamer.put": {
        "signature": "(self, value)"
    },
    "mindformers.generation.streamers.TextStreamer.get_printable_text": {
        "signature": "(self, text)"
    },
    "mindformers.generation.streamers.TextStreamer.end": {
        "signature": "(self)"
    },
    "mindformers.generation.streamers.TextStreamer.on_finalized_text": {
        "signature": "(self, text: str, stream_end: bool = False)"
    },
    "mindformers.generation.streamers.TextStreamer._is_chinese_char": {
        "signature": "(self, cp)"
    },
    "mindformers.generation.text_generator.GenerationMixin": {
        "signature": "()"
    },
    "mindformers.generation.text_generator.GenerationMixin._set_network_phase": {
        "signature": "(self, phase)"
    },
    "mindformers.generation.text_generator.GenerationMixin._set_block_mgr": {
        "signature": "(self, batch_size, seq_length)"
    },
    "mindformers.generation.text_generator.GenerationMixin._prepare_inputs_for_prefill_flatten": {
        "signature": "(self, input_ids, batch_valid_length, slot_mapping, model_inputs)"
    },
    "mindformers.generation.text_generator.GenerationMixin.prepare_inputs_for_generation": {
        "signature": "(self, input_ids, **kwargs)"
    },
    "mindformers.generation.text_generator.GenerationMixin.add_flags_custom": {
        "signature": "(self, is_first_iteration)"
    },
    "mindformers.generation.text_generator.GenerationMixin.update_model_kwargs_before_generate": {
        "signature": "(self, input_ids, model_kwargs: dict)"
    },
    "mindformers.generation.text_generator.GenerationMixin.slice_incremental_inputs": {
        "signature": "(self, model_inputs: dict, current_index)"
    },
    "mindformers.generation.text_generator.GenerationMixin.process_logits": {
        "signature": "(self, logits, current_index=None, keep_all=False)"
    },
    "mindformers.generation.text_generator.GenerationMixin.get_logits_processor": {
        "signature": "(self, generation_config: mindformers.generation.generation_config.GenerationConfig, input_ids_seq_length: int, logits_processor: Optional[mindformers.generation.logits_process.LogitsProcessorList])"
    },
    "mindformers.generation.text_generator.GenerationMixin._merge_processor_list": {
        "signature": "(self, default_list: mindformers.generation.logits_process.LogitsProcessorList, custom_list: mindformers.generation.logits_process.LogitsProcessorList)"
    },
    "mindformers.generation.text_generator.GenerationMixin.get_logits_warper": {
        "signature": "(self, generation_config: mindformers.generation.generation_config.GenerationConfig)"
    },
    "mindformers.generation.text_generator.GenerationMixin._get_generation_mode": {
        "signature": "(self, generation_config: mindformers.generation.generation_config.GenerationConfig)"
    },
    "mindformers.generation.text_generator.GenerationMixin._prepare_model_inputs_for_decoder": {
        "signature": "(self, input_ids, input_mask)"
    },
    "mindformers.generation.text_generator.GenerationMixin._pad_inputs_using_max_length": {
        "signature": "(self, origin_inputs, pad_token_id=0)"
    },
    "mindformers.generation.text_generator.GenerationMixin._incremental_infer": {
        "signature": "(self, model_inputs: dict, prefill, current_index, key_cache=None, value_cache=None)"
    },
    "mindformers.generation.text_generator.GenerationMixin._beam_search": {
        "signature": "(self, origin_inputs, beam_scorer: mindformers.generation.beam_search.BeamSearchScorer, generation_config: mindformers.generation.generation_config.GenerationConfig, logits_processor: Optional[mindformers.generation.logits_process.LogitsProcessorList] = None, streamer: mindformers.generation.streamers.BaseStreamer = None, **model_kwargs)"
    },
    "mindformers.generation.text_generator.GenerationMixin.generate": {
        "signature": "(self, input_ids: Union[List[List[int]], List[int], NoneType], generation_config: Optional[mindformers.generation.generation_config.GenerationConfig] = None, logits_processor: Optional[mindformers.generation.logits_process.LogitsProcessorList] = None, streamer: Optional[mindformers.generation.streamers.BaseStreamer] = None, seed: Optional[int] = None, **kwargs)"
    },
    "mindformers.generation.text_generator.GenerationMixin.infer": {
        "signature": "(self, input_ids: Union[List[List[int]], List[int]], valid_length_each_example: numpy.ndarray, generation_config: mindformers.generation.generation_config.GenerationConfig = None, logits_processor: Optional[mindformers.generation.logits_process.LogitsProcessorList] = None, logits_warper: Optional[mindformers.generation.logits_process.LogitsProcessorList] = None, block_tables: Optional[mindspore.common.tensor.Tensor] = None, slot_mapping: Optional[mindspore.common.tensor.Tensor] = None, prefill: bool = True, is_finished: List[bool] = None, encoder_mask: Optional[mindspore.common.tensor.Tensor] = None, encoder_output: Optional[mindspore.common.tensor.Tensor] = None, target_mask: Optional[mindspore.common.tensor.Tensor] = None, **model_kwargs)"
    },
    "mindformers.generation.text_generator.GenerationMixin.forward": {
        "signature": "(self, input_ids: [typing.Union[typing.List[int], typing.List[typing.List[int]]]], valid_length_each_example: numpy.ndarray, block_tables: Optional[mindspore.common.tensor.Tensor] = None, slot_mapping: Optional[mindspore.common.tensor.Tensor] = None, prefill: bool = None, use_past: bool = False, encoder_mask: Optional[mindspore.common.tensor.Tensor] = None, encoder_output: Optional[mindspore.common.tensor.Tensor] = None, target_mask: Optional[mindspore.common.tensor.Tensor] = None, key_cache: Optional[List[mindspore.common.tensor.Tensor]] = None, value_cache: Optional[List[mindspore.common.tensor.Tensor]] = None, **model_kwargs)"
    },
    "mindformers.generation.text_generator.GenerationMixin.chunk_prefill_infer": {
        "signature": "(self, input_ids: [typing.Union[typing.List[int], typing.List[typing.List[int]]]], batch_valid_length: numpy.ndarray, block_tables: numpy.ndarray, slot_mapping: numpy.ndarray, attention_mask: Optional[numpy.ndarray] = None, **model_kwargs)"
    },
    "mindformers.generation.text_generator.GenerationMixin.postprocess": {
        "signature": "(self, input_ids, is_finished, res, generation_config: mindformers.generation.generation_config.GenerationConfig, valid_length_each_example, current_index: Union[List[List[int]], List[int], NoneType], logits_processor: Optional[mindformers.generation.logits_process.LogitsProcessorList] = None, logits_warper: Optional[mindformers.generation.logits_process.LogitsProcessorList] = None, need_gather_logits: bool = True)"
    },
    "mindformers.generation.text_generator.GenerationMixin.chat": {
        "signature": "(self, tokenizer: mindformers.models.tokenization_utils.PreTrainedTokenizer, query: str, history: Optional[List[Dict[str, str]]] = None, system_role_name: Optional[str] = 'system', user_role_name: Optional[str] = 'user', assistant_role_name: Optional[str] = 'assistant', instruction: Optional[str] = '', max_length: Optional[int] = 512, max_new_tokens: Optional[int] = None, min_length: Optional[int] = 0, min_new_tokens: Optional[int] = None, do_sample: Optional[bool] = True, temperature: Optional[float] = 1.0, top_k: Optional[int] = 50, top_p: Optional[float] = 1.0, repetition_penalty: Optional[float] = 1.0)"
    },
    "mindformers.model_runner.ModelRunner": {
        "signature": "(model_path, npu_mem_size, cpu_mem_size, block_size, rank_id=0, world_size=1, npu_device_ids=None, plugin_params=None)"
    },
    "mindformers.models.AutoConfig": {
        "signature": "()"
    },
    "mindformers.models.AutoConfig.invalid_yaml_name": {
        "signature": "(yaml_name_or_path)"
    },
    "mindformers.models.AutoConfig.for_model": {
        "signature": "(model_type: str, *args, **kwargs)"
    },
    "mindformers.models.AutoConfig.from_pretrained": {
        "signature": "(yaml_name_or_path, **kwargs)"
    },
    "mindformers.models.AutoConfig.get_config_origin_mode": {
        "signature": "(yaml_name_or_path, **kwargs)"
    },
    "mindformers.models.AutoConfig.get_config_experimental_mode": {
        "signature": "(*args, **kwargs)"
    },
    "mindformers.models.AutoConfig.show_support_list": {
        "signature": "()"
    },
    "mindformers.models.AutoConfig.get_support_list": {
        "signature": "()"
    },
    "mindformers.models.AutoConfig.register": {
        "signature": "(model_type, config, exist_ok=False)"
    },
    "mindformers.models.AutoModel": {
        "signature": "(*args, **kwargs)"
    },
    "mindformers.models.AutoModel.from_config": {
        "signature": "(**kwargs)"
    },
    "mindformers.models.AutoModel.from_pretrained": {
        "signature": "(*model_args, **kwargs)"
    },
    "mindformers.models.AutoModelForCausalLM": {
        "signature": "(*args, **kwargs)"
    },
    "mindformers.models.AutoModelForCausalLM.from_config": {
        "signature": "(**kwargs)"
    },
    "mindformers.models.AutoModelForCausalLM.from_pretrained": {
        "signature": "(*model_args, **kwargs)"
    },
    "mindformers.models.AutoProcessor": {
        "signature": "()"
    },
    "mindformers.models.AutoProcessor.invalid_yaml_name": {
        "signature": "(yaml_name_or_path)"
    },
    "mindformers.models.AutoProcessor.from_pretrained": {
        "signature": "(yaml_name_or_path, **kwargs)"
    },
    "mindformers.models.AutoProcessor.from_pretrained_origin": {
        "signature": "(yaml_name_or_path, **kwargs)"
    },
    "mindformers.models.AutoProcessor.from_pretrained_experimental": {
        "signature": "(*args, **kwargs)"
    },
    "mindformers.models.AutoProcessor.register": {
        "signature": "(config_class, processor_class, exist_ok=False)"
    },
    "mindformers.models.AutoProcessor.show_support_list": {
        "signature": "()"
    },
    "mindformers.models.AutoProcessor.get_support_list": {
        "signature": "()"
    },
    "mindformers.models.AutoTokenizer": {
        "signature": "()"
    },
    "mindformers.models.AutoTokenizer.invalid_yaml_name": {
        "signature": "(yaml_name_or_path)"
    },
    "mindformers.models.AutoTokenizer._get_class_name_from_yaml": {
        "signature": "(yaml_name_or_path)"
    },
    "mindformers.models.AutoTokenizer.from_pretrained": {
        "signature": "(yaml_name_or_path, *args, **kwargs)"
    },
    "mindformers.models.AutoTokenizer.get_class_from_origin_mode": {
        "signature": "(yaml_name_or_path, **kwargs)"
    },
    "mindformers.models.AutoTokenizer.get_class_from_experimental_mode": {
        "signature": "(*args, **kwargs)"
    },
    "mindformers.models.AutoTokenizer.register": {
        "signature": "(config_class, slow_tokenizer_class=None, fast_tokenizer_class=None, exist_ok=False)"
    },
    "mindformers.models.AutoTokenizer.show_support_list": {
        "signature": "()"
    },
    "mindformers.models.AutoTokenizer.get_support_list": {
        "signature": "(cls)"
    },
    "mindformers.models.ChatGLM2Config": {
        "signature": "(batch_size=1, num_layers=28, padded_vocab_size=65024, hidden_size=4096, ffn_hidden_size=13696, kv_channels=128, num_attention_heads=32, seq_length=2048, hidden_dropout=0.0, attention_dropout=0.0, layernorm_epsilon=1e-05, rope_ratio=1, rmsnorm=True, apply_residual_connection_post_layernorm=False, post_layer_norm=True, add_bias_linear=False, add_qkv_bias=True, bias_dropout_fusion=True, multi_query_attention=True, multi_query_group_num=2, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=True, fp32_residual_connection=False, quantization_bit=0, pre_seq_len=None, prefix_projection=False, param_init_type: str = 'float16', compute_dtype: str = 'float16', layernorm_compute_type: str = 'float32', residual_dtype: str = 'float32', rotary_dtype: str = None, use_past=False, use_flash_attention=False, enable_high_performance=False, block_size=16, num_blocks=128, is_dynamic=False, eos_token_id=2, pad_token_id=0, gmask_token_id=None, bos_token_id=None, repetition_penalty=1.0, checkpoint_name_or_path=None, parallel_config: Union[dict, mindformers.modules.transformer.transformer.TransformerOpParallelConfig] = <mindformers.modules.transformer.transformer.TransformerOpParallelConfig object>, offset: int = 0, pp_interleave_num: int = 1, mlp_concat: bool = True, qkv_concat: bool = True, use_rearrange_rope: bool = False, mask_generate: str = None, fine_grain_interleave: int = 1, use_ring_attention: bool = False, **kwargs)"
    },
    "mindformers.models.ChatGLM2ForConditionalGeneration": {
        "signature": "(config: mindformers.models.glm2.glm2_config.ChatGLM2Config, **kwargs)"
    },
    "mindformers.models.ChatGLM2ForConditionalGeneration.prepare_inputs_for_predict_layout": {
        "signature": "(self, input_ids, **kwargs)"
    },
    "mindformers.models.ChatGLM2ForConditionalGeneration.set_dynamic_inputs": {
        "signature": "(self, **kwargs)"
    },
    "mindformers.models.ChatGLM2ForConditionalGeneration.add_flags_custom": {
        "signature": "(self, is_first_iteration)"
    },
    "mindformers.models.ChatGLM2ForConditionalGeneration.construct": {
        "signature": "(self, input_ids=None, labels=None, input_position=None, position_ids=None, attention_mask=None, input_embeds=None, init_reset=None, batch_valid_length=None, prefix_key_values=None, block_tables=None, slot_mapping=None, batch_index=None, zactivate_len=None, input_mask=None)"
    },
    "mindformers.models.ChatGLM2ForConditionalGeneration.clear_kv_cache": {
        "signature": "(self)"
    },
    "mindformers.models.ChatGLM3Tokenizer": {
        "signature": "(vocab_file, bos_token='<sop>', eos_token='<eop>', end_token='</s>', mask_token='[MASK]', gmask_token='[gMASK]', pad_token='<pad>', unk_token='<unk>', **kwargs)"
    },
    "mindformers.models.ChatGLM3Tokenizer.get_command": {
        "signature": "(self, token)"
    },
    "mindformers.models.ChatGLM3Tokenizer.get_vocab": {
        "signature": "(self)"
    },
    "mindformers.models.ChatGLM3Tokenizer.build_single_message": {
        "signature": "(self, role, metadata, message)"
    },
    "mindformers.models.ChatGLM3Tokenizer.build_chat_input": {
        "signature": "(self, query, history=None, role='user', return_tensors='np')"
    },
    "mindformers.models.ChatGLM3Tokenizer.build_batch_input": {
        "signature": "(self, queries, histories=None, roles='user', padding=True, return_tensors='np')"
    },
    "mindformers.models.ChatGLM3Tokenizer.tokenize": {
        "signature": "(self, text, pair=None, add_special_tokens=True, **kwargs)"
    },
    "mindformers.models.ChatGLM3Tokenizer._tokenize": {
        "signature": "(self, text, **kwargs)"
    },
    "mindformers.models.ChatGLM3Tokenizer._convert_token_to_id": {
        "signature": "(self, token)"
    },
    "mindformers.models.ChatGLM3Tokenizer.convert_tokens_to_ids": {
        "signature": "(self, tokens: List[str]) -> list"
    },
    "mindformers.models.ChatGLM3Tokenizer._decode": {
        "signature": "(self, token_ids: Union[List[int], int], skip_special_tokens: bool = False, clean_up_tokenization_spaces: bool = None, **kwargs) -> str"
    },
    "mindformers.models.ChatGLM3Tokenizer._convert_id_to_token": {
        "signature": "(self, index)"
    },
    "mindformers.models.ChatGLM3Tokenizer.convert_tokens_to_string": {
        "signature": "(self, tokens: List[str]) -> str"
    },
    "mindformers.models.ChatGLM3Tokenizer.save_vocabulary": {
        "signature": "(self, save_directory, filename_prefix=None)"
    },
    "mindformers.models.ChatGLM3Tokenizer.get_prefix_tokens": {
        "signature": "(self)"
    },
    "mindformers.models.ChatGLM3Tokenizer.build_inputs_with_special_tokens": {
        "signature": "(self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None) -> List[int]"
    },
    "mindformers.models.ChatGLM3Tokenizer._pad": {
        "signature": "(self, encoded_inputs: Union[Dict[str, List[int]], mindformers.models.tokenization_utils_base.BatchEncoding], max_length: Optional[int] = None, padding_strategy: mindformers.models.tokenization_utils_base.PaddingStrategy = <PaddingStrategy.DO_NOT_PAD: 'do_not_pad'>, pad_to_multiple_of: Optional[int] = None, return_attention_mask: Optional[bool] = None) -> dict"
    },
    "mindformers.models.ChatGLM3Tokenizer.apply_chat_template": {
        "signature": "(self, conversation, return_tensors=None, **tokenizer_kwargs)"
    },
    "mindformers.models.ChatGLM4Tokenizer": {
        "signature": "(vocab_file, clean_up_tokenization_spaces=False, encode_special_tokens=False, eos_token='<|endoftext|>', pad_token='<|endoftext|>', **kwargs)"
    },
    "mindformers.models.ChatGLM4Tokenizer.get_vocab": {
        "signature": "(self)"
    },
    "mindformers.models.ChatGLM4Tokenizer.convert_tokens_to_string": {
        "signature": "(self, tokens: List[Union[bytes, int, str]]) -> str"
    },
    "mindformers.models.ChatGLM4Tokenizer._tokenize": {
        "signature": "(self, text, **kwargs)"
    },
    "mindformers.models.ChatGLM4Tokenizer._convert_token_to_id": {
        "signature": "(self, token)"
    },
    "mindformers.models.ChatGLM4Tokenizer.convert_special_tokens_to_ids": {
        "signature": "(self, token)"
    },
    "mindformers.models.ChatGLM4Tokenizer._convert_id_to_token": {
        "signature": "(self, index)"
    },
    "mindformers.models.ChatGLM4Tokenizer.save_vocabulary": {
        "signature": "(self, save_directory, filename_prefix=None)"
    },
    "mindformers.models.ChatGLM4Tokenizer.get_prefix_tokens": {
        "signature": "(self)"
    },
    "mindformers.models.ChatGLM4Tokenizer.build_single_message": {
        "signature": "(self, role, metadata, message, tokenize=True)"
    },
    "mindformers.models.ChatGLM4Tokenizer.build_inputs_with_special_tokens": {
        "signature": "(self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None) -> List[int]"
    },
    "mindformers.models.ChatGLM4Tokenizer.build_batch_input": {
        "signature": "(self, queries, histories=None, roles='user', padding=True, return_tensors='np')"
    },
    "mindformers.models.ChatGLM4Tokenizer._pad": {
        "signature": "(self, encoded_inputs: Union[Dict[str, List[int]], mindformers.models.tokenization_utils_base.BatchEncoding], max_length: Optional[int] = None, padding_strategy: mindformers.models.tokenization_utils_base.PaddingStrategy = <PaddingStrategy.DO_NOT_PAD: 'do_not_pad'>, pad_to_multiple_of: Optional[int] = None, return_attention_mask: Optional[bool] = None) -> dict"
    },
    "mindformers.models.ChatGLM4Tokenizer.build_chat_input": {
        "signature": "(self, query, history=None, role='user', return_tensors='np')"
    },
    "mindformers.models.ChatGLM4Tokenizer.apply_chat_template": {
        "signature": "(self, conversation, return_tensors=None, **tokenizer_kwargs)"
    },
    "mindformers.models.LlamaConfig": {
        "signature": "(batch_size: int = 1, seq_length: int = 2048, hidden_size: int = 4096, num_layers: int = 32, num_heads: int = 32, n_kv_heads: Optional[int] = None, max_position_embedding: Optional[int] = None, intermediate_size: Optional[int] = None, vocab_size: int = 32000, multiple_of: int = 256, ffn_dim_multiplier: Optional[int] = None, rms_norm_eps: float = 1e-05, bos_token_id: int = 1, eos_token_id: int = 2, pad_token_id: int = 0, ignore_token_id: int = -100, theta: float = 10000.0, compute_dtype: str = 'float16', layernorm_compute_type: str = 'float32', softmax_compute_type: str = 'float32', rotary_dtype: str = 'float32', param_init_type: str = 'float16', residual_dtype: str = None, embedding_init_type=None, qkv_has_bias: bool = False, qkv_concat: bool = False, attn_proj_has_bias: bool = False, parallel_config: Union[dict, mindformers.modules.transformer.transformer.TransformerOpParallelConfig] = <mindformers.modules.transformer.transformer.TransformerOpParallelConfig object>, moe_config: Union[dict, mindformers.modules.transformer.moe.MoEConfig] = <mindformers.modules.transformer.moe.MoEConfig object>, use_past: bool = False, extend_method: str = 'None', scaling_factor: float = 1.0, is_dynamic: bool = False, use_rope_slice: bool = False, use_flash_attention: bool = False, use_ring_attention: bool = False, use_attn_mask_compression: bool = False, use_eod_attn_mask_compression: bool = False, parallel_optimizer: bool = False, fine_grain_interleave: int = 1, pp_interleave_num: int = 1, offset: int = 0, init_method_std: float = 0.01, checkpoint_name_or_path: str = '', repetition_penalty: float = 1.0, max_decode_length: int = 1024, block_size: int = 16, num_blocks: int = 512, top_k: int = 5, top_p: float = 1.0, do_sample: bool = True, quant_config: dict = None, tie_word_embeddings: bool = False, llm_backend: str = '', fused_rms_norm: bool = True, input_sliced_sig: bool = False, rmsnorm_compute_2d: bool = False, chunk_prefill: bool = False, calculate_per_token_loss: bool = False, pipeline_stage: dict = None, **kwargs)"
    },
    "mindformers.models.LlamaForCausalLM": {
        "signature": "(config: mindformers.models.llama.llama_config.LlamaConfig = None)"
    },
    "mindformers.models.LlamaForCausalLM.to_embeddings": {
        "signature": "(self, tokens)"
    },
    "mindformers.models.LlamaForCausalLM.prepare_inputs_for_predict_layout": {
        "signature": "(self, input_ids, **kwargs)"
    },
    "mindformers.models.LlamaForCausalLM.set_dynamic_inputs": {
        "signature": "(self, **kwargs)"
    },
    "mindformers.models.LlamaForCausalLM.add_flags_custom": {
        "signature": "(self, is_first_iteration)"
    },
    "mindformers.models.LlamaForCausalLM.pre_gather_func": {
        "signature": "(self, pre_gather, output, batch_valid_length, gather_index=None)"
    },
    "mindformers.models.LlamaForCausalLM.construct": {
        "signature": "(self, input_ids, labels=None, input_position=None, position_ids=None, attention_mask=None, input_embeds=None, init_reset=None, batch_valid_length=None, batch_index=None, zactivate_len=None, block_tables=None, slot_mapping=None, prefix_keys_values=None, llm_boost_inputs=None, q_seq_lens=None, loss_mask=None, gather_index=None, seq_range=None, actual_seq_len=None)"
    },
    "mindformers.models.LlamaForCausalLM.kvcache": {
        "signature": "(self, layer_idx)"
    },
    "mindformers.models.LlamaForCausalLM.convert_name": {
        "signature": "(weight_name)"
    },
    "mindformers.models.LlamaForCausalLM.convert_weight_dict": {
        "signature": "(source_dict, **kwargs)"
    },
    "mindformers.models.LlamaForCausalLM.convert_map_dict": {
        "signature": "(source_dict, **kwargs)"
    },
    "mindformers.models.LlamaForCausalLM.obtain_qkv_ffn_concat_keys": {
        "signature": "()"
    },
    "mindformers.models.LlamaForCausalLM.clear_kv_cache": {
        "signature": "(self)"
    },
    "mindformers.models.LlamaTokenizer": {
        "signature": "(vocab_file, unk_token='<unk>', bos_token='<s>', eos_token='</s>', pad_token='<unk>', sp_model_kwargs: Optional[Dict[str, Any]] = None, add_bos_token=True, add_eos_token=False, clean_up_tokenization_spaces=False, legacy=True, **kwargs)"
    },
    "mindformers.models.LlamaTokenizer.get_spm_processor": {
        "signature": "(self, from_slow=False)"
    },
    "mindformers.models.LlamaTokenizer.get_vocab": {
        "signature": "(self)"
    },
    "mindformers.models.LlamaTokenizer.tokenize": {
        "signature": "(self, text: 'TextInput', pair: Optional[str] = None, add_special_tokens: bool = False, **kwargs) -> List[str]"
    },
    "mindformers.models.LlamaTokenizer._tokenize": {
        "signature": "(self, text, **kwargs)"
    },
    "mindformers.models.LlamaTokenizer._convert_token_to_id": {
        "signature": "(self, token)"
    },
    "mindformers.models.LlamaTokenizer._convert_id_to_token": {
        "signature": "(self, index)"
    },
    "mindformers.models.LlamaTokenizer.convert_tokens_to_string": {
        "signature": "(self, tokens)"
    },
    "mindformers.models.LlamaTokenizer.save_vocabulary": {
        "signature": "(self, save_directory, filename_prefix=None)"
    },
    "mindformers.models.LlamaTokenizer.build_inputs_with_special_tokens": {
        "signature": "(self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None)"
    },
    "mindformers.models.LlamaTokenizer.get_special_tokens_mask": {
        "signature": "(self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False) -> List[int]"
    },
    "mindformers.models.LlamaTokenizer.create_token_type_ids_from_sequences": {
        "signature": "(self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None) -> List[int]"
    },
    "mindformers.models.LlamaTokenizerFast": {
        "signature": "(vocab_file=None, tokenizer_file=None, clean_up_tokenization_spaces=False, unk_token='<unk>', bos_token='<s>', eos_token='</s>', add_bos_token=True, add_eos_token=False, use_default_system_prompt=False, **kwargs)"
    },
    "mindformers.models.LlamaTokenizerFast.slow_tokenizer_class": {
        "signature": "(vocab_file, unk_token='<unk>', bos_token='<s>', eos_token='</s>', pad_token='<unk>', sp_model_kwargs: Optional[Dict[str, Any]] = None, add_bos_token=True, add_eos_token=False, clean_up_tokenization_spaces=False, legacy=True, **kwargs)"
    },
    "mindformers.models.LlamaTokenizerFast.update_post_processor": {
        "signature": "(self)"
    },
    "mindformers.models.LlamaTokenizerFast.save_vocabulary": {
        "signature": "(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]"
    },
    "mindformers.models.LlamaTokenizerFast.build_inputs_with_special_tokens": {
        "signature": "(self, token_ids_0, token_ids_1=None)"
    },
    "mindformers.models.ModalContentTransformTemplate": {
        "signature": "(output_columns: List[str] = None, tokenizer=None, mode='predict', vstack_columns: List[str] = None, modal_content_padding_size=1, max_length=2048, **kwargs)"
    },
    "mindformers.models.ModalContentTransformTemplate.process_predict_query": {
        "signature": "(self, query_ele_list: List[Dict], result_recorder: mindformers.models.multi_modal.utils.DataRecord)"
    },
    "mindformers.models.ModalContentTransformTemplate.process_train_item": {
        "signature": "(self, conversation_list: List[List], result_recorder: mindformers.models.multi_modal.utils.DataRecord)"
    },
    "mindformers.models.ModalContentTransformTemplate.build_conversation_input_text": {
        "signature": "(self, raw_inputs, result_recorder: mindformers.models.multi_modal.utils.DataRecord)"
    },
    "mindformers.models.ModalContentTransformTemplate.build_modal_context": {
        "signature": "(self, input_ids, result_recorder: mindformers.models.multi_modal.utils.DataRecord, **kwargs)"
    },
    "mindformers.models.ModalContentTransformTemplate.build_labels": {
        "signature": "(self, text_id_list, result_recorder, **kwargs)"
    },
    "mindformers.models.ModalContentTransformTemplate.generate_modal_context_positions": {
        "signature": "(self, input_ids, batch_index: int = 0, result_recorder: mindformers.models.multi_modal.utils.DataRecord = None, **kwargs)"
    },
    "mindformers.models.ModalContentTransformTemplate.check_modal_builder_tokens": {
        "signature": "(self, tokenizer)"
    },
    "mindformers.models.ModalContentTransformTemplate.get_need_update_output_items": {
        "signature": "(self, result: mindformers.models.multi_modal.utils.DataRecord) -> Dict[str, Any]"
    },
    "mindformers.models.ModalContentTransformTemplate.batch_input_ids": {
        "signature": "(self, input_ids_list, max_length)"
    },
    "mindformers.models.ModalContentTransformTemplate.stack_data": {
        "signature": "(self, data, need_vstack: bool = False)"
    },
    "mindformers.models.ModalContentTransformTemplate.try_to_batch": {
        "signature": "(self, data_list, column_name)"
    },
    "mindformers.models.ModalContentTransformTemplate.batch": {
        "signature": "(self, data_list, token_padding_length, **kwargs)"
    },
    "mindformers.models.ModalContentTransformTemplate.post_process": {
        "signature": "(self, output_ids, **kwargs)"
    },
    "mindformers.models.PreTrainedModel": {
        "signature": "(config: mindformers.models.configuration_utils.PretrainedConfig, *inputs, **kwargs)"
    },
    "mindformers.models.PreTrainedModel.post_init": {
        "signature": "(self)"
    },
    "mindformers.models.PreTrainedModel._from_config": {
        "signature": "(config, **kwargs)"
    },
    "mindformers.models.PreTrainedModel.can_generate": {
        "signature": "() -> bool"
    },
    "mindformers.models.PreTrainedModel.save_pretrained": {
        "signature": "(self, save_directory: Union[os.PathLike, str], save_name: str = 'mindspore_model', **kwargs)"
    },
    "mindformers.models.PreTrainedModel.save_pretrained_experimental_mode": {
        "signature": "(self, save_directory: Union[os.PathLike, str], is_main_process: bool = True, state_dict: Optional[dict] = None, push_to_hub: bool = False, max_shard_size: Union[int, str] = '5GB', variant: Optional[str] = None, token: Union[NoneType, bool, str] = None, **kwargs)"
    },
    "mindformers.models.PreTrainedModel.save_pretrained_origin_mode": {
        "signature": "(self, save_directory: Optional[str] = None, save_name: str = 'mindspore_model')"
    },
    "mindformers.models.PreTrainedModel.remove_type": {
        "signature": "(self, config)"
    },
    "mindformers.models.PreTrainedModel.prepare_inputs_for_predict_layout": {
        "signature": "(self, input_ids, **kwargs)"
    },
    "mindformers.models.PreTrainedModel.set_dynamic_inputs": {
        "signature": "(self, **kwargs)"
    },
    "mindformers.models.PreTrainedModel._inverse_parse_config": {
        "signature": "(self, config)"
    },
    "mindformers.models.PreTrainedModel._wrap_config": {
        "signature": "(self, config)"
    },
    "mindformers.models.PreTrainedModel._get_config_args": {
        "signature": "(pretrained_model_name_or_dir, **kwargs)"
    },
    "mindformers.models.PreTrainedModel.is_experimental_mode": {
        "signature": "(pretrained_model_name_or_dir)"
    },
    "mindformers.models.PreTrainedModel.from_pretrained": {
        "signature": "(pretrained_model_name_or_dir: str, *model_args, **kwargs)"
    },
    "mindformers.models.PreTrainedModel.from_pretrained_origin_mode": {
        "signature": "(pretrained_model_name_or_dir: str, **kwargs)"
    },
    "mindformers.models.PreTrainedModel.from_pretrained_experimental_mode": {
        "signature": "(pretrained_model_name_or_path: Union[NoneType, os.PathLike, str], *model_args, config: Union[NoneType, mindformers.models.configuration_utils.PretrainedConfig, os.PathLike, str] = None, cache_dir: Union[NoneType, os.PathLike, str] = None, ignore_mismatched_sizes: bool = False, force_download: bool = False, local_files_only: bool = False, token: Union[NoneType, bool, str] = None, revision: str = 'main', **kwargs)"
    },
    "mindformers.models.PreTrainedModel._get_src_checkpoint": {
        "signature": "(state_dict, resolved_archive_file, src_checkpoint)"
    },
    "mindformers.models.PreTrainedModel._load_pretrained_model": {
        "signature": "(model, state_dict, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes=False)"
    },
    "mindformers.models.PreTrainedModel.register_for_auto_class": {
        "signature": "(auto_class='AutoModel')"
    },
    "mindformers.models.PreTrainedModel.load_checkpoint": {
        "signature": "(self, config)"
    },
    "mindformers.models.PreTrainedModel.show_support_list": {
        "signature": "()"
    },
    "mindformers.models.PreTrainedModel.get_support_list": {
        "signature": "()"
    },
    "mindformers.models.PreTrainedModel.kvcache": {
        "signature": "(self, layer_idx)"
    },
    "mindformers.models.PreTrainedModel.fuse_weight_from_ckpt": {
        "signature": "(self, ckpt_dict)"
    },
    "mindformers.models.PreTrainedModel.convert_name": {
        "signature": "(weight_name)"
    },
    "mindformers.models.PreTrainedModel.convert_weight_dict": {
        "signature": "(source_dict, **kwargs)"
    },
    "mindformers.models.PreTrainedModel.convert_map_dict": {
        "signature": "(source_dict, **kwargs)"
    },
    "mindformers.models.PreTrainedModel.obtain_qkv_ffn_concat_keys": {
        "signature": "()"
    },
    "mindformers.models.PreTrainedTokenizer": {
        "signature": "(**kwargs)"
    },
    "mindformers.models.PreTrainedTokenizer.get_added_vocab": {
        "signature": "(self) -> Dict[str, int]"
    },
    "mindformers.models.PreTrainedTokenizer._add_tokens": {
        "signature": "(self, new_tokens: Union[List[str], List[tokenizers.AddedToken]], special_tokens: bool = False) -> int"
    },
    "mindformers.models.PreTrainedTokenizer._update_trie": {
        "signature": "(self, unique_no_split_tokens: Optional[str] = None)"
    },
    "mindformers.models.PreTrainedTokenizer.num_special_tokens_to_add": {
        "signature": "(self, pair: bool = False) -> int"
    },
    "mindformers.models.PreTrainedTokenizer.tokenize": {
        "signature": "(self, text: str, pair: Optional[str] = None, add_special_tokens: bool = False, **kwargs) -> List[str]"
    },
    "mindformers.models.PreTrainedTokenizer.tokenize_atom": {
        "signature": "(self, tokens, no_split_token)"
    },
    "mindformers.models.PreTrainedTokenizer._tokenize": {
        "signature": "(self, text, **kwargs)"
    },
    "mindformers.models.PreTrainedTokenizer.convert_tokens_to_ids": {
        "signature": "(self, tokens: Union[List[str], str]) -> Union[List[int], int]"
    },
    "mindformers.models.PreTrainedTokenizer._convert_token_to_id_with_added_voc": {
        "signature": "(self, token)"
    },
    "mindformers.models.PreTrainedTokenizer._convert_token_to_id": {
        "signature": "(self, token)"
    },
    "mindformers.models.PreTrainedTokenizer._encode_plus": {
        "signature": "(self, text: Union[List[int], List[str], str], text_pair: Union[List[int], List[str], NoneType, str] = None, add_special_tokens: bool = True, padding_strategy: mindformers.models.tokenization_utils_base.PaddingStrategy = <PaddingStrategy.DO_NOT_PAD: 'do_not_pad'>, truncation_strategy: mindformers.models.tokenization_utils_base.TruncationStrategy = <TruncationStrategy.DO_NOT_TRUNCATE: 'do_not_truncate'>, max_length: Optional[int] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Optional[int] = None, return_tensors: Union[NoneType, mindformers.models.tokenization_utils_base.TensorType, str] = None, return_token_type_ids: Optional[bool] = None, return_attention_mask: Optional[bool] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> mindformers.models.tokenization_utils_base.BatchEncoding"
    },
    "mindformers.models.PreTrainedTokenizer._batch_encode_plus": {
        "signature": "(self, batch_text_or_text_pairs: Union[List[List[int]], List[List[str]], List[Tuple[List[int], List[int]]], List[Tuple[List[str], List[str]]], List[Tuple[str, str]], List[str]], add_special_tokens: bool = True, padding_strategy: mindformers.models.tokenization_utils_base.PaddingStrategy = <PaddingStrategy.DO_NOT_PAD: 'do_not_pad'>, truncation_strategy: mindformers.models.tokenization_utils_base.TruncationStrategy = <TruncationStrategy.DO_NOT_TRUNCATE: 'do_not_truncate'>, max_length: Optional[int] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Optional[int] = None, return_tensors: Union[NoneType, mindformers.models.tokenization_utils_base.TensorType, str] = None, return_token_type_ids: Optional[bool] = None, return_attention_mask: Optional[bool] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> mindformers.models.tokenization_utils_base.BatchEncoding"
    },
    "mindformers.models.PreTrainedTokenizer._batch_prepare_for_model": {
        "signature": "(self, batch_ids_pairs: List[Union[Tuple[List[int], NoneType], Tuple[List[str], List[str]]]], add_special_tokens: bool = True, padding_strategy: mindformers.models.tokenization_utils_base.PaddingStrategy = <PaddingStrategy.DO_NOT_PAD: 'do_not_pad'>, truncation_strategy: mindformers.models.tokenization_utils_base.TruncationStrategy = <TruncationStrategy.DO_NOT_TRUNCATE: 'do_not_truncate'>, max_length: Optional[int] = None, stride: int = 0, pad_to_multiple_of: Optional[int] = None, return_tensors: Optional[str] = None, return_token_type_ids: Optional[bool] = None, return_attention_mask: Optional[bool] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_length: bool = False, verbose: bool = True) -> mindformers.models.tokenization_utils_base.BatchEncoding"
    },
    "mindformers.models.PreTrainedTokenizer.prepare_for_tokenization": {
        "signature": "(self, text: str, **kwargs) -> Tuple[str, Dict[str, Any]]"
    },
    "mindformers.models.PreTrainedTokenizer.get_special_tokens_mask": {
        "signature": "(self, token_ids_0: List, token_ids_1: Optional[List] = None, already_has_special_tokens: bool = False) -> List[int]"
    },
    "mindformers.models.PreTrainedTokenizer.convert_ids_to_tokens": {
        "signature": "(self, ids: Union[List[int], int], skip_special_tokens: bool = False) -> Union[List[str], str]"
    },
    "mindformers.models.PreTrainedTokenizer._convert_id_to_token": {
        "signature": "(self, index: int) -> str"
    },
    "mindformers.models.PreTrainedTokenizer.convert_tokens_to_string": {
        "signature": "(self, tokens: List[str]) -> str"
    },
    "mindformers.models.PreTrainedTokenizer._decode": {
        "signature": "(self, token_ids: List[int], skip_special_tokens: bool = False, clean_up_tokenization_spaces: bool = None, **kwargs) -> str"
    },
    "mindformers.models.PreTrainedTokenizerFast": {
        "signature": "(*args, **kwargs)"
    },
    "mindformers.models.PreTrainedTokenizerFast.init_atom_1": {
        "signature": "(self, *args, **kwargs)"
    },
    "mindformers.models.PreTrainedTokenizerFast.init_atom_2": {
        "signature": "(self, **kwargs)"
    },
    "mindformers.models.PreTrainedTokenizerFast.get_vocab": {
        "signature": "(self) -> Dict[str, int]"
    },
    "mindformers.models.PreTrainedTokenizerFast.get_added_vocab": {
        "signature": "(self) -> Dict[str, int]"
    },
    "mindformers.models.PreTrainedTokenizerFast._convert_encoding": {
        "signature": "(self, encoding: tokenizers.Encoding, return_token_type_ids: Optional[bool] = None, return_attention_mask: Optional[bool] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False) -> Tuple[Dict[str, Any], List[tokenizers.Encoding]]"
    },
    "mindformers.models.PreTrainedTokenizerFast.convert_tokens_to_ids": {
        "signature": "(self, tokens: Union[List[str], str]) -> Union[List[int], int]"
    },
    "mindformers.models.PreTrainedTokenizerFast._convert_token_to_id_with_added_voc": {
        "signature": "(self, token: str) -> int"
    },
    "mindformers.models.PreTrainedTokenizerFast._convert_id_to_token": {
        "signature": "(self, index: int) -> Optional[str]"
    },
    "mindformers.models.PreTrainedTokenizerFast._add_tokens": {
        "signature": "(self, new_tokens: List[Union[str, tokenizers.AddedToken]], special_tokens=False) -> int"
    },
    "mindformers.models.PreTrainedTokenizerFast.num_special_tokens_to_add": {
        "signature": "(self, pair: bool = False) -> int"
    },
    "mindformers.models.PreTrainedTokenizerFast.convert_ids_to_tokens": {
        "signature": "(self, ids: Union[List[int], int], skip_special_tokens: bool = False) -> Union[List[str], str]"
    },
    "mindformers.models.PreTrainedTokenizerFast.tokenize": {
        "signature": "(self, text: str, pair: Optional[str] = None, add_special_tokens: bool = False, **kwargs) -> List[str]"
    },
    "mindformers.models.PreTrainedTokenizerFast.set_truncation_and_padding": {
        "signature": "(self, padding_strategy: mindformers.models.tokenization_utils_base.PaddingStrategy, truncation_strategy: mindformers.models.tokenization_utils_base.TruncationStrategy, max_length: int, stride: int, pad_to_multiple_of: Optional[int])"
    },
    "mindformers.models.PreTrainedTokenizerFast._batch_encode_plus": {
        "signature": "(self, batch_text_or_text_pairs: Union[List[List[str]], List[Tuple[List[str], List[str]]], List[Tuple[str, str]], List[str]], add_special_tokens: bool = True, padding_strategy: mindformers.models.tokenization_utils_base.PaddingStrategy = <PaddingStrategy.DO_NOT_PAD: 'do_not_pad'>, truncation_strategy: mindformers.models.tokenization_utils_base.TruncationStrategy = <TruncationStrategy.DO_NOT_TRUNCATE: 'do_not_truncate'>, max_length: Optional[int] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Optional[int] = None, return_tensors: Optional[str] = None, return_token_type_ids: Optional[bool] = None, return_attention_mask: Optional[bool] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> mindformers.models.tokenization_utils_base.BatchEncoding"
    },
    "mindformers.models.PreTrainedTokenizerFast._encode_plus": {
        "signature": "(self, text: Union[List[str], str], text_pair: Union[List[str], NoneType, str] = None, add_special_tokens: bool = True, padding_strategy: mindformers.models.tokenization_utils_base.PaddingStrategy = <PaddingStrategy.DO_NOT_PAD: 'do_not_pad'>, truncation_strategy: mindformers.models.tokenization_utils_base.TruncationStrategy = <TruncationStrategy.DO_NOT_TRUNCATE: 'do_not_truncate'>, max_length: Optional[int] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Optional[int] = None, return_tensors: Optional[bool] = None, return_token_type_ids: Optional[bool] = None, return_attention_mask: Optional[bool] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> mindformers.models.tokenization_utils_base.BatchEncoding"
    },
    "mindformers.models.PreTrainedTokenizerFast.convert_tokens_to_string": {
        "signature": "(self, tokens: List[str]) -> str"
    },
    "mindformers.models.PreTrainedTokenizerFast._decode": {
        "signature": "(self, token_ids: Union[List[int], int], skip_special_tokens: bool = False, clean_up_tokenization_spaces: bool = None, **kwargs) -> str"
    },
    "mindformers.models.PreTrainedTokenizerFast._save_pretrained": {
        "signature": "(self, save_directory: Union[os.PathLike, str], file_names: Tuple[str], legacy_format: Optional[bool] = None, filename_prefix: Optional[str] = None) -> Tuple[str]"
    },
    "mindformers.models.PreTrainedTokenizerFast.train_new_from_iterator": {
        "signature": "(self, text_iterator, vocab_size, length=None, new_special_tokens=None, special_tokens_map=None, **kwargs)"
    },
    "mindformers.models.PreTrainedTokenizerFast.save_vocabulary": {
        "signature": "(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]"
    },
    "mindformers.models.PretrainedConfig": {
        "signature": "(**kwargs)"
    },
    "mindformers.models.PretrainedConfig._to_dict_helper": {
        "signature": "(self, output)"
    },
    "mindformers.models.PretrainedConfig.to_dict": {
        "signature": "(self) -> Dict[str, Any]"
    },
    "mindformers.models.PretrainedConfig.from_pretrained": {
        "signature": "(yaml_name_or_path, **kwargs) -> 'PretrainedConfig'"
    },
    "mindformers.models.PretrainedConfig.get_config_experimental_mode": {
        "signature": "(*args, **kwargs)"
    },
    "mindformers.models.PretrainedConfig.get_config_origin_mode": {
        "signature": "(yaml_name_or_path, **kwargs)"
    },
    "mindformers.models.PretrainedConfig.save_pretrained": {
        "signature": "(self, save_directory=None, save_name='mindspore_model', **kwargs)"
    },
    "mindformers.models.PretrainedConfig.save_config_experimental_mode": {
        "signature": "(cls, *args, **kwargs)"
    },
    "mindformers.models.PretrainedConfig.save_config_origin_mode": {
        "signature": "(self, save_directory, save_name)"
    },
    "mindformers.models.PretrainedConfig.remove_type": {
        "signature": "(self)"
    },
    "mindformers.models.PretrainedConfig.inverse_parse_config": {
        "signature": "(self)"
    },
    "mindformers.models.PretrainedConfig._inverse_parse_config": {
        "signature": "(self)"
    },
    "mindformers.models.PretrainedConfig._wrap_config": {
        "signature": "(self, config)"
    },
    "mindformers.models.PretrainedConfig.show_support_list": {
        "signature": "()"
    },
    "mindformers.models.PretrainedConfig.get_support_list": {
        "signature": "()"
    },
    "mindformers.models.PretrainedConfig.get_config_dict": {
        "signature": "(pretrained_model_name_or_path: Union[os.PathLike, str], **kwargs) -> Tuple[Dict[str, Any], Dict[str, Any]]"
    },
    "mindformers.models.PretrainedConfig._get_config_dict": {
        "signature": "(pretrained_model_name_or_path: Union[os.PathLike, str], **kwargs) -> Tuple[Dict[str, Any], Dict[str, Any]]"
    },
    "mindformers.models.PretrainedConfig.from_dict": {
        "signature": "(config_dict: Dict[str, Any], **kwargs) -> 'PretrainedConfig'"
    },
    "mindformers.models.PretrainedConfig.from_json_file": {
        "signature": "(json_file: Union[os.PathLike, str]) -> 'PretrainedConfig'"
    },
    "mindformers.models.PretrainedConfig._dict_from_json_file": {
        "signature": "(json_file: Union[os.PathLike, str])"
    },
    "mindformers.models.PretrainedConfig.to_json_file": {
        "signature": "(self, json_file_path: Union[os.PathLike, str], use_diff: bool = True)"
    },
    "mindformers.models.PretrainedConfig.to_json_string": {
        "signature": "(self, use_diff: bool = True) -> str"
    },
    "mindformers.models.PretrainedConfig._to_diff_dict_helper": {
        "signature": "(self, serializable_config_dict)"
    },
    "mindformers.models.PretrainedConfig.to_diff_dict": {
        "signature": "(self) -> Dict[str, Any]"
    },
    "mindformers.models.PretrainedConfig.delete_from_dict": {
        "signature": "(self, config_dict)"
    },
    "mindformers.models.PretrainedConfig._delete_by_keys": {
        "signature": "(self, config_dict, keys)"
    },
    "mindformers.models.PretrainedConfig.dict_ms_dtype_to_str": {
        "signature": "(self, d: Dict[str, Any]) -> None"
    },
    "mindformers.models.PretrainedConfig.register_for_auto_class": {
        "signature": "(auto_class='AutoConfig')"
    },
    "mindformers.models.auto.AutoConfig": {
        "signature": "()"
    },
    "mindformers.models.auto.AutoConfig.invalid_yaml_name": {
        "signature": "(yaml_name_or_path)"
    },
    "mindformers.models.auto.AutoConfig.for_model": {
        "signature": "(model_type: str, *args, **kwargs)"
    },
    "mindformers.models.auto.AutoConfig.from_pretrained": {
        "signature": "(yaml_name_or_path, **kwargs)"
    },
    "mindformers.models.auto.AutoConfig.get_config_origin_mode": {
        "signature": "(yaml_name_or_path, **kwargs)"
    },
    "mindformers.models.auto.AutoConfig.get_config_experimental_mode": {
        "signature": "(*args, **kwargs)"
    },
    "mindformers.models.auto.AutoConfig.show_support_list": {
        "signature": "()"
    },
    "mindformers.models.auto.AutoConfig.get_support_list": {
        "signature": "()"
    },
    "mindformers.models.auto.AutoConfig.register": {
        "signature": "(model_type, config, exist_ok=False)"
    },
    "mindformers.models.auto.AutoModel": {
        "signature": "(*args, **kwargs)"
    },
    "mindformers.models.auto.AutoModel.from_config": {
        "signature": "(**kwargs)"
    },
    "mindformers.models.auto.AutoModel.from_pretrained": {
        "signature": "(*model_args, **kwargs)"
    },
    "mindformers.models.auto.AutoModelForCausalLM": {
        "signature": "(*args, **kwargs)"
    },
    "mindformers.models.auto.AutoModelForCausalLM.from_config": {
        "signature": "(**kwargs)"
    },
    "mindformers.models.auto.AutoModelForCausalLM.from_pretrained": {
        "signature": "(*model_args, **kwargs)"
    },
    "mindformers.models.auto.AutoProcessor": {
        "signature": "()"
    },
    "mindformers.models.auto.AutoProcessor.invalid_yaml_name": {
        "signature": "(yaml_name_or_path)"
    },
    "mindformers.models.auto.AutoProcessor.from_pretrained": {
        "signature": "(yaml_name_or_path, **kwargs)"
    },
    "mindformers.models.auto.AutoProcessor.from_pretrained_origin": {
        "signature": "(yaml_name_or_path, **kwargs)"
    },
    "mindformers.models.auto.AutoProcessor.from_pretrained_experimental": {
        "signature": "(*args, **kwargs)"
    },
    "mindformers.models.auto.AutoProcessor.register": {
        "signature": "(config_class, processor_class, exist_ok=False)"
    },
    "mindformers.models.auto.AutoProcessor.show_support_list": {
        "signature": "()"
    },
    "mindformers.models.auto.AutoProcessor.get_support_list": {
        "signature": "()"
    },
    "mindformers.models.auto.AutoTokenizer": {
        "signature": "()"
    },
    "mindformers.models.auto.AutoTokenizer.invalid_yaml_name": {
        "signature": "(yaml_name_or_path)"
    },
    "mindformers.models.auto.AutoTokenizer._get_class_name_from_yaml": {
        "signature": "(yaml_name_or_path)"
    },
    "mindformers.models.auto.AutoTokenizer.from_pretrained": {
        "signature": "(yaml_name_or_path, *args, **kwargs)"
    },
    "mindformers.models.auto.AutoTokenizer.get_class_from_origin_mode": {
        "signature": "(yaml_name_or_path, **kwargs)"
    },
    "mindformers.models.auto.AutoTokenizer.get_class_from_experimental_mode": {
        "signature": "(*args, **kwargs)"
    },
    "mindformers.models.auto.AutoTokenizer.register": {
        "signature": "(config_class, slow_tokenizer_class=None, fast_tokenizer_class=None, exist_ok=False)"
    },
    "mindformers.models.auto.AutoTokenizer.show_support_list": {
        "signature": "()"
    },
    "mindformers.models.auto.AutoTokenizer.get_support_list": {
        "signature": "(cls)"
    },
    "mindformers.models.bert.bert.BertConfig": {
        "signature": "(model_type: str = 'bert', use_one_hot_embeddings: bool = False, num_labels: int = 1, assessment_method: str = '', dropout_prob: float = 0.1, batch_size: int = 16, seq_length: int = 128, vocab_size: int = 30522, hidden_size: int = 768, num_hidden_layers: int = 12, num_attention_heads: int = 12, intermediate_size: int = 3072, hidden_act: str = 'gelu', post_layernorm_residual: bool = True, hidden_dropout_prob: float = 0.1, attention_probs_dropout_prob: float = 0.1, max_position_embeddings: int = 128, type_vocab_size: int = 2, initializer_range: float = 0.02, use_relative_positions: bool = False, dtype: str = 'float32', layernorm_dtype: str = 'float32', softmax_dtype: str = 'float32', compute_dtype: str = 'float16', use_past: bool = False, parallel_config: Union[dict, mindformers.modules.transformer.transformer.TransformerOpParallelConfig] = <mindformers.modules.transformer.transformer.TransformerOpParallelConfig object>, checkpoint_name_or_path: str = '', moe_config: Union[dict, mindformers.modules.transformer.moe.MoEConfig] = <mindformers.modules.transformer.moe.MoEConfig object>, **kwargs)"
    },
    "mindformers.models.bert.bert.BertForMultipleChoice": {
        "signature": "(config=BertConfig {\n  \"assessment_method\": \"\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"batch_size\": 16,\n  \"checkpoint_name_or_path\": \"\",\n  \"compute_dtype\": \"float16\",\n  \"dropout_prob\": 0.1,\n  \"dtype\": \"float32\",\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layernorm_dtype\": \"float32\",\n  \"max_position_embeddings\": 128,\n  \"mindformers_version\": \"1.4.0.beta1\",\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"num_labels\": 1,\n  \"post_layernorm_residual\": true,\n  \"seq_length\": 128,\n  \"softmax_dtype\": \"float32\",\n  \"type_vocab_size\": 2,\n  \"use_one_hot_embeddings\": false,\n  \"use_past\": false,\n  \"use_relative_positions\": false,\n  \"vocab_size\": 30522\n}\n)"
    },
    "mindformers.models.bert.bert.BertForMultipleChoice.construct": {
        "signature": "(self, input_ids, input_mask, token_type_id, label_ids=None)"
    },
    "mindformers.models.bert.bert.BertForPreTraining": {
        "signature": "(config=BertConfig {\n  \"assessment_method\": \"\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"batch_size\": 16,\n  \"checkpoint_name_or_path\": \"\",\n  \"compute_dtype\": \"float16\",\n  \"dropout_prob\": 0.1,\n  \"dtype\": \"float32\",\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layernorm_dtype\": \"float32\",\n  \"max_position_embeddings\": 128,\n  \"mindformers_version\": \"1.4.0.beta1\",\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"num_labels\": 1,\n  \"post_layernorm_residual\": true,\n  \"seq_length\": 128,\n  \"softmax_dtype\": \"float32\",\n  \"type_vocab_size\": 2,\n  \"use_one_hot_embeddings\": false,\n  \"use_past\": false,\n  \"use_relative_positions\": false,\n  \"vocab_size\": 30522\n}\n)"
    },
    "mindformers.models.bert.bert.BertForPreTraining.bert_forward": {
        "signature": "(self, input_ids, input_mask, token_type_id, masked_lm_positions=None)"
    },
    "mindformers.models.bert.bert.BertForPreTraining.construct": {
        "signature": "(self, input_ids, input_mask, token_type_id, next_sentence_labels=None, masked_lm_positions=None, masked_lm_ids=None, masked_lm_weights=None)"
    },
    "mindformers.models.bert.bert.BertForQuestionAnswering": {
        "signature": "(config=BertConfig {\n  \"assessment_method\": \"\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"batch_size\": 16,\n  \"checkpoint_name_or_path\": \"\",\n  \"compute_dtype\": \"float16\",\n  \"dropout_prob\": 0.1,\n  \"dtype\": \"float32\",\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layernorm_dtype\": \"float32\",\n  \"max_position_embeddings\": 128,\n  \"mindformers_version\": \"1.4.0.beta1\",\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"num_labels\": 1,\n  \"post_layernorm_residual\": true,\n  \"seq_length\": 128,\n  \"softmax_dtype\": \"float32\",\n  \"type_vocab_size\": 2,\n  \"use_one_hot_embeddings\": false,\n  \"use_past\": false,\n  \"use_relative_positions\": false,\n  \"vocab_size\": 30522\n}\n)"
    },
    "mindformers.models.bert.bert.BertForQuestionAnswering.construct": {
        "signature": "(self, input_ids, input_mask, token_type_id, start_position=None, end_position=None, unique_id=None)"
    },
    "mindformers.models.bert.bert.BertForTokenClassification": {
        "signature": "(config=BertConfig {\n  \"assessment_method\": \"\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"batch_size\": 16,\n  \"checkpoint_name_or_path\": \"\",\n  \"compute_dtype\": \"float16\",\n  \"dropout_prob\": 0.1,\n  \"dtype\": \"float32\",\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layernorm_dtype\": \"float32\",\n  \"max_position_embeddings\": 128,\n  \"mindformers_version\": \"1.4.0.beta1\",\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"num_labels\": 1,\n  \"post_layernorm_residual\": true,\n  \"seq_length\": 128,\n  \"softmax_dtype\": \"float32\",\n  \"type_vocab_size\": 2,\n  \"use_one_hot_embeddings\": false,\n  \"use_past\": false,\n  \"use_relative_positions\": false,\n  \"vocab_size\": 30522\n}\n)"
    },
    "mindformers.models.bert.bert.BertForTokenClassification.construct": {
        "signature": "(self, input_ids, input_mask, token_type_ids, label_ids=None)"
    },
    "mindformers.models.bert.bert.BertModel": {
        "signature": "(config=None, use_one_hot_embeddings=False)"
    },
    "mindformers.models.bert.bert.BertModel.construct": {
        "signature": "(self, input_ids, token_type_ids, input_mask)"
    },
    "mindformers.models.bert.bert_config.BertConfig": {
        "signature": "(model_type: str = 'bert', use_one_hot_embeddings: bool = False, num_labels: int = 1, assessment_method: str = '', dropout_prob: float = 0.1, batch_size: int = 16, seq_length: int = 128, vocab_size: int = 30522, hidden_size: int = 768, num_hidden_layers: int = 12, num_attention_heads: int = 12, intermediate_size: int = 3072, hidden_act: str = 'gelu', post_layernorm_residual: bool = True, hidden_dropout_prob: float = 0.1, attention_probs_dropout_prob: float = 0.1, max_position_embeddings: int = 128, type_vocab_size: int = 2, initializer_range: float = 0.02, use_relative_positions: bool = False, dtype: str = 'float32', layernorm_dtype: str = 'float32', softmax_dtype: str = 'float32', compute_dtype: str = 'float16', use_past: bool = False, parallel_config: Union[dict, mindformers.modules.transformer.transformer.TransformerOpParallelConfig] = <mindformers.modules.transformer.transformer.TransformerOpParallelConfig object>, checkpoint_name_or_path: str = '', moe_config: Union[dict, mindformers.modules.transformer.moe.MoEConfig] = <mindformers.modules.transformer.moe.MoEConfig object>, **kwargs)"
    },
    "mindformers.models.bert.bert_processor.BertProcessor": {
        "signature": "(tokenizer=None, max_length=128, padding='max_length', return_tensors='ms')"
    },
    "mindformers.models.bert.bert_tokenizer.BasicTokenizer": {
        "signature": "(do_lower_case=True, never_split=None, tokenize_chinese_chars=True, strip_accents=None, do_split_on_punc=True)"
    },
    "mindformers.models.bert.bert_tokenizer.BasicTokenizer.tokenize": {
        "signature": "(self, text, never_split=None)"
    },
    "mindformers.models.bert.bert_tokenizer.BasicTokenizer._run_strip_accents": {
        "signature": "(self, text)"
    },
    "mindformers.models.bert.bert_tokenizer.BasicTokenizer._run_split_on_punc": {
        "signature": "(self, text, never_split=None)"
    },
    "mindformers.models.bert.bert_tokenizer.BasicTokenizer._tokenize_chinese_chars": {
        "signature": "(self, text)"
    },
    "mindformers.models.bert.bert_tokenizer.BasicTokenizer._is_chinese_char": {
        "signature": "(self, cp)"
    },
    "mindformers.models.bert.bert_tokenizer.BasicTokenizer._clean_text": {
        "signature": "(self, text)"
    },
    "mindformers.models.bert.bert_tokenizer.BertTokenizer": {
        "signature": "(vocab_file, do_lower_case=True, do_basic_tokenize=True, never_split=None, unk_token='[UNK]', sep_token='[SEP]', pad_token='[PAD]', cls_token='[CLS]', mask_token='[MASK]', tokenize_chinese_chars=True, strip_accents=None, **kwargs)"
    },
    "mindformers.models.bert.bert_tokenizer.BertTokenizer.get_vocab": {
        "signature": "(self)"
    },
    "mindformers.models.bert.bert_tokenizer.BertTokenizer._tokenize": {
        "signature": "(self, text, **kwargs)"
    },
    "mindformers.models.bert.bert_tokenizer.BertTokenizer._convert_token_to_id": {
        "signature": "(self, token)"
    },
    "mindformers.models.bert.bert_tokenizer.BertTokenizer._convert_id_to_token": {
        "signature": "(self, index)"
    },
    "mindformers.models.bert.bert_tokenizer.BertTokenizer.convert_tokens_to_string": {
        "signature": "(self, tokens)"
    },
    "mindformers.models.bert.bert_tokenizer.BertTokenizer.build_inputs_with_special_tokens": {
        "signature": "(self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None) -> List[int]"
    },
    "mindformers.models.bert.bert_tokenizer.BertTokenizer.get_special_tokens_mask": {
        "signature": "(self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False) -> List[int]"
    },
    "mindformers.models.bert.bert_tokenizer.BertTokenizer.create_token_type_ids_from_sequences": {
        "signature": "(self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None) -> List[int]"
    },
    "mindformers.models.bert.bert_tokenizer.BertTokenizer.save_vocabulary": {
        "signature": "(self, save_directory, filename_prefix=None)"
    },
    "mindformers.models.bert.bert_tokenizer.BertTokenizer.tokenize": {
        "signature": "(self, text: str, pair: Optional[str] = None, add_special_tokens: bool = False, **kwargs) -> List[str]"
    },
    "mindformers.models.bert.bert_tokenizer_fast.BertTokenizerFast": {
        "signature": "(vocab_file=None, tokenizer_file=None, do_lower_case=True, unk_token='[UNK]', sep_token='[SEP]', pad_token='[PAD]', cls_token='[CLS]', mask_token='[MASK]', tokenize_chinese_chars=True, strip_accents=None, **kwargs)"
    },
    "mindformers.models.bert.bert_tokenizer_fast.BertTokenizerFast.slow_tokenizer_class": {
        "signature": "(vocab_file, do_lower_case=True, do_basic_tokenize=True, never_split=None, unk_token='[UNK]', sep_token='[SEP]', pad_token='[PAD]', cls_token='[CLS]', mask_token='[MASK]', tokenize_chinese_chars=True, strip_accents=None, **kwargs)"
    },
    "mindformers.models.bert.bert_tokenizer_fast.BertTokenizerFast.build_inputs_with_special_tokens": {
        "signature": "(self, token_ids_0, token_ids_1=None)"
    },
    "mindformers.models.bert.bert_tokenizer_fast.BertTokenizerFast.create_token_type_ids_from_sequences": {
        "signature": "(self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None) -> List[int]"
    },
    "mindformers.models.bert.bert_tokenizer_fast.BertTokenizerFast.save_vocabulary": {
        "signature": "(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]"
    },
    "mindformers.models.blip2.BertLMHeadModel": {
        "signature": "(config: mindformers.models.blip2.qformer_config.QFormerConfig)"
    },
    "mindformers.models.blip2.BertLMHeadModel.convert_bert_model_params": {
        "signature": "(self, bert_model_params: collections.OrderedDict)"
    },
    "mindformers.models.blip2.BertLMHeadModel.load_bert_model_params": {
        "signature": "(self, config: mindformers.models.blip2.qformer_config.QFormerConfig, param)"
    },
    "mindformers.models.blip2.BertLMHeadModel.load_checkpoint": {
        "signature": "(self, config: mindformers.models.blip2.qformer_config.QFormerConfig)"
    },
    "mindformers.models.blip2.BertLMHeadModel.get_input_embeddings": {
        "signature": "(self) -> mindspore.nn.cell.Cell"
    },
    "mindformers.models.blip2.BertLMHeadModel.set_input_embeddings": {
        "signature": "(self, value)"
    },
    "mindformers.models.blip2.BertLMHeadModel.get_output_embeddings": {
        "signature": "(self) -> mindspore.nn.cell.Cell"
    },
    "mindformers.models.blip2.BertLMHeadModel.set_output_embeddings": {
        "signature": "(self, value)"
    },
    "mindformers.models.blip2.BertLMHeadModel.resize_token_embeddings": {
        "signature": "(self, new_num_tokens: Optional[int] = None) -> mindspore.nn.layer.embedding.Embedding"
    },
    "mindformers.models.blip2.BertLMHeadModel.tie_weights": {
        "signature": "(self)"
    },
    "mindformers.models.blip2.BertLMHeadModel.construct": {
        "signature": "(self, input_ids=None, attention_mask=None, position_ids=None, head_mask=None, query_embeds=None, encoder_hidden_states=None, encoder_attention_mask=None, labels=None, past_key_values=None, use_cache=True, output_attentions=None, output_hidden_states=None, return_dict=None, return_logits=False, is_decoder=True)"
    },
    "mindformers.models.blip2.Blip2Classifier": {
        "signature": "(config: mindformers.models.blip2.blip2_config.Blip2Config, **kwargs)"
    },
    "mindformers.models.blip2.Blip2Classifier.construct": {
        "signature": "(self, image: mindspore.common.tensor.Tensor, text_input_ids: mindspore.common.tensor.Tensor, return_tuple: bool = False)"
    },
    "mindformers.models.blip2.Blip2Config": {
        "signature": "(batch_size: int = 8, freeze_vision: bool = True, freeze_text: bool = True, max_txt_len: int = 32, prompt: bool = False, prompt_length: int = 0, checkpoint_name_or_path: str = None, dtype: str = 'float32', compute_dtype: str = 'float16', layernorm_compute_type: str = 'float32', softmax_compute_type: str = 'float32', vision_config: Union[dict, mindformers.models.vit.vit_config.ViTConfig] = ViTConfig {\n  \"attention_probs_dropout_prob\": 0.0, \n  \"checkpoint_name_or_path\": \"\", \n  \"drop_path_rate\": 0.1, \n  \"encoder_stride\": 16, \n  \"hidden_act\": \"gelu\", \n  \"hidden_dropout_prob\": 0.0, \n  \"hidden_size\": 768, \n  \"image_size\": 224, \n  \"init_values\": null, \n  \"initializer_range\": 0.02, \n  \"intermediate_size\": 3072, \n  \"layer_norm_eps\": 1e-12, \n  \"layernorm_compute_type\": \"float32\", \n  \"loss_type\": \"SoftTargetCrossEntropy\", \n  \"mindformers_version\": \"1.4.0.beta1\", \n  \"model_type\": \"vit\", \n  \"num_attention_heads\": 12, \n  \"num_channels\": 3, \n  \"num_classes\": 1000, \n  \"num_hidden_layers\": 12, \n  \"param_init_type\": \"float32\", \n  \"patch_size\": 16, \n  \"post_layernorm_residual\": false, \n  \"qkv_bias\": true, \n  \"softmax_compute_type\": \"float32\", \n  \"use_mean_pooling\": true\n}\n, qformer_config: Union[dict, mindformers.models.blip2.qformer_config.QFormerConfig] = QFormerConfig {\n  \"add_cross_attention\": true, \n  \"assessment_method\": \"\", \n  \"attention_probs_dropout_prob\": 0.1, \n  \"batch_size\": 16, \n  \"bos_token_id\": 30522, \n  \"checkpoint_name_or_path\": \"\", \n  \"chunk_size_feed_forward\": 0, \n  \"compute_dtype\": \"float16\", \n  \"convert_param_from_bert\": false, \n  \"cross_attention_freq\": 2, \n  \"dropout_prob\": 0.1, \n  \"dtype\": \"float32\", \n  \"encoder_width\": 1408, \n  \"head_embed_dim\": 256, \n  \"hidden_act\": \"gelu\", \n  \"hidden_dropout_prob\": 0.1, \n  \"hidden_size\": 768, \n  \"initializer_range\": 0.02, \n  \"intermediate_size\": 3072, \n  \"layer_norm_eps\": 1e-12, \n  \"layernorm_dtype\": \"float32\", \n  \"loss_reduction\": \"mean\", \n  \"max_position_embeddings\": 512, \n  \"mindformers_version\": \"1.4.0.beta1\", \n  \"model_type\": \"blip_2_qformer\", \n  \"num_attention_heads\": 12, \n  \"num_hidden_layers\": 12, \n  \"num_labels\": 1, \n  \"output_attentions\": false, \n  \"output_hidden_states\": false, \n  \"pad_token_id\": 0, \n  \"position_embedding_type\": \"absolute\", \n  \"post_layernorm_residual\": true, \n  \"query_length\": 32, \n  \"resize_token_embeddings\": true, \n  \"sep_token_id\": 102, \n  \"seq_length\": 128, \n  \"softmax_dtype\": \"float32\", \n  \"special_token_nums\": 1, \n  \"tie_word_embeddings\": true, \n  \"type_vocab_size\": 2, \n  \"use_one_hot_embeddings\": false, \n  \"use_past\": false, \n  \"use_relative_positions\": false, \n  \"vocab_size\": 30523\n}\n, text_config: Optional[mindformers.models.llama.llama_config.LlamaConfig] = None, parallel_config: Union[dict, mindformers.modules.transformer.transformer.TransformerOpParallelConfig] = <mindformers.modules.transformer.transformer.TransformerOpParallelConfig object>, is_training: bool = True, micro_batch_interleave_num=1, use_past=False, **kwargs)"
    },
    "mindformers.models.blip2.Blip2ImageProcessor": {
        "signature": "(image_size: Optional[int] = 224, interpolation: Optional[str] = 'bicubic', mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711), is_hwc=False, **kwargs)"
    },
    "mindformers.models.blip2.Blip2ImageProcessor.preprocess": {
        "signature": "(self, images: Union[List[PIL.Image.Image], PIL.Image.Image, mindspore.common.tensor.Tensor, numpy.ndarray], **kwargs)"
    },
    "mindformers.models.blip2.Blip2ImageProcessor._bhwc_check": {
        "signature": "(self, image_batch: Union[List[PIL.Image.Image], PIL.Image.Image, mindspore.common.tensor.Tensor, numpy.ndarray])"
    },
    "mindformers.models.blip2.Blip2ImageToTextGeneration": {
        "signature": "(config: mindformers.models.blip2.blip2_config.Blip2Config, **kwargs)"
    },
    "mindformers.models.blip2.Blip2ImageToTextGeneration.construct": {
        "signature": "(self, image: mindspore.common.tensor.Tensor, text_input_ids: mindspore.common.tensor.Tensor)"
    },
    "mindformers.models.blip2.Blip2ImageToTextGeneration.generate_text_for_image": {
        "signature": "(self, image: mindspore.common.tensor.Tensor, prompt_input_ids: mindspore.common.tensor.Tensor, **kwargs)"
    },
    "mindformers.models.blip2.Blip2ItmEvaluator": {
        "signature": "(config: mindformers.models.blip2.blip2_config.Blip2Config, blip2_qformer: mindformers.models.blip2.blip2_qformer.Blip2Qformer = None, **kwargs)"
    },
    "mindformers.models.blip2.Blip2ItmEvaluator.load_checkpoint": {
        "signature": "(self, config)"
    },
    "mindformers.models.blip2.Blip2ItmEvaluator.construct": {
        "signature": "(self, image_feats, text_feats, vit_outputs=None, text_ids=None, add_extra_itm_score: bool = False)"
    },
    "mindformers.models.blip2.Blip2Llm": {
        "signature": "(config: mindformers.models.blip2.blip2_config.Blip2Config, **kwargs)"
    },
    "mindformers.models.blip2.Blip2Llm.construct": {
        "signature": "(self, image: mindspore.common.tensor.Tensor, text_input_ids: mindspore.common.tensor.Tensor)"
    },
    "mindformers.models.blip2.Blip2Llm.forward_qformer_and_proj": {
        "signature": "(self, image: mindspore.common.tensor.Tensor)"
    },
    "mindformers.models.blip2.Blip2Processor": {
        "signature": "(image_processor, tokenizer, max_length=32, padding='max_length', return_tensors='ms')"
    },
    "mindformers.models.blip2.Blip2Qformer": {
        "signature": "(config: mindformers.models.blip2.blip2_config.Blip2Config, **kwargs)"
    },
    "mindformers.models.blip2.Blip2Qformer.construct": {
        "signature": "(self, image: mindspore.common.tensor.Tensor, text_input_ids: mindspore.common.tensor.Tensor, return_tuple: bool = False)"
    },
    "mindformers.models.blip2.Blip2Qformer.forward_image": {
        "signature": "(self, image, use_cache=False)"
    },
    "mindformers.models.blip2.Blip2Qformer.forward_text": {
        "signature": "(self, text_input_ids)"
    },
    "mindformers.models.blip2.Blip2Qformer.forward_text_and_image": {
        "signature": "(self, image_inputs, text_ids, vit_computed=False)"
    },
    "mindformers.models.blip2.Blip2Qformer.fill_masked_weight": {
        "signature": "(self, sim_t2i, sim_i2t, batch_size)"
    },
    "mindformers.models.blip2.Blip2Qformer.choose_negative_targets": {
        "signature": "(self, weights_t2i, weights_i2t, batch_size, image_embeds, text_input_ids)"
    },
    "mindformers.models.blip2.Blip2Qformer.compute_itm": {
        "signature": "(self, image_inputs, text_ids, vit_computed=False)"
    },
    "mindformers.models.blip2.Blip2Qformer.get_image_feature": {
        "signature": "(self, image, output_past_keys=False)"
    },
    "mindformers.models.blip2.Blip2Qformer.get_text_feature": {
        "signature": "(self, input_ids)"
    },
    "mindformers.models.blip2.Blip2Qformer.extract_features": {
        "signature": "(self, samples, mode='multimodal')"
    },
    "mindformers.models.blip2.blip2_config.Blip2Config": {
        "signature": "(batch_size: int = 8, freeze_vision: bool = True, freeze_text: bool = True, max_txt_len: int = 32, prompt: bool = False, prompt_length: int = 0, checkpoint_name_or_path: str = None, dtype: str = 'float32', compute_dtype: str = 'float16', layernorm_compute_type: str = 'float32', softmax_compute_type: str = 'float32', vision_config: Union[dict, mindformers.models.vit.vit_config.ViTConfig] = ViTConfig {\n  \"attention_probs_dropout_prob\": 0.0, \n  \"checkpoint_name_or_path\": \"\", \n  \"drop_path_rate\": 0.1, \n  \"encoder_stride\": 16, \n  \"hidden_act\": \"gelu\", \n  \"hidden_dropout_prob\": 0.0, \n  \"hidden_size\": 768, \n  \"image_size\": 224, \n  \"init_values\": null, \n  \"initializer_range\": 0.02, \n  \"intermediate_size\": 3072, \n  \"layer_norm_eps\": 1e-12, \n  \"layernorm_compute_type\": \"float32\", \n  \"loss_type\": \"SoftTargetCrossEntropy\", \n  \"mindformers_version\": \"1.4.0.beta1\", \n  \"model_type\": \"vit\", \n  \"num_attention_heads\": 12, \n  \"num_channels\": 3, \n  \"num_classes\": 1000, \n  \"num_hidden_layers\": 12, \n  \"param_init_type\": \"float32\", \n  \"patch_size\": 16, \n  \"post_layernorm_residual\": false, \n  \"qkv_bias\": true, \n  \"softmax_compute_type\": \"float32\", \n  \"use_mean_pooling\": true\n}\n, qformer_config: Union[dict, mindformers.models.blip2.qformer_config.QFormerConfig] = QFormerConfig {\n  \"add_cross_attention\": true, \n  \"assessment_method\": \"\", \n  \"attention_probs_dropout_prob\": 0.1, \n  \"batch_size\": 16, \n  \"bos_token_id\": 30522, \n  \"checkpoint_name_or_path\": \"\", \n  \"chunk_size_feed_forward\": 0, \n  \"compute_dtype\": \"float16\", \n  \"convert_param_from_bert\": false, \n  \"cross_attention_freq\": 2, \n  \"dropout_prob\": 0.1, \n  \"dtype\": \"float32\", \n  \"encoder_width\": 1408, \n  \"head_embed_dim\": 256, \n  \"hidden_act\": \"gelu\", \n  \"hidden_dropout_prob\": 0.1, \n  \"hidden_size\": 768, \n  \"initializer_range\": 0.02, \n  \"intermediate_size\": 3072, \n  \"layer_norm_eps\": 1e-12, \n  \"layernorm_dtype\": \"float32\", \n  \"loss_reduction\": \"mean\", \n  \"max_position_embeddings\": 512, \n  \"mindformers_version\": \"1.4.0.beta1\", \n  \"model_type\": \"blip_2_qformer\", \n  \"num_attention_heads\": 12, \n  \"num_hidden_layers\": 12, \n  \"num_labels\": 1, \n  \"output_attentions\": false, \n  \"output_hidden_states\": false, \n  \"pad_token_id\": 0, \n  \"position_embedding_type\": \"absolute\", \n  \"post_layernorm_residual\": true, \n  \"query_length\": 32, \n  \"resize_token_embeddings\": true, \n  \"sep_token_id\": 102, \n  \"seq_length\": 128, \n  \"softmax_dtype\": \"float32\", \n  \"special_token_nums\": 1, \n  \"tie_word_embeddings\": true, \n  \"type_vocab_size\": 2, \n  \"use_one_hot_embeddings\": false, \n  \"use_past\": false, \n  \"use_relative_positions\": false, \n  \"vocab_size\": 30523\n}\n, text_config: Optional[mindformers.models.llama.llama_config.LlamaConfig] = None, parallel_config: Union[dict, mindformers.modules.transformer.transformer.TransformerOpParallelConfig] = <mindformers.modules.transformer.transformer.TransformerOpParallelConfig object>, is_training: bool = True, micro_batch_interleave_num=1, use_past=False, **kwargs)"
    },
    "mindformers.models.blip2.blip2_llm.Blip2ImageToTextGeneration": {
        "signature": "(config: mindformers.models.blip2.blip2_config.Blip2Config, **kwargs)"
    },
    "mindformers.models.blip2.blip2_llm.Blip2ImageToTextGeneration.construct": {
        "signature": "(self, image: mindspore.common.tensor.Tensor, text_input_ids: mindspore.common.tensor.Tensor)"
    },
    "mindformers.models.blip2.blip2_llm.Blip2ImageToTextGeneration.generate_text_for_image": {
        "signature": "(self, image: mindspore.common.tensor.Tensor, prompt_input_ids: mindspore.common.tensor.Tensor, **kwargs)"
    },
    "mindformers.models.blip2.blip2_llm.Blip2Llm": {
        "signature": "(config: mindformers.models.blip2.blip2_config.Blip2Config, **kwargs)"
    },
    "mindformers.models.blip2.blip2_llm.Blip2Llm.construct": {
        "signature": "(self, image: mindspore.common.tensor.Tensor, text_input_ids: mindspore.common.tensor.Tensor)"
    },
    "mindformers.models.blip2.blip2_llm.Blip2Llm.forward_qformer_and_proj": {
        "signature": "(self, image: mindspore.common.tensor.Tensor)"
    },
    "mindformers.models.blip2.qformer_config.QFormerConfig": {
        "signature": "(num_hidden_layers: int = 12, num_attention_heads: int = 12, query_length: int = 32, resize_token_embeddings: bool = True, special_token_nums: int = 1, vocab_size: int = 30523, hidden_size: int = 768, encoder_width: int = 1408, head_embed_dim: int = 256, bos_token_id: int = 30522, sep_token_id: int = 102, pad_token_id: int = 0, max_position_embeddings: int = 512, layer_norm_eps: float = 1e-12, hidden_dropout_prob: float = 0.1, attention_probs_dropout_prob: float = 0.1, chunk_size_feed_forward: int = 0, cross_attention_freq: int = 2, intermediate_size: int = 3072, initializer_range: float = 0.02, hidden_act: str = 'gelu', dtype: str = 'float32', layernorm_dtype: str = 'float32', softmax_dtype: str = 'float32', compute_dtype: str = 'float16', add_cross_attention: bool = True, use_relative_positions: bool = False, tie_word_embeddings: bool = True, output_attentions: bool = False, output_hidden_states: bool = False, convert_param_from_bert: bool = False, parallel_config: str = 'default', moe_config: str = 'default', **kwargs)"
    },
    "mindformers.models.bloom.bloom_config.BloomConfig": {
        "signature": "(embedding_dropout_prob: float = 0.0, batch_size: int = 1, seq_length: int = 1024, vocab_size: int = 250880, hidden_size: int = 64, num_layers: int = 2, num_heads: int = 8, expand_ratio: int = 4, hidden_dropout_rate: float = 0.1, attention_dropout_rate: float = 0.1, param_init_type: str = 'float32', embedding_init_type: str = 'float32', layernorm_compute_type: str = 'float32', softmax_compute_type: str = 'float32', compute_dtype: str = 'float16', hidden_act: str = 'gelu', parallel_config: Union[dict, mindformers.modules.transformer.transformer.TransformerOpParallelConfig] = <mindformers.modules.transformer.transformer.TransformerOpParallelConfig object>, checkpoint_name_or_path: str = '', moe_config: Union[dict, mindformers.modules.transformer.moe.MoEConfig] = <mindformers.modules.transformer.moe.MoEConfig object>, use_seq_parallel: bool = False, use_select_recompute: bool = False, use_past: bool = False, unk_token_id: int = 0, bos_token_id: int = 1, eos_token_id: int = 2, pad_token_id: int = 3, repetition_penalty: int = 1, max_decode_length: int = 1024, top_k: int = 5, top_p: int = 1, do_sample: bool = True, is_sample_acceleration: bool = False, use_flash_attention: bool = False, **kwargs)"
    },
    "mindformers.models.bloom.bloom_processor.BloomProcessor": {
        "signature": "(tokenizer=None, max_length=128, padding='max_length', return_tensors='ms')"
    },
    "mindformers.models.bloom.bloom_reward.BloomRewardModel": {
        "signature": "(config=None)"
    },
    "mindformers.models.bloom.bloom_reward.BloomRewardModel.construct": {
        "signature": "(self, input_ids, position_id=None, attention_mask=None, loss_mask=None, end_ind=None)"
    },
    "mindformers.models.bloom.bloom_reward.VHead": {
        "signature": "(config=None)"
    },
    "mindformers.models.bloom.bloom_reward.VHead.construct": {
        "signature": "(self, output_states)"
    },
    "mindformers.models.bloom.bloom_tokenizer.BloomTokenizer": {
        "signature": "(vocab_file, unk_token='<unk>', bos_token='<s>', eos_token='</s>', pad_token='<pad>', add_prefix_space=False, add_bos_token=False, add_eos_token=False, **kwargs)"
    },
    "mindformers.models.bloom.bloom_tokenizer.BloomTokenizer.bpe": {
        "signature": "(self, token)"
    },
    "mindformers.models.bloom.bloom_tokenizer.BloomTokenizer._tokenize": {
        "signature": "(self, text, **kwargs)"
    },
    "mindformers.models.bloom.bloom_tokenizer.BloomTokenizer._convert_token_to_id": {
        "signature": "(self, token)"
    },
    "mindformers.models.bloom.bloom_tokenizer.BloomTokenizer._convert_id_to_token": {
        "signature": "(self, index)"
    },
    "mindformers.models.bloom.bloom_tokenizer.BloomTokenizer._convert_tokens_to_string": {
        "signature": "(self, tokens)"
    },
    "mindformers.models.bloom.bloom_tokenizer.BloomTokenizer.convert_tokens_to_string": {
        "signature": "(self, tokens)"
    },
    "mindformers.models.bloom.bloom_tokenizer.BloomTokenizer.prepare_for_tokenization": {
        "signature": "(self, text, **kwargs)"
    },
    "mindformers.models.bloom.bloom_tokenizer.BloomTokenizer.save_vocabulary": {
        "signature": "(self, save_directory, filename_prefix=None)"
    },
    "mindformers.models.bloom.bloom_tokenizer.BloomTokenizer.build_inputs_with_special_tokens": {
        "signature": "(self, token_ids_0, token_ids_1=None)"
    },
    "mindformers.models.bloom.bloom_tokenizer.BloomTokenizer.create_token_type_ids_from_sequences": {
        "signature": "(self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None) -> List[int]"
    },
    "mindformers.models.bloom.bloom_tokenizer.BloomTokenizer.get_vocab": {
        "signature": "(self)"
    },
    "mindformers.models.bloom.bloom_tokenizer_fast.BloomTokenizerFast": {
        "signature": "(vocab_file=None, merges_file=None, tokenizer_file=None, unk_token='<unk>', bos_token='<s>', eos_token='</s>', pad_token='<pad>', add_prefix_space=False, clean_up_tokenization_spaces=False, **kwargs)"
    },
    "mindformers.models.bloom.bloom_tokenizer_fast.BloomTokenizerFast._batch_encode_plus": {
        "signature": "(self, *args, **kwargs) -> mindformers.models.tokenization_utils_base.BatchEncoding"
    },
    "mindformers.models.bloom.bloom_tokenizer_fast.BloomTokenizerFast._encode_plus": {
        "signature": "(self, *args, **kwargs) -> mindformers.models.tokenization_utils_base.BatchEncoding"
    },
    "mindformers.models.bloom.bloom_tokenizer_fast.BloomTokenizerFast.save_vocabulary": {
        "signature": "(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]"
    },
    "mindformers.models.clip.clip_tokenizer.CLIPTokenizer": {
        "signature": "(vocab_file, eos_token='<|endoftext|>', bos_token='<|startoftext|>', pad_token='<|endoftext|>', unk_token='<|endoftext|>', add_bos_token=True, add_eos_token=True, **kwargs)"
    },
    "mindformers.models.clip.clip_tokenizer.CLIPTokenizer._read_merge_files": {
        "signature": "(text_path, start_pos=1, end_pos=48895)"
    },
    "mindformers.models.clip.clip_tokenizer.CLIPTokenizer._tokenize": {
        "signature": "(self, text, **kwargs)"
    },
    "mindformers.models.clip.clip_tokenizer.CLIPTokenizer.build_inputs_with_special_tokens": {
        "signature": "(self, token_ids_0, token_ids_1=None)"
    },
    "mindformers.models.clip.clip_tokenizer.CLIPTokenizer.save_vocabulary": {
        "signature": "(self, save_directory, filename_prefix=None)"
    },
    "mindformers.models.clip.clip_tokenizer.CLIPTokenizer._convert_token_to_id": {
        "signature": "(self, token)"
    },
    "mindformers.models.clip.clip_tokenizer.CLIPTokenizer._convert_id_to_token": {
        "signature": "(self, index)"
    },
    "mindformers.models.clip.clip_tokenizer.CLIPTokenizer.create_token_type_ids_from_sequences": {
        "signature": "(self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None) -> List[int]"
    },
    "mindformers.models.clip.clip_tokenizer.CLIPTokenizer.get_vocab": {
        "signature": "(self)"
    },
    "mindformers.models.cogvlm2.cogvlm2.CogVLM2ForCausalLM": {
        "signature": "(config: mindformers.models.cogvlm2.cogvlm2_config.CogVLM2Config, **kwargs)"
    },
    "mindformers.models.cogvlm2.cogvlm2.CogVLM2ForCausalLM.freeze_component": {
        "signature": "(self)"
    },
    "mindformers.models.cogvlm2.cogvlm2.CogVLM2ForCausalLM._flatten_inputs": {
        "signature": "(self, input_ids, video_context_pos, batch_valid_length, slot_mapping)"
    },
    "mindformers.models.cogvlm2.cogvlm2.CogVLM2ForCausalLM.prepare_inputs_for_generation": {
        "signature": "(self, input_ids, **kwargs)"
    },
    "mindformers.models.cogvlm2.cogvlm2.CogVLM2ForCausalLM.prepare_inputs_for_mindie_generation": {
        "signature": "(self, input_ids, **kwargs)"
    },
    "mindformers.models.cogvlm2.cogvlm2.CogVLM2ForCausalLM.prepare_inputs_for_predict_layout": {
        "signature": "(self, input_ids, **kwargs)"
    },
    "mindformers.models.cogvlm2.cogvlm2.CogVLM2ForCausalLM.set_dynamic_inputs": {
        "signature": "(self, **kwargs)"
    },
    "mindformers.models.cogvlm2.cogvlm2.CogVLM2ForCausalLM.add_flags_custom": {
        "signature": "(self, is_first_iteration)"
    },
    "mindformers.models.cogvlm2.cogvlm2.CogVLM2ForCausalLM.kvcache": {
        "signature": "(self, layer_idx)"
    },
    "mindformers.models.cogvlm2.cogvlm2.CogVLM2ForCausalLM.construct": {
        "signature": "(self, input_ids, images, video_context_pos=None, position_ids=None, labels=None, input_position=None, attention_mask=None, init_reset=None, batch_valid_length=None, batch_index=None, zactivate_len=None, block_tables=None, slot_mapping=None)"
    },
    "mindformers.models.cogvlm2.cogvlm2.CogVLM2ImageForCausalLM": {
        "signature": "(config: mindformers.models.cogvlm2.cogvlm2_config.CogVLM2Config, **kwargs)"
    },
    "mindformers.models.cogvlm2.cogvlm2.CogVLM2ImageForCausalLM.prepare_inputs_for_predict_layout": {
        "signature": "(self, input_ids, **kwargs)"
    },
    "mindformers.models.cogvlm2.cogvlm2.CogVLM2ImageForCausalLM._generate_context_positions": {
        "signature": "(token_mask, target_token_id, batch_index=0)"
    },
    "mindformers.models.cogvlm2.cogvlm2.CogVLM2ImageForCausalLM.prepare_inputs_for_generation": {
        "signature": "(self, input_ids, **kwargs)"
    },
    "mindformers.models.cogvlm2.cogvlm2.CogVLM2ImageForCausalLM._to_tensor": {
        "signature": "(self, x, dtype=None)"
    },
    "mindformers.models.cogvlm2.cogvlm2.CogVLM2ImageForCausalLM.add_flags_custom": {
        "signature": "(self, is_first_iteration)"
    },
    "mindformers.models.cogvlm2.cogvlm2.CogVLM2ImageForCausalLM.kvcache": {
        "signature": "(self, layer_idx)"
    },
    "mindformers.models.cogvlm2.cogvlm2.CogVLM2ImageForCausalLM.construct": {
        "signature": "(self, input_ids, images, image_context_pos: mindspore.common.tensor.Tensor = None, labels=None, input_position=None, position_ids=None, attention_mask=None, init_reset=True, batch_valid_length=None, batch_index=None, zactivate_len=None, block_tables=None, slot_mapping=None, vision_token_mask=None, language_token_mask=None, vision_indices=None, language_indices=None)"
    },
    "mindformers.models.cogvlm2.cogvlm2_config.CogVLM2Config": {
        "signature": "(vision_model: mindformers.tools.register.config.DictConfig = None, llm_model: mindformers.tools.register.config.DictConfig = None, layers_per_stage: list = None, freeze_vision: bool = False, freeze_adapter: bool = False, freeze_llm: bool = False, is_video: bool = False, use_past: bool = False, is_dynamic: bool = False, num_queries: int = 66, proj_output_dim: int = 4096, image_start_id: int = 151857, image_pad_id: int = 151859, video_downsample: int = 1, batch_size: int = 1, parallel_config: mindformers.modules.transformer.transformer.TransformerOpParallelConfig = <mindformers.modules.transformer.transformer.TransformerOpParallelConfig object>, **kwargs)"
    },
    "mindformers.models.cogvlm2.cogvlm2_llm.CogVLM2VideoLM": {
        "signature": "(config: mindformers.models.llama.llama_config.LlamaConfig = None)"
    },
    "mindformers.models.cogvlm2.cogvlm2_llm.CogVLM2VideoLM.prepare_inputs_for_predict_layout": {
        "signature": "(self, input_ids, **kwargs)"
    },
    "mindformers.models.cogvlm2.cogvlm2_llm.CogVLM2VideoLM.set_dynamic_inputs": {
        "signature": "(self, **kwargs)"
    },
    "mindformers.models.cogvlm2.cogvlm2_llm.CogVLM2VideoLM.add_flags_custom": {
        "signature": "(self, is_first_iteration)"
    },
    "mindformers.models.cogvlm2.cogvlm2_llm.CogVLM2VideoLM.to_embeddings": {
        "signature": "(self, input_ids)"
    },
    "mindformers.models.cogvlm2.cogvlm2_llm.CogVLM2VideoLM.construct": {
        "signature": "(self, input_ids=None, labels=None, input_position=None, position_ids=None, attention_mask=None, input_embeds=None, init_reset=None, batch_valid_length=None, batch_index=None, zactivate_len=None, block_tables=None, slot_mapping=None, prefix_keys_values=None)"
    },
    "mindformers.models.cogvlm2.cogvlm2_llm.CogVLM2VideoLM.kvcache": {
        "signature": "(self, layer_idx)"
    },
    "mindformers.models.cogvlm2.cogvlm2_llm.CogVLM2VideoLMModel": {
        "signature": "(config: mindformers.models.llama.llama_config.LlamaConfig = None)"
    },
    "mindformers.models.cogvlm2.cogvlm2_llm.CogVLM2VideoLMModel.build_decoderlayer": {
        "signature": "(self, layer_id, config)"
    },
    "mindformers.models.cogvlm2.cogvlm2_llm.CogVLM2VideoLMModel.construct": {
        "signature": "(self, tokens: mindspore.common.tensor.Tensor = None, batch_valid_length=None, batch_index=None, zactivate_len=None, block_tables=None, slot_mapping=None, prefix_keys_values=None, input_embeds=None, input_attention_masks=None, position_ids=None)"
    },
    "mindformers.models.cogvlm2.cogvlm2_tokenizer.CogVLM2Tokenizer": {
        "signature": "(vocab_file, bos_token='<|begin_of_text|>', eos_token='<|end_of_text|>', pad_token='<|reserved_special_token_0|>', add_bos_token=False, add_eos_token=False, errors='replace', num_reserved_special_tokens=256, **kwargs)"
    },
    "mindformers.models.cogvlm2.cogvlm2_tokenizer.CogVLM2Tokenizer.get_vocab": {
        "signature": "(self)"
    },
    "mindformers.models.cogvlm2.cogvlm2_tokenizer.CogVLM2Tokenizer.convert_tokens_to_string": {
        "signature": "(self, tokens: List[Union[bytes, str]]) -> str"
    },
    "mindformers.models.cogvlm2.cogvlm2_tokenizer.CogVLM2Tokenizer._convert_tokens_to_ids": {
        "signature": "(self, tokens: Union[List[Union[bytes, str]], bytes, str]) -> Union[List[int], int]"
    },
    "mindformers.models.cogvlm2.cogvlm2_tokenizer.CogVLM2Tokenizer._convert_token_to_id": {
        "signature": "(self, token: Union[bytes, str]) -> int"
    },
    "mindformers.models.cogvlm2.cogvlm2_tokenizer.CogVLM2Tokenizer._convert_ids_to_tokens": {
        "signature": "(self, input_id: int)"
    },
    "mindformers.models.cogvlm2.cogvlm2_tokenizer.CogVLM2Tokenizer._convert_id_to_token": {
        "signature": "(self, index: int) -> Union[bytes, str]"
    },
    "mindformers.models.cogvlm2.cogvlm2_tokenizer.CogVLM2Tokenizer.tokenize": {
        "signature": "(self, text: str, allowed_special: Union[Set, str] = 'all', disallowed_special: Union[Collection, str] = (), **kwargs) -> List[Union[bytes, str]]"
    },
    "mindformers.models.cogvlm2.cogvlm2_tokenizer.CogVLM2Tokenizer._decode": {
        "signature": "(self, token_ids: Union[List[int], int], skip_special_tokens: bool = False, errors: str = None, **kwargs) -> str"
    },
    "mindformers.models.configuration_utils.PretrainedConfig": {
        "signature": "(**kwargs)"
    },
    "mindformers.models.configuration_utils.PretrainedConfig._to_dict_helper": {
        "signature": "(self, output)"
    },
    "mindformers.models.configuration_utils.PretrainedConfig.to_dict": {
        "signature": "(self) -> Dict[str, Any]"
    },
    "mindformers.models.configuration_utils.PretrainedConfig.from_pretrained": {
        "signature": "(yaml_name_or_path, **kwargs) -> 'PretrainedConfig'"
    },
    "mindformers.models.configuration_utils.PretrainedConfig.get_config_experimental_mode": {
        "signature": "(*args, **kwargs)"
    },
    "mindformers.models.configuration_utils.PretrainedConfig.get_config_origin_mode": {
        "signature": "(yaml_name_or_path, **kwargs)"
    },
    "mindformers.models.configuration_utils.PretrainedConfig.save_pretrained": {
        "signature": "(self, save_directory=None, save_name='mindspore_model', **kwargs)"
    },
    "mindformers.models.configuration_utils.PretrainedConfig.save_config_experimental_mode": {
        "signature": "(cls, *args, **kwargs)"
    },
    "mindformers.models.configuration_utils.PretrainedConfig.save_config_origin_mode": {
        "signature": "(self, save_directory, save_name)"
    },
    "mindformers.models.configuration_utils.PretrainedConfig.remove_type": {
        "signature": "(self)"
    },
    "mindformers.models.configuration_utils.PretrainedConfig.inverse_parse_config": {
        "signature": "(self)"
    },
    "mindformers.models.configuration_utils.PretrainedConfig._inverse_parse_config": {
        "signature": "(self)"
    },
    "mindformers.models.configuration_utils.PretrainedConfig._wrap_config": {
        "signature": "(self, config)"
    },
    "mindformers.models.configuration_utils.PretrainedConfig.show_support_list": {
        "signature": "()"
    },
    "mindformers.models.configuration_utils.PretrainedConfig.get_support_list": {
        "signature": "()"
    },
    "mindformers.models.configuration_utils.PretrainedConfig.get_config_dict": {
        "signature": "(pretrained_model_name_or_path: Union[os.PathLike, str], **kwargs) -> Tuple[Dict[str, Any], Dict[str, Any]]"
    },
    "mindformers.models.configuration_utils.PretrainedConfig._get_config_dict": {
        "signature": "(pretrained_model_name_or_path: Union[os.PathLike, str], **kwargs) -> Tuple[Dict[str, Any], Dict[str, Any]]"
    },
    "mindformers.models.configuration_utils.PretrainedConfig.from_dict": {
        "signature": "(config_dict: Dict[str, Any], **kwargs) -> 'PretrainedConfig'"
    },
    "mindformers.models.configuration_utils.PretrainedConfig.from_json_file": {
        "signature": "(json_file: Union[os.PathLike, str]) -> 'PretrainedConfig'"
    },
    "mindformers.models.configuration_utils.PretrainedConfig._dict_from_json_file": {
        "signature": "(json_file: Union[os.PathLike, str])"
    },
    "mindformers.models.configuration_utils.PretrainedConfig.to_json_file": {
        "signature": "(self, json_file_path: Union[os.PathLike, str], use_diff: bool = True)"
    },
    "mindformers.models.configuration_utils.PretrainedConfig.to_json_string": {
        "signature": "(self, use_diff: bool = True) -> str"
    },
    "mindformers.models.configuration_utils.PretrainedConfig._to_diff_dict_helper": {
        "signature": "(self, serializable_config_dict)"
    },
    "mindformers.models.configuration_utils.PretrainedConfig.to_diff_dict": {
        "signature": "(self) -> Dict[str, Any]"
    },
    "mindformers.models.configuration_utils.PretrainedConfig.delete_from_dict": {
        "signature": "(self, config_dict)"
    },
    "mindformers.models.configuration_utils.PretrainedConfig._delete_by_keys": {
        "signature": "(self, config_dict, keys)"
    },
    "mindformers.models.configuration_utils.PretrainedConfig.dict_ms_dtype_to_str": {
        "signature": "(self, d: Dict[str, Any]) -> None"
    },
    "mindformers.models.configuration_utils.PretrainedConfig.register_for_auto_class": {
        "signature": "(auto_class='AutoConfig')"
    },
    "mindformers.models.eva02.EVA02Config": {
        "signature": "(image_size: int = 224, patch_size: int = 16, num_channels: int = 3, hidden_size: int = 768, num_hidden_layers: int = 12, num_attention_heads: int = 12, intermediate_size: int = 2048, ref_feat_shape: int = 16, num_classes: int = 512, offset: int = 0, hidden_dropout_prob: float = 0.0, attention_dropout_prob: float = 0.0, drop_path_rate: float = 0.0, drop_rate: float = 0.0, layer_norm: str = 'layernorm', layer_norm_eps: float = 1e-06, class_token: bool = True, use_abs_pos_emb: bool = True, use_rot_pos_emb: bool = True, qkv_bias: bool = True, use_swiglu: bool = True, use_scale_mlp: bool = True, use_qkv_fused: bool = False, use_qkv_simple: bool = False, use_attn_norm: bool = True, use_post_norm: bool = False, post_norm: bool = False, with_cls_token: bool = False, checkpoint_name_or_path: str = '', compute_dtype: mindspore.common.dtype = mindspore.float16, parallel_config: Union[dict, mindformers.modules.transformer.transformer.TransformerOpParallelConfig] = <mindformers.modules.transformer.transformer.TransformerOpParallelConfig object>, moe_config: Union[dict, mindformers.modules.transformer.moe.MoEConfig] = <mindformers.modules.transformer.moe.MoEConfig object>, init_values=None, pipeline_stage=None, **kwargs)"
    },
    "mindformers.models.eva02.EVAModel": {
        "signature": "(config=None)"
    },
    "mindformers.models.eva02.EVAModel._pos_embed": {
        "signature": "(self, x)"
    },
    "mindformers.models.eva02.EVAModel.construct_features": {
        "signature": "(self, image)"
    },
    "mindformers.models.eva02.EVAModel.construct": {
        "signature": "(self, image)"
    },
    "mindformers.models.eva02.eva.EVAModel": {
        "signature": "(config=None)"
    },
    "mindformers.models.eva02.eva.EVAModel._pos_embed": {
        "signature": "(self, x)"
    },
    "mindformers.models.eva02.eva.EVAModel.construct_features": {
        "signature": "(self, image)"
    },
    "mindformers.models.eva02.eva.EVAModel.construct": {
        "signature": "(self, image)"
    },
    "mindformers.models.eva02.eva_config.EVA02Config": {
        "signature": "(image_size: int = 224, patch_size: int = 16, num_channels: int = 3, hidden_size: int = 768, num_hidden_layers: int = 12, num_attention_heads: int = 12, intermediate_size: int = 2048, ref_feat_shape: int = 16, num_classes: int = 512, offset: int = 0, hidden_dropout_prob: float = 0.0, attention_dropout_prob: float = 0.0, drop_path_rate: float = 0.0, drop_rate: float = 0.0, layer_norm: str = 'layernorm', layer_norm_eps: float = 1e-06, class_token: bool = True, use_abs_pos_emb: bool = True, use_rot_pos_emb: bool = True, qkv_bias: bool = True, use_swiglu: bool = True, use_scale_mlp: bool = True, use_qkv_fused: bool = False, use_qkv_simple: bool = False, use_attn_norm: bool = True, use_post_norm: bool = False, post_norm: bool = False, with_cls_token: bool = False, checkpoint_name_or_path: str = '', compute_dtype: mindspore.common.dtype = mindspore.float16, parallel_config: Union[dict, mindformers.modules.transformer.transformer.TransformerOpParallelConfig] = <mindformers.modules.transformer.transformer.TransformerOpParallelConfig object>, moe_config: Union[dict, mindformers.modules.transformer.moe.MoEConfig] = <mindformers.modules.transformer.moe.MoEConfig object>, init_values=None, pipeline_stage=None, **kwargs)"
    },
    "mindformers.models.glm.chatglm_6b_tokenizer.ChatGLMTokenizer": {
        "signature": "(vocab_file, do_lower_case=False, remove_space=False, bos_token='<sop>', eos_token='<eop>', end_token='</s>', mask_token='[MASK]', gmask_token='[gMASK]', pad_token='<pad>', unk_token='<unk>', num_image_tokens=0, **kwargs) -> None"
    },
    "mindformers.models.glm.chatglm_6b_tokenizer.ChatGLMTokenizer.get_vocab": {
        "signature": "(self)"
    },
    "mindformers.models.glm.chatglm_6b_tokenizer.ChatGLMTokenizer.preprocess_text": {
        "signature": "(self, inputs)"
    },
    "mindformers.models.glm.chatglm_6b_tokenizer.ChatGLMTokenizer._tokenize": {
        "signature": "(self, text, **kwargs)"
    },
    "mindformers.models.glm.chatglm_6b_tokenizer.ChatGLMTokenizer._decode": {
        "signature": "(self, token_ids, skip_special_tokens=False, clean_up_tokenization_spaces=None, **kwargs)"
    },
    "mindformers.models.glm.chatglm_6b_tokenizer.ChatGLMTokenizer.convert_tokens_to_ids": {
        "signature": "(self, tokens: Union[List[str], str]) -> Union[List[int], int]"
    },
    "mindformers.models.glm.chatglm_6b_tokenizer.ChatGLMTokenizer._convert_tokens_to_ids": {
        "signature": "(self, tokens: Union[List[str], str]) -> Union[List[int], int]"
    },
    "mindformers.models.glm.chatglm_6b_tokenizer.ChatGLMTokenizer._convert_token_to_id_with_added_voc": {
        "signature": "(self, token)"
    },
    "mindformers.models.glm.chatglm_6b_tokenizer.ChatGLMTokenizer._convert_token_to_id": {
        "signature": "(self, token)"
    },
    "mindformers.models.glm.chatglm_6b_tokenizer.ChatGLMTokenizer._convert_id_to_token": {
        "signature": "(self, index)"
    },
    "mindformers.models.glm.chatglm_6b_tokenizer.ChatGLMTokenizer.build_inputs_with_special_tokens": {
        "signature": "(self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None)"
    },
    "mindformers.models.glm.chatglm_6b_tokenizer.ChatGLMTokenizer.save_vocabulary": {
        "signature": "(self, save_directory, filename_prefix=None)"
    },
    "mindformers.models.glm.chatglm_6b_tokenizer.ChatGLMTokenizer.create_token_type_ids_from_sequences": {
        "signature": "(self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None) -> List[int]"
    },
    "mindformers.models.glm.glm.GLMChatModel": {
        "signature": "(config: mindformers.models.glm.glm_config.GLMConfig)"
    },
    "mindformers.models.glm.glm.GLMChatModel.sample": {
        "signature": "(self, log_probs)"
    },
    "mindformers.models.glm.glm.GLMChatModel.construct": {
        "signature": "(self, input_ids, position_ids=None, attention_mask=None, init_reset=True, batch_valid_length=None, labels=None, input_position=None, input_embeds=None, batch_index=None, zactivate_len=None, block_tables=None, slot_mapping=None)"
    },
    "mindformers.models.glm.glm.GLMForPreTraining": {
        "signature": "(config: mindformers.models.glm.glm_config.GLMConfig)"
    },
    "mindformers.models.glm.glm.GLMForPreTraining.get_masks_np": {
        "signature": "(self, input_ids)"
    },
    "mindformers.models.glm.glm.GLMForPreTraining.get_position_ids_np": {
        "signature": "(self, input_ids, mask_positions, use_gmasks=None)"
    },
    "mindformers.models.glm.glm.GLMForPreTraining.create_position_ids_np": {
        "signature": "(self, input_ids)"
    },
    "mindformers.models.glm.glm.GLMForPreTraining.update_model_kwargs_before_generate": {
        "signature": "(self, input_ids, model_kwargs: dict)"
    },
    "mindformers.models.glm.glm.GLMForPreTraining.prepare_inputs_for_generation": {
        "signature": "(self, input_ids, **kwargs)"
    },
    "mindformers.models.glm.glm.GLMForPreTraining.slice_incremental_inputs": {
        "signature": "(self, model_inputs: dict, current_index)"
    },
    "mindformers.models.glm.glm.GLMForPreTraining.add_flags_custom": {
        "signature": "(self, is_first_iteration)"
    },
    "mindformers.models.glm.glm.GLMForPreTraining.construct": {
        "signature": "(self, input_ids, labels=None, position_ids=None, attention_mask=None, input_position=None, input_embeds=None, init_reset=True, batch_valid_length=None, batch_index=None, zactivate_len=None, block_tables=None, slot_mapping=None)"
    },
    "mindformers.models.glm.glm_config.GLMConfig": {
        "signature": "(batch_size: int = 1, vocab_size: int = 130528, hidden_size: int = 4096, num_layers: int = 28, num_heads: int = 32, inner_hidden_size: int = 16384, seq_length: int = 512, embedding_dropout_prob: float = 0.0, attention_dropout_rate: float = 0.0, hidden_dropout_rate: float = 0.0, hidden_size_per_attention_head: int = None, layernorm_order: str = 'post', layernorm_epsilon: float = 1e-05, use_final_layernorm: bool = True, op_parallel_config: Union[dict, mindformers.modules.transformer.op_parallel_config.OpParallelConfig] = <mindformers.modules.transformer.op_parallel_config.OpParallelConfig object>, embed_parallel_config: Union[dict, mindformers.modules.transformer.transformer.EmbeddingOpParallelConfig] = <mindformers.modules.transformer.transformer.EmbeddingOpParallelConfig object>, parallel_config: Union[dict, mindformers.modules.transformer.transformer.TransformerOpParallelConfig] = <mindformers.modules.transformer.transformer.TransformerOpParallelConfig object>, moe_config: Union[dict, mindformers.modules.transformer.moe.MoEConfig] = <mindformers.modules.transformer.moe.MoEConfig object>, use_past: bool = False, activation_func: str = 'GELU', position_encoding_2d: bool = True, param_init_type: str = 'float16', layernorm_compute_type: str = 'float32', softmax_compute_type: str = 'float32', compute_dtype: str = 'float16', bos_token_id: int = 130004, eos_token_id: int = 130005, mask_token_id: int = 130000, gmask_token_id: int = 130001, pad_token_id: int = 3, is_enhanced_encoder: bool = True, is_sample_acceleration: bool = False, checkpoint_name_or_path: str = '', max_decode_length: int = 2048, top_k: int = 1, top_p: float = 1, repetition_penalty: float = 1.0, do_sample: bool = True, ignore_index: int = -100, ignore_token_id=None, **kwargs)"
    },
    "mindformers.models.glm.glm_processor.GLMProcessor": {
        "signature": "(tokenizer=None, max_length=128, padding='max_length', return_tensors='ms')"
    },
    "mindformers.models.glm2.ChatGLM2Config": {
        "signature": "(batch_size=1, num_layers=28, padded_vocab_size=65024, hidden_size=4096, ffn_hidden_size=13696, kv_channels=128, num_attention_heads=32, seq_length=2048, hidden_dropout=0.0, attention_dropout=0.0, layernorm_epsilon=1e-05, rope_ratio=1, rmsnorm=True, apply_residual_connection_post_layernorm=False, post_layer_norm=True, add_bias_linear=False, add_qkv_bias=True, bias_dropout_fusion=True, multi_query_attention=True, multi_query_group_num=2, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=True, fp32_residual_connection=False, quantization_bit=0, pre_seq_len=None, prefix_projection=False, param_init_type: str = 'float16', compute_dtype: str = 'float16', layernorm_compute_type: str = 'float32', residual_dtype: str = 'float32', rotary_dtype: str = None, use_past=False, use_flash_attention=False, enable_high_performance=False, block_size=16, num_blocks=128, is_dynamic=False, eos_token_id=2, pad_token_id=0, gmask_token_id=None, bos_token_id=None, repetition_penalty=1.0, checkpoint_name_or_path=None, parallel_config: Union[dict, mindformers.modules.transformer.transformer.TransformerOpParallelConfig] = <mindformers.modules.transformer.transformer.TransformerOpParallelConfig object>, offset: int = 0, pp_interleave_num: int = 1, mlp_concat: bool = True, qkv_concat: bool = True, use_rearrange_rope: bool = False, mask_generate: str = None, fine_grain_interleave: int = 1, use_ring_attention: bool = False, **kwargs)"
    },
    "mindformers.models.glm2.ChatGLM2ForConditionalGeneration": {
        "signature": "(config: mindformers.models.glm2.glm2_config.ChatGLM2Config, **kwargs)"
    },
    "mindformers.models.glm2.ChatGLM2ForConditionalGeneration.prepare_inputs_for_predict_layout": {
        "signature": "(self, input_ids, **kwargs)"
    },
    "mindformers.models.glm2.ChatGLM2ForConditionalGeneration.set_dynamic_inputs": {
        "signature": "(self, **kwargs)"
    },
    "mindformers.models.glm2.ChatGLM2ForConditionalGeneration.add_flags_custom": {
        "signature": "(self, is_first_iteration)"
    },
    "mindformers.models.glm2.ChatGLM2ForConditionalGeneration.construct": {
        "signature": "(self, input_ids=None, labels=None, input_position=None, position_ids=None, attention_mask=None, input_embeds=None, init_reset=None, batch_valid_length=None, prefix_key_values=None, block_tables=None, slot_mapping=None, batch_index=None, zactivate_len=None, input_mask=None)"
    },
    "mindformers.models.glm2.ChatGLM2ForConditionalGeneration.clear_kv_cache": {
        "signature": "(self)"
    },
    "mindformers.models.glm2.ChatGLM3Tokenizer": {
        "signature": "(vocab_file, bos_token='<sop>', eos_token='<eop>', end_token='</s>', mask_token='[MASK]', gmask_token='[gMASK]', pad_token='<pad>', unk_token='<unk>', **kwargs)"
    },
    "mindformers.models.glm2.ChatGLM3Tokenizer.get_command": {
        "signature": "(self, token)"
    },
    "mindformers.models.glm2.ChatGLM3Tokenizer.get_vocab": {
        "signature": "(self)"
    },
    "mindformers.models.glm2.ChatGLM3Tokenizer.build_single_message": {
        "signature": "(self, role, metadata, message)"
    },
    "mindformers.models.glm2.ChatGLM3Tokenizer.build_chat_input": {
        "signature": "(self, query, history=None, role='user', return_tensors='np')"
    },
    "mindformers.models.glm2.ChatGLM3Tokenizer.build_batch_input": {
        "signature": "(self, queries, histories=None, roles='user', padding=True, return_tensors='np')"
    },
    "mindformers.models.glm2.ChatGLM3Tokenizer.tokenize": {
        "signature": "(self, text, pair=None, add_special_tokens=True, **kwargs)"
    },
    "mindformers.models.glm2.ChatGLM3Tokenizer._tokenize": {
        "signature": "(self, text, **kwargs)"
    },
    "mindformers.models.glm2.ChatGLM3Tokenizer._convert_token_to_id": {
        "signature": "(self, token)"
    },
    "mindformers.models.glm2.ChatGLM3Tokenizer.convert_tokens_to_ids": {
        "signature": "(self, tokens: List[str]) -> list"
    },
    "mindformers.models.glm2.ChatGLM3Tokenizer._decode": {
        "signature": "(self, token_ids: Union[List[int], int], skip_special_tokens: bool = False, clean_up_tokenization_spaces: bool = None, **kwargs) -> str"
    },
    "mindformers.models.glm2.ChatGLM3Tokenizer._convert_id_to_token": {
        "signature": "(self, index)"
    },
    "mindformers.models.glm2.ChatGLM3Tokenizer.convert_tokens_to_string": {
        "signature": "(self, tokens: List[str]) -> str"
    },
    "mindformers.models.glm2.ChatGLM3Tokenizer.save_vocabulary": {
        "signature": "(self, save_directory, filename_prefix=None)"
    },
    "mindformers.models.glm2.ChatGLM3Tokenizer.get_prefix_tokens": {
        "signature": "(self)"
    },
    "mindformers.models.glm2.ChatGLM3Tokenizer.build_inputs_with_special_tokens": {
        "signature": "(self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None) -> List[int]"
    },
    "mindformers.models.glm2.ChatGLM3Tokenizer._pad": {
        "signature": "(self, encoded_inputs: Union[Dict[str, List[int]], mindformers.models.tokenization_utils_base.BatchEncoding], max_length: Optional[int] = None, padding_strategy: mindformers.models.tokenization_utils_base.PaddingStrategy = <PaddingStrategy.DO_NOT_PAD: 'do_not_pad'>, pad_to_multiple_of: Optional[int] = None, return_attention_mask: Optional[bool] = None) -> dict"
    },
    "mindformers.models.glm2.ChatGLM3Tokenizer.apply_chat_template": {
        "signature": "(self, conversation, return_tensors=None, **tokenizer_kwargs)"
    },
    "mindformers.models.glm2.ChatGLM4Tokenizer": {
        "signature": "(vocab_file, clean_up_tokenization_spaces=False, encode_special_tokens=False, eos_token='<|endoftext|>', pad_token='<|endoftext|>', **kwargs)"
    },
    "mindformers.models.glm2.ChatGLM4Tokenizer.get_vocab": {
        "signature": "(self)"
    },
    "mindformers.models.glm2.ChatGLM4Tokenizer.convert_tokens_to_string": {
        "signature": "(self, tokens: List[Union[bytes, int, str]]) -> str"
    },
    "mindformers.models.glm2.ChatGLM4Tokenizer._tokenize": {
        "signature": "(self, text, **kwargs)"
    },
    "mindformers.models.glm2.ChatGLM4Tokenizer._convert_token_to_id": {
        "signature": "(self, token)"
    },
    "mindformers.models.glm2.ChatGLM4Tokenizer.convert_special_tokens_to_ids": {
        "signature": "(self, token)"
    },
    "mindformers.models.glm2.ChatGLM4Tokenizer._convert_id_to_token": {
        "signature": "(self, index)"
    },
    "mindformers.models.glm2.ChatGLM4Tokenizer.save_vocabulary": {
        "signature": "(self, save_directory, filename_prefix=None)"
    },
    "mindformers.models.glm2.ChatGLM4Tokenizer.get_prefix_tokens": {
        "signature": "(self)"
    },
    "mindformers.models.glm2.ChatGLM4Tokenizer.build_single_message": {
        "signature": "(self, role, metadata, message, tokenize=True)"
    },
    "mindformers.models.glm2.ChatGLM4Tokenizer.build_inputs_with_special_tokens": {
        "signature": "(self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None) -> List[int]"
    },
    "mindformers.models.glm2.ChatGLM4Tokenizer.build_batch_input": {
        "signature": "(self, queries, histories=None, roles='user', padding=True, return_tensors='np')"
    },
    "mindformers.models.glm2.ChatGLM4Tokenizer._pad": {
        "signature": "(self, encoded_inputs: Union[Dict[str, List[int]], mindformers.models.tokenization_utils_base.BatchEncoding], max_length: Optional[int] = None, padding_strategy: mindformers.models.tokenization_utils_base.PaddingStrategy = <PaddingStrategy.DO_NOT_PAD: 'do_not_pad'>, pad_to_multiple_of: Optional[int] = None, return_attention_mask: Optional[bool] = None) -> dict"
    },
    "mindformers.models.glm2.ChatGLM4Tokenizer.build_chat_input": {
        "signature": "(self, query, history=None, role='user', return_tensors='np')"
    },
    "mindformers.models.glm2.ChatGLM4Tokenizer.apply_chat_template": {
        "signature": "(self, conversation, return_tensors=None, **tokenizer_kwargs)"
    },
    "mindformers.models.glm2.glm2.ChatGLM2ForConditionalGeneration": {
        "signature": "(config: mindformers.models.glm2.glm2_config.ChatGLM2Config, **kwargs)"
    },
    "mindformers.models.glm2.glm2.ChatGLM2ForConditionalGeneration.prepare_inputs_for_predict_layout": {
        "signature": "(self, input_ids, **kwargs)"
    },
    "mindformers.models.glm2.glm2.ChatGLM2ForConditionalGeneration.set_dynamic_inputs": {
        "signature": "(self, **kwargs)"
    },
    "mindformers.models.glm2.glm2.ChatGLM2ForConditionalGeneration.add_flags_custom": {
        "signature": "(self, is_first_iteration)"
    },
    "mindformers.models.glm2.glm2.ChatGLM2ForConditionalGeneration.construct": {
        "signature": "(self, input_ids=None, labels=None, input_position=None, position_ids=None, attention_mask=None, input_embeds=None, init_reset=None, batch_valid_length=None, prefix_key_values=None, block_tables=None, slot_mapping=None, batch_index=None, zactivate_len=None, input_mask=None)"
    },
    "mindformers.models.glm2.glm2.ChatGLM2ForConditionalGeneration.clear_kv_cache": {
        "signature": "(self)"
    },
    "mindformers.models.glm2.glm2.ChatGLM2Model": {
        "signature": "(config: mindformers.models.glm2.glm2_config.ChatGLM2Config, **kwargs)"
    },
    "mindformers.models.glm2.glm2.ChatGLM2Model.shard_head_tail": {
        "signature": "(self, config)"
    },
    "mindformers.models.glm2.glm2.ChatGLM2Model.get_masks": {
        "signature": "(self, batch_size, seq_len, padding_mask=None, input_position=None)"
    },
    "mindformers.models.glm2.glm2.ChatGLM2Model.construct": {
        "signature": "(self, input_ids, input_position=None, position_ids=None, attention_mask=None, input_embeds=None, batch_valid_length=None, full_attention_mask=None, prefix_key_values=None, block_tables=None, slot_mapping=None)"
    },
    "mindformers.models.glm2.glm2.ChatGLM2Model.clear_kv_cache": {
        "signature": "(self)"
    },
    "mindformers.models.glm2.glm2.ChatGLM2WithPtuning2": {
        "signature": "(config: mindformers.models.glm2.glm2_config.ChatGLM2Config = None, **kwargs)"
    },
    "mindformers.models.glm2.glm2.ChatGLM2WithPtuning2.construct": {
        "signature": "(self, input_ids=None, labels=None, input_position=None, position_ids=None, attention_mask=None, input_embeds=None, init_reset=True, batch_valid_length=None, prefix_key_values=None, block_tables=None, slot_mapping=None, batch_index=None, zactivate_len=None)"
    },
    "mindformers.models.glm2.glm2.ChatGLM2WithPtuning2.kvcache": {
        "signature": "(self, layer_idx)"
    },
    "mindformers.models.glm2.glm2_config.ChatGLM2Config": {
        "signature": "(batch_size=1, num_layers=28, padded_vocab_size=65024, hidden_size=4096, ffn_hidden_size=13696, kv_channels=128, num_attention_heads=32, seq_length=2048, hidden_dropout=0.0, attention_dropout=0.0, layernorm_epsilon=1e-05, rope_ratio=1, rmsnorm=True, apply_residual_connection_post_layernorm=False, post_layer_norm=True, add_bias_linear=False, add_qkv_bias=True, bias_dropout_fusion=True, multi_query_attention=True, multi_query_group_num=2, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=True, fp32_residual_connection=False, quantization_bit=0, pre_seq_len=None, prefix_projection=False, param_init_type: str = 'float16', compute_dtype: str = 'float16', layernorm_compute_type: str = 'float32', residual_dtype: str = 'float32', rotary_dtype: str = None, use_past=False, use_flash_attention=False, enable_high_performance=False, block_size=16, num_blocks=128, is_dynamic=False, eos_token_id=2, pad_token_id=0, gmask_token_id=None, bos_token_id=None, repetition_penalty=1.0, checkpoint_name_or_path=None, parallel_config: Union[dict, mindformers.modules.transformer.transformer.TransformerOpParallelConfig] = <mindformers.modules.transformer.transformer.TransformerOpParallelConfig object>, offset: int = 0, pp_interleave_num: int = 1, mlp_concat: bool = True, qkv_concat: bool = True, use_rearrange_rope: bool = False, mask_generate: str = None, fine_grain_interleave: int = 1, use_ring_attention: bool = False, **kwargs)"
    },
    "mindformers.models.glm2.glm2_tokenizer.ChatGLM2Tokenizer": {
        "signature": "(vocab_file, bos_token='<sop>', eos_token='<eop>', end_token='</s>', mask_token='[MASK]', gmask_token='[gMASK]', pad_token='<pad>', unk_token='<unk>', **kwargs)"
    },
    "mindformers.models.glm2.glm2_tokenizer.ChatGLM2Tokenizer.get_command": {
        "signature": "(self, token)"
    },
    "mindformers.models.glm2.glm2_tokenizer.ChatGLM2Tokenizer.get_vocab": {
        "signature": "(self)"
    },
    "mindformers.models.glm2.glm2_tokenizer.ChatGLM2Tokenizer.build_prompt": {
        "signature": "(self, query, history=None)"
    },
    "mindformers.models.glm2.glm2_tokenizer.ChatGLM2Tokenizer.tokenize": {
        "signature": "(self, text, pair: Optional[str] = None, add_special_tokens: bool = False, **kwargs)"
    },
    "mindformers.models.glm2.glm2_tokenizer.ChatGLM2Tokenizer._tokenize": {
        "signature": "(self, text, **kwargs)"
    },
    "mindformers.models.glm2.glm2_tokenizer.ChatGLM2Tokenizer._convert_token_to_id": {
        "signature": "(self, token)"
    },
    "mindformers.models.glm2.glm2_tokenizer.ChatGLM2Tokenizer.convert_tokens_to_ids": {
        "signature": "(self, tokens: List[str]) -> list"
    },
    "mindformers.models.glm2.glm2_tokenizer.ChatGLM2Tokenizer._decode": {
        "signature": "(self, token_ids: Union[List[int], int], skip_special_tokens: bool = False, clean_up_tokenization_spaces: bool = None, **kwargs) -> str"
    },
    "mindformers.models.glm2.glm2_tokenizer.ChatGLM2Tokenizer._convert_id_to_token": {
        "signature": "(self, index)"
    },
    "mindformers.models.glm2.glm2_tokenizer.ChatGLM2Tokenizer.convert_tokens_to_string": {
        "signature": "(self, tokens: List[str]) -> str"
    },
    "mindformers.models.glm2.glm2_tokenizer.ChatGLM2Tokenizer.save_vocabulary": {
        "signature": "(self, save_directory, filename_prefix=None)"
    },
    "mindformers.models.glm2.glm2_tokenizer.ChatGLM2Tokenizer.get_prefix_tokens": {
        "signature": "(self)"
    },
    "mindformers.models.glm2.glm2_tokenizer.ChatGLM2Tokenizer.build_inputs_with_special_tokens": {
        "signature": "(self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None) -> List[int]"
    },
    "mindformers.models.glm2.glm3_tokenizer.ChatGLM3Tokenizer": {
        "signature": "(vocab_file, bos_token='<sop>', eos_token='<eop>', end_token='</s>', mask_token='[MASK]', gmask_token='[gMASK]', pad_token='<pad>', unk_token='<unk>', **kwargs)"
    },
    "mindformers.models.glm2.glm3_tokenizer.ChatGLM3Tokenizer.get_command": {
        "signature": "(self, token)"
    },
    "mindformers.models.glm2.glm3_tokenizer.ChatGLM3Tokenizer.get_vocab": {
        "signature": "(self)"
    },
    "mindformers.models.glm2.glm3_tokenizer.ChatGLM3Tokenizer.build_single_message": {
        "signature": "(self, role, metadata, message)"
    },
    "mindformers.models.glm2.glm3_tokenizer.ChatGLM3Tokenizer.build_chat_input": {
        "signature": "(self, query, history=None, role='user', return_tensors='np')"
    },
    "mindformers.models.glm2.glm3_tokenizer.ChatGLM3Tokenizer.build_batch_input": {
        "signature": "(self, queries, histories=None, roles='user', padding=True, return_tensors='np')"
    },
    "mindformers.models.glm2.glm3_tokenizer.ChatGLM3Tokenizer.tokenize": {
        "signature": "(self, text, pair=None, add_special_tokens=True, **kwargs)"
    },
    "mindformers.models.glm2.glm3_tokenizer.ChatGLM3Tokenizer._tokenize": {
        "signature": "(self, text, **kwargs)"
    },
    "mindformers.models.glm2.glm3_tokenizer.ChatGLM3Tokenizer._convert_token_to_id": {
        "signature": "(self, token)"
    },
    "mindformers.models.glm2.glm3_tokenizer.ChatGLM3Tokenizer.convert_tokens_to_ids": {
        "signature": "(self, tokens: List[str]) -> list"
    },
    "mindformers.models.glm2.glm3_tokenizer.ChatGLM3Tokenizer._decode": {
        "signature": "(self, token_ids: Union[List[int], int], skip_special_tokens: bool = False, clean_up_tokenization_spaces: bool = None, **kwargs) -> str"
    },
    "mindformers.models.glm2.glm3_tokenizer.ChatGLM3Tokenizer._convert_id_to_token": {
        "signature": "(self, index)"
    },
    "mindformers.models.glm2.glm3_tokenizer.ChatGLM3Tokenizer.convert_tokens_to_string": {
        "signature": "(self, tokens: List[str]) -> str"
    },
    "mindformers.models.glm2.glm3_tokenizer.ChatGLM3Tokenizer.save_vocabulary": {
        "signature": "(self, save_directory, filename_prefix=None)"
    },
    "mindformers.models.glm2.glm3_tokenizer.ChatGLM3Tokenizer.get_prefix_tokens": {
        "signature": "(self)"
    },
    "mindformers.models.glm2.glm3_tokenizer.ChatGLM3Tokenizer.build_inputs_with_special_tokens": {
        "signature": "(self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None) -> List[int]"
    },
    "mindformers.models.glm2.glm3_tokenizer.ChatGLM3Tokenizer._pad": {
        "signature": "(self, encoded_inputs: Union[Dict[str, List[int]], mindformers.models.tokenization_utils_base.BatchEncoding], max_length: Optional[int] = None, padding_strategy: mindformers.models.tokenization_utils_base.PaddingStrategy = <PaddingStrategy.DO_NOT_PAD: 'do_not_pad'>, pad_to_multiple_of: Optional[int] = None, return_attention_mask: Optional[bool] = None) -> dict"
    },
    "mindformers.models.glm2.glm3_tokenizer.ChatGLM3Tokenizer.apply_chat_template": {
        "signature": "(self, conversation, return_tensors=None, **tokenizer_kwargs)"
    },
    "mindformers.models.glm2.glm4_tokenizer.ChatGLM4Tokenizer": {
        "signature": "(vocab_file, clean_up_tokenization_spaces=False, encode_special_tokens=False, eos_token='<|endoftext|>', pad_token='<|endoftext|>', **kwargs)"
    },
    "mindformers.models.glm2.glm4_tokenizer.ChatGLM4Tokenizer.get_vocab": {
        "signature": "(self)"
    },
    "mindformers.models.glm2.glm4_tokenizer.ChatGLM4Tokenizer.convert_tokens_to_string": {
        "signature": "(self, tokens: List[Union[bytes, int, str]]) -> str"
    },
    "mindformers.models.glm2.glm4_tokenizer.ChatGLM4Tokenizer._tokenize": {
        "signature": "(self, text, **kwargs)"
    },
    "mindformers.models.glm2.glm4_tokenizer.ChatGLM4Tokenizer._convert_token_to_id": {
        "signature": "(self, token)"
    },
    "mindformers.models.glm2.glm4_tokenizer.ChatGLM4Tokenizer.convert_special_tokens_to_ids": {
        "signature": "(self, token)"
    },
    "mindformers.models.glm2.glm4_tokenizer.ChatGLM4Tokenizer._convert_id_to_token": {
        "signature": "(self, index)"
    },
    "mindformers.models.glm2.glm4_tokenizer.ChatGLM4Tokenizer.save_vocabulary": {
        "signature": "(self, save_directory, filename_prefix=None)"
    },
    "mindformers.models.glm2.glm4_tokenizer.ChatGLM4Tokenizer.get_prefix_tokens": {
        "signature": "(self)"
    },
    "mindformers.models.glm2.glm4_tokenizer.ChatGLM4Tokenizer.build_single_message": {
        "signature": "(self, role, metadata, message, tokenize=True)"
    },
    "mindformers.models.glm2.glm4_tokenizer.ChatGLM4Tokenizer.build_inputs_with_special_tokens": {
        "signature": "(self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None) -> List[int]"
    },
    "mindformers.models.glm2.glm4_tokenizer.ChatGLM4Tokenizer.build_batch_input": {
        "signature": "(self, queries, histories=None, roles='user', padding=True, return_tensors='np')"
    },
    "mindformers.models.glm2.glm4_tokenizer.ChatGLM4Tokenizer._pad": {
        "signature": "(self, encoded_inputs: Union[Dict[str, List[int]], mindformers.models.tokenization_utils_base.BatchEncoding], max_length: Optional[int] = None, padding_strategy: mindformers.models.tokenization_utils_base.PaddingStrategy = <PaddingStrategy.DO_NOT_PAD: 'do_not_pad'>, pad_to_multiple_of: Optional[int] = None, return_attention_mask: Optional[bool] = None) -> dict"
    },
    "mindformers.models.glm2.glm4_tokenizer.ChatGLM4Tokenizer.build_chat_input": {
        "signature": "(self, query, history=None, role='user', return_tensors='np')"
    },
    "mindformers.models.glm2.glm4_tokenizer.ChatGLM4Tokenizer.apply_chat_template": {
        "signature": "(self, conversation, return_tensors=None, **tokenizer_kwargs)"
    },
    "mindformers.models.gpt2.gpt2.GPT2ForSequenceClassification": {
        "signature": "(config: mindformers.models.gpt2.gpt2_config.GPT2Config = None)"
    },
    "mindformers.models.gpt2.gpt2.GPT2ForSequenceClassification.construct": {
        "signature": "(self, input_ids, attention_mask, labels=None, input_embeds=None, input_position=None, position_ids=None, init_reset=True, batch_valid_length=None)"
    },
    "mindformers.models.gpt2.gpt2.GPT2LMHeadModel": {
        "signature": "(config: mindformers.models.gpt2.gpt2_config.GPT2Config = None)"
    },
    "mindformers.models.gpt2.gpt2.GPT2LMHeadModel.prepare_inputs_for_generation": {
        "signature": "(self, input_ids, **kwargs)"
    },
    "mindformers.models.gpt2.gpt2.GPT2LMHeadModel.add_flags_custom": {
        "signature": "(self, is_first_iteration)"
    },
    "mindformers.models.gpt2.gpt2.GPT2LMHeadModel.construct": {
        "signature": "(self, input_ids, attention_mask=None, input_embeds=None, labels=None, input_position=None, position_ids=None, init_reset=True, batch_valid_length=None, batch_index=None, zactivate_len=None, block_tables=None, slot_mapping=None)"
    },
    "mindformers.models.gpt2.gpt2.GPT2Model": {
        "signature": "(config)"
    },
    "mindformers.models.gpt2.gpt2.GPT2Model.construct": {
        "signature": "(self, input_ids, attention_mask=None, input_position=None, init_reset=True, batch_valid_length=None, labels=None, input_embeds=None, batch_index=None, zactivate_len=None, position_ids=None, block_tables=None, slot_mapping=None)"
    },
    "mindformers.models.gpt2.gpt2.GPTHead": {
        "signature": "(hidden_size, vocab_size, compute_type=mindspore.float16, parallel_config=None)"
    },
    "mindformers.models.gpt2.gpt2.GPTHead.construct": {
        "signature": "(self, state, embedding_table)"
    },
    "mindformers.models.gpt2.gpt2_config.GPT2Config": {
        "signature": "(batch_size: int = 1, eos_token_id: int = 50256, pad_token_id: int = 50256, bos_token_id: int = 50256, unk_token_id: int = 50256, seq_length: int = 1024, max_position_embeddings: int = None, vocab_size: int = 50257, hidden_size: int = 768, num_layers: int = 12, num_heads: int = 12, num_labels: int = 2, expand_ratio: int = 4, embedding_dropout_prob: float = 0.1, hidden_dropout_rate: float = 0.1, attention_dropout_rate: float = 0.1, param_init_type: str = 'float32', layernorm_compute_type: str = 'float32', softmax_compute_type: str = 'float32', compute_dtype: str = 'float16', hidden_act: str = 'gelu', use_past: bool = False, post_layernorm_residual: bool = False, offset: int = 0, parallel_config: Union[dict, mindformers.modules.transformer.transformer.TransformerOpParallelConfig] = <mindformers.modules.transformer.transformer.TransformerOpParallelConfig object>, checkpoint_name_or_path: str = '', moe_config: Union[dict, mindformers.modules.transformer.moe.MoEConfig] = <mindformers.modules.transformer.moe.MoEConfig object>, repetition_penalty: float = 1.0, max_decode_length: int = 1024, top_k: int = 5, top_p: float = 1.0, do_sample: bool = True, use_flash_attention: bool = False, use_prompt_flash_attention: bool = False, is_dynamic=False, block_size: int = 16, num_blocks: int = 512, **kwargs)"
    },
    "mindformers.models.gpt2.gpt2_processor.GPT2Processor": {
        "signature": "(tokenizer=None, max_length=128, padding='max_length', return_tensors='ms')"
    },
    "mindformers.models.gpt2.gpt2_tokenizer.GPT2Tokenizer": {
        "signature": "(vocab_file, merges_file, errors='replace', unk_token='<|endoftext|>', bos_token='<|endoftext|>', eos_token='<|endoftext|>', pad_token='<|endoftext|>', add_prefix_space=False, add_bos_token=False, add_eos_token=False, **kwargs)"
    },
    "mindformers.models.gpt2.gpt2_tokenizer.GPT2Tokenizer.get_vocab": {
        "signature": "(self)"
    },
    "mindformers.models.gpt2.gpt2_tokenizer.GPT2Tokenizer.bpe": {
        "signature": "(self, token)"
    },
    "mindformers.models.gpt2.gpt2_tokenizer.GPT2Tokenizer.build_inputs_with_special_tokens": {
        "signature": "(self, token_ids_0, token_ids_1=None)"
    },
    "mindformers.models.gpt2.gpt2_tokenizer.GPT2Tokenizer.get_special_tokens_mask": {
        "signature": "(self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False) -> List[int]"
    },
    "mindformers.models.gpt2.gpt2_tokenizer.GPT2Tokenizer._tokenize": {
        "signature": "(self, text, **kwargs)"
    },
    "mindformers.models.gpt2.gpt2_tokenizer.GPT2Tokenizer._convert_token_to_id": {
        "signature": "(self, token)"
    },
    "mindformers.models.gpt2.gpt2_tokenizer.GPT2Tokenizer._convert_id_to_token": {
        "signature": "(self, index)"
    },
    "mindformers.models.gpt2.gpt2_tokenizer.GPT2Tokenizer.convert_tokens_to_string": {
        "signature": "(self, tokens)"
    },
    "mindformers.models.gpt2.gpt2_tokenizer.GPT2Tokenizer.save_vocabulary": {
        "signature": "(self, save_directory, filename_prefix=None)"
    },
    "mindformers.models.gpt2.gpt2_tokenizer.GPT2Tokenizer.prepare_for_tokenization": {
        "signature": "(self, text, **kwargs)"
    },
    "mindformers.models.gpt2.gpt2_tokenizer_fast.GPT2TokenizerFast": {
        "signature": "(vocab_file=None, merges_file=None, tokenizer_file=None, unk_token='<|endoftext|>', bos_token='<|endoftext|>', pad_token='<|endoftext|>', eos_token='<|endoftext|>', add_prefix_space=False, **kwargs)"
    },
    "mindformers.models.gpt2.gpt2_tokenizer_fast.GPT2TokenizerFast.slow_tokenizer_class": {
        "signature": "(vocab_file, merges_file, errors='replace', unk_token='<|endoftext|>', bos_token='<|endoftext|>', eos_token='<|endoftext|>', pad_token='<|endoftext|>', add_prefix_space=False, add_bos_token=False, add_eos_token=False, **kwargs)"
    },
    "mindformers.models.gpt2.gpt2_tokenizer_fast.GPT2TokenizerFast._batch_encode_plus": {
        "signature": "(self, batch_text_or_text_pairs, add_special_tokens: bool = True, padding_strategy: mindformers.models.tokenization_utils_base.PaddingStrategy = <PaddingStrategy.DO_NOT_PAD: 'do_not_pad'>, truncation_strategy: mindformers.models.tokenization_utils_base.TruncationStrategy = <TruncationStrategy.DO_NOT_TRUNCATE: 'do_not_truncate'>, max_length: Optional[int] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Optional[int] = None, return_tensors: Optional[str] = None, return_token_type_ids: Optional[bool] = None, return_attention_mask: Optional[bool] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> mindformers.models.tokenization_utils_base.BatchEncoding"
    },
    "mindformers.models.gpt2.gpt2_tokenizer_fast.GPT2TokenizerFast._encode_plus": {
        "signature": "(self, text, text_pair=None, add_special_tokens: bool = True, padding_strategy: mindformers.models.tokenization_utils_base.PaddingStrategy = <PaddingStrategy.DO_NOT_PAD: 'do_not_pad'>, truncation_strategy: mindformers.models.tokenization_utils_base.TruncationStrategy = <TruncationStrategy.DO_NOT_TRUNCATE: 'do_not_truncate'>, max_length: Optional[int] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Optional[int] = None, return_tensors: Optional[bool] = None, return_token_type_ids: Optional[bool] = None, return_attention_mask: Optional[bool] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> mindformers.models.tokenization_utils_base.BatchEncoding"
    },
    "mindformers.models.gpt2.gpt2_tokenizer_fast.GPT2TokenizerFast.save_vocabulary": {
        "signature": "(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]"
    },
    "mindformers.models.llama.LlamaConfig": {
        "signature": "(batch_size: int = 1, seq_length: int = 2048, hidden_size: int = 4096, num_layers: int = 32, num_heads: int = 32, n_kv_heads: Optional[int] = None, max_position_embedding: Optional[int] = None, intermediate_size: Optional[int] = None, vocab_size: int = 32000, multiple_of: int = 256, ffn_dim_multiplier: Optional[int] = None, rms_norm_eps: float = 1e-05, bos_token_id: int = 1, eos_token_id: int = 2, pad_token_id: int = 0, ignore_token_id: int = -100, theta: float = 10000.0, compute_dtype: str = 'float16', layernorm_compute_type: str = 'float32', softmax_compute_type: str = 'float32', rotary_dtype: str = 'float32', param_init_type: str = 'float16', residual_dtype: str = None, embedding_init_type=None, qkv_has_bias: bool = False, qkv_concat: bool = False, attn_proj_has_bias: bool = False, parallel_config: Union[dict, mindformers.modules.transformer.transformer.TransformerOpParallelConfig] = <mindformers.modules.transformer.transformer.TransformerOpParallelConfig object>, moe_config: Union[dict, mindformers.modules.transformer.moe.MoEConfig] = <mindformers.modules.transformer.moe.MoEConfig object>, use_past: bool = False, extend_method: str = 'None', scaling_factor: float = 1.0, is_dynamic: bool = False, use_rope_slice: bool = False, use_flash_attention: bool = False, use_ring_attention: bool = False, use_attn_mask_compression: bool = False, use_eod_attn_mask_compression: bool = False, parallel_optimizer: bool = False, fine_grain_interleave: int = 1, pp_interleave_num: int = 1, offset: int = 0, init_method_std: float = 0.01, checkpoint_name_or_path: str = '', repetition_penalty: float = 1.0, max_decode_length: int = 1024, block_size: int = 16, num_blocks: int = 512, top_k: int = 5, top_p: float = 1.0, do_sample: bool = True, quant_config: dict = None, tie_word_embeddings: bool = False, llm_backend: str = '', fused_rms_norm: bool = True, input_sliced_sig: bool = False, rmsnorm_compute_2d: bool = False, chunk_prefill: bool = False, calculate_per_token_loss: bool = False, pipeline_stage: dict = None, **kwargs)"
    },
    "mindformers.models.llama.LlamaForCausalLM": {
        "signature": "(config: mindformers.models.llama.llama_config.LlamaConfig = None)"
    },
    "mindformers.models.llama.LlamaForCausalLM.to_embeddings": {
        "signature": "(self, tokens)"
    },
    "mindformers.models.llama.LlamaForCausalLM.prepare_inputs_for_predict_layout": {
        "signature": "(self, input_ids, **kwargs)"
    },
    "mindformers.models.llama.LlamaForCausalLM.set_dynamic_inputs": {
        "signature": "(self, **kwargs)"
    },
    "mindformers.models.llama.LlamaForCausalLM.add_flags_custom": {
        "signature": "(self, is_first_iteration)"
    },
    "mindformers.models.llama.LlamaForCausalLM.pre_gather_func": {
        "signature": "(self, pre_gather, output, batch_valid_length, gather_index=None)"
    },
    "mindformers.models.llama.LlamaForCausalLM.construct": {
        "signature": "(self, input_ids, labels=None, input_position=None, position_ids=None, attention_mask=None, input_embeds=None, init_reset=None, batch_valid_length=None, batch_index=None, zactivate_len=None, block_tables=None, slot_mapping=None, prefix_keys_values=None, llm_boost_inputs=None, q_seq_lens=None, loss_mask=None, gather_index=None, seq_range=None, actual_seq_len=None)"
    },
    "mindformers.models.llama.LlamaForCausalLM.kvcache": {
        "signature": "(self, layer_idx)"
    },
    "mindformers.models.llama.LlamaForCausalLM.convert_name": {
        "signature": "(weight_name)"
    },
    "mindformers.models.llama.LlamaForCausalLM.convert_weight_dict": {
        "signature": "(source_dict, **kwargs)"
    },
    "mindformers.models.llama.LlamaForCausalLM.convert_map_dict": {
        "signature": "(source_dict, **kwargs)"
    },
    "mindformers.models.llama.LlamaForCausalLM.obtain_qkv_ffn_concat_keys": {
        "signature": "()"
    },
    "mindformers.models.llama.LlamaForCausalLM.clear_kv_cache": {
        "signature": "(self)"
    },
    "mindformers.models.llama.LlamaTokenizer": {
        "signature": "(vocab_file, unk_token='<unk>', bos_token='<s>', eos_token='</s>', pad_token='<unk>', sp_model_kwargs: Optional[Dict[str, Any]] = None, add_bos_token=True, add_eos_token=False, clean_up_tokenization_spaces=False, legacy=True, **kwargs)"
    },
    "mindformers.models.llama.LlamaTokenizer.get_spm_processor": {
        "signature": "(self, from_slow=False)"
    },
    "mindformers.models.llama.LlamaTokenizer.get_vocab": {
        "signature": "(self)"
    },
    "mindformers.models.llama.LlamaTokenizer.tokenize": {
        "signature": "(self, text: 'TextInput', pair: Optional[str] = None, add_special_tokens: bool = False, **kwargs) -> List[str]"
    },
    "mindformers.models.llama.LlamaTokenizer._tokenize": {
        "signature": "(self, text, **kwargs)"
    },
    "mindformers.models.llama.LlamaTokenizer._convert_token_to_id": {
        "signature": "(self, token)"
    },
    "mindformers.models.llama.LlamaTokenizer._convert_id_to_token": {
        "signature": "(self, index)"
    },
    "mindformers.models.llama.LlamaTokenizer.convert_tokens_to_string": {
        "signature": "(self, tokens)"
    },
    "mindformers.models.llama.LlamaTokenizer.save_vocabulary": {
        "signature": "(self, save_directory, filename_prefix=None)"
    },
    "mindformers.models.llama.LlamaTokenizer.build_inputs_with_special_tokens": {
        "signature": "(self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None)"
    },
    "mindformers.models.llama.LlamaTokenizer.get_special_tokens_mask": {
        "signature": "(self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False) -> List[int]"
    },
    "mindformers.models.llama.LlamaTokenizer.create_token_type_ids_from_sequences": {
        "signature": "(self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None) -> List[int]"
    },
    "mindformers.models.llama.LlamaTokenizerFast": {
        "signature": "(vocab_file=None, tokenizer_file=None, clean_up_tokenization_spaces=False, unk_token='<unk>', bos_token='<s>', eos_token='</s>', add_bos_token=True, add_eos_token=False, use_default_system_prompt=False, **kwargs)"
    },
    "mindformers.models.llama.LlamaTokenizerFast.slow_tokenizer_class": {
        "signature": "(vocab_file, unk_token='<unk>', bos_token='<s>', eos_token='</s>', pad_token='<unk>', sp_model_kwargs: Optional[Dict[str, Any]] = None, add_bos_token=True, add_eos_token=False, clean_up_tokenization_spaces=False, legacy=True, **kwargs)"
    },
    "mindformers.models.llama.LlamaTokenizerFast.update_post_processor": {
        "signature": "(self)"
    },
    "mindformers.models.llama.LlamaTokenizerFast.save_vocabulary": {
        "signature": "(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]"
    },
    "mindformers.models.llama.LlamaTokenizerFast.build_inputs_with_special_tokens": {
        "signature": "(self, token_ids_0, token_ids_1=None)"
    },
    "mindformers.models.llama.llama.LlamaForCausalLM": {
        "signature": "(config: mindformers.models.llama.llama_config.LlamaConfig = None)"
    },
    "mindformers.models.llama.llama.LlamaForCausalLM.to_embeddings": {
        "signature": "(self, tokens)"
    },
    "mindformers.models.llama.llama.LlamaForCausalLM.prepare_inputs_for_predict_layout": {
        "signature": "(self, input_ids, **kwargs)"
    },
    "mindformers.models.llama.llama.LlamaForCausalLM.set_dynamic_inputs": {
        "signature": "(self, **kwargs)"
    },
    "mindformers.models.llama.llama.LlamaForCausalLM.add_flags_custom": {
        "signature": "(self, is_first_iteration)"
    },
    "mindformers.models.llama.llama.LlamaForCausalLM.pre_gather_func": {
        "signature": "(self, pre_gather, output, batch_valid_length, gather_index=None)"
    },
    "mindformers.models.llama.llama.LlamaForCausalLM.construct": {
        "signature": "(self, input_ids, labels=None, input_position=None, position_ids=None, attention_mask=None, input_embeds=None, init_reset=None, batch_valid_length=None, batch_index=None, zactivate_len=None, block_tables=None, slot_mapping=None, prefix_keys_values=None, llm_boost_inputs=None, q_seq_lens=None, loss_mask=None, gather_index=None, seq_range=None, actual_seq_len=None)"
    },
    "mindformers.models.llama.llama.LlamaForCausalLM.kvcache": {
        "signature": "(self, layer_idx)"
    },
    "mindformers.models.llama.llama.LlamaForCausalLM.convert_name": {
        "signature": "(weight_name)"
    },
    "mindformers.models.llama.llama.LlamaForCausalLM.convert_weight_dict": {
        "signature": "(source_dict, **kwargs)"
    },
    "mindformers.models.llama.llama.LlamaForCausalLM.convert_map_dict": {
        "signature": "(source_dict, **kwargs)"
    },
    "mindformers.models.llama.llama.LlamaForCausalLM.obtain_qkv_ffn_concat_keys": {
        "signature": "()"
    },
    "mindformers.models.llama.llama.LlamaForCausalLM.clear_kv_cache": {
        "signature": "(self)"
    },
    "mindformers.models.llama.llama.LlamaModel": {
        "signature": "(config: mindformers.models.llama.llama_config.LlamaConfig = None)"
    },
    "mindformers.models.llama.llama.LlamaModel.construct": {
        "signature": "(self, tokens: mindspore.common.tensor.Tensor, input_embeds=None, batch_valid_length=None, batch_index=None, zactivate_len=None, block_tables=None, slot_mapping=None, prefix_keys_values=None, attention_mask=None, position_ids=None, q_seq_lens=None, seq_range=None, actual_seq_len=None)"
    },
    "mindformers.models.llama.llama.LlamaModel.clear_kv_cache": {
        "signature": "(self)"
    },
    "mindformers.models.llama.llama_config.LlamaConfig": {
        "signature": "(batch_size: int = 1, seq_length: int = 2048, hidden_size: int = 4096, num_layers: int = 32, num_heads: int = 32, n_kv_heads: Optional[int] = None, max_position_embedding: Optional[int] = None, intermediate_size: Optional[int] = None, vocab_size: int = 32000, multiple_of: int = 256, ffn_dim_multiplier: Optional[int] = None, rms_norm_eps: float = 1e-05, bos_token_id: int = 1, eos_token_id: int = 2, pad_token_id: int = 0, ignore_token_id: int = -100, theta: float = 10000.0, compute_dtype: str = 'float16', layernorm_compute_type: str = 'float32', softmax_compute_type: str = 'float32', rotary_dtype: str = 'float32', param_init_type: str = 'float16', residual_dtype: str = None, embedding_init_type=None, qkv_has_bias: bool = False, qkv_concat: bool = False, attn_proj_has_bias: bool = False, parallel_config: Union[dict, mindformers.modules.transformer.transformer.TransformerOpParallelConfig] = <mindformers.modules.transformer.transformer.TransformerOpParallelConfig object>, moe_config: Union[dict, mindformers.modules.transformer.moe.MoEConfig] = <mindformers.modules.transformer.moe.MoEConfig object>, use_past: bool = False, extend_method: str = 'None', scaling_factor: float = 1.0, is_dynamic: bool = False, use_rope_slice: bool = False, use_flash_attention: bool = False, use_ring_attention: bool = False, use_attn_mask_compression: bool = False, use_eod_attn_mask_compression: bool = False, parallel_optimizer: bool = False, fine_grain_interleave: int = 1, pp_interleave_num: int = 1, offset: int = 0, init_method_std: float = 0.01, checkpoint_name_or_path: str = '', repetition_penalty: float = 1.0, max_decode_length: int = 1024, block_size: int = 16, num_blocks: int = 512, top_k: int = 5, top_p: float = 1.0, do_sample: bool = True, quant_config: dict = None, tie_word_embeddings: bool = False, llm_backend: str = '', fused_rms_norm: bool = True, input_sliced_sig: bool = False, rmsnorm_compute_2d: bool = False, chunk_prefill: bool = False, calculate_per_token_loss: bool = False, pipeline_stage: dict = None, **kwargs)"
    },
    "mindformers.models.llama.llama_processor.LlamaProcessor": {
        "signature": "(tokenizer=None, max_length=128, padding='max_length', return_tensors='ms')"
    },
    "mindformers.models.llama.llama_tokenizer.LlamaTokenizer": {
        "signature": "(vocab_file, unk_token='<unk>', bos_token='<s>', eos_token='</s>', pad_token='<unk>', sp_model_kwargs: Optional[Dict[str, Any]] = None, add_bos_token=True, add_eos_token=False, clean_up_tokenization_spaces=False, legacy=True, **kwargs)"
    },
    "mindformers.models.llama.llama_tokenizer.LlamaTokenizer.get_spm_processor": {
        "signature": "(self, from_slow=False)"
    },
    "mindformers.models.llama.llama_tokenizer.LlamaTokenizer.get_vocab": {
        "signature": "(self)"
    },
    "mindformers.models.llama.llama_tokenizer.LlamaTokenizer.tokenize": {
        "signature": "(self, text: 'TextInput', pair: Optional[str] = None, add_special_tokens: bool = False, **kwargs) -> List[str]"
    },
    "mindformers.models.llama.llama_tokenizer.LlamaTokenizer._tokenize": {
        "signature": "(self, text, **kwargs)"
    },
    "mindformers.models.llama.llama_tokenizer.LlamaTokenizer._convert_token_to_id": {
        "signature": "(self, token)"
    },
    "mindformers.models.llama.llama_tokenizer.LlamaTokenizer._convert_id_to_token": {
        "signature": "(self, index)"
    },
    "mindformers.models.llama.llama_tokenizer.LlamaTokenizer.convert_tokens_to_string": {
        "signature": "(self, tokens)"
    },
    "mindformers.models.llama.llama_tokenizer.LlamaTokenizer.save_vocabulary": {
        "signature": "(self, save_directory, filename_prefix=None)"
    },
    "mindformers.models.llama.llama_tokenizer.LlamaTokenizer.build_inputs_with_special_tokens": {
        "signature": "(self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None)"
    },
    "mindformers.models.llama.llama_tokenizer.LlamaTokenizer.get_special_tokens_mask": {
        "signature": "(self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False) -> List[int]"
    },
    "mindformers.models.llama.llama_tokenizer.LlamaTokenizer.create_token_type_ids_from_sequences": {
        "signature": "(self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None) -> List[int]"
    },
    "mindformers.models.llama.llama_tokenizer_fast.LlamaTokenizerFast": {
        "signature": "(vocab_file=None, tokenizer_file=None, clean_up_tokenization_spaces=False, unk_token='<unk>', bos_token='<s>', eos_token='</s>', add_bos_token=True, add_eos_token=False, use_default_system_prompt=False, **kwargs)"
    },
    "mindformers.models.llama.llama_tokenizer_fast.LlamaTokenizerFast.slow_tokenizer_class": {
        "signature": "(vocab_file, unk_token='<unk>', bos_token='<s>', eos_token='</s>', pad_token='<unk>', sp_model_kwargs: Optional[Dict[str, Any]] = None, add_bos_token=True, add_eos_token=False, clean_up_tokenization_spaces=False, legacy=True, **kwargs)"
    },
    "mindformers.models.llama.llama_tokenizer_fast.LlamaTokenizerFast.update_post_processor": {
        "signature": "(self)"
    },
    "mindformers.models.llama.llama_tokenizer_fast.LlamaTokenizerFast.save_vocabulary": {
        "signature": "(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]"
    },
    "mindformers.models.llama.llama_tokenizer_fast.LlamaTokenizerFast.build_inputs_with_special_tokens": {
        "signature": "(self, token_ids_0, token_ids_1=None)"
    },
    "mindformers.models.mae.mae.ViTMAEForPreTraining": {
        "signature": "(config=None)"
    },
    "mindformers.models.mae.mae.ViTMAEForPreTraining.init_pos_emd": {
        "signature": "(self)"
    },
    "mindformers.models.mae.mae.ViTMAEForPreTraining.init_weights": {
        "signature": "(self)"
    },
    "mindformers.models.mae.mae.ViTMAEForPreTraining.construct": {
        "signature": "(self, image, mask, ids_restore, unmask_index)"
    },
    "mindformers.models.mae.mae.ViTMAEModel": {
        "signature": "(config=None)"
    },
    "mindformers.models.mae.mae.ViTMAEModel.init_pos_emd": {
        "signature": "(self)"
    },
    "mindformers.models.mae.mae.ViTMAEModel.init_weights": {
        "signature": "(self)"
    },
    "mindformers.models.mae.mae.ViTMAEModel.construct": {
        "signature": "(self, image, unmask_index)"
    },
    "mindformers.models.mae.mae_config.ViTMAEConfig": {
        "signature": "(mask_ratio: float = 0.75, image_size: int = 224, patch_size: int = 16, num_channels: int = 3, initializer_range: float = 0.02, hidden_size: int = 768, num_hidden_layers: int = 12, num_attention_heads: int = 12, intermediate_size: int = 3072, qkv_bias: bool = True, hidden_act: str = 'gelu', post_layernorm_residual: bool = False, layer_norm_eps: float = 1e-06, attention_probs_dropout_prob: float = 0.0, hidden_dropout_prob: float = 0.0, drop_path_rate: float = 0.0, decoder_hidden_size: int = 512, decoder_num_hidden_layers: int = 8, decoder_num_attention_heads: int = 16, decoder_intermediate_size: int = 2048, norm_pix_loss: bool = True, checkpoint_name_or_path: str = '', layernorm_compute_type: mindspore.common.dtype = mindspore.float32, parallel_config: Union[dict, mindformers.modules.transformer.transformer.TransformerOpParallelConfig] = <mindformers.modules.transformer.transformer.TransformerOpParallelConfig object>, moe_config: Union[dict, mindformers.modules.transformer.moe.MoEConfig] = <mindformers.modules.transformer.moe.MoEConfig object>, init_values=None, window_size=None, **kwargs)"
    },
    "mindformers.models.mae.mae_processor.ViTMAEImageProcessor": {
        "signature": "(size=224, patch_size=16, mask_ratio=0.75, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), is_hwc=False, interpolation='cubic', **kwargs)"
    },
    "mindformers.models.mae.mae_processor.ViTMAEImageProcessor.preprocess": {
        "signature": "(self, images, **kwargs)"
    },
    "mindformers.models.mae.mae_processor.ViTMAEImageProcessor._format_inputs": {
        "signature": "(self, inputs)"
    },
    "mindformers.models.mae.mae_processor.ViTMAEImageProcessor._chw2hwc": {
        "signature": "(inputs)"
    },
    "mindformers.models.mae.mae_processor.ViTMAEProcessor": {
        "signature": "(image_processor=None, return_tensors='ms')"
    },
    "mindformers.models.mllama.MllamaConfig": {
        "signature": "(model_type: str = 'mllama', batch_size: int = 8, seq_length: int = 4096, freeze_vision: bool = True, checkpoint_name_or_path: str = None, compute_dtype: str = 'float16', layernorm_compute_type: str = 'float32', softmax_compute_type: str = 'float32', rotary_dtype: str = 'float16', param_init_type: str = 'float16', vision_model: Optional[mindformers.models.mllama.mllama_config.MllamaVisionConfig] = None, text_model: Optional[mindformers.models.mllama.mllama_config.MllamaTextConfig] = None, parallel_config: Union[dict, mindformers.modules.transformer.transformer.TransformerOpParallelConfig] = <mindformers.modules.transformer.transformer.TransformerOpParallelConfig object>, is_training: bool = True, use_past: bool = False, is_dynamic: bool = False, repetition_penalty: float = 1.0, block_size: int = 16, num_blocks: int = 512, top_k: int = 5, top_p: float = 1.0, do_sample: bool = False, use_flash_attention: bool = False, max_decode_length: int = 512, ignore_token_id: int = -100, **kwargs)"
    },
    "mindformers.models.mllama.MllamaForConditionalGeneration": {
        "signature": "(config)"
    },
    "mindformers.models.mllama.MllamaForConditionalGeneration.freeze_component": {
        "signature": "(self)"
    },
    "mindformers.models.mllama.MllamaForConditionalGeneration.to_embeddings": {
        "signature": "(self, tokens)"
    },
    "mindformers.models.mllama.MllamaForConditionalGeneration.prepare_inputs_for_predict_layout": {
        "signature": "(self, input_ids, **kwargs)"
    },
    "mindformers.models.mllama.MllamaForConditionalGeneration.set_dynamic_inputs": {
        "signature": "(self, **kwargs)"
    },
    "mindformers.models.mllama.MllamaForConditionalGeneration.add_flags_custom": {
        "signature": "(self, is_first_iteration)"
    },
    "mindformers.models.mllama.MllamaForConditionalGeneration.prepare_inputs_for_generation": {
        "signature": "(self, input_ids, **kwargs)"
    },
    "mindformers.models.mllama.MllamaForConditionalGeneration.prepare_cross_attention_mask": {
        "signature": "(self, cross_attention_mask, num_vision_tokens, dtype)"
    },
    "mindformers.models.mllama.MllamaForConditionalGeneration.construct": {
        "signature": "(self, input_ids, labels=None, pixel_values=None, aspect_ratio_mask=None, aspect_ratio_ids=None, cross_attention_mask=None, cross_attention_states=None, input_position=None, position_ids=None, attention_mask=None, input_embeds=None, init_reset=None, batch_valid_length=None, batch_index=None, zactivate_len=None, block_tables=None, slot_mapping=None, prefix_keys_values=None, llm_boost_inputs=None, q_seq_lens=None, loss_mask=None, gather_index=None, seq_range=None)"
    },
    "mindformers.models.mllama.MllamaProcessor": {
        "signature": "(output_columns, tokenizer, mode='predict', signal_type='base', *args, **kwargs)"
    },
    "mindformers.models.mllama.MllamaProcessor.build_conversation_input_text": {
        "signature": "(self, raw_inputs, result_recorder)"
    },
    "mindformers.models.mllama.MllamaProcessor.build_train_conversation": {
        "signature": "(self, raw_inputs, result_recorder)"
    },
    "mindformers.models.mllama.MllamaProcessor.build_labels": {
        "signature": "(self, text_id_list, result_recorder, **kwargs)"
    },
    "mindformers.models.mllama.MllamaProcessor.get_need_update_output_items": {
        "signature": "(self, result: mindformers.models.multi_modal.utils.DataRecord)"
    },
    "mindformers.models.mllama.MllamaProcessor.get_prompt": {
        "signature": "(self, conversations)"
    },
    "mindformers.models.mllama.MllamaProcessor.build_string_from_input": {
        "signature": "(self, prompt, bos_token, image_token)"
    },
    "mindformers.models.mllama.MllamaProcessor.build_predict_conversation": {
        "signature": "(self, raw_inputs, result_recorder)"
    },
    "mindformers.models.mllama.MllamaProcessor.check_header": {
        "signature": "(self, targets, seq)"
    },
    "mindformers.models.mllama.MllamaProcessor.replace_target": {
        "signature": "(self, target, seq)"
    },
    "mindformers.models.mllama.MllamaProcessor.batch_input_ids": {
        "signature": "(self, input_ids_list, max_length)"
    },
    "mindformers.models.mllama.MllamaProcessor.batch": {
        "signature": "(self, data_list, token_padding_length, **kwargs)"
    },
    "mindformers.models.mllama.MllamaTextConfig": {
        "signature": "(cross_attention_layers: Optional[List[int]] = None, **kwargs)"
    },
    "mindformers.models.mllama.MllamaTextModel": {
        "signature": "(config=None)"
    },
    "mindformers.models.mllama.MllamaTextModel.construct": {
        "signature": "(self, tokens: mindspore.common.tensor.Tensor, cross_attention_mask=None, cross_attention_states=None, full_text_row_masked_out_mask=None, input_embeds=None, batch_valid_length=None, batch_index=None, zactivate_len=None, block_tables=None, slot_mapping=None, prefix_keys_values=None, attention_mask=None, position_ids=None, q_seq_lens=None, seq_range=None)"
    },
    "mindformers.models.mllama.MllamaTokenizer": {
        "signature": "(vocab_file, bos_token='<|begin_of_text|>', eos_token='<|end_of_text|>', pad_token='<|finetune_right_pad_id|>', add_bos_token=False, add_eos_token=False, errors='replace', num_reserved_special_tokens=256, num_reserved_start_pos=2, special_tokens_used_num=9, **kwargs)"
    },
    "mindformers.models.mllama.MllamaVisionConfig": {
        "signature": "(hidden_size: Optional[int] = 1280, hidden_act: Optional[str] = 'gelu', num_hidden_layers: Optional[int] = 32, num_global_layers: Optional[int] = 8, num_attention_heads: Optional[int] = 16, num_channels: Optional[int] = 3, intermediate_size: Optional[int] = 5120, vision_output_dim: Optional[int] = 7680, image_size: Optional[int] = 560, patch_size: Optional[int] = 14, norm_eps: Optional[float] = 1e-05, max_num_tiles: Optional[int] = 4, max_num_images: Optional[int] = 1, output_attentions: bool = False, intermediate_layers_indices: Optional[List[int]] = None, supported_aspect_ratios: Optional[List[List[int]]] = None, initializer_range: Optional[float] = 0.02, **kwargs)"
    },
    "mindformers.models.mllama.MllamaVisionModel": {
        "signature": "(config: mindformers.models.mllama.mllama_config.MllamaVisionConfig)"
    },
    "mindformers.models.mllama.MllamaVisionModel.config_class": {
        "signature": "(hidden_size: Optional[int] = 1280, hidden_act: Optional[str] = 'gelu', num_hidden_layers: Optional[int] = 32, num_global_layers: Optional[int] = 8, num_attention_heads: Optional[int] = 16, num_channels: Optional[int] = 3, intermediate_size: Optional[int] = 5120, vision_output_dim: Optional[int] = 7680, image_size: Optional[int] = 560, patch_size: Optional[int] = 14, norm_eps: Optional[float] = 1e-05, max_num_tiles: Optional[int] = 4, max_num_images: Optional[int] = 1, output_attentions: bool = False, intermediate_layers_indices: Optional[List[int]] = None, supported_aspect_ratios: Optional[List[List[int]]] = None, initializer_range: Optional[float] = 0.02, **kwargs)"
    },
    "mindformers.models.mllama.MllamaVisionModel.apply_class_embedding": {
        "signature": "(self, hidden_state)"
    },
    "mindformers.models.mllama.MllamaVisionModel.prepare_aspect_ratio_attention_mask": {
        "signature": "(self, aspect_ratio_mask, num_patches, target_length, dtype)"
    },
    "mindformers.models.mllama.MllamaVisionModel.construct": {
        "signature": "(self, pixel_values, aspect_ratio_ids, aspect_ratio_mask)"
    },
    "mindformers.models.mllama.mllama.MllamaForCausalLM": {
        "signature": "(config=None)"
    },
    "mindformers.models.mllama.mllama.MllamaForCausalLM.add_flags_custom": {
        "signature": "(self, is_first_iteration)"
    },
    "mindformers.models.mllama.mllama.MllamaForCausalLM.pre_gather_func": {
        "signature": "(self, pre_gather, output, batch_valid_length)"
    },
    "mindformers.models.mllama.mllama.MllamaForCausalLM.construct": {
        "signature": "(self, tokens, cross_attention_mask=None, cross_attention_states=None, full_text_row_masked_out_mask=None, input_embeds=None, batch_valid_length=None, batch_index=None, zactivate_len=None, block_tables=None, slot_mapping=None, prefix_keys_values=None, attention_mask=None, position_ids=None, q_seq_lens=None, seq_range=None)"
    },
    "mindformers.models.mllama.mllama.MllamaForCausalLM.kvcache": {
        "signature": "(self, layer_idx)"
    },
    "mindformers.models.mllama.mllama.MllamaForConditionalGeneration": {
        "signature": "(config)"
    },
    "mindformers.models.mllama.mllama.MllamaForConditionalGeneration.freeze_component": {
        "signature": "(self)"
    },
    "mindformers.models.mllama.mllama.MllamaForConditionalGeneration.to_embeddings": {
        "signature": "(self, tokens)"
    },
    "mindformers.models.mllama.mllama.MllamaForConditionalGeneration.prepare_inputs_for_predict_layout": {
        "signature": "(self, input_ids, **kwargs)"
    },
    "mindformers.models.mllama.mllama.MllamaForConditionalGeneration.set_dynamic_inputs": {
        "signature": "(self, **kwargs)"
    },
    "mindformers.models.mllama.mllama.MllamaForConditionalGeneration.add_flags_custom": {
        "signature": "(self, is_first_iteration)"
    },
    "mindformers.models.mllama.mllama.MllamaForConditionalGeneration.prepare_inputs_for_generation": {
        "signature": "(self, input_ids, **kwargs)"
    },
    "mindformers.models.mllama.mllama.MllamaForConditionalGeneration.prepare_cross_attention_mask": {
        "signature": "(self, cross_attention_mask, num_vision_tokens, dtype)"
    },
    "mindformers.models.mllama.mllama.MllamaForConditionalGeneration.construct": {
        "signature": "(self, input_ids, labels=None, pixel_values=None, aspect_ratio_mask=None, aspect_ratio_ids=None, cross_attention_mask=None, cross_attention_states=None, input_position=None, position_ids=None, attention_mask=None, input_embeds=None, init_reset=None, batch_valid_length=None, batch_index=None, zactivate_len=None, block_tables=None, slot_mapping=None, prefix_keys_values=None, llm_boost_inputs=None, q_seq_lens=None, loss_mask=None, gather_index=None, seq_range=None)"
    },
    "mindformers.models.mllama.mllama.MllamaTextModel": {
        "signature": "(config=None)"
    },
    "mindformers.models.mllama.mllama.MllamaTextModel.construct": {
        "signature": "(self, tokens: mindspore.common.tensor.Tensor, cross_attention_mask=None, cross_attention_states=None, full_text_row_masked_out_mask=None, input_embeds=None, batch_valid_length=None, batch_index=None, zactivate_len=None, block_tables=None, slot_mapping=None, prefix_keys_values=None, attention_mask=None, position_ids=None, q_seq_lens=None, seq_range=None)"
    },
    "mindformers.models.mllama.mllama.MllamaVisionModel": {
        "signature": "(config: mindformers.models.mllama.mllama_config.MllamaVisionConfig)"
    },
    "mindformers.models.mllama.mllama.MllamaVisionModel.config_class": {
        "signature": "(hidden_size: Optional[int] = 1280, hidden_act: Optional[str] = 'gelu', num_hidden_layers: Optional[int] = 32, num_global_layers: Optional[int] = 8, num_attention_heads: Optional[int] = 16, num_channels: Optional[int] = 3, intermediate_size: Optional[int] = 5120, vision_output_dim: Optional[int] = 7680, image_size: Optional[int] = 560, patch_size: Optional[int] = 14, norm_eps: Optional[float] = 1e-05, max_num_tiles: Optional[int] = 4, max_num_images: Optional[int] = 1, output_attentions: bool = False, intermediate_layers_indices: Optional[List[int]] = None, supported_aspect_ratios: Optional[List[List[int]]] = None, initializer_range: Optional[float] = 0.02, **kwargs)"
    },
    "mindformers.models.mllama.mllama.MllamaVisionModel.apply_class_embedding": {
        "signature": "(self, hidden_state)"
    },
    "mindformers.models.mllama.mllama.MllamaVisionModel.prepare_aspect_ratio_attention_mask": {
        "signature": "(self, aspect_ratio_mask, num_patches, target_length, dtype)"
    },
    "mindformers.models.mllama.mllama.MllamaVisionModel.construct": {
        "signature": "(self, pixel_values, aspect_ratio_ids, aspect_ratio_mask)"
    },
    "mindformers.models.mllama.mllama_config.MllamaConfig": {
        "signature": "(model_type: str = 'mllama', batch_size: int = 8, seq_length: int = 4096, freeze_vision: bool = True, checkpoint_name_or_path: str = None, compute_dtype: str = 'float16', layernorm_compute_type: str = 'float32', softmax_compute_type: str = 'float32', rotary_dtype: str = 'float16', param_init_type: str = 'float16', vision_model: Optional[mindformers.models.mllama.mllama_config.MllamaVisionConfig] = None, text_model: Optional[mindformers.models.mllama.mllama_config.MllamaTextConfig] = None, parallel_config: Union[dict, mindformers.modules.transformer.transformer.TransformerOpParallelConfig] = <mindformers.modules.transformer.transformer.TransformerOpParallelConfig object>, is_training: bool = True, use_past: bool = False, is_dynamic: bool = False, repetition_penalty: float = 1.0, block_size: int = 16, num_blocks: int = 512, top_k: int = 5, top_p: float = 1.0, do_sample: bool = False, use_flash_attention: bool = False, max_decode_length: int = 512, ignore_token_id: int = -100, **kwargs)"
    },
    "mindformers.models.mllama.mllama_tokenizer.MllamaTokenizer": {
        "signature": "(vocab_file, bos_token='<|begin_of_text|>', eos_token='<|end_of_text|>', pad_token='<|finetune_right_pad_id|>', add_bos_token=False, add_eos_token=False, errors='replace', num_reserved_special_tokens=256, num_reserved_start_pos=2, special_tokens_used_num=9, **kwargs)"
    },
    "mindformers.models.modeling_utils.PreTrainedModel": {
        "signature": "(config: mindformers.models.configuration_utils.PretrainedConfig, *inputs, **kwargs)"
    },
    "mindformers.models.modeling_utils.PreTrainedModel.post_init": {
        "signature": "(self)"
    },
    "mindformers.models.modeling_utils.PreTrainedModel._from_config": {
        "signature": "(config, **kwargs)"
    },
    "mindformers.models.modeling_utils.PreTrainedModel.can_generate": {
        "signature": "() -> bool"
    },
    "mindformers.models.modeling_utils.PreTrainedModel.save_pretrained": {
        "signature": "(self, save_directory: Union[os.PathLike, str], save_name: str = 'mindspore_model', **kwargs)"
    },
    "mindformers.models.modeling_utils.PreTrainedModel.save_pretrained_experimental_mode": {
        "signature": "(self, save_directory: Union[os.PathLike, str], is_main_process: bool = True, state_dict: Optional[dict] = None, push_to_hub: bool = False, max_shard_size: Union[int, str] = '5GB', variant: Optional[str] = None, token: Union[NoneType, bool, str] = None, **kwargs)"
    },
    "mindformers.models.modeling_utils.PreTrainedModel.save_pretrained_origin_mode": {
        "signature": "(self, save_directory: Optional[str] = None, save_name: str = 'mindspore_model')"
    },
    "mindformers.models.modeling_utils.PreTrainedModel.remove_type": {
        "signature": "(self, config)"
    },
    "mindformers.models.modeling_utils.PreTrainedModel.prepare_inputs_for_predict_layout": {
        "signature": "(self, input_ids, **kwargs)"
    },
    "mindformers.models.modeling_utils.PreTrainedModel.set_dynamic_inputs": {
        "signature": "(self, **kwargs)"
    },
    "mindformers.models.modeling_utils.PreTrainedModel._inverse_parse_config": {
        "signature": "(self, config)"
    },
    "mindformers.models.modeling_utils.PreTrainedModel._wrap_config": {
        "signature": "(self, config)"
    },
    "mindformers.models.modeling_utils.PreTrainedModel._get_config_args": {
        "signature": "(pretrained_model_name_or_dir, **kwargs)"
    },
    "mindformers.models.modeling_utils.PreTrainedModel.is_experimental_mode": {
        "signature": "(pretrained_model_name_or_dir)"
    },
    "mindformers.models.modeling_utils.PreTrainedModel.from_pretrained": {
        "signature": "(pretrained_model_name_or_dir: str, *model_args, **kwargs)"
    },
    "mindformers.models.modeling_utils.PreTrainedModel.from_pretrained_origin_mode": {
        "signature": "(pretrained_model_name_or_dir: str, **kwargs)"
    },
    "mindformers.models.modeling_utils.PreTrainedModel.from_pretrained_experimental_mode": {
        "signature": "(pretrained_model_name_or_path: Union[NoneType, os.PathLike, str], *model_args, config: Union[NoneType, mindformers.models.configuration_utils.PretrainedConfig, os.PathLike, str] = None, cache_dir: Union[NoneType, os.PathLike, str] = None, ignore_mismatched_sizes: bool = False, force_download: bool = False, local_files_only: bool = False, token: Union[NoneType, bool, str] = None, revision: str = 'main', **kwargs)"
    },
    "mindformers.models.modeling_utils.PreTrainedModel._get_src_checkpoint": {
        "signature": "(state_dict, resolved_archive_file, src_checkpoint)"
    },
    "mindformers.models.modeling_utils.PreTrainedModel._load_pretrained_model": {
        "signature": "(model, state_dict, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes=False)"
    },
    "mindformers.models.modeling_utils.PreTrainedModel.register_for_auto_class": {
        "signature": "(auto_class='AutoModel')"
    },
    "mindformers.models.modeling_utils.PreTrainedModel.load_checkpoint": {
        "signature": "(self, config)"
    },
    "mindformers.models.modeling_utils.PreTrainedModel.show_support_list": {
        "signature": "()"
    },
    "mindformers.models.modeling_utils.PreTrainedModel.get_support_list": {
        "signature": "()"
    },
    "mindformers.models.modeling_utils.PreTrainedModel.kvcache": {
        "signature": "(self, layer_idx)"
    },
    "mindformers.models.modeling_utils.PreTrainedModel.fuse_weight_from_ckpt": {
        "signature": "(self, ckpt_dict)"
    },
    "mindformers.models.modeling_utils.PreTrainedModel.convert_name": {
        "signature": "(weight_name)"
    },
    "mindformers.models.modeling_utils.PreTrainedModel.convert_weight_dict": {
        "signature": "(source_dict, **kwargs)"
    },
    "mindformers.models.modeling_utils.PreTrainedModel.convert_map_dict": {
        "signature": "(source_dict, **kwargs)"
    },
    "mindformers.models.modeling_utils.PreTrainedModel.obtain_qkv_ffn_concat_keys": {
        "signature": "()"
    },
    "mindformers.models.multi_modal.ModalContentTransformTemplate": {
        "signature": "(output_columns: List[str] = None, tokenizer=None, mode='predict', vstack_columns: List[str] = None, modal_content_padding_size=1, max_length=2048, **kwargs)"
    },
    "mindformers.models.multi_modal.ModalContentTransformTemplate.process_predict_query": {
        "signature": "(self, query_ele_list: List[Dict], result_recorder: mindformers.models.multi_modal.utils.DataRecord)"
    },
    "mindformers.models.multi_modal.ModalContentTransformTemplate.process_train_item": {
        "signature": "(self, conversation_list: List[List], result_recorder: mindformers.models.multi_modal.utils.DataRecord)"
    },
    "mindformers.models.multi_modal.ModalContentTransformTemplate.build_conversation_input_text": {
        "signature": "(self, raw_inputs, result_recorder: mindformers.models.multi_modal.utils.DataRecord)"
    },
    "mindformers.models.multi_modal.ModalContentTransformTemplate.build_modal_context": {
        "signature": "(self, input_ids, result_recorder: mindformers.models.multi_modal.utils.DataRecord, **kwargs)"
    },
    "mindformers.models.multi_modal.ModalContentTransformTemplate.build_labels": {
        "signature": "(self, text_id_list, result_recorder, **kwargs)"
    },
    "mindformers.models.multi_modal.ModalContentTransformTemplate.generate_modal_context_positions": {
        "signature": "(self, input_ids, batch_index: int = 0, result_recorder: mindformers.models.multi_modal.utils.DataRecord = None, **kwargs)"
    },
    "mindformers.models.multi_modal.ModalContentTransformTemplate.check_modal_builder_tokens": {
        "signature": "(self, tokenizer)"
    },
    "mindformers.models.multi_modal.ModalContentTransformTemplate.get_need_update_output_items": {
        "signature": "(self, result: mindformers.models.multi_modal.utils.DataRecord) -> Dict[str, Any]"
    },
    "mindformers.models.multi_modal.ModalContentTransformTemplate.batch_input_ids": {
        "signature": "(self, input_ids_list, max_length)"
    },
    "mindformers.models.multi_modal.ModalContentTransformTemplate.stack_data": {
        "signature": "(self, data, need_vstack: bool = False)"
    },
    "mindformers.models.multi_modal.ModalContentTransformTemplate.try_to_batch": {
        "signature": "(self, data_list, column_name)"
    },
    "mindformers.models.multi_modal.ModalContentTransformTemplate.batch": {
        "signature": "(self, data_list, token_padding_length, **kwargs)"
    },
    "mindformers.models.multi_modal.ModalContentTransformTemplate.post_process": {
        "signature": "(self, output_ids, **kwargs)"
    },
    "mindformers.models.pangualpha.pangualpha.PanguAlphaHeadModel": {
        "signature": "(config: mindformers.models.pangualpha.pangualpha_config.PanguAlphaConfig = None)"
    },
    "mindformers.models.pangualpha.pangualpha.PanguAlphaHeadModel.prepare_inputs_for_generation": {
        "signature": "(self, input_ids, **kwargs)"
    },
    "mindformers.models.pangualpha.pangualpha.PanguAlphaHeadModel.construct": {
        "signature": "(self, input_ids, input_position=None, attention_mask=None, position_ids=None, input_embeds=None, labels=None, init_reset=True, batch_valid_length=None)"
    },
    "mindformers.models.pangualpha.pangualpha.PanguAlphaModel": {
        "signature": "(config)"
    },
    "mindformers.models.pangualpha.pangualpha.PanguAlphaModel.construct": {
        "signature": "(self, input_ids, input_position, encoder_masks, init_reset=True, batch_valid_length=None)"
    },
    "mindformers.models.pangualpha.pangualpha.PanguAlphaModel.reshape_to_2d": {
        "signature": "(self, x)"
    },
    "mindformers.models.pangualpha.pangualpha.PanguAlphaPromptTextClassificationModel": {
        "signature": "(config: mindformers.models.pangualpha.pangualpha_config.PanguAlphaConfig = None)"
    },
    "mindformers.models.pangualpha.pangualpha.PanguAlphaPromptTextClassificationModel.construct": {
        "signature": "(self, input_ids=None, labels=None, attention_mask=None, position_ids=None, input_embeds=None, input_position=None, init_reset=True, batch_valid_length=None)"
    },
    "mindformers.models.pangualpha.pangualpha_config.PanguAlphaConfig": {
        "signature": "(batch_size: int = 1, seq_length: int = 1024, vocab_size: int = 40000, hidden_size: int = 2560, ffn_hidden_size: int = 10240, num_layers: int = 32, num_heads: int = 32, pad_token_id: int = 6, eos_token_id: int = 8, post_layernorm_residual: bool = False, param_init_type: str = 'float32', compute_dtype: str = 'float16', softmax_compute_type: str = 'float16', embedding_dropout_prob: float = 0.1, hidden_dropout_rate: float = 0.1, attention_dropout_rate: float = 0.1, hidden_act: str = 'fast_gelu', use_past: bool = False, parallel_config: Union[dict, mindformers.modules.transformer.transformer.TransformerOpParallelConfig] = <mindformers.modules.transformer.transformer.TransformerOpParallelConfig object>, moe_config: Union[dict, mindformers.modules.transformer.moe.MoEConfig] = <mindformers.modules.transformer.moe.MoEConfig object>, use_moe: bool = False, expert_num: int = 1, per_token_num_experts_chosen: int = 1, checkpoint_name_or_path: str = '', repetition_penalty: float = 1.0, max_decode_length: int = 1024, top_k: int = 5, top_p: float = 1.0, do_sample: bool = True, **kwargs)"
    },
    "mindformers.models.pangualpha.pangualpha_processor.PanguAlphaProcessor": {
        "signature": "(tokenizer=None, max_length=128, padding='max_length', return_tensors='ms')"
    },
    "mindformers.models.pangualpha.pangualpha_tokenizer.PanguAlphaTokenizer": {
        "signature": "(vocab_file, eos_token='<eod>', bos_token='<s>', unk_token='<unk>', pad_token='<pad>', add_bos_token=False, add_eos_token=False, **kwargs)"
    },
    "mindformers.models.pangualpha.pangualpha_tokenizer.PanguAlphaTokenizer._tokenize": {
        "signature": "(self, text, **kwargs)"
    },
    "mindformers.models.pangualpha.pangualpha_tokenizer.PanguAlphaTokenizer.convert_tokens_to_ids": {
        "signature": "(self, tokens)"
    },
    "mindformers.models.pangualpha.pangualpha_tokenizer.PanguAlphaTokenizer._convert_token_to_id": {
        "signature": "(self, token)"
    },
    "mindformers.models.pangualpha.pangualpha_tokenizer.PanguAlphaTokenizer._convert_id_to_token": {
        "signature": "(self, index)"
    },
    "mindformers.models.pangualpha.pangualpha_tokenizer.PanguAlphaTokenizer.convert_tokens_to_string": {
        "signature": "(self, tokens)"
    },
    "mindformers.models.pangualpha.pangualpha_tokenizer.PanguAlphaTokenizer.process_tokens": {
        "signature": "(self, text)"
    },
    "mindformers.models.pangualpha.pangualpha_tokenizer.PanguAlphaTokenizer.pangu_encode": {
        "signature": "(self, text)"
    },
    "mindformers.models.pangualpha.pangualpha_tokenizer.PanguAlphaTokenizer.pangu_decode": {
        "signature": "(self, tokens)"
    },
    "mindformers.models.pangualpha.pangualpha_tokenizer.PanguAlphaTokenizer.save_vocabulary": {
        "signature": "(self, save_directory, filename_prefix=None)"
    },
    "mindformers.models.pangualpha.pangualpha_tokenizer.PanguAlphaTokenizer.build_inputs_with_special_tokens": {
        "signature": "(self, token_ids_0, token_ids_1=None)"
    },
    "mindformers.models.pangualpha.pangualpha_tokenizer.PanguAlphaTokenizer.create_token_type_ids_from_sequences": {
        "signature": "(self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None) -> List[int]"
    },
    "mindformers.models.pangualpha.pangualpha_tokenizer.PanguAlphaTokenizer.get_vocab": {
        "signature": "(self)"
    },
    "mindformers.models.sam.sam_utils.MaskData": {
        "signature": "(**kwargs) -> None"
    },
    "mindformers.models.sam.sam_utils.MaskData.items": {
        "signature": "(self) -> ItemsView[str, Any]"
    },
    "mindformers.models.sam.sam_utils.MaskData.filter": {
        "signature": "(self, keep: numpy.ndarray) -> None"
    },
    "mindformers.models.sam.sam_utils.MaskData.cat": {
        "signature": "(self, new_stats: 'MaskData') -> None"
    },
    "mindformers.models.sam.sam_utils.area_from_rle": {
        "signature": "(rle: Dict[str, Any]) -> int"
    },
    "mindformers.models.sam.sam_utils.batch_iterator": {
        "signature": "(batch_size: int, *args) -> Generator[List[Any], NoneType, NoneType]"
    },
    "mindformers.models.sam.sam_utils.batched_mask_to_box": {
        "signature": "(masks)"
    },
    "mindformers.models.sam.sam_utils.box_area": {
        "signature": "(boxes)"
    },
    "mindformers.models.sam.sam_utils.box_xyxy_to_xywh": {
        "signature": "(box_xyxy: numpy.ndarray) -> numpy.ndarray"
    },
    "mindformers.models.sam.sam_utils.build_all_layer_point_grids": {
        "signature": "(n_per_side: int, n_layers: int, scale_per_layer: int) -> List[numpy.ndarray]"
    },
    "mindformers.models.sam.sam_utils.calculate_stability_score": {
        "signature": "(masks: numpy.ndarray, mask_threshold: float, threshold_offset: float) -> numpy.ndarray"
    },
    "mindformers.models.sam.sam_utils.coco_encode_rle": {
        "signature": "(uncompressed_rle: Dict[str, Any]) -> Dict[str, Any]"
    },
    "mindformers.models.sam.sam_utils.generate_crop_boxes": {
        "signature": "(im_size: Tuple[int, ...], n_layers: int, overlap_ratio: float) -> Tuple[List[List[int]], List[int]]"
    },
    "mindformers.models.sam.sam_utils.is_box_near_crop_edge": {
        "signature": "(boxes: numpy.ndarray, crop_box: List[int], orig_box: List[int], atol: float = 20.0) -> numpy.ndarray"
    },
    "mindformers.models.sam.sam_utils.mask_to_rle": {
        "signature": "(tensor: numpy.ndarray) -> List[Dict[str, Any]]"
    },
    "mindformers.models.sam.sam_utils.nms": {
        "signature": "(boxes: numpy.ndarray, scores: numpy.ndarray, iou_threshold: float)"
    },
    "mindformers.models.sam.sam_utils.remove_small_regions": {
        "signature": "(mask: numpy.ndarray, area_thresh: float, mode: str) -> Tuple[numpy.ndarray, bool]"
    },
    "mindformers.models.sam.sam_utils.rle_to_mask": {
        "signature": "(rle: Dict[str, Any]) -> numpy.ndarray"
    },
    "mindformers.models.sam.sam_utils.uncrop_boxes_xyxy": {
        "signature": "(boxes: numpy.ndarray, crop_box: List[int]) -> numpy.ndarray"
    },
    "mindformers.models.sam.sam_utils.uncrop_masks": {
        "signature": "(masks: numpy.ndarray, crop_box: List[int], orig_h: int, orig_w: int) -> numpy.ndarray"
    },
    "mindformers.models.sam.sam_utils.uncrop_points": {
        "signature": "(points: numpy.ndarray, crop_box: List[int]) -> numpy.ndarray"
    },
    "mindformers.models.swin.swin.SwinForImageClassification": {
        "signature": "(config: mindformers.models.configuration_utils.PretrainedConfig = None)"
    },
    "mindformers.models.swin.swin.SwinForImageClassification.construct": {
        "signature": "(self, image, target=None)"
    },
    "mindformers.models.swin.swin.SwinForImageClassification.load_checkpoint": {
        "signature": "(self, config)"
    },
    "mindformers.models.swin.swin.SwinForImageClassification.remap_pretrained_keys_swin": {
        "signature": "(self, checkpoint_model)"
    },
    "mindformers.models.swin.swin.SwinForImageClassification._del_unused_keys": {
        "signature": "(checkpoint_model, key_name)"
    },
    "mindformers.models.swin.swin.SwinModel": {
        "signature": "(config: mindformers.models.configuration_utils.PretrainedConfig = None)"
    },
    "mindformers.models.swin.swin.SwinModel.construct": {
        "signature": "(self, x)"
    },
    "mindformers.models.swin.swin_config.SwinConfig": {
        "signature": "(image_size: int = 224, patch_size: int = 4, num_channels: int = 3, embed_dim: int = 128, depths: list = (2, 2, 18, 2), num_heads: list = (4, 8, 16, 32), window_size: int = 7, shift_size: int = 0, mlp_ratio: float = 4.0, qkv_bias: bool = True, layer_norm_eps: float = 1e-05, hidden_dropout_prob: float = 0.0, attention_probs_dropout_prob: float = 0.0, drop_path_rate: float = 0.1, use_absolute_embeddings: bool = False, patch_norm: bool = True, hidden_act: str = 'gelu', weight_init: str = 'normal', num_labels: int = 1000, loss_type: str = 'SoftTargetCrossEntropy', param_init_type: mindspore.common.dtype = mindspore.float32, moe_config: Union[dict, mindformers.modules.transformer.moe.MoEConfig] = <mindformers.modules.transformer.moe.MoEConfig object>, parallel_config: Union[dict, mindformers.modules.transformer.transformer.TransformerOpParallelConfig] = <mindformers.modules.transformer.transformer.TransformerOpParallelConfig object>, checkpoint_name_or_path: str = '', **kwargs)"
    },
    "mindformers.models.swin.swin_processor.SwinImageProcessor": {
        "signature": "(size=224, resize=256, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), is_hwc=False, interpolation='cubic', **kwargs)"
    },
    "mindformers.models.swin.swin_processor.SwinImageProcessor.preprocess": {
        "signature": "(self, images, **kwargs)"
    },
    "mindformers.models.swin.swin_processor.SwinImageProcessor._format_inputs": {
        "signature": "(self, inputs)"
    },
    "mindformers.models.swin.swin_processor.SwinImageProcessor._chw2hwc": {
        "signature": "(inputs)"
    },
    "mindformers.models.swin.swin_processor.SwinProcessor": {
        "signature": "(image_processor=None, return_tensors='ms')"
    },
    "mindformers.models.t5.mt5.MT5ForConditionalGeneration": {
        "signature": "(config: mindformers.models.t5.t5_config.T5Config)"
    },
    "mindformers.models.t5.mt5.MT5ForConditionalGeneration._add_start_to_inputs": {
        "signature": "(self, target_ids)"
    },
    "mindformers.models.t5.mt5.MT5ForConditionalGeneration._add_eos_to_inputs": {
        "signature": "(self, target_ids)"
    },
    "mindformers.models.t5.mt5.MT5ForConditionalGeneration.encoder_forward": {
        "signature": "(self, source_ids, source_mask)"
    },
    "mindformers.models.t5.mt5.MT5ForConditionalGeneration.construct": {
        "signature": "(self, input_ids, attention_mask, labels=None, decoder_input_ids=None, decoder_attention_mask=None, memory_mask=None, encoder_outputs=None)"
    },
    "mindformers.models.t5.t5.T5ForConditionalGeneration": {
        "signature": "(config: mindformers.models.t5.t5_config.T5Config)"
    },
    "mindformers.models.t5.t5.T5ForConditionalGeneration._add_start_to_inputs": {
        "signature": "(self, target_ids)"
    },
    "mindformers.models.t5.t5.T5ForConditionalGeneration._add_eos_to_inputs": {
        "signature": "(self, target_ids)"
    },
    "mindformers.models.t5.t5.T5ForConditionalGeneration.encoder_forward": {
        "signature": "(self, source_ids, source_mask)"
    },
    "mindformers.models.t5.t5.T5ForConditionalGeneration.construct": {
        "signature": "(self, input_ids, attention_mask, labels=None, decoder_input_ids=None, decoder_attention_mask=None, memory_mask=None, encoder_outputs=None, return_loss=False)"
    },
    "mindformers.models.t5.t5_config.T5Config": {
        "signature": "(vocab_size: int = 32128, hidden_size: int = 512, d_kv: int = 64, d_ff: int = 2048, num_layers: int = 6, num_decoder_layers: int = None, num_heads: int = 8, relative_attention_num_buckets: int = 32, hidden_dropout_rate: float = 0.1, attention_dropout_rate: float = 0.1, embedding_dropout_prob: float = 0.1, layer_norm_epsilon: float = 1e-06, initializer_factor: float = 1.0, is_encoder_decoder: bool = True, use_cache: bool = True, pad_token_id: int = 0, start_token_id: int = 0, eos_token_id: int = 1, batch_size: int = 1, seq_length: int = 1024, max_position_embeddings: int = 1024, initializer_range: float = 0.02, max_decode_length: int = 128, length_penalty_weight: float = 1.0, compute_dtype: str = 'float32', has_relative_bias: bool = True, scale_output: bool = True, parallel_config: Union[dict, mindformers.modules.transformer.transformer.TransformerOpParallelConfig] = <mindformers.modules.transformer.transformer.TransformerOpParallelConfig object>, checkpoint_name_or_path: str = None, top_p: float = 0.95, top_k: int = 1, repetition_penalty: float = 1.0, max_length: int = 20, do_sample: bool = False, param_init_type: str = 'float32', layernorm_compute_type: str = 'float32', softmax_compute_type: str = 'float32', hidden_act: str = 'relu', post_layernorm_residual: bool = False, offset: int = 0, use_past: bool = False, moe_config: Union[dict, mindformers.modules.transformer.moe.MoEConfig] = <mindformers.modules.transformer.moe.MoEConfig object>, dtype=None, **kwargs)"
    },
    "mindformers.models.t5.t5_processor.T5Processor": {
        "signature": "(tokenizer=None, max_length=77, tgt_max_length=128, padding='max_length', return_tensors='ms')"
    },
    "mindformers.models.t5.t5_tokenizer.T5PegasusTokenizer": {
        "signature": "(vocab_file, *args, pre_tokenizer=<function T5PegasusTokenizer.<lambda>>, **kwargs)"
    },
    "mindformers.models.t5.t5_tokenizer.T5PegasusTokenizer._tokenize": {
        "signature": "(self, text, **kwargs)"
    },
    "mindformers.models.t5.t5_tokenizer.T5PegasusTokenizer.convert_tokens_to_string": {
        "signature": "(self, tokens: list)"
    },
    "mindformers.models.t5.t5_tokenizer.T5PegasusTokenizer._decode": {
        "signature": "(self, token_ids, skip_special_tokens=False, clean_up_tokenization_spaces=None, **kwargs)"
    },
    "mindformers.models.t5.t5_tokenizer.T5Tokenizer": {
        "signature": "(vocab_file, eos_token='</s>', unk_token='<unk>', pad_token='<pad>', extra_ids=100, additional_special_tokens=None, sp_model_kwargs: Optional[Dict[str, Any]] = None, legacy=True, **kwargs) -> None"
    },
    "mindformers.models.t5.t5_tokenizer.T5Tokenizer.get_spm_processor": {
        "signature": "(self, from_slow=False)"
    },
    "mindformers.models.t5.t5_tokenizer.T5Tokenizer._eventually_correct_t5_max_length": {
        "signature": "(pretrained_model_name_or_path, max_model_length, init_max_model_length)"
    },
    "mindformers.models.t5.t5_tokenizer.T5Tokenizer.get_vocab": {
        "signature": "(self)"
    },
    "mindformers.models.t5.t5_tokenizer.T5Tokenizer.get_special_tokens_mask": {
        "signature": "(self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False) -> List[int]"
    },
    "mindformers.models.t5.t5_tokenizer.T5Tokenizer.get_sentinel_tokens": {
        "signature": "(self)"
    },
    "mindformers.models.t5.t5_tokenizer.T5Tokenizer.get_sentinel_token_ids": {
        "signature": "(self)"
    },
    "mindformers.models.t5.t5_tokenizer.T5Tokenizer._add_eos_if_not_present": {
        "signature": "(self, token_ids: List[int]) -> List[int]"
    },
    "mindformers.models.t5.t5_tokenizer.T5Tokenizer.create_token_type_ids_from_sequences": {
        "signature": "(self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None) -> List[int]"
    },
    "mindformers.models.t5.t5_tokenizer.T5Tokenizer.build_inputs_with_special_tokens": {
        "signature": "(self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None) -> List[int]"
    },
    "mindformers.models.t5.t5_tokenizer.T5Tokenizer.tokenize": {
        "signature": "(self, text: 'TextInput', pair: Optional[str] = None, add_special_tokens: bool = False, **kwargs) -> List[str]"
    },
    "mindformers.models.t5.t5_tokenizer.T5Tokenizer._tokenize": {
        "signature": "(self, text, **kwargs)"
    },
    "mindformers.models.t5.t5_tokenizer.T5Tokenizer._convert_token_to_id": {
        "signature": "(self, token)"
    },
    "mindformers.models.t5.t5_tokenizer.T5Tokenizer._convert_id_to_token": {
        "signature": "(self, index)"
    },
    "mindformers.models.t5.t5_tokenizer.T5Tokenizer.convert_tokens_to_string": {
        "signature": "(self, tokens)"
    },
    "mindformers.models.t5.t5_tokenizer.T5Tokenizer.save_vocabulary": {
        "signature": "(self, save_directory, filename_prefix=None)"
    },
    "mindformers.models.t5.t5_tokenizer_fast.T5TokenizerFast": {
        "signature": "(vocab_file=None, tokenizer_file=None, eos_token='</s>', unk_token='<unk>', pad_token='<pad>', extra_ids=100, additional_special_tokens=None, **kwargs)"
    },
    "mindformers.models.t5.t5_tokenizer_fast.T5TokenizerFast.slow_tokenizer_class": {
        "signature": "(vocab_file, eos_token='</s>', unk_token='<unk>', pad_token='<pad>', extra_ids=100, additional_special_tokens=None, sp_model_kwargs: Optional[Dict[str, Any]] = None, legacy=True, **kwargs) -> None"
    },
    "mindformers.models.t5.t5_tokenizer_fast.T5TokenizerFast._eventually_correct_t5_max_length": {
        "signature": "(pretrained_model_name_or_path, max_model_length, init_max_model_length)"
    },
    "mindformers.models.t5.t5_tokenizer_fast.T5TokenizerFast.save_vocabulary": {
        "signature": "(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]"
    },
    "mindformers.models.t5.t5_tokenizer_fast.T5TokenizerFast.build_inputs_with_special_tokens": {
        "signature": "(self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None) -> List[int]"
    },
    "mindformers.models.t5.t5_tokenizer_fast.T5TokenizerFast.create_token_type_ids_from_sequences": {
        "signature": "(self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None) -> List[int]"
    },
    "mindformers.models.t5.t5_tokenizer_fast.T5TokenizerFast.get_sentinel_tokens": {
        "signature": "(self)"
    },
    "mindformers.models.t5.t5_tokenizer_fast.T5TokenizerFast.get_sentinel_token_ids": {
        "signature": "(self)"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizer": {
        "signature": "(**kwargs)"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizer.get_added_vocab": {
        "signature": "(self) -> Dict[str, int]"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizer._add_tokens": {
        "signature": "(self, new_tokens: Union[List[str], List[tokenizers.AddedToken]], special_tokens: bool = False) -> int"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizer._update_trie": {
        "signature": "(self, unique_no_split_tokens: Optional[str] = None)"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizer.num_special_tokens_to_add": {
        "signature": "(self, pair: bool = False) -> int"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizer.tokenize": {
        "signature": "(self, text: str, pair: Optional[str] = None, add_special_tokens: bool = False, **kwargs) -> List[str]"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizer.tokenize_atom": {
        "signature": "(self, tokens, no_split_token)"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizer._tokenize": {
        "signature": "(self, text, **kwargs)"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids": {
        "signature": "(self, tokens: Union[List[str], str]) -> Union[List[int], int]"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizer._convert_token_to_id_with_added_voc": {
        "signature": "(self, token)"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizer._convert_token_to_id": {
        "signature": "(self, token)"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizer._encode_plus": {
        "signature": "(self, text: Union[List[int], List[str], str], text_pair: Union[List[int], List[str], NoneType, str] = None, add_special_tokens: bool = True, padding_strategy: mindformers.models.tokenization_utils_base.PaddingStrategy = <PaddingStrategy.DO_NOT_PAD: 'do_not_pad'>, truncation_strategy: mindformers.models.tokenization_utils_base.TruncationStrategy = <TruncationStrategy.DO_NOT_TRUNCATE: 'do_not_truncate'>, max_length: Optional[int] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Optional[int] = None, return_tensors: Union[NoneType, mindformers.models.tokenization_utils_base.TensorType, str] = None, return_token_type_ids: Optional[bool] = None, return_attention_mask: Optional[bool] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> mindformers.models.tokenization_utils_base.BatchEncoding"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizer._batch_encode_plus": {
        "signature": "(self, batch_text_or_text_pairs: Union[List[List[int]], List[List[str]], List[Tuple[List[int], List[int]]], List[Tuple[List[str], List[str]]], List[Tuple[str, str]], List[str]], add_special_tokens: bool = True, padding_strategy: mindformers.models.tokenization_utils_base.PaddingStrategy = <PaddingStrategy.DO_NOT_PAD: 'do_not_pad'>, truncation_strategy: mindformers.models.tokenization_utils_base.TruncationStrategy = <TruncationStrategy.DO_NOT_TRUNCATE: 'do_not_truncate'>, max_length: Optional[int] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Optional[int] = None, return_tensors: Union[NoneType, mindformers.models.tokenization_utils_base.TensorType, str] = None, return_token_type_ids: Optional[bool] = None, return_attention_mask: Optional[bool] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> mindformers.models.tokenization_utils_base.BatchEncoding"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizer._batch_prepare_for_model": {
        "signature": "(self, batch_ids_pairs: List[Union[Tuple[List[int], NoneType], Tuple[List[str], List[str]]]], add_special_tokens: bool = True, padding_strategy: mindformers.models.tokenization_utils_base.PaddingStrategy = <PaddingStrategy.DO_NOT_PAD: 'do_not_pad'>, truncation_strategy: mindformers.models.tokenization_utils_base.TruncationStrategy = <TruncationStrategy.DO_NOT_TRUNCATE: 'do_not_truncate'>, max_length: Optional[int] = None, stride: int = 0, pad_to_multiple_of: Optional[int] = None, return_tensors: Optional[str] = None, return_token_type_ids: Optional[bool] = None, return_attention_mask: Optional[bool] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_length: bool = False, verbose: bool = True) -> mindformers.models.tokenization_utils_base.BatchEncoding"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizer.prepare_for_tokenization": {
        "signature": "(self, text: str, **kwargs) -> Tuple[str, Dict[str, Any]]"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizer.get_special_tokens_mask": {
        "signature": "(self, token_ids_0: List, token_ids_1: Optional[List] = None, already_has_special_tokens: bool = False) -> List[int]"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizer.convert_ids_to_tokens": {
        "signature": "(self, ids: Union[List[int], int], skip_special_tokens: bool = False) -> Union[List[str], str]"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizer._convert_id_to_token": {
        "signature": "(self, index: int) -> str"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_string": {
        "signature": "(self, tokens: List[str]) -> str"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizer._decode": {
        "signature": "(self, token_ids: List[int], skip_special_tokens: bool = False, clean_up_tokenization_spaces: bool = None, **kwargs) -> str"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizerBase": {
        "signature": "(**kwargs)"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizerBase._set_processor_class": {
        "signature": "(self, processor_class: str)"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizerBase.get_vocab": {
        "signature": "(self) -> Dict[str, int]"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizerBase.apply_chat_template": {
        "signature": "(self, conversation: Union[ForwardRef('Conversation'), List[Dict[str, str]]], chat_template: Optional[str] = None, add_generation_prompt: bool = False, tokenize: bool = True, padding: bool = False, truncation: bool = False, max_length: Optional[int] = None, return_tensors: Union[NoneType, mindformers.models.tokenization_utils_base.TensorType, str] = None, **tokenizer_kwargs) -> Union[List[int], str]"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizerBase._compile_jinja_template": {
        "signature": "(self, chat_template)"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizerBase.from_pretrained": {
        "signature": "(name_or_path, *args, **kwargs)"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizerBase.from_origin_pretrained": {
        "signature": "(name_or_path: str, **kwargs)"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizerBase._download_using_name": {
        "signature": "(name_or_path)"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizerBase.cache_vocab_files": {
        "signature": "(name_or_path, cache_path=None)"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizerBase._get_class_name_and_args_form_config": {
        "signature": "(config)"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizerBase.read_files_according_specific_by_tokenizer": {
        "signature": "(name_or_path)"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizerBase.from_experimental_pretrained": {
        "signature": "(pretrained_model_name_or_path, *args, **kwargs)"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizerBase._from_experimental_pretrained": {
        "signature": "(resolved_vocab_files, pretrained_model_name_or_path, init_configuration, *init_inputs, token=None, cache_dir=None, local_files_only=False, commit_hash=None, is_local=False, **kwargs)"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizerBase._eventually_correct_t5_max_length": {
        "signature": "(pretrained_model_name_or_path, max_model_length, init_max_model_length)"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizerBase.convert_added_tokens": {
        "signature": "(obj: Union[Any, tokenizers.AddedToken], save=False, add_type_field=True)"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizerBase.save_pretrained": {
        "signature": "(self, save_directory: Optional[str] = None, save_name: str = 'mindspore_model', file_format: str = 'yaml', save_json: bool = False, **kwargs)"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizerBase.save_origin_pretrained": {
        "signature": "(self, save_directory: Optional[str] = None, save_name: str = 'mindspore_model', file_format: str = 'yaml')"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizerBase.save_experimental_pretrained": {
        "signature": "(self, save_directory: Union[os.PathLike, str], filename_prefix: Optional[str] = None, **kwargs) -> Tuple[str]"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizerBase._save_pretrained": {
        "signature": "(self, save_directory: Union[os.PathLike, str], file_names: Tuple[str], legacy_format: Optional[bool] = None, filename_prefix: Optional[str] = None) -> Tuple[str]"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizerBase.save_vocabulary": {
        "signature": "(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizerBase.tokenize": {
        "signature": "(self, text: str, pair: Optional[str] = None, add_special_tokens: bool = False, **kwargs) -> List[str]"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizerBase.encode": {
        "signature": "(self, text: Union[List[int], List[str], str], text_pair: Union[List[int], List[str], NoneType, str] = None, add_special_tokens: bool = True, padding: Union[bool, mindformers.models.tokenization_utils_base.PaddingStrategy, str] = False, truncation: Union[bool, mindformers.models.tokenization_utils_base.TruncationStrategy, str] = None, max_length: Optional[int] = None, stride: int = 0, return_tensors: Union[NoneType, mindformers.models.tokenization_utils_base.TensorType, str] = None, **kwargs) -> List[int]"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizerBase.num_special_tokens_to_add": {
        "signature": "(self, pair: bool = False) -> int"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizerBase._get_padding_truncation_strategies": {
        "signature": "(self, padding=False, truncation=None, max_length=None, pad_to_multiple_of=None, verbose=True, **kwargs)"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizerBase._call_one": {
        "signature": "(self, text: Union[List[List[str]], List[str], str], text_pair: Union[List[List[str]], List[str], NoneType, str] = None, add_special_tokens: bool = True, padding: Union[bool, mindformers.models.tokenization_utils_base.PaddingStrategy, str] = False, truncation: Union[bool, mindformers.models.tokenization_utils_base.TruncationStrategy, str] = None, max_length: Optional[int] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Optional[int] = None, return_tensors: Union[NoneType, mindformers.models.tokenization_utils_base.TensorType, str] = None, return_token_type_ids: Optional[bool] = None, return_attention_mask: Optional[bool] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> mindformers.models.tokenization_utils_base.BatchEncoding"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizerBase.encode_plus": {
        "signature": "(self, text: Union[List[int], List[str], str], text_pair: Union[List[int], List[str], NoneType, str] = None, add_special_tokens: bool = True, padding: Union[bool, mindformers.models.tokenization_utils_base.PaddingStrategy, str] = False, truncation: Union[bool, mindformers.models.tokenization_utils_base.TruncationStrategy, str] = None, max_length: Optional[int] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Optional[int] = None, return_tensors: Union[NoneType, mindformers.models.tokenization_utils_base.TensorType, str] = None, return_token_type_ids: Optional[bool] = None, return_attention_mask: Optional[bool] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> mindformers.models.tokenization_utils_base.BatchEncoding"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizerBase._encode_plus": {
        "signature": "(self, text: Union[List[int], List[str], str], text_pair: Union[List[int], List[str], NoneType, str] = None, add_special_tokens: bool = True, padding_strategy: mindformers.models.tokenization_utils_base.PaddingStrategy = <PaddingStrategy.DO_NOT_PAD: 'do_not_pad'>, truncation_strategy: mindformers.models.tokenization_utils_base.TruncationStrategy = <TruncationStrategy.DO_NOT_TRUNCATE: 'do_not_truncate'>, max_length: Optional[int] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Optional[int] = None, return_tensors: Union[NoneType, mindformers.models.tokenization_utils_base.TensorType, str] = None, return_token_type_ids: Optional[bool] = None, return_attention_mask: Optional[bool] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> mindformers.models.tokenization_utils_base.BatchEncoding"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizerBase.batch_encode_plus": {
        "signature": "(self, batch_text_or_text_pairs: Union[List[List[int]], List[List[str]], List[Tuple[List[int], List[int]]], List[Tuple[List[str], List[str]]], List[Tuple[str, str]], List[str]], add_special_tokens: bool = True, padding: Union[bool, mindformers.models.tokenization_utils_base.PaddingStrategy, str] = False, truncation: Union[bool, mindformers.models.tokenization_utils_base.TruncationStrategy, str] = None, max_length: Optional[int] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Optional[int] = None, return_tensors: Union[NoneType, mindformers.models.tokenization_utils_base.TensorType, str] = None, return_token_type_ids: Optional[bool] = None, return_attention_mask: Optional[bool] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> mindformers.models.tokenization_utils_base.BatchEncoding"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizerBase._batch_encode_plus": {
        "signature": "(self, batch_text_or_text_pairs: Union[List[List[int]], List[List[str]], List[Tuple[List[int], List[int]]], List[Tuple[List[str], List[str]]], List[Tuple[str, str]], List[str]], add_special_tokens: bool = True, padding_strategy: mindformers.models.tokenization_utils_base.PaddingStrategy = <PaddingStrategy.DO_NOT_PAD: 'do_not_pad'>, truncation_strategy: mindformers.models.tokenization_utils_base.TruncationStrategy = <TruncationStrategy.DO_NOT_TRUNCATE: 'do_not_truncate'>, max_length: Optional[int] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Optional[int] = None, return_tensors: Union[NoneType, mindformers.models.tokenization_utils_base.TensorType, str] = None, return_token_type_ids: Optional[bool] = None, return_attention_mask: Optional[bool] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> mindformers.models.tokenization_utils_base.BatchEncoding"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizerBase.pad": {
        "signature": "(self, encoded_inputs: Union[Dict[str, List[List[int]]], Dict[str, List[int]], List[Dict[str, List[int]]], List[mindformers.models.tokenization_utils_base.BatchEncoding], mindformers.models.tokenization_utils_base.BatchEncoding], padding: Union[bool, mindformers.models.tokenization_utils_base.PaddingStrategy, str] = True, max_length: Optional[int] = None, pad_to_multiple_of: Optional[int] = None, return_attention_mask: Optional[bool] = None, return_tensors: Union[NoneType, mindformers.models.tokenization_utils_base.TensorType, str] = None, verbose: bool = True) -> mindformers.models.tokenization_utils_base.BatchEncoding"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizerBase.create_token_type_ids_from_sequences": {
        "signature": "(self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None) -> List[int]"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizerBase.build_inputs_with_special_tokens": {
        "signature": "(self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None) -> List[int]"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizerBase.prepare_for_model": {
        "signature": "(self, ids: List[int], pair_ids: Optional[List[int]] = None, add_special_tokens: bool = True, padding: Union[bool, mindformers.models.tokenization_utils_base.PaddingStrategy, str] = False, truncation: Union[bool, mindformers.models.tokenization_utils_base.TruncationStrategy, str] = None, max_length: Optional[int] = None, stride: int = 0, pad_to_multiple_of: Optional[int] = None, return_tensors: Union[NoneType, mindformers.models.tokenization_utils_base.TensorType, str] = None, return_token_type_ids: Optional[bool] = None, return_attention_mask: Optional[bool] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_length: bool = False, verbose: bool = True, prepend_batch_axis: bool = False, **kwargs) -> mindformers.models.tokenization_utils_base.BatchEncoding"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizerBase.truncate_sequences": {
        "signature": "(self, ids: List[int], pair_ids: Optional[List[int]] = None, num_tokens_to_remove: int = 0, truncation_strategy: Union[mindformers.models.tokenization_utils_base.TruncationStrategy, str] = 'longest_first', stride: int = 0) -> Tuple[List[int], List[int], List[int]]"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizerBase._pad": {
        "signature": "(self, encoded_inputs: Union[Dict[str, List[int]], mindformers.models.tokenization_utils_base.BatchEncoding], max_length: Optional[int] = None, padding_strategy: mindformers.models.tokenization_utils_base.PaddingStrategy = <PaddingStrategy.DO_NOT_PAD: 'do_not_pad'>, pad_to_multiple_of: Optional[int] = None, return_attention_mask: Optional[bool] = None) -> dict"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizerBase.convert_tokens_to_string": {
        "signature": "(self, tokens: List[str]) -> str"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizerBase.batch_decode": {
        "signature": "(self, sequences: Union[ForwardRef('np.ndarray'), ForwardRef('tf.Tensor'), ForwardRef('torch.Tensor'), List[List[int]], List[int]], skip_special_tokens: bool = False, clean_up_tokenization_spaces: bool = None, **kwargs) -> List[str]"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizerBase.decode": {
        "signature": "(self, token_ids: Union[ForwardRef('np.ndarray'), List[int], int], skip_special_tokens: bool = False, clean_up_tokenization_spaces: bool = None, **kwargs) -> str"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizerBase._decode": {
        "signature": "(self, token_ids: Union[List[int], int], skip_special_tokens: bool = False, clean_up_tokenization_spaces: bool = None, **kwargs) -> str"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizerBase.get_special_tokens_mask": {
        "signature": "(self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False) -> List[int]"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizerBase.clean_up_tokenization": {
        "signature": "(out_string: str) -> str"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizerBase._eventual_warn_about_too_long_sequence": {
        "signature": "(self, ids: List[int], max_length: Optional[int], verbose: bool)"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizerBase._switch_to_input_mode": {
        "signature": "(self)"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizerBase._switch_to_target_mode": {
        "signature": "(self)"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizerBase.as_target_tokenizer": {
        "signature": "(self)"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizerBase.register_for_auto_class": {
        "signature": "(auto_class='AutoTokenizer')"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizerBase.prepare_seq2seq_batch": {
        "signature": "(self, src_texts: List[str], tgt_texts: Optional[List[str]] = None, max_length: Optional[int] = None, max_target_length: Optional[int] = None, padding: str = 'longest', return_tensors: str = None, truncation: bool = True, **kwargs) -> mindformers.models.tokenization_utils_base.BatchEncoding"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizerBase.show_support_list": {
        "signature": "()"
    },
    "mindformers.models.tokenization_utils.PreTrainedTokenizerBase.get_support_list": {
        "signature": "()"
    },
    "mindformers.modules.activation.CELU": {
        "signature": "(alpha=1.0)"
    },
    "mindformers.modules.activation.CELU.construct": {
        "signature": "(self, x)"
    },
    "mindformers.modules.activation.ELU": {
        "signature": "(alpha=1.0)"
    },
    "mindformers.modules.activation.ELU.construct": {
        "signature": "(self, x)"
    },
    "mindformers.modules.activation.FastGelu": {
        "signature": "()"
    },
    "mindformers.modules.activation.FastGelu.construct": {
        "signature": "(self, x)"
    },
    "mindformers.modules.activation.GELU": {
        "signature": "(approximate=True)"
    },
    "mindformers.modules.activation.GELU.construct": {
        "signature": "(self, x)"
    },
    "mindformers.modules.activation.GLU": {
        "signature": "(axis=-1)"
    },
    "mindformers.modules.activation.GLU.construct": {
        "signature": "(self, x)"
    },
    "mindformers.modules.activation.HShrink": {
        "signature": "(lambd=0.5)"
    },
    "mindformers.modules.activation.HShrink.construct": {
        "signature": "(self, input_x)"
    },
    "mindformers.modules.activation.HSigmoid": {
        "signature": "()"
    },
    "mindformers.modules.activation.HSigmoid.construct": {
        "signature": "(self, input_x)"
    },
    "mindformers.modules.activation.HSwish": {
        "signature": "()"
    },
    "mindformers.modules.activation.HSwish.construct": {
        "signature": "(self, x)"
    },
    "mindformers.modules.activation.Hardtanh": {
        "signature": "(min_val=-1.0, max_val=1.0)"
    },
    "mindformers.modules.activation.Hardtanh.construct": {
        "signature": "(self, x)"
    },
    "mindformers.modules.activation.LRN": {
        "signature": "(depth_radius=5, bias=1.0, alpha=1.0, beta=0.5, norm_region='ACROSS_CHANNELS')"
    },
    "mindformers.modules.activation.LRN.construct": {
        "signature": "(self, input_x)"
    },
    "mindformers.modules.activation.LeakyReLU": {
        "signature": "(alpha=0.2)"
    },
    "mindformers.modules.activation.LeakyReLU.construct": {
        "signature": "(self, x)"
    },
    "mindformers.modules.activation.LogSigmoid": {
        "signature": "()"
    },
    "mindformers.modules.activation.LogSigmoid.construct": {
        "signature": "(self, input_x)"
    },
    "mindformers.modules.activation.LogSoftmax": {
        "signature": "(axis=-1)"
    },
    "mindformers.modules.activation.LogSoftmax.construct": {
        "signature": "(self, x)"
    },
    "mindformers.modules.activation.Mish": {
        "signature": "()"
    },
    "mindformers.modules.activation.Mish.construct": {
        "signature": "(self, input_x)"
    },
    "mindformers.modules.activation.PReLU": {
        "signature": "(channel=1, w=0.25)"
    },
    "mindformers.modules.activation.PReLU.construct": {
        "signature": "(self, x)"
    },
    "mindformers.modules.activation.RReLU": {
        "signature": "(lower=0.125, upper=0.3333333333333333)"
    },
    "mindformers.modules.activation.RReLU.construct": {
        "signature": "(self, x)"
    },
    "mindformers.modules.activation.ReLU": {
        "signature": "()"
    },
    "mindformers.modules.activation.ReLU.construct": {
        "signature": "(self, x)"
    },
    "mindformers.modules.activation.ReLU6": {
        "signature": "()"
    },
    "mindformers.modules.activation.ReLU6.construct": {
        "signature": "(self, x)"
    },
    "mindformers.modules.activation.SeLU": {
        "signature": "()"
    },
    "mindformers.modules.activation.SeLU.construct": {
        "signature": "(self, input_x)"
    },
    "mindformers.modules.activation.SiLU": {
        "signature": "(auto_prefix=True, flags=None)"
    },
    "mindformers.modules.activation.SiLU.construct": {
        "signature": "(self, x)"
    },
    "mindformers.modules.activation.Sigmoid": {
        "signature": "()"
    },
    "mindformers.modules.activation.Sigmoid.construct": {
        "signature": "(self, x)"
    },
    "mindformers.modules.activation.SoftShrink": {
        "signature": "(lambd=0.5)"
    },
    "mindformers.modules.activation.SoftShrink.construct": {
        "signature": "(self, input_x)"
    },
    "mindformers.modules.activation.Softmax": {
        "signature": "(axis=-1)"
    },
    "mindformers.modules.activation.Softmax.construct": {
        "signature": "(self, x)"
    },
    "mindformers.modules.activation.Softmax2d": {
        "signature": "()"
    },
    "mindformers.modules.activation.Softmax2d._check_input_dim": {
        "signature": "(*args, **kwargs)"
    },
    "mindformers.modules.activation.Softmax2d.construct": {
        "signature": "(self, x)"
    },
    "mindformers.modules.activation.Softmin": {
        "signature": "(axis=-1)"
    },
    "mindformers.modules.activation.Softmin.construct": {
        "signature": "(self, x)"
    },
    "mindformers.modules.activation.Softsign": {
        "signature": "()"
    },
    "mindformers.modules.activation.Softsign.construct": {
        "signature": "(self, x)"
    },
    "mindformers.modules.activation.Tanh": {
        "signature": "()"
    },
    "mindformers.modules.activation.Tanh.construct": {
        "signature": "(self, x)"
    },
    "mindformers.modules.activation.Tanhshrink": {
        "signature": "(auto_prefix=True, flags=None)"
    },
    "mindformers.modules.activation.Tanhshrink.construct": {
        "signature": "(self, x)"
    },
    "mindformers.modules.activation.Threshold": {
        "signature": "(threshold, value)"
    },
    "mindformers.modules.activation.Threshold.construct": {
        "signature": "(self, input_x)"
    },
    "mindformers.modules.activation.get_activation": {
        "signature": "(name, prim_name=None)"
    },
    "mindformers.modules.layers.AlibiTensor": {
        "signature": "(seq_length, num_heads, parallel_config=<mindformers.modules.transformer.op_parallel_config.OpParallelConfig object>)"
    },
    "mindformers.modules.layers.AlibiTensor.construct": {
        "signature": "(self, attention_mask, dtype)"
    },
    "mindformers.modules.layers.AlibiTensorV2": {
        "signature": "(num_heads)"
    },
    "mindformers.modules.layers.AlibiTensorV2.construct": {
        "signature": "(self, attention_mask, dtype=mindspore.float32)"
    },
    "mindformers.modules.layers.AlibiTensorV2.shard": {
        "signature": "(self, parallel_config)"
    },
    "mindformers.modules.layers.Dropout": {
        "signature": "(keep_prob=0.5, dtype=mindspore.float32)"
    },
    "mindformers.modules.layers.Dropout.construct": {
        "signature": "(self, x)"
    },
    "mindformers.modules.layers.Dropout.extend_repr": {
        "signature": "(self)"
    },
    "mindformers.modules.layers.Dropout.shard": {
        "signature": "(self, strategy)"
    },
    "mindformers.modules.layers.FixedSparseAttention": {
        "signature": "(batch_size, num_heads, size_per_head, block_size, seq_length=1024, num_different_global_patterns=4, parallel_config=<mindformers.modules.transformer.op_parallel_config.OpParallelConfig object>)"
    },
    "mindformers.modules.layers.FixedSparseAttention.construct": {
        "signature": "(self, q, k, v, attention_mask)"
    },
    "mindformers.modules.layers.FixedSparseAttention._generate_attention_mask": {
        "signature": "(self, attention_mask)"
    },
    "mindformers.modules.layers.FixedSparseAttention._transpose_inputs": {
        "signature": "(self, q, k, v)"
    },
    "mindformers.modules.layers.LayerNorm": {
        "signature": "(normalized_shape, eps=1e-05, param_init_type=mindspore.float32, is_self_defined=False)"
    },
    "mindformers.modules.layers.LayerNorm.construct": {
        "signature": "(self, x)"
    },
    "mindformers.modules.layers.LayerNorm.shard": {
        "signature": "(self, strategy)"
    },
    "mindformers.modules.layers.Linear": {
        "signature": "(in_channels, out_channels, init_method_std=0.01, weight_init='normal', bias_init='zeros', has_bias=True, activation=None, transpose_b=True, expert_num=1, outer_batch=1, expert_group_size=None, use_gmm=False, param_init_type=mindspore.float32, compute_dtype=mindspore.float16)"
    },
    "mindformers.modules.layers.Linear.construct": {
        "signature": "(self, x, group_list=None)"
    },
    "mindformers.modules.layers.Linear.shard": {
        "signature": "(self, strategy_matmul, strategy_bias=None, strategy_activation=None, out_strategy_matmul=None, enable_nd_tp=False)"
    },
    "mindformers.modules.layers.RotaryEmbedding": {
        "signature": "(head_dim=128, compute_dtype=mindspore.float32, use_rope_slice=False, use_3d_tensor_parallel=False, tp_x=1, tp_y=1, tp_z=1)"
    },
    "mindformers.modules.layers.RotaryEmbedding.rotate_half": {
        "signature": "(self, x, swap_mask)"
    },
    "mindformers.modules.layers.RotaryEmbedding.slice_half": {
        "signature": "(self, x)"
    },
    "mindformers.modules.layers.RotaryEmbedding.construct": {
        "signature": "(self, xq: mindspore.common.tensor.Tensor, xk: mindspore.common.tensor.Tensor, freqs_cis)"
    },
    "mindformers.modules.layers.RotaryEmbedding.shard": {
        "signature": "(self, parallel_config)"
    },
    "mindformers.modules.local_block_sparse_attention.LocalBlockSparseAttention": {
        "signature": "(seq_len, size_per_head, local_size, block_size, dropout_rate=0.1, softmax_compute_type=mindspore.float16, parallel_config=<mindformers.modules.transformer.op_parallel_config.OpParallelConfig object>)"
    },
    "mindformers.modules.local_block_sparse_attention.LocalBlockSparseAttention.construct": {
        "signature": "(self, q, k, v, attention_mask=None)"
    },
    "mindformers.modules.transformer.moe.MoEConfig": {
        "signature": "(expert_num=1, capacity_factor=1.1, aux_loss_factor=0.05, num_experts_chosen=1, expert_group_size=None, group_wise_a2a=False, comp_comm_parallel=False, comp_comm_parallel_degree=2, save_token_distribution=False, cur_layer=0, enable_cold_hot_expert=False, update_step=10000, hot_expert_num=0, cold_token_percent=1.0, moe_module_name='', routing_policy='TopkRouterV1', norm_topk_prob=True, enable_sdrop=False, use_fused_ops_topkrouter=False, router_dense_type='float32', shared_expert_num=0, use_shared_expert_gating=False, max_router_load=131072, topk_method='greedy', topk_group=None, n_group=None, first_k_dense_replace=True, moe_intermediate_size=1407, routed_scaling_factor=1.0, aux_loss_types=None, aux_loss_factors=None, z_loss_factor=0.0, balance_via_topk_bias=False, topk_bias_update_rate=0.0, use_allgather_dispatcher=False, moe_shared_expert_overlap=False, expert_model_parallel=None, use_gating_sigmoid=False, use_gmm=False, enable_gmm_safe_tokens=False, use_fused_ops_permute=False)"
    },
    "mindformers.modules.transformer.moe.MoEConfig.to_diff_dict": {
        "signature": "(self)"
    },
    "mindformers.modules.transformer.moe.MoEConfig.to_dict": {
        "signature": "(self)"
    },
    "mindformers.modules.transformer.moe_utils.ZLoss": {
        "signature": "(parallel_config=<mindformers.modules.transformer.op_parallel_config.OpParallelConfig object>)"
    },
    "mindformers.modules.transformer.moe_utils.ZLoss.construct": {
        "signature": "(self, logits, weight)"
    },
    "mindformers.modules.transformer.op_parallel_config.OpParallelConfig": {
        "signature": "(data_parallel=1, model_parallel=1, use_seq_parallel=False, context_parallel=1, select_recompute=False, context_parallel_algo: str = 'colossalai_cp')"
    },
    "mindformers.modules.transformer.op_parallel_config.OpParallelConfig._check_context_parallel": {
        "signature": "(self)"
    },
    "mindformers.modules.transformer.op_parallel_config.OpParallelConfig.get_ulysses_cp_num": {
        "signature": "(self)"
    },
    "mindformers.modules.transformer.op_parallel_config.OpParallelConfig.to_dict": {
        "signature": "(self)"
    },
    "mindformers.modules.transformer.op_parallel_config.OpParallelConfig.to_diff_dict": {
        "signature": "(self)"
    },
    "mindformers.modules.transformer.transformer.AttentionMask": {
        "signature": "(seq_length, parallel_config=<mindformers.modules.transformer.op_parallel_config.OpParallelConfig object>, compute_dtype=mindspore.float16)"
    },
    "mindformers.modules.transformer.transformer.AttentionMask.construct": {
        "signature": "(self, input_mask)"
    },
    "mindformers.modules.transformer.transformer.AttentionMaskHF": {
        "signature": "(seq_length, parallel_config=<mindformers.modules.transformer.op_parallel_config.OpParallelConfig object>, compute_dtype=mindspore.float16)"
    },
    "mindformers.modules.transformer.transformer.AttentionMaskHF.construct": {
        "signature": "(self, input_mask)"
    },
    "mindformers.modules.transformer.transformer.EmbeddingOpParallelConfig": {
        "signature": "(data_parallel=1, model_parallel=1, context_parallel=1, use_seq_parallel=False, select_recompute=False, vocab_emb_dp=True)"
    },
    "mindformers.modules.transformer.transformer.EmbeddingOpParallelConfig.to_diff_dict": {
        "signature": "(self)"
    },
    "mindformers.modules.transformer.transformer.EmbeddingOpParallelConfig.to_dict": {
        "signature": "(self)"
    },
    "mindformers.modules.transformer.transformer.FeedForward": {
        "signature": "(hidden_size, ffn_hidden_size, dropout_rate, hidden_act='gelu', expert_num=1, expert_group_size=None, param_init_type=mindspore.float32, parallel_config=<mindformers.modules.transformer.op_parallel_config.OpParallelConfig object>, compute_dtype=mindspore.float16)"
    },
    "mindformers.modules.transformer.transformer.FeedForward.construct": {
        "signature": "(self, x)"
    },
    "mindformers.modules.transformer.transformer.LowerTriangularMaskWithDynamic": {
        "signature": "(seq_length, batch_size=1, compute_type=mindspore.float16, is_dynamic=False, pad_token_id=0, use_flash_attention=False, use_attn_mask_compression=False, use_past=False, seq_split_num=1, chunk_prefill=False)"
    },
    "mindformers.modules.transformer.transformer.LowerTriangularMaskWithDynamic.construct": {
        "signature": "(self, tokens=None, masks=None, seq_chunk=None)"
    },
    "mindformers.modules.transformer.transformer.LowerTriangularMaskWithDynamic.prefill": {
        "signature": "(self)"
    },
    "mindformers.modules.transformer.transformer.LowerTriangularMaskWithDynamic.chunk_masks": {
        "signature": "(self, seq_range)"
    },
    "mindformers.modules.transformer.transformer.LowerTriangularMaskWithDynamic.shard": {
        "signature": "(self, parallel_config)"
    },
    "mindformers.modules.transformer.transformer.MultiHeadAttention": {
        "signature": "(batch_size, src_seq_length, tgt_seq_length, hidden_size, num_heads, hidden_dropout_rate=0.1, attention_dropout_rate=0.1, compute_dtype=mindspore.float16, softmax_compute_type=mindspore.float32, param_init_type=mindspore.float32, use_past=False, parallel_config=<mindformers.modules.transformer.op_parallel_config.OpParallelConfig object>, use_flash_attention=False, use_prompt_flash_attention=False)"
    },
    "mindformers.modules.transformer.transformer.MultiHeadAttention.construct": {
        "signature": "(self, query_tensor, key_tensor, value_tensor, attention_mask, key_past=None, value_past=None, batch_valid_length=None)"
    },
    "mindformers.modules.transformer.transformer.MultiHeadAttention._get_batch_size_from_query": {
        "signature": "(self, query)"
    },
    "mindformers.modules.transformer.transformer.MultiHeadAttention._get_seq_length_under_incremental": {
        "signature": "(self, length)"
    },
    "mindformers.modules.transformer.transformer.MultiHeadAttention._pfa_ifa_data_preprocess": {
        "signature": "(self, query, key, attention_mask, batch_valid_length)"
    },
    "mindformers.modules.transformer.transformer.MultiHeadAttention._check_inputs": {
        "signature": "(self, query_tensor, key_tensor, value_tensor, attention_mask, key_past=None, value_past=None, batch_valid_length=None)"
    },
    "mindformers.modules.transformer.transformer.MultiHeadAttention._convert_to_2d_tensor": {
        "signature": "(self, query_tensor, key_tensor, value_tensor)"
    },
    "mindformers.modules.transformer.transformer.MultiHeadAttention._merge_heads": {
        "signature": "(self, x)"
    },
    "mindformers.modules.transformer.transformer.MultiHeadAttention._softmax": {
        "signature": "(self, attention_scores)"
    },
    "mindformers.modules.transformer.transformer.MultiHeadAttention._flash_attn": {
        "signature": "(self, query, key, value, attention_mask)"
    },
    "mindformers.modules.transformer.transformer.MultiHeadAttention._attn": {
        "signature": "(self, query, key, value, attention_mask)"
    },
    "mindformers.modules.transformer.transformer.Transformer": {
        "signature": "(hidden_size, batch_size, ffn_hidden_size, src_seq_length, tgt_seq_length, encoder_layers=3, decoder_layers=3, num_heads=2, attention_dropout_rate=0.1, hidden_dropout_rate=0.1, hidden_act='gelu', post_layernorm_residual=False, layernorm_compute_type=mindspore.float32, softmax_compute_type=mindspore.float32, param_init_type=mindspore.float32, lambda_func=None, use_past=False, moe_config=<mindformers.modules.transformer.moe.MoEConfig object>, parallel_config=<mindformers.modules.transformer.transformer.TransformerOpParallelConfig object>)"
    },
    "mindformers.modules.transformer.transformer.Transformer.construct": {
        "signature": "(self, encoder_inputs, encoder_masks, decoder_inputs=None, decoder_masks=None, memory_mask=None, init_reset=True, batch_valid_length=None)"
    },
    "mindformers.modules.transformer.transformer.TransformerDecoder": {
        "signature": "(num_layers, batch_size, hidden_size, ffn_hidden_size, src_seq_length, tgt_seq_length, num_heads, attention_dropout_rate=0.1, hidden_dropout_rate=0.1, post_layernorm_residual=False, layernorm_compute_type=mindspore.float32, softmax_compute_type=mindspore.float32, param_init_type=mindspore.float32, hidden_act='gelu', lambda_func=None, use_past=False, offset=0, moe_config=<mindformers.modules.transformer.moe.MoEConfig object>, parallel_config=<mindformers.modules.transformer.transformer.TransformerOpParallelConfig object>)"
    },
    "mindformers.modules.transformer.transformer.TransformerDecoder.construct": {
        "signature": "(self, hidden_states, attention_mask, encoder_output=None, memory_mask=None, init_reset=True, batch_valid_length=None)"
    },
    "mindformers.modules.transformer.transformer.TransformerDecoderLayer": {
        "signature": "(hidden_size, ffn_hidden_size, num_heads, batch_size, src_seq_length, tgt_seq_length, attention_dropout_rate=0.1, hidden_dropout_rate=0.1, post_layernorm_residual=False, use_past=False, layernorm_compute_type=mindspore.float32, softmax_compute_type=mindspore.float32, param_init_type=mindspore.float32, hidden_act='gelu', moe_config=<mindformers.modules.transformer.moe.MoEConfig object>, parallel_config=<mindformers.modules.transformer.op_parallel_config.OpParallelConfig object>)"
    },
    "mindformers.modules.transformer.transformer.TransformerDecoderLayer.construct": {
        "signature": "(self, hidden_stats, decoder_mask, encoder_output=None, memory_mask=None, init_reset=True, batch_valid_length=None)"
    },
    "mindformers.modules.transformer.transformer.TransformerDecoderLayer._check_input": {
        "signature": "(self, hidden_states, attention_mask, encoder_output, memory_mask, init_reset, batch_valid_length)"
    },
    "mindformers.modules.transformer.transformer.TransformerEncoder": {
        "signature": "(batch_size, num_layers, hidden_size, ffn_hidden_size, seq_length, num_heads, attention_dropout_rate=0.1, hidden_dropout_rate=0.1, hidden_act='gelu', post_layernorm_residual=False, layernorm_compute_type=mindspore.float32, softmax_compute_type=mindspore.float32, param_init_type=mindspore.float32, lambda_func=None, offset=0, use_past=False, moe_config=<mindformers.modules.transformer.moe.MoEConfig object>, parallel_config=<mindformers.modules.transformer.transformer.TransformerOpParallelConfig object>)"
    },
    "mindformers.modules.transformer.transformer.TransformerEncoder.construct": {
        "signature": "(self, hidden_states, attention_mask, init_reset=True, batch_valid_length=None)"
    },
    "mindformers.modules.transformer.transformer.TransformerEncoderLayer": {
        "signature": "(batch_size, hidden_size, ffn_hidden_size, num_heads, seq_length, attention_dropout_rate=0.1, hidden_dropout_rate=0.1, post_layernorm_residual=False, layernorm_compute_type=mindspore.float32, softmax_compute_type=mindspore.float32, param_init_type=mindspore.float32, hidden_act='gelu', use_past=False, moe_config=<mindformers.modules.transformer.moe.MoEConfig object>, parallel_config=<mindformers.modules.transformer.op_parallel_config.OpParallelConfig object>, use_flash_attention=False, use_prompt_flash_attention=False, compute_dtype=mindspore.float16)"
    },
    "mindformers.modules.transformer.transformer.TransformerEncoderLayer.construct": {
        "signature": "(self, x, input_mask=None, init_reset=True, batch_valid_length=None)"
    },
    "mindformers.modules.transformer.transformer.TransformerEncoderLayer._check_input": {
        "signature": "(self, x, input_mask, init_reset, batch_valid_length)"
    },
    "mindformers.modules.transformer.transformer.TransformerOpParallelConfig": {
        "signature": "(data_parallel=1, model_parallel=1, context_parallel=1, expert_parallel=1, pipeline_stage=1, micro_batch_num=1, seq_split_num=1, recompute: Union[dict, mindformers.modules.transformer.transformer.TransformerRecomputeConfig] = <mindformers.modules.transformer.transformer.TransformerRecomputeConfig object>, use_seq_parallel=False, optimizer_shard=None, gradient_aggregation_group=4, vocab_emb_dp=True, context_parallel_algo: str = 'colossalai_cp', ulysses_degree_in_cp=1, mem_coeff=0.1)"
    },
    "mindformers.modules.transformer.transformer.TransformerOpParallelConfig._check_context_parallel": {
        "signature": "(self)"
    },
    "mindformers.modules.transformer.transformer.TransformerOpParallelConfig.get_ulysses_cp_num": {
        "signature": "(self)"
    },
    "mindformers.modules.transformer.transformer.TransformerOpParallelConfig.to_diff_dict": {
        "signature": "(self)"
    },
    "mindformers.modules.transformer.transformer.TransformerOpParallelConfig.to_dict": {
        "signature": "(self)"
    },
    "mindformers.modules.transformer.transformer.TransformerRecomputeConfig": {
        "signature": "(recompute=False, select_recompute=False, parallel_optimizer_comm_recompute=False, select_comm_recompute=False, mp_comm_recompute=True, recompute_slice_activation=False, select_recompute_off=False, select_comm_recompute_off=False)"
    },
    "mindformers.modules.transformer.transformer.TransformerRecomputeConfig.to_diff_dict": {
        "signature": "(self)"
    },
    "mindformers.modules.transformer.transformer.TransformerRecomputeConfig.to_dict": {
        "signature": "(self)"
    },
    "mindformers.modules.transformer.transformer.VocabEmbedding": {
        "signature": "(vocab_size, embedding_size, parallel_config=<mindformers.modules.transformer.transformer.EmbeddingOpParallelConfig object>, param_init='normal', param_init_type=mindspore.float32)"
    },
    "mindformers.modules.transformer.transformer.VocabEmbedding.construct": {
        "signature": "(self, input_ids)"
    },
    "mindformers.pet.LoraConfig": {
        "signature": "(lora_rank: int = 8, lora_alpha: int = 16, lora_dropout: float = 0.01, lora_a_init: str = 'normal', lora_b_init: str = 'zero', param_init_type: str = None, compute_dtype: str = None, target_modules: str = None, exclude_layers: str = None, freeze_include: List[str] = None, freeze_exclude: List[str] = None, **kwargs)"
    },
    "mindformers.pet.LoraModel": {
        "signature": "(config: mindformers.pet.pet_config.LoraConfig, base_model: mindformers.models.modeling_utils.PreTrainedModel)"
    },
    "mindformers.pet.LoraModel.add_adapter": {
        "signature": "(self, base_model: mindformers.models.modeling_utils.PreTrainedModel)"
    },
    "mindformers.pet.LoraModel._check_config": {
        "signature": "(self)"
    },
    "mindformers.pet.LoraModel.update_model_kwargs_before_generate": {
        "signature": "(self, input_ids, model_kwargs: dict)"
    },
    "mindformers.pet.LoraModel.prepare_inputs_for_generation": {
        "signature": "(self, input_ids, **kwargs)"
    },
    "mindformers.pet.LoraModel.prepare_inputs_for_predict_layout": {
        "signature": "(self, input_ids, **kwargs)"
    },
    "mindformers.pet.LoraModel.slice_incremental_inputs": {
        "signature": "(self, model_inputs: dict, current_index)"
    },
    "mindformers.pet.LoraModel.set_dynamic_inputs": {
        "signature": "(self, **kwargs)"
    },
    "mindformers.pet.LoraModel.to_embeddings": {
        "signature": "(self, tokens)"
    },
    "mindformers.pet.LoraModel.convert_name": {
        "signature": "(self, weight_name)"
    },
    "mindformers.pet.LoraModel.convert_weight_dict": {
        "signature": "(self, source_dict, **kwargs)"
    },
    "mindformers.pet.LoraModel.convert_map_dict": {
        "signature": "(self, source_dict, **kwargs)"
    },
    "mindformers.pet.LoraModel.construct": {
        "signature": "(self, *inputs, **kwargs)"
    },
    "mindformers.pet.PetConfig": {
        "signature": "(pet_type: str = None, **kwargs)"
    },
    "mindformers.pet.models.LoraModel": {
        "signature": "(config: mindformers.pet.pet_config.LoraConfig, base_model: mindformers.models.modeling_utils.PreTrainedModel)"
    },
    "mindformers.pet.models.LoraModel.add_adapter": {
        "signature": "(self, base_model: mindformers.models.modeling_utils.PreTrainedModel)"
    },
    "mindformers.pet.models.LoraModel._check_config": {
        "signature": "(self)"
    },
    "mindformers.pet.models.LoraModel.update_model_kwargs_before_generate": {
        "signature": "(self, input_ids, model_kwargs: dict)"
    },
    "mindformers.pet.models.LoraModel.prepare_inputs_for_generation": {
        "signature": "(self, input_ids, **kwargs)"
    },
    "mindformers.pet.models.LoraModel.prepare_inputs_for_predict_layout": {
        "signature": "(self, input_ids, **kwargs)"
    },
    "mindformers.pet.models.LoraModel.slice_incremental_inputs": {
        "signature": "(self, model_inputs: dict, current_index)"
    },
    "mindformers.pet.models.LoraModel.set_dynamic_inputs": {
        "signature": "(self, **kwargs)"
    },
    "mindformers.pet.models.LoraModel.to_embeddings": {
        "signature": "(self, tokens)"
    },
    "mindformers.pet.models.LoraModel.convert_name": {
        "signature": "(self, weight_name)"
    },
    "mindformers.pet.models.LoraModel.convert_weight_dict": {
        "signature": "(self, source_dict, **kwargs)"
    },
    "mindformers.pet.models.LoraModel.convert_map_dict": {
        "signature": "(self, source_dict, **kwargs)"
    },
    "mindformers.pet.models.LoraModel.construct": {
        "signature": "(self, *inputs, **kwargs)"
    },
    "mindformers.pet.pet_config.LoraConfig": {
        "signature": "(lora_rank: int = 8, lora_alpha: int = 16, lora_dropout: float = 0.01, lora_a_init: str = 'normal', lora_b_init: str = 'zero', param_init_type: str = None, compute_dtype: str = None, target_modules: str = None, exclude_layers: str = None, freeze_include: List[str] = None, freeze_exclude: List[str] = None, **kwargs)"
    },
    "mindformers.pet.pet_config.PetConfig": {
        "signature": "(pet_type: str = None, **kwargs)"
    },
    "mindformers.pet.pet_config.PrefixTuningConfig": {
        "signature": "(prefix_token_num: int = 128, mid_dim: int = 512, dropout_rate: float = 0.1, **kwargs)"
    },
    "mindformers.pet.pet_config.Ptuning2Config": {
        "signature": "(pre_seq_len: int = 128, prefix_projection: bool = False, projection_dim: int = 128, dropout_prob: float = 0.01, **kwargs)"
    },
    "mindformers.pipeline.MultiModalToTextPipeline": {
        "signature": "(model: Union[mindformers.models.modeling_utils.PreTrainedModel, mindspore.train.model.Model], processor: Optional[mindformers.models.multi_modal.base_multi_modal_processor.BaseXModalToTextProcessor] = None, **kwargs)"
    },
    "mindformers.pipeline.MultiModalToTextPipeline._sanitize_parameters": {
        "signature": "(self, **pipeline_parameters)"
    },
    "mindformers.pipeline.MultiModalToTextPipeline.preprocess": {
        "signature": "(self, inputs: Union[List[Dict[str, str]], List[List[Dict[str, str]]]], **preprocess_params)"
    },
    "mindformers.pipeline.MultiModalToTextPipeline._forward": {
        "signature": "(self, model_inputs: dict, **forward_params)"
    },
    "mindformers.pipeline.MultiModalToTextPipeline.postprocess": {
        "signature": "(self, model_outputs, **postprocess_params)"
    },
    "mindformers.pipeline.pipeline": {
        "signature": "(task: str = None, model: Union[NoneType, Tuple[str, str], mindformers.models.modeling_utils.PreTrainedModel, mindspore.train.model.Model, str] = None, tokenizer: Optional[mindformers.models.tokenization_utils_base.PreTrainedTokenizerBase] = None, image_processor: Optional[mindformers.models.image_processing_utils.BaseImageProcessor] = None, audio_processor: Optional[mindformers.models.base_processor.BaseAudioProcessor] = None, backend: Optional[str] = 'ms', **kwargs: Any)"
    },
    "mindformers.pipeline.fill_mask_pipeline.FillMaskPipeline": {
        "signature": "(model, tokenizer=None, **kwargs)"
    },
    "mindformers.pipeline.fill_mask_pipeline.FillMaskPipeline._sanitize_parameters": {
        "signature": "(self, **pipeline_parameters)"
    },
    "mindformers.pipeline.fill_mask_pipeline.FillMaskPipeline.preprocess": {
        "signature": "(self, inputs, **preprocess_params)"
    },
    "mindformers.pipeline.fill_mask_pipeline.FillMaskPipeline._forward": {
        "signature": "(self, model_inputs, **forward_params)"
    },
    "mindformers.pipeline.fill_mask_pipeline.FillMaskPipeline.postprocess": {
        "signature": "(self, model_outputs, **postprocess_params)"
    },
    "mindformers.pipeline.image_to_text_generation_pipeline.ImageToTextPipeline": {
        "signature": "(model: Union[mindformers.models.modeling_utils.PreTrainedModel, mindspore.train.model.Model], processor: Optional[mindformers.models.base_processor.BaseProcessor] = None, **kwargs)"
    },
    "mindformers.pipeline.image_to_text_generation_pipeline.ImageToTextPipeline._sanitize_parameters": {
        "signature": "(self, **pipeline_parameters)"
    },
    "mindformers.pipeline.image_to_text_generation_pipeline.ImageToTextPipeline.preprocess": {
        "signature": "(self, inputs: Union[List[Dict[str, str]], List[List[Dict[str, str]]]], **preprocess_params)"
    },
    "mindformers.pipeline.image_to_text_generation_pipeline.ImageToTextPipeline._forward": {
        "signature": "(self, model_inputs: dict, **forward_params)"
    },
    "mindformers.pipeline.image_to_text_generation_pipeline.ImageToTextPipeline.postprocess": {
        "signature": "(self, model_outputs, **postprocess_params)"
    },
    "mindformers.pipeline.masked_image_modeling_pipeline.MaskedImageModelingPipeline": {
        "signature": "(model: Union[mindformers.models.modeling_utils.PreTrainedModel, mindspore.train.model.Model], image_processor: Optional[mindformers.models.image_processing_utils.BaseImageProcessor] = None, **kwargs)"
    },
    "mindformers.pipeline.masked_image_modeling_pipeline.MaskedImageModelingPipeline._sanitize_parameters": {
        "signature": "(self, **pipeline_parameters)"
    },
    "mindformers.pipeline.masked_image_modeling_pipeline.MaskedImageModelingPipeline.preprocess": {
        "signature": "(self, inputs: Union[PIL.Image.Image, mindspore.common.tensor.Tensor, numpy.ndarray, str], **preprocess_params)"
    },
    "mindformers.pipeline.masked_image_modeling_pipeline.MaskedImageModelingPipeline._forward": {
        "signature": "(self, model_inputs: dict, **forward_params)"
    },
    "mindformers.pipeline.masked_image_modeling_pipeline.MaskedImageModelingPipeline.postprocess": {
        "signature": "(self, model_outputs, **postprocess_params)"
    },
    "mindformers.pipeline.multi_modal_to_text_generation_pipeline.MultiModalToTextPipeline": {
        "signature": "(model: Union[mindformers.models.modeling_utils.PreTrainedModel, mindspore.train.model.Model], processor: Optional[mindformers.models.multi_modal.base_multi_modal_processor.BaseXModalToTextProcessor] = None, **kwargs)"
    },
    "mindformers.pipeline.multi_modal_to_text_generation_pipeline.MultiModalToTextPipeline._sanitize_parameters": {
        "signature": "(self, **pipeline_parameters)"
    },
    "mindformers.pipeline.multi_modal_to_text_generation_pipeline.MultiModalToTextPipeline.preprocess": {
        "signature": "(self, inputs: Union[List[Dict[str, str]], List[List[Dict[str, str]]]], **preprocess_params)"
    },
    "mindformers.pipeline.multi_modal_to_text_generation_pipeline.MultiModalToTextPipeline._forward": {
        "signature": "(self, model_inputs: dict, **forward_params)"
    },
    "mindformers.pipeline.multi_modal_to_text_generation_pipeline.MultiModalToTextPipeline.postprocess": {
        "signature": "(self, model_outputs, **postprocess_params)"
    },
    "mindformers.pipeline.question_answering_pipeline.QuestionAnsweringPipeline": {
        "signature": "(model, tokenizer, doc_stride=128, max_question_len=64, max_seq_len=384, top_k=1, n_best_size=20, max_answer_len=30, **kwargs)"
    },
    "mindformers.pipeline.question_answering_pipeline.QuestionAnsweringPipeline._sanitize_parameters": {
        "signature": "(self, **pipeline_parameters)"
    },
    "mindformers.pipeline.question_answering_pipeline.QuestionAnsweringPipeline.preprocess": {
        "signature": "(self, inputs, **preprocess_params)"
    },
    "mindformers.pipeline.question_answering_pipeline.QuestionAnsweringPipeline._forward": {
        "signature": "(self, model_inputs, **forward_params)"
    },
    "mindformers.pipeline.question_answering_pipeline.QuestionAnsweringPipeline.postprocess": {
        "signature": "(self, model_outputs, **postprocess_params)"
    },
    "mindformers.pipeline.question_answering_pipeline.QuestionAnsweringPipeline._get_prelim_predictions": {
        "signature": "(self, features, unique_id_to_result, max_answer_len, n_best_size)"
    },
    "mindformers.pipeline.question_answering_pipeline.QuestionAnsweringPipeline._get_nbest": {
        "signature": "(self, prelim_predictions, features, example, n_best_size)"
    },
    "mindformers.pipeline.question_answering_pipeline.QuestionAnsweringPipeline._get_answer_index": {
        "signature": "(self, context_text, orig_doc_start, orig_doc_end)"
    },
    "mindformers.pipeline.question_answering_pipeline.QuestionAnsweringPipeline._compute_softmax": {
        "signature": "(self, scores)"
    },
    "mindformers.pipeline.question_answering_pipeline.QuestionAnsweringPipeline._get_final_text": {
        "signature": "(self, pred_text, orig_text)"
    },
    "mindformers.pipeline.question_answering_pipeline.QuestionAnsweringPipeline._get_best_indexes": {
        "signature": "(self, logits, n_best_size)"
    },
    "mindformers.pipeline.segment_anything_pipeline.SegmentAnythingPipeline": {
        "signature": "(model: Union[mindformers.models.modeling_utils.PreTrainedModel, mindspore.train.model.Model], image_processor: Optional[mindformers.models.image_processing_utils.BaseImageProcessor] = None, **kwargs)"
    },
    "mindformers.pipeline.segment_anything_pipeline.SegmentAnythingPipeline._sanitize_parameters": {
        "signature": "(self, **pipeline_parameters)"
    },
    "mindformers.pipeline.segment_anything_pipeline.SegmentAnythingPipeline.preprocess": {
        "signature": "(self, inputs, **preprocess_params)"
    },
    "mindformers.pipeline.segment_anything_pipeline.SegmentAnythingPipeline._forward": {
        "signature": "(self, model_inputs, **forward_params)"
    },
    "mindformers.pipeline.segment_anything_pipeline.SegmentAnythingPipeline.postprocess": {
        "signature": "(self, model_outputs, **postprocess_params)"
    },
    "mindformers.pipeline.segment_anything_pipeline.SegmentAnythingPipeline.preprocess_image": {
        "signature": "(self, image, image_format: str = 'RGB') -> None"
    },
    "mindformers.pipeline.segment_anything_pipeline.SegmentAnythingPipeline.preprocess_prompts": {
        "signature": "(self, original_size, point_coords=None, point_labels=None, boxes=None, masks=None, multi_seg=False)"
    },
    "mindformers.pipeline.segment_anything_pipeline.SegmentAnythingPipeline.set_image": {
        "signature": "(self, image, image_format: str = 'RGB')"
    },
    "mindformers.pipeline.segment_anything_pipeline.SegmentAnythingPipeline.reset_image": {
        "signature": "(self) -> None"
    },
    "mindformers.pipeline.segment_anything_pipeline.SegmentAnythingPipeline.get_image_embedding": {
        "signature": "(self)"
    },
    "mindformers.pipeline.segment_anything_pipeline.SegmentAnythingPipeline.reset_config": {
        "signature": "(self, points_per_side=32, points_per_batch=64, pred_iou_thresh=0.88, stability_score_thresh=0.95, stability_score_offset=1.0, box_nms_thresh=0.7, crop_n_layers=0, crop_nms_thresh=0.7, crop_overlap_ratio=0.3413, crop_n_points_downscale_factor=1, point_grids=None, min_mask_region_area=0, output_mode='binary_mask', **kwargs) -> None"
    },
    "mindformers.pipeline.segment_anything_pipeline.SegmentAnythingPipeline.postprocess_small_regions": {
        "signature": "(self, mask_data: mindformers.models.sam.sam_utils.MaskData, min_area: int, nms_thresh: float) -> mindformers.models.sam.sam_utils.MaskData"
    },
    "mindformers.pipeline.text_classification_pipeline.TextClassificationPipeline": {
        "signature": "(model, tokenizer=None, **kwargs)"
    },
    "mindformers.pipeline.text_classification_pipeline.TextClassificationPipeline._sanitize_parameters": {
        "signature": "(self, **pipeline_parameters)"
    },
    "mindformers.pipeline.text_classification_pipeline.TextClassificationPipeline.inputs_process": {
        "signature": "(self, inputs_zero, inputs_one)"
    },
    "mindformers.pipeline.text_classification_pipeline.TextClassificationPipeline.preprocess": {
        "signature": "(self, inputs, **preprocess_params)"
    },
    "mindformers.pipeline.text_classification_pipeline.TextClassificationPipeline._forward": {
        "signature": "(self, model_inputs, **forward_params)"
    },
    "mindformers.pipeline.text_classification_pipeline.TextClassificationPipeline.softmax": {
        "signature": "(self, outputs)"
    },
    "mindformers.pipeline.text_classification_pipeline.TextClassificationPipeline.postprocess": {
        "signature": "(self, model_outputs, **postprocess_params)"
    },
    "mindformers.pipeline.text_generation_pipeline.TextGenerationPipeline": {
        "signature": "(model: Union[mindformers.models.modeling_utils.PreTrainedModel, mindspore.train.model.Model], tokenizer: Optional[mindformers.models.tokenization_utils.PreTrainedTokenizer] = None, **kwargs)"
    },
    "mindformers.pipeline.text_generation_pipeline.TextGenerationPipeline._sanitize_parameters": {
        "signature": "(self, **pipeline_parameters)"
    },
    "mindformers.pipeline.text_generation_pipeline.TextGenerationPipeline.preprocess": {
        "signature": "(self, inputs: Union[dict, mindspore.common.tensor.Tensor, str], **preprocess_params)"
    },
    "mindformers.pipeline.text_generation_pipeline.TextGenerationPipeline.run_multi": {
        "signature": "(self, inputs: Union[list, tuple], batch_size: int, preprocess_params: dict, forward_params: dict, postprocess_params: dict)"
    },
    "mindformers.pipeline.text_generation_pipeline.TextGenerationPipeline._forward": {
        "signature": "(self, model_inputs: dict, **forward_params)"
    },
    "mindformers.pipeline.text_generation_pipeline.TextGenerationPipeline.postprocess": {
        "signature": "(self, model_outputs: dict, **postprocess_params)"
    },
    "mindformers.pipeline.token_classification_pipeline.TokenClassificationPipeline": {
        "signature": "(model, id2label, tokenizer=None, **kwargs)"
    },
    "mindformers.pipeline.token_classification_pipeline.TokenClassificationPipeline._sanitize_parameters": {
        "signature": "(self, **pipeline_parameters)"
    },
    "mindformers.pipeline.token_classification_pipeline.TokenClassificationPipeline.preprocess": {
        "signature": "(self, inputs, **preprocess_params)"
    },
    "mindformers.pipeline.token_classification_pipeline.TokenClassificationPipeline._forward": {
        "signature": "(self, model_inputs, **forward_params)"
    },
    "mindformers.pipeline.token_classification_pipeline.TokenClassificationPipeline.postprocess": {
        "signature": "(self, model_outputs, **postprocess_params)"
    },
    "mindformers.pipeline.token_classification_pipeline.TokenClassificationPipeline.get_entities_bios": {
        "signature": "(self, seq)"
    },
    "mindformers.pipeline.translation_pipeline.TranslationPipeline": {
        "signature": "(model: Union[mindformers.models.modeling_utils.PreTrainedModel, mindspore.train.model.Model], tokenizer: Optional[mindformers.models.tokenization_utils.PreTrainedTokenizer] = None, **kwargs)"
    },
    "mindformers.pipeline.translation_pipeline.TranslationPipeline._sanitize_parameters": {
        "signature": "(self, **pipeline_parameters)"
    },
    "mindformers.pipeline.translation_pipeline.TranslationPipeline.preprocess": {
        "signature": "(self, inputs: Union[dict, mindspore.common.tensor.Tensor, str], **preprocess_params)"
    },
    "mindformers.pipeline.translation_pipeline.TranslationPipeline._forward": {
        "signature": "(self, model_inputs: dict, **forward_params)"
    },
    "mindformers.pipeline.translation_pipeline.TranslationPipeline.postprocess": {
        "signature": "(self, model_outputs: dict, **postprocess_params)"
    },
    "mindformers.reinforcement_learning.DPOConfig": {
        "signature": "(dpo_alpha: float = 0.5, dpo_beta: float = 1.0, **kwargs)"
    },
    "mindformers.reinforcement_learning.RlConfig": {
        "signature": "(rl_type: str = None, **kwargs)"
    },
    "mindformers.reinforcement_learning.rl_config.DPOConfig": {
        "signature": "(dpo_alpha: float = 0.5, dpo_beta: float = 1.0, **kwargs)"
    },
    "mindformers.reinforcement_learning.rl_config.RlConfig": {
        "signature": "(rl_type: str = None, **kwargs)"
    },
    "mindformers.tools.MindFormerConfig": {
        "signature": "(*args, **kwargs)"
    },
    "mindformers.tools.MindFormerConfig.merge_from_dict": {
        "signature": "(self, options)"
    },
    "mindformers.tools.MindFormerConfig._merge_a_into_b": {
        "signature": "(a, b)"
    },
    "mindformers.tools.MindFormerConfig._file2dict": {
        "signature": "(filename=None)"
    },
    "mindformers.tools.MindFormerConfig._dict2config": {
        "signature": "(config, dic)"
    },
    "mindformers.tools.MindFormerConfig.get_value": {
        "signature": "(self, levels: Union[list, str], default=None)"
    },
    "mindformers.tools.MindFormerConfig.set_value": {
        "signature": "(self, levels: Union[list, str], value)"
    },
    "mindformers.tools.MindFormerModuleType": {
        "signature": "()"
    },
    "mindformers.tools.MindFormerRegister": {
        "signature": "()"
    },
    "mindformers.tools.MindFormerRegister.register": {
        "signature": "(module_type='tools', alias=None)"
    },
    "mindformers.tools.MindFormerRegister.register_cls": {
        "signature": "(register_class, module_type='tools', alias=None)"
    },
    "mindformers.tools.MindFormerRegister.is_exist": {
        "signature": "(module_type, class_name=None)"
    },
    "mindformers.tools.MindFormerRegister.get_cls": {
        "signature": "(module_type, class_name=None)"
    },
    "mindformers.tools.MindFormerRegister.get_instance_from_cfg": {
        "signature": "(cfg, module_type='tools', default_args=None)"
    },
    "mindformers.tools.MindFormerRegister.get_instance": {
        "signature": "(module_type='tools', class_name=None, **kwargs)"
    },
    "mindformers.tools.MindFormerRegister.auto_register": {
        "signature": "(class_reference: str, module_type='tools')"
    },
    "mindformers.tools.ckpt_transform.TransformCkpt": {
        "signature": "(auto_trans_ckpt: bool = False, rank_id: Optional[int] = None, world_size: Optional[int] = None, transform_process_num: int = 1, transform_by_rank: bool = False, npu_num_per_node: int = None)"
    },
    "mindformers.tools.ckpt_transform.TransformCkpt.transform_ckpt": {
        "signature": "(self, src_checkpoint, dst_checkpoint_dir, src_strategy=None, dst_strategy=None, prefix='checkpoint_')"
    },
    "mindformers.tools.ckpt_transform.TransformCkpt.transform_checkpoints": {
        "signature": "(self, src_checkpoint, dst_checkpoint, prefix, src_strategy, dst_strategy)"
    },
    "mindformers.tools.ckpt_transform.TransformCkpt.transform_checkpoint_by_rank": {
        "signature": "(self, src_checkpoint, dst_checkpoint, prefix, src_strategy, dst_strategy)"
    },
    "mindformers.tools.ckpt_transform.TransformCkpt.build_soft_link_of_checkpoint": {
        "signature": "(self, checkpoint, soft_link_dir)"
    },
    "mindformers.tools.ckpt_transform.TransformCkpt.clear_cache": {
        "signature": "(self)"
    },
    "mindformers.tools.ckpt_transform.TransformCkpt.get_strategy": {
        "signature": "(self, strategy_path, rank_id=None)"
    },
    "mindformers.tools.ckpt_transform.TransformCkpt.get_dst_strategy": {
        "signature": "(self, dst_strategy)"
    },
    "mindformers.tools.ckpt_transform.TransformCkpt.check_src_checkpoint_and_strategy": {
        "signature": "(self, src_checkpoint, src_strategy)"
    },
    "mindformers.tools.ckpt_transform.TransformCkpt.send_strategy_to_obs": {
        "signature": "(self, strategy)"
    },
    "mindformers.tools.ckpt_transform.TransformCkpt.send_transformed_checkpoint_to_obs": {
        "signature": "(self, dst_checkpoint_dir)"
    },
    "mindformers.tools.ckpt_transform.TransformCkpt.wait_collect_all_strategy": {
        "signature": "(self)"
    },
    "mindformers.tools.ckpt_transform.TransformCkpt.wait_transform": {
        "signature": "(self, ckpt_dir)"
    },
    "mindformers.tools.ckpt_transform.transform_checkpoint.TransformCkpt": {
        "signature": "(auto_trans_ckpt: bool = False, rank_id: Optional[int] = None, world_size: Optional[int] = None, transform_process_num: int = 1, transform_by_rank: bool = False, npu_num_per_node: int = None)"
    },
    "mindformers.tools.ckpt_transform.transform_checkpoint.TransformCkpt.transform_ckpt": {
        "signature": "(self, src_checkpoint, dst_checkpoint_dir, src_strategy=None, dst_strategy=None, prefix='checkpoint_')"
    },
    "mindformers.tools.ckpt_transform.transform_checkpoint.TransformCkpt.transform_checkpoints": {
        "signature": "(self, src_checkpoint, dst_checkpoint, prefix, src_strategy, dst_strategy)"
    },
    "mindformers.tools.ckpt_transform.transform_checkpoint.TransformCkpt.transform_checkpoint_by_rank": {
        "signature": "(self, src_checkpoint, dst_checkpoint, prefix, src_strategy, dst_strategy)"
    },
    "mindformers.tools.ckpt_transform.transform_checkpoint.TransformCkpt.build_soft_link_of_checkpoint": {
        "signature": "(self, checkpoint, soft_link_dir)"
    },
    "mindformers.tools.ckpt_transform.transform_checkpoint.TransformCkpt.clear_cache": {
        "signature": "(self)"
    },
    "mindformers.tools.ckpt_transform.transform_checkpoint.TransformCkpt.get_strategy": {
        "signature": "(self, strategy_path, rank_id=None)"
    },
    "mindformers.tools.ckpt_transform.transform_checkpoint.TransformCkpt.get_dst_strategy": {
        "signature": "(self, dst_strategy)"
    },
    "mindformers.tools.ckpt_transform.transform_checkpoint.TransformCkpt.check_src_checkpoint_and_strategy": {
        "signature": "(self, src_checkpoint, src_strategy)"
    },
    "mindformers.tools.ckpt_transform.transform_checkpoint.TransformCkpt.send_strategy_to_obs": {
        "signature": "(self, strategy)"
    },
    "mindformers.tools.ckpt_transform.transform_checkpoint.TransformCkpt.send_transformed_checkpoint_to_obs": {
        "signature": "(self, dst_checkpoint_dir)"
    },
    "mindformers.tools.ckpt_transform.transform_checkpoint.TransformCkpt.wait_collect_all_strategy": {
        "signature": "(self)"
    },
    "mindformers.tools.ckpt_transform.transform_checkpoint.TransformCkpt.wait_transform": {
        "signature": "(self, ckpt_dir)"
    },
    "mindformers.tools.cloud_adapter.cloud_adapter.Local2ObsMonitor": {
        "signature": "(src_dir, target_dir, step_upload_frequence: int = 100, epoch_upload_frequence: int = -1, keep_last=True, retry=3, retry_time=5, log=<class 'mindformers.tools.logger.logger'>)"
    },
    "mindformers.tools.cloud_adapter.cloud_adapter.Local2ObsMonitor.step_end": {
        "signature": "(self, run_context)"
    },
    "mindformers.tools.cloud_adapter.cloud_adapter.Local2ObsMonitor.epoch_end": {
        "signature": "(self, run_context)"
    },
    "mindformers.tools.cloud_adapter.cloud_adapter.Local2ObsMonitor.upload": {
        "signature": "(self)"
    },
    "mindformers.tools.cloud_adapter.cloud_adapter.Local2ObsMonitor.sync2obs": {
        "signature": "(*args, **kwargs)"
    },
    "mindformers.tools.cloud_adapter.cloud_adapter.Obs2Local": {
        "signature": "(rank_id=0, retry=3, retry_time=5, log=<class 'mindformers.tools.logger.logger'>)"
    },
    "mindformers.tools.cloud_adapter.cloud_adapter.Obs2Local.obs2local": {
        "signature": "(self, obs_url, local_url, special_id=None)"
    },
    "mindformers.tools.cloud_adapter.cloud_adapter.mox_adapter": {
        "signature": "(src_dir, target_dir, retry=3, retry_time=5, log=<class 'mindformers.tools.logger.logger'>)"
    },
    "mindformers.tools.register.MindFormerConfig": {
        "signature": "(*args, **kwargs)"
    },
    "mindformers.tools.register.MindFormerConfig.merge_from_dict": {
        "signature": "(self, options)"
    },
    "mindformers.tools.register.MindFormerConfig._merge_a_into_b": {
        "signature": "(a, b)"
    },
    "mindformers.tools.register.MindFormerConfig._file2dict": {
        "signature": "(filename=None)"
    },
    "mindformers.tools.register.MindFormerConfig._dict2config": {
        "signature": "(config, dic)"
    },
    "mindformers.tools.register.MindFormerConfig.get_value": {
        "signature": "(self, levels: Union[list, str], default=None)"
    },
    "mindformers.tools.register.MindFormerConfig.set_value": {
        "signature": "(self, levels: Union[list, str], value)"
    },
    "mindformers.tools.register.MindFormerModuleType": {
        "signature": "()"
    },
    "mindformers.tools.register.MindFormerRegister": {
        "signature": "()"
    },
    "mindformers.tools.register.MindFormerRegister.register": {
        "signature": "(module_type='tools', alias=None)"
    },
    "mindformers.tools.register.MindFormerRegister.register_cls": {
        "signature": "(register_class, module_type='tools', alias=None)"
    },
    "mindformers.tools.register.MindFormerRegister.is_exist": {
        "signature": "(module_type, class_name=None)"
    },
    "mindformers.tools.register.MindFormerRegister.get_cls": {
        "signature": "(module_type, class_name=None)"
    },
    "mindformers.tools.register.MindFormerRegister.get_instance_from_cfg": {
        "signature": "(cfg, module_type='tools', default_args=None)"
    },
    "mindformers.tools.register.MindFormerRegister.get_instance": {
        "signature": "(module_type='tools', class_name=None, **kwargs)"
    },
    "mindformers.tools.register.MindFormerRegister.auto_register": {
        "signature": "(class_reference: str, module_type='tools')"
    },
    "mindformers.trainer.Trainer": {
        "signature": "(args: Union[NoneType, mindformers.tools.register.config.MindFormerConfig, mindformers.trainer.training_args.TrainingArguments, str] = None, task: Optional[str] = 'general', model: Union[NoneType, mindformers.models.modeling_utils.PreTrainedModel, str] = None, model_name: Optional[str] = None, tokenizer: Optional[mindformers.models.tokenization_utils_base.PreTrainedTokenizerBase] = None, train_dataset: Union[NoneType, collections.abc.Iterable, mindformers.dataset.base_dataset.BaseDataset, mindspore.dataset.engine.datasets.Dataset, str] = None, eval_dataset: Union[NoneType, collections.abc.Iterable, mindformers.dataset.base_dataset.BaseDataset, mindspore.dataset.engine.datasets.Dataset, str] = None, data_collator: Optional[Callable] = None, optimizers: Optional[mindspore.nn.optim.optimizer.Optimizer] = None, compute_metrics: Union[NoneType, dict, set] = None, callbacks: Union[List[mindspore.train.callback._callback.Callback], NoneType, mindspore.train.callback._callback.Callback] = None, eval_callbacks: Union[List[mindspore.train.callback._callback.Callback], NoneType, mindspore.train.callback._callback.Callback] = None, pet_method: Optional[str] = '', image_processor: Optional[mindformers.models.image_processing_utils.BaseImageProcessor] = None, audio_processor: Optional[mindformers.models.base_processor.BaseAudioProcessor] = None, save_config: bool = False, reset_model: bool = False)"
    },
    "mindformers.trainer.Trainer._reassign_monitor_config": {
        "signature": "(self)"
    },
    "mindformers.trainer.Trainer.train": {
        "signature": "(self, train_checkpoint: Union[NoneType, bool, str] = False, resume_from_checkpoint: Union[NoneType, bool, str] = None, resume_training: Union[NoneType, bool, str] = None, ignore_data_skip: Optional[bool] = None, data_skip_steps: Optional[int] = None, auto_trans_ckpt: Optional[bool] = None, src_strategy: Optional[str] = None, transform_process_num: Optional[int] = None, do_eval: Optional[bool] = False)"
    },
    "mindformers.trainer.Trainer.finetune": {
        "signature": "(self, finetune_checkpoint: Union[NoneType, bool, str] = False, resume_from_checkpoint: Union[NoneType, bool, str] = None, resume_training: Union[NoneType, bool, str] = None, ignore_data_skip: Optional[bool] = None, data_skip_steps: Optional[int] = None, auto_trans_ckpt: Optional[bool] = None, src_strategy: Optional[str] = None, transform_process_num: Optional[int] = None, do_eval: bool = False)"
    },
    "mindformers.trainer.Trainer.evaluate": {
        "signature": "(self, eval_dataset: Union[NoneType, collections.abc.Iterable, mindformers.dataset.base_dataset.BaseDataset, mindspore.dataset.engine.datasets.Dataset, str] = None, eval_checkpoint: Union[NoneType, bool, str] = False, auto_trans_ckpt: Optional[bool] = None, src_strategy: Optional[str] = None, transform_process_num: Optional[int] = None, **kwargs)"
    },
    "mindformers.trainer.Trainer.predict": {
        "signature": "(self, predict_checkpoint: Union[NoneType, bool, str] = None, auto_trans_ckpt: Optional[bool] = None, src_strategy: Optional[str] = None, transform_process_num: Optional[int] = None, input_data: Union[NoneType, PIL.Image.Image, list, mindspore.common.tensor.Tensor, mindspore.dataset.engine.datasets_user_defined.GeneratorDataset, numpy.ndarray, str] = None, batch_size: int = None, **kwargs)"
    },
    "mindformers.trainer.Trainer.add_callback": {
        "signature": "(self, callback)"
    },
    "mindformers.trainer.Trainer.pop_callback": {
        "signature": "(self, callback)"
    },
    "mindformers.trainer.Trainer.remove_callback": {
        "signature": "(self, callback)"
    },
    "mindformers.trainer.Trainer.set_parallel_config": {
        "signature": "(self, data_parallel=1, model_parallel=1, context_parallel=1, expert_parallel=1, pipeline_stage=1, micro_batch_interleave_num=1, micro_batch_num=1, use_seq_parallel=False, optimizer_shard=False, gradient_aggregation_group=4, vocab_emb_dp=True)"
    },
    "mindformers.trainer.Trainer.set_recompute_config": {
        "signature": "(self, recompute=False, parallel_optimizer_comm_recompute=False, select_recompute=False, mp_comm_recompute=True, recompute_slice_activation=False)"
    },
    "mindformers.trainer.Trainer._set_moe_config": {
        "signature": "(self, expert_num=1, capacity_factor=1.1, aux_loss_factor=0.05, num_experts_chosen=1, expert_group_size=None, group_wise_a2a=False, comp_comm_parallel=False, comp_comm_parallel_degree=2)"
    },
    "mindformers.trainer.Trainer._reset_model_instance": {
        "signature": "(self, is_train=False)"
    },
    "mindformers.trainer.Trainer.get_task_config": {
        "signature": "(task, model_name)"
    },
    "mindformers.trainer.Trainer.get_train_dataloader": {
        "signature": "(self)"
    },
    "mindformers.trainer.Trainer.get_eval_dataloader": {
        "signature": "(self)"
    },
    "mindformers.trainer.Trainer.get_last_checkpoint": {
        "signature": "(self)"
    },
    "mindformers.trainer.Trainer.get_load_checkpoint": {
        "signature": "(self, checkpoint)"
    },
    "mindformers.trainer.Trainer._config_init": {
        "signature": "(self, args: Union[NoneType, mindformers.tools.register.config.MindFormerConfig, mindformers.trainer.training_args.TrainingArguments, str] = None, task_config: dict = None)"
    },
    "mindformers.trainer.Trainer._build_profile_cb": {
        "signature": "(self)"
    },
    "mindformers.trainer.Trainer._init_model": {
        "signature": "(self, is_train=False)"
    },
    "mindformers.trainer.Trainer._init_tokenizer": {
        "signature": "(self)"
    },
    "mindformers.trainer.Trainer._init_dataset": {
        "signature": "(self)"
    },
    "mindformers.trainer.Trainer._init_callbacks": {
        "signature": "(self)"
    },
    "mindformers.trainer.Trainer.init_openmind_repo": {
        "signature": "(self)"
    },
    "mindformers.trainer.Trainer.save_model": {
        "signature": "(self, output_dir: Optional[str] = None, internal_call: bool = False)"
    },
    "mindformers.trainer.Trainer._save": {
        "signature": "(self, output_dir: Optional[str] = None, state_dict=None)"
    },
    "mindformers.trainer.Trainer._save_config_to_yaml": {
        "signature": "(self, config: dict = None, config_dir: Optional[str] = None)"
    },
    "mindformers.trainer.Trainer._load_model_checkpoint": {
        "signature": "(self)"
    },
    "mindformers.trainer.Trainer._check_checkpoint_config": {
        "signature": "(self, checkpoint: Union[NoneType, bool, str] = None)"
    },
    "mindformers.trainer.Trainer._check_config_type": {
        "signature": "(self)"
    },
    "mindformers.trainer.Trainer._check_config_rules": {
        "signature": "(self)"
    },
    "mindformers.trainer.Trainer._check_args_task_and_model": {
        "signature": "(self)"
    },
    "mindformers.trainer.Trainer.push_to_hub": {
        "signature": "(self, commit_message: Optional[str] = 'End of training', blocking: bool = True) -> str"
    },
    "mindformers.trainer.TrainingArguments": {
        "signature": "(output_dir: str = './output', overwrite_output_dir: bool = False, seed: int = 42, data_seed: Optional[int] = None, only_save_strategy: bool = False, auto_trans_ckpt: bool = False, src_strategy: Optional[str] = None, transform_process_num: int = 1, resume_from_checkpoint: Optional[str] = None, resume_training: Union[NoneType, bool, str] = None, ignore_data_skip: bool = False, data_skip_steps: Optional[int] = None, do_train: bool = False, do_eval: bool = False, do_predict: bool = False, check_for_nan_in_loss_and_grad: bool = False, calculate_per_token_loss: bool = False, remote_save_url: Optional[str] = None, batch_size: Optional[int] = None, num_train_epochs: float = 3.0, sink_mode: bool = True, sink_size: int = 2, gradient_accumulation_steps: int = 1, mode: int = 0, use_cpu: bool = False, device_id: int = 0, device_target: str = 'Ascend', max_call_depth: int = 10000, max_device_memory: str = '1024GB', save_graphs: bool = False, save_graphs_path: str = './graph', use_parallel: bool = False, parallel_mode: int = 1, gradients_mean: bool = False, loss_repeated_mean: bool = False, enable_alltoall: bool = False, full_batch: bool = True, dataset_strategy: Union[str, tuple] = 'full_batch', search_mode: str = 'sharding_propagation', enable_parallel_optimizer: bool = False, gradient_accumulation_shard: bool = False, parallel_optimizer_threshold: int = 64, optimizer_weight_shard_size: int = -1, strategy_ckpt_save_file: str = './ckpt_strategy.ckpt', data_parallel: int = 1, model_parallel: int = 1, expert_parallel: int = 1, pipeline_stage: int = 1, micro_batch_num: int = 1, gradient_aggregation_group: int = 4, micro_batch_interleave_num: int = 1, use_seq_parallel: bool = False, vocab_emb_dp: bool = True, expert_num: int = 1, capacity_factor: float = 1.05, aux_loss_factor: float = 0.05, num_experts_chosen: int = 1, recompute: bool = False, select_recompute: bool = False, parallel_optimizer_comm_recompute: bool = False, mp_comm_recompute: bool = True, recompute_slice_activation: bool = False, optim: Union[mindformers.trainer.utils.OptimizerType, str] = 'fp32_adamw', adam_beta1: float = 0.9, adam_beta2: float = 0.999, adam_epsilon: float = 1e-08, weight_decay: float = 0.0, layer_scale: bool = False, layer_decay: float = 0.65, lr_scheduler_type: Union[mindformers.trainer.utils.LrSchedulerType, str] = 'cosine', learning_rate: float = 5e-05, lr_end: float = 1e-06, warmup_lr_init: float = 0.0, warmup_epochs: Optional[int] = None, warmup_ratio: Optional[float] = None, warmup_steps: int = 0, total_steps: int = -1, lr_scale: bool = False, lr_scale_factor: int = 256, dataset_task: Optional[str] = None, dataset_type: Optional[str] = None, train_dataset: Optional[str] = None, train_dataset_in_columns: Optional[List[str]] = None, train_dataset_out_columns: Optional[List[str]] = None, eval_dataset: Optional[str] = None, eval_dataset_in_columns: Optional[List[str]] = None, eval_dataset_out_columns: Optional[List[str]] = None, shuffle: bool = True, dataloader_drop_last: bool = True, repeat: int = 1, per_device_train_batch_size: int = 8, per_device_eval_batch_size: int = 8, dataloader_num_workers: int = 8, python_multiprocessing: bool = False, numa_enable: bool = False, prefetch_size: int = 1, wrapper_type: str = 'MFTrainOneStepCell', scale_sense: Union[float, str] = 'DynamicLossScaleUpdateCell', loss_scale_value: int = 65536, loss_scale_factor: int = 2, loss_scale_window: int = 1000, use_clip_grad: bool = True, max_grad_norm: float = 1.0, max_scale_window: int = 1000, min_scale_window: int = 20, metric_type: Union[List[str], NoneType, str] = None, logging_strategy: Union[mindformers.trainer.utils.LoggingIntervalStrategy, str] = 'steps', logging_steps: int = 1, save_prefix: str = 'CKP', save_directory: Optional[str] = None, save_strategy: Union[mindformers.trainer.utils.SaveIntervalStrategy, str] = 'steps', save_steps: int = 500, save_seconds: Optional[int] = None, save_total_limit: Optional[int] = 5, keep_checkpoint_per_n_minutes: int = 0, save_on_each_node: bool = True, integrated_save: bool = None, save_network_params: bool = True, save_trainable_params: bool = False, async_save: bool = False, evaluation_strategy: Union[mindformers.trainer.utils.IntervalStrategy, str] = 'no', eval_steps: Optional[float] = None, eval_epochs: Optional[int] = None, profile: bool = False, profile_start_step: int = 1, profile_end_step: int = 10, init_start_profile: bool = False, profile_communication: bool = False, profile_memory: bool = True, auto_tune: bool = False, filepath_prefix: str = './autotune', autotune_per_step: int = 10, push_to_hub: bool = False, hub_model_id: Optional[str] = None, hub_strategy: Union[mindformers.trainer.utils.HubStrategy, str] = 'every_save', hub_token: Optional[str] = None, hub_private_repo: bool = False, hub_always_push: bool = False) -> None"
    },
    "mindformers.trainer.TrainingArguments._check_rules": {
        "signature": "(self)"
    },
    "mindformers.trainer.TrainingArguments._check_strategy_rules": {
        "signature": "(self)"
    },
    "mindformers.trainer.TrainingArguments._check_dataset_rules": {
        "signature": "(self)"
    },
    "mindformers.trainer.TrainingArguments._check_metric_rules": {
        "signature": "(self)"
    },
    "mindformers.trainer.TrainingArguments.check_step_rules": {
        "signature": "(steps, info='steps')"
    },
    "mindformers.trainer.TrainingArguments.get_warmup_steps": {
        "signature": "(self, num_training_steps: int)"
    },
    "mindformers.trainer.TrainingArguments.get_recompute_config": {
        "signature": "(self)"
    },
    "mindformers.trainer.TrainingArguments.get_parallel_config": {
        "signature": "(self)"
    },
    "mindformers.trainer.TrainingArguments.get_moe_config": {
        "signature": "(self)"
    },
    "mindformers.trainer.TrainingArguments.to_dict": {
        "signature": "(self)"
    },
    "mindformers.trainer.TrainingArguments.to_json_string": {
        "signature": "(self)"
    },
    "mindformers.trainer.TrainingArguments.set_training": {
        "signature": "(self, learning_rate: float = 5e-05, batch_size: int = 8, weight_decay: float = 0, num_epochs: float = 3.0, gradient_accumulation_steps: int = 1, seed: int = 42, **kwargs)"
    },
    "mindformers.trainer.TrainingArguments.set_evaluate": {
        "signature": "(self, strategy: Union[mindformers.trainer.utils.IntervalStrategy, str] = 'no', steps: int = 500, batch_size: int = 8, **kwargs)"
    },
    "mindformers.trainer.TrainingArguments.set_testing": {
        "signature": "(self, batch_size: int = 8, loss_only: bool = False, **kwargs)"
    },
    "mindformers.trainer.TrainingArguments.set_save": {
        "signature": "(self, strategy: Union[mindformers.trainer.utils.IntervalStrategy, str] = 'steps', steps: int = 500, total_limit: Optional[int] = None, on_each_node: bool = True, **kwargs)"
    },
    "mindformers.trainer.TrainingArguments.set_logging": {
        "signature": "(self, strategy: Union[mindformers.trainer.utils.IntervalStrategy, str] = 'steps', steps: int = 500, **kwargs)"
    },
    "mindformers.trainer.TrainingArguments.set_push_to_hub": {
        "signature": "(self, model_id: str, strategy: Union[mindformers.trainer.utils.HubStrategy, str] = 'every_save', token: Optional[str] = None, private_repo: bool = False, always_push: bool = False, **kwargs)"
    },
    "mindformers.trainer.TrainingArguments.set_optimizer": {
        "signature": "(self, name: Union[mindformers.trainer.utils.OptimizerType, str] = 'adamw', learning_rate: float = 5e-05, lr_end: float = 1e-06, weight_decay: float = 0, beta1: float = 0.9, beta2: float = 0.999, epsilon: float = 1e-08, **kwargs)"
    },
    "mindformers.trainer.TrainingArguments.set_lr_scheduler": {
        "signature": "(self, name: Union[mindformers.trainer.utils.LrSchedulerType, str] = 'linear', num_epochs: float = 3.0, warmup_lr_init: float = 0.0, warmup_epochs: Optional[int] = None, warmup_ratio: Optional[float] = None, warmup_steps: int = 0, total_steps: int = -1, **kwargs)"
    },
    "mindformers.trainer.TrainingArguments.set_dataloader": {
        "signature": "(self, train_batch_size: int = 8, eval_batch_size: int = 8, drop_last: bool = False, num_workers: int = 0, ignore_data_skip: bool = False, data_skip_steps: Optional[int] = None, sampler_seed: Optional[int] = None, **kwargs)"
    },
    "mindformers.trainer.TrainingArguments.print_kwargs_unused": {
        "signature": "(**kwargs)"
    },
    "mindformers.trainer.TrainingArguments.convert_args_to_mindformers_config": {
        "signature": "(self, task_config: mindformers.tools.register.config.MindFormerConfig = None)"
    },
    "mindformers.trainer.TrainingArguments._adapt_common_config": {
        "signature": "(self, task_config)"
    },
    "mindformers.trainer.TrainingArguments._adapt_runner_config": {
        "signature": "(self, task_config)"
    },
    "mindformers.trainer.TrainingArguments._adapt_context_config": {
        "signature": "(self, task_config)"
    },
    "mindformers.trainer.TrainingArguments._adapt_parallel_config": {
        "signature": "(self, task_config)"
    },
    "mindformers.trainer.TrainingArguments._adapt_moe_config": {
        "signature": "(self, task_config)"
    },
    "mindformers.trainer.TrainingArguments._adapt_recompute_config": {
        "signature": "(self, task_config)"
    },
    "mindformers.trainer.TrainingArguments._adapt_optimizer_config": {
        "signature": "(self, task_config)"
    },
    "mindformers.trainer.TrainingArguments._adapt_lr_schedule_config": {
        "signature": "(self, task_config)"
    },
    "mindformers.trainer.TrainingArguments._adapt_dataset_config": {
        "signature": "(self, task_config)"
    },
    "mindformers.trainer.TrainingArguments._adapt_wrapper_config": {
        "signature": "(self, task_config)"
    },
    "mindformers.trainer.TrainingArguments._adapt_metric_config": {
        "signature": "(self, task_config)"
    },
    "mindformers.trainer.TrainingArguments._adapt_callback_config": {
        "signature": "(self, task_config)"
    },
    "mindformers.trainer.TrainingArguments._adapt_eval_config": {
        "signature": "(self, task_config)"
    },
    "mindformers.trainer.TrainingArguments._adapt_profile_config": {
        "signature": "(self, task_config)"
    },
    "mindformers.trainer.TrainingArguments._adapt_auto_tune_config": {
        "signature": "(self, task_config)"
    },
    "mindformers.trainer.TrainingArguments._adapt_hub_config": {
        "signature": "(self, task_config)"
    },
    "mindformers.trainer.config_args.BaseArgsConfig": {
        "signature": "(**kwargs)"
    },
    "mindformers.trainer.config_args.BaseArgsConfig.get_supported_kwargs": {
        "signature": "()"
    },
    "mindformers.trainer.config_args.BaseArgsConfig.filter_kwargs": {
        "signature": "(**kwargs)"
    },
    "mindformers.trainer.config_args.CheckpointConfig": {
        "signature": "(prefix: str = 'mindformers', directory: str = None, save_checkpoint_steps: int = 1, keep_checkpoint_max: int = 1, integrated_save: bool = True, async_save: bool = False, saved_network: bool = None, **kwargs)"
    },
    "mindformers.trainer.config_args.CloudConfig": {
        "signature": "(obs_path: str = None, root_path: str = '/cache', rank_id: int = None, upload_frequence: int = 1, keep_last: bool = False, **kwargs)"
    },
    "mindformers.trainer.config_args.ConfigArguments": {
        "signature": "(output_dir: str = './output', profile: bool = False, auto_tune: bool = False, filepath_prefix: str = './autotune', autotune_per_step: int = 10, seed: int = None, train_dataset: Union[NoneType, dict, mindformers.trainer.config_args.BaseArgsConfig] = None, eval_dataset: Union[NoneType, dict, mindformers.trainer.config_args.BaseArgsConfig] = None, runner_config: Union[NoneType, dict, mindformers.trainer.config_args.BaseArgsConfig] = None, optimizer: Union[NoneType, dict, mindformers.trainer.config_args.BaseArgsConfig] = None, runner_wrapper: Union[NoneType, dict, mindformers.trainer.config_args.BaseArgsConfig] = None, lr_schedule: Union[NoneType, dict, mindformers.trainer.config_args.BaseArgsConfig] = None, save_checkpoint: Union[NoneType, dict, mindformers.trainer.config_args.BaseArgsConfig] = None, cloud_config: Union[NoneType, dict, mindformers.trainer.config_args.BaseArgsConfig] = None)"
    },
    "mindformers.trainer.config_args.ContextConfig": {
        "signature": "(mode: Union[int, str] = 0, device_target: str = 'Ascend', device_id: int = 0, save_graphs: bool = False, save_graphs_path: str = '.', **kwargs)"
    },
    "mindformers.trainer.config_args.DataLoaderConfig": {
        "signature": "(dataloader_type: str = None, dataset_dir: str = None, **kwargs)"
    },
    "mindformers.trainer.config_args.DatasetConfig": {
        "signature": "(data_loader: Union[NoneType, dict, mindformers.trainer.config_args.BaseArgsConfig] = None, input_columns: Union[NoneType, list, str] = None, output_columns: Union[NoneType, list, str] = None, column_order: Union[NoneType, list, str] = None, drop_remainder: bool = True, repeat: int = 1, batch_size: int = None, image_size: Union[NoneType, int, list, tuple] = None, **kwargs)"
    },
    "mindformers.trainer.config_args.LRConfig": {
        "signature": "(lr_type: str = None, **kwargs)"
    },
    "mindformers.trainer.config_args.OptimizerConfig": {
        "signature": "(optim_type: str = None, learning_rate: Union[NoneType, float, mindformers.trainer.config_args.BaseArgsConfig] = None, **kwargs)"
    },
    "mindformers.trainer.config_args.ParallelContextConfig": {
        "signature": "(parallel_mode: str = 'STAND_ALONE', device_num: int = 1, gradients_mean: bool = False, auto_pipeline: bool = False, **kwargs)"
    },
    "mindformers.trainer.config_args.RunnerConfig": {
        "signature": "(epochs: int = None, batch_size: int = None, sink_mode: bool = None, sink_size: int = None, initial_epoch: int = None, has_trained_epoches: int = None, has_trained_steps: int = None, **kwargs)"
    },
    "mindformers.trainer.config_args.WrapperConfig": {
        "signature": "(wrapper_type: str = None, **kwargs)"
    },
    "mindformers.trainer.image_classification.image_classification.ImageClassificationTrainer": {
        "signature": "(model_name: str = None)"
    },
    "mindformers.trainer.image_classification.image_classification.ImageClassificationTrainer.train": {
        "signature": "(self, config: Union[NoneType, dict, mindformers.tools.register.config.MindFormerConfig, mindformers.trainer.config_args.ConfigArguments, mindformers.trainer.training_args.TrainingArguments] = None, network: Union[NoneType, mindformers.models.modeling_utils.PreTrainedModel, mindspore.nn.cell.Cell] = None, dataset: Union[NoneType, mindformers.dataset.base_dataset.BaseDataset, mindspore.dataset.engine.datasets_user_defined.GeneratorDataset] = None, wrapper: Optional[mindspore.nn.wrap.cell_wrapper.TrainOneStepCell] = None, optimizer: Optional[mindspore.nn.optim.optimizer.Optimizer] = None, callbacks: Union[List[mindspore.train.callback._callback.Callback], NoneType, mindspore.train.callback._callback.Callback] = None, **kwargs)"
    },
    "mindformers.trainer.image_classification.image_classification.ImageClassificationTrainer.evaluate": {
        "signature": "(self, config: Union[NoneType, dict, mindformers.tools.register.config.MindFormerConfig, mindformers.trainer.config_args.ConfigArguments, mindformers.trainer.training_args.TrainingArguments] = None, network: Union[NoneType, mindformers.models.modeling_utils.PreTrainedModel, mindspore.nn.cell.Cell] = None, dataset: Union[NoneType, mindformers.dataset.base_dataset.BaseDataset, mindspore.dataset.engine.datasets_user_defined.GeneratorDataset] = None, callbacks: Union[List[mindspore.train.callback._callback.Callback], NoneType, mindspore.train.callback._callback.Callback] = None, compute_metrics: Union[NoneType, dict, set] = None, **kwargs)"
    },
    "mindformers.trainer.image_classification.image_classification.ImageClassificationTrainer.predict": {
        "signature": "(self, config: Union[NoneType, dict, mindformers.tools.register.config.MindFormerConfig, mindformers.trainer.config_args.ConfigArguments, mindformers.trainer.training_args.TrainingArguments] = None, input_data: Union[NoneType, PIL.Image.Image, list, mindspore.common.tensor.Tensor, numpy.ndarray, str] = None, network: Union[NoneType, mindformers.models.modeling_utils.PreTrainedModel, mindspore.nn.cell.Cell] = None, image_processor: Optional[mindformers.models.image_processing_utils.BaseImageProcessor] = None, **kwargs)"
    },
    "mindformers.trainer.image_classification.zero_shot_image_classification.ZeroShotImageClassificationTrainer": {
        "signature": "(model_name: str = None)"
    },
    "mindformers.trainer.image_classification.zero_shot_image_classification.ZeroShotImageClassificationTrainer.train": {
        "signature": "(self, *args, **kwargs)"
    },
    "mindformers.trainer.image_classification.zero_shot_image_classification.ZeroShotImageClassificationTrainer.evaluate": {
        "signature": "(self, config: Union[NoneType, dict, mindformers.tools.register.config.MindFormerConfig, mindformers.trainer.config_args.ConfigArguments, mindformers.trainer.training_args.TrainingArguments] = None, network: Union[NoneType, mindformers.models.modeling_utils.PreTrainedModel, mindspore.nn.cell.Cell] = None, dataset: Union[NoneType, mindformers.dataset.base_dataset.BaseDataset, mindspore.dataset.engine.datasets_user_defined.GeneratorDataset] = None, callbacks: Union[List[mindspore.train.callback._callback.Callback], NoneType, mindspore.train.callback._callback.Callback] = None, compute_metrics: Union[NoneType, dict, set] = None, **kwargs)"
    },
    "mindformers.trainer.image_classification.zero_shot_image_classification.ZeroShotImageClassificationTrainer.predict": {
        "signature": "(self, config: Union[NoneType, dict, mindformers.tools.register.config.MindFormerConfig, mindformers.trainer.config_args.ConfigArguments, mindformers.trainer.training_args.TrainingArguments] = None, input_data: Union[NoneType, PIL.Image.Image, list, mindspore.common.tensor.Tensor, mindspore.dataset.engine.datasets_user_defined.GeneratorDataset, numpy.ndarray, str] = None, network: Union[NoneType, mindformers.models.modeling_utils.PreTrainedModel, mindspore.nn.cell.Cell] = None, tokenizer: Optional[mindformers.models.tokenization_utils_base.PreTrainedTokenizerBase] = None, image_processor: Optional[mindformers.models.image_processing_utils.BaseImageProcessor] = None, **kwargs)"
    },
    "mindformers.trainer.image_to_text_generation.image_to_text_generation.ImageToTextGenerationTrainer": {
        "signature": "(model_name: str = None)"
    },
    "mindformers.trainer.image_to_text_generation.image_to_text_generation.ImageToTextGenerationTrainer.train": {
        "signature": "(self, config: Union[NoneType, dict, mindformers.tools.register.config.MindFormerConfig, mindformers.trainer.config_args.ConfigArguments, mindformers.trainer.training_args.TrainingArguments] = None, network: Union[NoneType, mindformers.models.modeling_utils.PreTrainedModel, mindspore.nn.cell.Cell] = None, dataset: Union[NoneType, mindformers.dataset.base_dataset.BaseDataset, mindspore.dataset.engine.datasets_user_defined.GeneratorDataset] = None, optimizer: Optional[mindspore.nn.optim.optimizer.Optimizer] = None, wrapper: Optional[mindspore.nn.wrap.cell_wrapper.TrainOneStepCell] = None, callbacks: Union[List[mindspore.train.callback._callback.Callback], NoneType, mindspore.train.callback._callback.Callback] = None, **kwargs)"
    },
    "mindformers.trainer.image_to_text_generation.image_to_text_generation.ImageToTextGenerationTrainer.evaluate": {
        "signature": "(self, *args, **kwargs)"
    },
    "mindformers.trainer.image_to_text_generation.image_to_text_generation.ImageToTextGenerationTrainer.predict": {
        "signature": "(self, config: Union[NoneType, dict, mindformers.tools.register.config.MindFormerConfig, mindformers.trainer.config_args.ConfigArguments, mindformers.trainer.training_args.TrainingArguments] = None, input_data: Union[NoneType, list, mindspore.dataset.engine.datasets_user_defined.GeneratorDataset] = None, network: Union[NoneType, mindformers.models.modeling_utils.PreTrainedModel, mindspore.nn.cell.Cell] = None, processor: Optional[mindformers.models.base_processor.BaseProcessor] = None, **kwargs)"
    },
    "mindformers.trainer.multi_modal_to_text_generation.multi_modal_to_text_generation.MultiModalToTextGenerationTrainer": {
        "signature": "(model_name: str = None)"
    },
    "mindformers.trainer.multi_modal_to_text_generation.multi_modal_to_text_generation.MultiModalToTextGenerationTrainer.train": {
        "signature": "(self, config: Union[NoneType, dict, mindformers.tools.register.config.MindFormerConfig, mindformers.trainer.config_args.ConfigArguments, mindformers.trainer.training_args.TrainingArguments] = None, network: Union[NoneType, mindformers.models.modeling_utils.PreTrainedModel, mindspore.nn.cell.Cell] = None, dataset: Union[NoneType, mindformers.dataset.base_dataset.BaseDataset, mindspore.dataset.engine.datasets_user_defined.GeneratorDataset] = None, optimizer: Optional[mindspore.nn.optim.optimizer.Optimizer] = None, wrapper: Optional[mindspore.nn.wrap.cell_wrapper.TrainOneStepCell] = None, callbacks: Union[List[mindspore.train.callback._callback.Callback], NoneType, mindspore.train.callback._callback.Callback] = None, **kwargs)"
    },
    "mindformers.trainer.multi_modal_to_text_generation.multi_modal_to_text_generation.MultiModalToTextGenerationTrainer.evaluate": {
        "signature": "(self, *args, **kwargs)"
    },
    "mindformers.trainer.multi_modal_to_text_generation.multi_modal_to_text_generation.MultiModalToTextGenerationTrainer.predict": {
        "signature": "(self, config: Union[NoneType, dict, mindformers.tools.register.config.MindFormerConfig, mindformers.trainer.config_args.ConfigArguments, mindformers.trainer.training_args.TrainingArguments] = None, input_data: Union[NoneType, list, mindspore.dataset.engine.datasets_user_defined.GeneratorDataset] = None, network: Union[NoneType, mindformers.models.modeling_utils.PreTrainedModel, mindspore.nn.cell.Cell] = None, processor: Optional[mindformers.models.base_processor.BaseProcessor] = None, **kwargs)"
    },
    "mindformers.trainer.question_answering.question_answering.QuestionAnsweringTrainer": {
        "signature": "(model_name: str = None)"
    },
    "mindformers.trainer.question_answering.question_answering.QuestionAnsweringTrainer.train": {
        "signature": "(self, config: Union[NoneType, dict, mindformers.tools.register.config.MindFormerConfig, mindformers.trainer.config_args.ConfigArguments, mindformers.trainer.training_args.TrainingArguments] = None, network: Union[NoneType, mindformers.models.modeling_utils.PreTrainedModel, mindspore.nn.cell.Cell] = None, dataset: Union[NoneType, mindformers.dataset.base_dataset.BaseDataset, mindspore.dataset.engine.datasets_user_defined.GeneratorDataset] = None, wrapper: Optional[mindspore.nn.wrap.cell_wrapper.TrainOneStepCell] = None, optimizer: Optional[mindspore.nn.optim.optimizer.Optimizer] = None, callbacks: Union[List[mindspore.train.callback._callback.Callback], NoneType, mindspore.train.callback._callback.Callback] = None, **kwargs)"
    },
    "mindformers.trainer.question_answering.question_answering.QuestionAnsweringTrainer.evaluate": {
        "signature": "(self, config: Union[NoneType, dict, mindformers.tools.register.config.MindFormerConfig, mindformers.trainer.config_args.ConfigArguments, mindformers.trainer.training_args.TrainingArguments] = None, network: Union[NoneType, mindformers.models.modeling_utils.PreTrainedModel, mindspore.nn.cell.Cell] = None, dataset: Union[NoneType, mindformers.dataset.base_dataset.BaseDataset, mindspore.dataset.engine.datasets_user_defined.GeneratorDataset] = None, callbacks: Union[List[mindspore.train.callback._callback.Callback], NoneType, mindspore.train.callback._callback.Callback] = None, compute_metrics: Union[NoneType, dict, set] = None, **kwargs)"
    },
    "mindformers.trainer.question_answering.question_answering.QuestionAnsweringTrainer.predict": {
        "signature": "(self, config: Union[NoneType, dict, mindformers.tools.register.config.MindFormerConfig, mindformers.trainer.config_args.ConfigArguments, mindformers.trainer.training_args.TrainingArguments] = None, input_data: Union[NoneType, list, str] = None, network: Union[NoneType, mindformers.models.modeling_utils.PreTrainedModel, mindspore.nn.cell.Cell] = None, tokenizer: Optional[mindformers.models.tokenization_utils_base.PreTrainedTokenizerBase] = None, **kwargs)"
    },
    "mindformers.trainer.trainer.Trainer": {
        "signature": "(args: Union[NoneType, mindformers.tools.register.config.MindFormerConfig, mindformers.trainer.training_args.TrainingArguments, str] = None, task: Optional[str] = 'general', model: Union[NoneType, mindformers.models.modeling_utils.PreTrainedModel, str] = None, model_name: Optional[str] = None, tokenizer: Optional[mindformers.models.tokenization_utils_base.PreTrainedTokenizerBase] = None, train_dataset: Union[NoneType, collections.abc.Iterable, mindformers.dataset.base_dataset.BaseDataset, mindspore.dataset.engine.datasets.Dataset, str] = None, eval_dataset: Union[NoneType, collections.abc.Iterable, mindformers.dataset.base_dataset.BaseDataset, mindspore.dataset.engine.datasets.Dataset, str] = None, data_collator: Optional[Callable] = None, optimizers: Optional[mindspore.nn.optim.optimizer.Optimizer] = None, compute_metrics: Union[NoneType, dict, set] = None, callbacks: Union[List[mindspore.train.callback._callback.Callback], NoneType, mindspore.train.callback._callback.Callback] = None, eval_callbacks: Union[List[mindspore.train.callback._callback.Callback], NoneType, mindspore.train.callback._callback.Callback] = None, pet_method: Optional[str] = '', image_processor: Optional[mindformers.models.image_processing_utils.BaseImageProcessor] = None, audio_processor: Optional[mindformers.models.base_processor.BaseAudioProcessor] = None, save_config: bool = False, reset_model: bool = False)"
    },
    "mindformers.trainer.trainer.Trainer._reassign_monitor_config": {
        "signature": "(self)"
    },
    "mindformers.trainer.trainer.Trainer.train": {
        "signature": "(self, train_checkpoint: Union[NoneType, bool, str] = False, resume_from_checkpoint: Union[NoneType, bool, str] = None, resume_training: Union[NoneType, bool, str] = None, ignore_data_skip: Optional[bool] = None, data_skip_steps: Optional[int] = None, auto_trans_ckpt: Optional[bool] = None, src_strategy: Optional[str] = None, transform_process_num: Optional[int] = None, do_eval: Optional[bool] = False)"
    },
    "mindformers.trainer.trainer.Trainer.finetune": {
        "signature": "(self, finetune_checkpoint: Union[NoneType, bool, str] = False, resume_from_checkpoint: Union[NoneType, bool, str] = None, resume_training: Union[NoneType, bool, str] = None, ignore_data_skip: Optional[bool] = None, data_skip_steps: Optional[int] = None, auto_trans_ckpt: Optional[bool] = None, src_strategy: Optional[str] = None, transform_process_num: Optional[int] = None, do_eval: bool = False)"
    },
    "mindformers.trainer.trainer.Trainer.evaluate": {
        "signature": "(self, eval_dataset: Union[NoneType, collections.abc.Iterable, mindformers.dataset.base_dataset.BaseDataset, mindspore.dataset.engine.datasets.Dataset, str] = None, eval_checkpoint: Union[NoneType, bool, str] = False, auto_trans_ckpt: Optional[bool] = None, src_strategy: Optional[str] = None, transform_process_num: Optional[int] = None, **kwargs)"
    },
    "mindformers.trainer.trainer.Trainer.predict": {
        "signature": "(self, predict_checkpoint: Union[NoneType, bool, str] = None, auto_trans_ckpt: Optional[bool] = None, src_strategy: Optional[str] = None, transform_process_num: Optional[int] = None, input_data: Union[NoneType, PIL.Image.Image, list, mindspore.common.tensor.Tensor, mindspore.dataset.engine.datasets_user_defined.GeneratorDataset, numpy.ndarray, str] = None, batch_size: int = None, **kwargs)"
    },
    "mindformers.trainer.trainer.Trainer.add_callback": {
        "signature": "(self, callback)"
    },
    "mindformers.trainer.trainer.Trainer.pop_callback": {
        "signature": "(self, callback)"
    },
    "mindformers.trainer.trainer.Trainer.remove_callback": {
        "signature": "(self, callback)"
    },
    "mindformers.trainer.trainer.Trainer.set_parallel_config": {
        "signature": "(self, data_parallel=1, model_parallel=1, context_parallel=1, expert_parallel=1, pipeline_stage=1, micro_batch_interleave_num=1, micro_batch_num=1, use_seq_parallel=False, optimizer_shard=False, gradient_aggregation_group=4, vocab_emb_dp=True)"
    },
    "mindformers.trainer.trainer.Trainer.set_recompute_config": {
        "signature": "(self, recompute=False, parallel_optimizer_comm_recompute=False, select_recompute=False, mp_comm_recompute=True, recompute_slice_activation=False)"
    },
    "mindformers.trainer.trainer.Trainer._set_moe_config": {
        "signature": "(self, expert_num=1, capacity_factor=1.1, aux_loss_factor=0.05, num_experts_chosen=1, expert_group_size=None, group_wise_a2a=False, comp_comm_parallel=False, comp_comm_parallel_degree=2)"
    },
    "mindformers.trainer.trainer.Trainer._reset_model_instance": {
        "signature": "(self, is_train=False)"
    },
    "mindformers.trainer.trainer.Trainer.get_task_config": {
        "signature": "(task, model_name)"
    },
    "mindformers.trainer.trainer.Trainer.get_train_dataloader": {
        "signature": "(self)"
    },
    "mindformers.trainer.trainer.Trainer.get_eval_dataloader": {
        "signature": "(self)"
    },
    "mindformers.trainer.trainer.Trainer.get_last_checkpoint": {
        "signature": "(self)"
    },
    "mindformers.trainer.trainer.Trainer.get_load_checkpoint": {
        "signature": "(self, checkpoint)"
    },
    "mindformers.trainer.trainer.Trainer._config_init": {
        "signature": "(self, args: Union[NoneType, mindformers.tools.register.config.MindFormerConfig, mindformers.trainer.training_args.TrainingArguments, str] = None, task_config: dict = None)"
    },
    "mindformers.trainer.trainer.Trainer._build_profile_cb": {
        "signature": "(self)"
    },
    "mindformers.trainer.trainer.Trainer._init_model": {
        "signature": "(self, is_train=False)"
    },
    "mindformers.trainer.trainer.Trainer._init_tokenizer": {
        "signature": "(self)"
    },
    "mindformers.trainer.trainer.Trainer._init_dataset": {
        "signature": "(self)"
    },
    "mindformers.trainer.trainer.Trainer._init_callbacks": {
        "signature": "(self)"
    },
    "mindformers.trainer.trainer.Trainer.init_openmind_repo": {
        "signature": "(self)"
    },
    "mindformers.trainer.trainer.Trainer.save_model": {
        "signature": "(self, output_dir: Optional[str] = None, internal_call: bool = False)"
    },
    "mindformers.trainer.trainer.Trainer._save": {
        "signature": "(self, output_dir: Optional[str] = None, state_dict=None)"
    },
    "mindformers.trainer.trainer.Trainer._save_config_to_yaml": {
        "signature": "(self, config: dict = None, config_dir: Optional[str] = None)"
    },
    "mindformers.trainer.trainer.Trainer._load_model_checkpoint": {
        "signature": "(self)"
    },
    "mindformers.trainer.trainer.Trainer._check_checkpoint_config": {
        "signature": "(self, checkpoint: Union[NoneType, bool, str] = None)"
    },
    "mindformers.trainer.trainer.Trainer._check_config_type": {
        "signature": "(self)"
    },
    "mindformers.trainer.trainer.Trainer._check_config_rules": {
        "signature": "(self)"
    },
    "mindformers.trainer.trainer.Trainer._check_args_task_and_model": {
        "signature": "(self)"
    },
    "mindformers.trainer.trainer.Trainer.push_to_hub": {
        "signature": "(self, commit_message: Optional[str] = 'End of training', blocking: bool = True) -> str"
    },
    "mindformers.wrapper.GradAccumulationCellWithTwoOutput": {
        "signature": "(network, micro_size)"
    },
    "mindformers.wrapper.GradAccumulationCellWithTwoOutput.construct": {
        "signature": "(self, *inputs)"
    },
    "mindformers.wrapper.MFPipelineWithLossScaleCell": {
        "signature": "(network, optimizer, use_clip_grad=True, max_grad_norm=1.0, scale_sense=1.0, micro_batch_num=1, local_norm=False, calculate_per_token_loss=False, **kwargs)"
    },
    "mindformers.wrapper.MFPipelineWithLossScaleCell.construct": {
        "signature": "(self, *inputs)"
    },
    "mindformers.wrapper.MFTrainOneStepCell": {
        "signature": "(network, optimizer, use_clip_grad=False, max_grad_norm=1.0, scale_sense=1.0, local_norm=False, calculate_per_token_loss=False, **kwargs)"
    },
    "mindformers.wrapper.MFTrainOneStepCell.construct": {
        "signature": "(self, *inputs)"
    },
    "mindformers.wrapper.PipelineCellWithTwoOutput": {
        "signature": "(network, micro_size)"
    },
    "mindformers.wrapper.PipelineCellWithTwoOutput.construct": {
        "signature": "(self, *inputs)"
    },
    "mindformers.wrapper.adaptive_loss_scale.AdaptiveLossScaleUpdateCell": {
        "signature": "(loss_scale_value, scale_factor, scale_window, max_scale_window=1000, min_scale_window=20)"
    },
    "mindformers.wrapper.adaptive_loss_scale.AdaptiveLossScaleUpdateCell.get_loss_scale": {
        "signature": "(self)"
    },
    "mindformers.wrapper.adaptive_loss_scale.AdaptiveLossScaleUpdateCell.construct": {
        "signature": "(self, loss_scale, overflow)"
    },
    "mindformers.wrapper.wrapper.GradAccumulationCellWithTwoOutput": {
        "signature": "(network, micro_size)"
    },
    "mindformers.wrapper.wrapper.GradAccumulationCellWithTwoOutput.construct": {
        "signature": "(self, *inputs)"
    },
    "mindformers.wrapper.wrapper.MFPipelineWithLossScaleCell": {
        "signature": "(network, optimizer, use_clip_grad=True, max_grad_norm=1.0, scale_sense=1.0, micro_batch_num=1, local_norm=False, calculate_per_token_loss=False, **kwargs)"
    },
    "mindformers.wrapper.wrapper.MFPipelineWithLossScaleCell.construct": {
        "signature": "(self, *inputs)"
    },
    "mindformers.wrapper.wrapper.MFTrainOneStepCell": {
        "signature": "(network, optimizer, use_clip_grad=False, max_grad_norm=1.0, scale_sense=1.0, local_norm=False, calculate_per_token_loss=False, **kwargs)"
    },
    "mindformers.wrapper.wrapper.MFTrainOneStepCell.construct": {
        "signature": "(self, *inputs)"
    },
    "mindformers.wrapper.wrapper.PipelineCellWithTwoOutput": {
        "signature": "(network, micro_size)"
    },
    "mindformers.wrapper.wrapper.PipelineCellWithTwoOutput.construct": {
        "signature": "(self, *inputs)"
    }
}