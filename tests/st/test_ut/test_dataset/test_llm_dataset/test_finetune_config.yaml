# Test configuration for finetune dataset
train_dataset:
  # data_loader: 数据加载器配置，定义如何加载和处理训练数据
  data_loader:
    # type: 数据加载器类型，HFDataLoader用于加载HuggingFace格式的数据集
    type: HFDataLoader

    # load_func: 数据集加载函数名，指定使用哪个函数来加载数据集，此处复用HuggingFace Datasets的load_dataset函数
    load_func: 'load_dataset'

    # path: 数据集路径或名称，指定要加载的HuggingFace数据集ID 或者 已经下载好的数据集的格式指定
    path: "llm-wizard/alpaca-gpt4-data-zh" # HuggingFace数据集ID，自动下载该数据集
    # path: 'json' # 数据集路径或格式类型，当加载本地文件时可指定为'json'、'csv'等格式，配合data_files使用

    # data_files: 数据文件路径，指定要加载的本地数据文件的完整路径
    # 支持单个文件路径或文件路径列表，用于加载本地存储的数据集文件，此处代表的是已经离线下载好的Hugging Face数据集
    # data_files: '/path/alpaca-gpt4-data.json'

    # create_attention_mask: 是否创建注意力掩码，用于控制模型在自注意力计算中哪些位置可以被关注
    create_attention_mask: True

    # create_compressed_eod_mask: 是否创建压缩的EOD掩码，用于节省内存
    create_compressed_eod_mask: False

    # compressed_eod_mask_length: 压缩EOD掩码的长度，当create_compressed_eod_mask为True时生效
    compressed_eod_mask_length: 128

    # use_broadcast_data: 是否使用广播数据传输，在分布式训练中用于数据分发，分布式场景必须打开
    use_broadcast_data: True

    # shuffle: 是否对数据集进行随机打乱
    shuffle: False

    # handler: 数据处理管道，定义数据预处理的步骤序列
    handler:
      # take数据集采样操作：限制数据集大小，只取前n个样本
      # 通常用于快速测试或调试，正式训练时建议删除此配置项以使用完整数据集
      - type: take
        # n: 要保留的样本数量
        n: 2000

      # AlpacaInstructDataHandler操作：处理Alpaca格式的指令数据
      - type: AlpacaInstructDataHandler
        # seq_length: 序列长度，指定处理后每个样本的token序列长度
        seq_length: 4096
        # padding: 是否进行填充操作
        padding: False
        # tokenizer: 分词器配置
        tokenizer:
          # trust_remote_code: 是否信任远程代码，允许执行远程定义的分词器逻辑
          trust_remote_code: True
          # padding_side: 填充方向，'right'表示在右侧填充
          padding_side: 'right'

      # PackingHandler操作：将多个短序列打包成一个长序列以提高训练效率
      # 微调场景推荐使用，可以有效提高GPU利用率，减少padding浪费
      # 如果不需要序列打包功能，删除此配置项即可
      - type: PackingHandler
        # seq_length: 打包后的序列长度
        seq_length: 4096
        # pack_strategy: 打包策略，'pack'表示使用打包算法将多个短序列组合成长序列
        pack_strategy: 'pack'

  # input_columns: 输入列名，指定数据集中包含的字段名称，输入列名，指定数据集中包含的字段名称, 套件内部会根据不同的场景自动生成，如果用户自定义DataLoader并手动指定，则会优先使用用户自定义的
  input_columns: [ "input_ids", "labels", "loss_mask", "position_ids", "attention_mask" ]

  # construct_args_key: 构造参数键名，指定传给模型构造函数的参数名称，输入列名，指定数据集中包含的字段名称, 套件内部会根据不同的场景自动生成，如果用户自定义网络入参，则会优先使用用户自定义的
  construct_args_key: [ "input_ids", "labels", "loss_mask", "position_ids", "attention_mask" ]

  # drop_remainder: 是否丢弃最后一个不完整的batch，设置为True时会丢弃不足batch大小的数据
  drop_remainder: True

  # num_parallel_workers: 并行数据处理工作线程数，控制数据预处理的并行度
  num_parallel_workers: 8

  # python_multiprocessing: 是否启用Python多进程，设置为True时使用多进程处理数据
  python_multiprocessing: False

  # numa_enable: 是否启用NUMA优化，用于提升多核CPU环境下的数据处理性能
  numa_enable: False

  # prefetch_size: 数据预取大小，控制预加载到内存中的数据batch数量
  # 设置较大的值可以提高数据加载效率，减少训练等待时间，但会增加内存占用
  # 通常建议设置为1-4之间，根据可用内存和数据集大小进行调整
  prefetch_size: 1
