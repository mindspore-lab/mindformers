# Token Classification

> ## ğŸš¨ å¼ƒç”¨è¯´æ˜
>
> æœ¬æ–‡æ¡£å·²è¿‡æ—¶ï¼Œä¸å†è¿›è¡Œç»´æŠ¤ï¼Œå¹¶å°†åœ¨ *1.6.0* ç‰ˆæœ¬ä¸‹æ¶ï¼Œå…¶ä¸­å¯èƒ½åŒ…å«è¿‡æ—¶çš„ä¿¡æ¯æˆ–å·²è¢«æ›´æ–°çš„åŠŸèƒ½æ›¿ä»£ã€‚å»ºè®®å‚è€ƒæœ€æ–°çš„ **[å®˜æ–¹æ–‡æ¡£](https://www.mindspore.cn/mindformers/docs/zh-CN/r1.5.0/index.html)** ï¼Œä»¥è·å–å‡†ç¡®çš„ä¿¡æ¯ã€‚
>
> å¦‚æœæ‚¨ä»éœ€ä½¿ç”¨æœ¬æ–‡æ¡£ä¸­çš„å†…å®¹ï¼Œè¯·ä»”ç»†æ ¸å¯¹å…¶é€‚ç”¨æ€§ï¼Œå¹¶ç»“åˆæœ€æ–°ç‰ˆæœ¬çš„ç›¸å…³èµ„æºè¿›è¡ŒéªŒè¯ã€‚
>
> å¦‚æœ‰ä»»ä½•é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·é€šè¿‡ **[ç¤¾åŒºIssue](https://gitee.com/mindspore/mindformers/issues/new)** æäº¤åé¦ˆã€‚æ„Ÿè°¢æ‚¨çš„ç†è§£ä¸æ”¯æŒï¼

## ä»»åŠ¡æè¿°

å‘½åå®ä½“è¯†åˆ«ï¼šæ¨¡å‹åœ¨åŸºäºå‘½åå®ä½“è¯†åˆ«æ•°æ®é›†çš„å¾®è°ƒåï¼Œå¯ä»¥åœ¨ç»™å®šä»»æ„æ–‡æœ¬ä¸å€™é€‰æ ‡ç­¾åˆ—è¡¨çš„æƒ…å†µä¸‹ï¼Œå®Œæˆå¯¹æ–‡æœ¬ä¸­å‘½åå®ä½“çš„è¯†åˆ«ã€‚

[ç›¸å…³è®ºæ–‡](https://arxiv.org/abs/2001.04351) Xu, Liang and Dong, Qianqian and Yu, Cong and Tian, Yin and Liu, Weitang and Li, Lu and Zhang, Xuanwei, CLUENER2020: Fine-grained Name Entity Recognition for Chinese, 2020.

## å·²æ”¯æŒæ•°æ®é›†æ€§èƒ½

| model  |               type               | datasets | Entity F1 |            stage            | example |
|:------:|:--------------------------------:|:--------:|:---------:|:---------------------------:|:-------:|
| tokcls | tokcls_bert_case_chinese_cluener | CLUENER  |  0.7905   | finetune<br>eval<br>predict |    -    |

### [CLUENER](https://github.com/CLUEbenchmark/CLUENER2020)

- æ•°æ®é›†ï¼šè®­ç»ƒé›†å¤§å°ä¸º10748ä¸ªæ ·æœ¬ï¼ŒéªŒè¯é›†å¤§å°ä¸º1343ä¸ªæ ·æœ¬ï¼Œ10ä¸ªç±»åˆ«ï¼Œåˆ†åˆ«ä¸º: åœ°å€ï¼ˆaddressï¼‰ï¼Œä¹¦åï¼ˆbookï¼‰ï¼Œå…¬å¸ï¼ˆcompanyï¼‰ï¼Œæ¸¸æˆï¼ˆgameï¼‰ï¼Œæ”¿åºœï¼ˆgovernmentï¼‰ï¼Œç”µå½±ï¼ˆmovieï¼‰ï¼Œå§“åï¼ˆnameï¼‰ï¼Œç»„ç»‡æœºæ„ï¼ˆorganizationï¼‰ï¼ŒèŒä½ï¼ˆpositionï¼‰ï¼Œæ™¯ç‚¹ï¼ˆsceneï¼‰ã€‚
- æ•°æ®æ ¼å¼ï¼šjsonæ–‡ä»¶

 ```bash
æ•°æ®é›†ç›®å½•æ ¼å¼
â””â”€cluener
    â”œâ”€train.json
    â”œâ”€dev.json
    â”œâ”€test.json
    â”œâ”€cluener_predict.json
 ```

## å¿«é€Ÿä»»åŠ¡æ¥å£

### è„šæœ¬å¯åŠ¨

> éœ€å¼€å‘è€…æå‰cloneå·¥ç¨‹ã€‚

- è¯·å‚è€ƒ[ä½¿ç”¨è„šæœ¬å¯åŠ¨](../../README.md#æ–¹å¼ä¸€ä½¿ç”¨å·²æœ‰è„šæœ¬å¯åŠ¨)

- åœ¨è„šæœ¬æ‰§è¡Œç›®å½•åˆ›å»º `cluener` æ–‡ä»¶å¤¹ï¼Œç„¶åå°†æ•°æ®é›†æ”¾å…¥å…¶ä¸­

- è„šæœ¬è¿è¡Œæµ‹è¯•

```shell
# finetune
python run_mindformer.py --config ./configs/tokcls/run_tokcls_bert_base_chinese.yaml --run_mode finetune --load_checkpoint tokcls_bert_base_chinese

# evaluate
python run_mindformer.py --config ./configs/tokcls/run_tokcls_bert_base_chinese.yaml --run_mode eval --load_checkpoint tokcls_bert_base_chinese_cluener

# predict
python run_mindformer.py --config ./configs/tokcls/run_tokcls_bert_base_chinese.yaml --run_mode predict --load_checkpoint tokcls_bert_base_chinese_cluener --predict_data [TEXT]
```

### è°ƒç”¨APIå¯åŠ¨

- Traineræ¥å£å¼€å¯è®­ç»ƒ/è¯„ä¼°/æ¨ç†ï¼š

```python
import mindspore; mindspore.set_context(mode=0, device_id=0)
from mindformers.trainer import Trainer

# åˆå§‹åŒ–trainer
trainer = Trainer(task='token_classification',
                  model='tokcls_bert_base_chinese',
                  train_dataset='./cluener/',
                  eval_dataset='./cluener/')
# æµ‹è¯•æ•°æ®
input_data = ["ç»“æœä¸Šå‘¨å…­ä»–ä»¬ä¸»åœº0ï¼š3æƒ¨è´¥ç»™äº†ä¸­æ¸¸çƒé˜Ÿç“¦æ‹‰å¤šåˆ©å¾·ï¼Œè¿‘7ä¸ªå¤šæœˆä»¥æ¥è¥¿ç”²é¦–æ¬¡è¾“çƒã€‚"]

#æ–¹å¼1ï¼šä½¿ç”¨ç°æœ‰çš„é¢„è®­ç»ƒæƒé‡è¿›è¡Œfinetuneï¼Œ å¹¶ä½¿ç”¨finetuneè·å¾—çš„æƒé‡è¿›è¡Œevalå’Œæ¨ç†
trainer.finetune(finetune_checkpoint="tokcls_bert_base_chinese")
trainer.evaluate(eval_checkpoint=True)
trainer.predict(predict_checkpoint=True, input_data=input_data)

# æ–¹å¼2ï¼š ä»obsä¸‹è½½è®­ç»ƒå¥½çš„æƒé‡å¹¶è¿›è¡Œevalå’Œæ¨ç†
trainer.evaluate()
# INFO - Entity F1=0.7853
trainer.predict(input_data=input_data)
# INFO - output result is [[{'entity_group': 'organization', 'start': 20, 'end': 24, 'score': 0.94914, 'word': 'ç“¦æ‹‰å¤šåˆ©å¾·'},
#                           {'entity_group': 'organization', 'start': 33, 'end': 34, 'score': 0.9496, 'word': 'è¥¿ç”²'}]]
```

- pipelineæ¥å£å¼€å¯å¿«é€Ÿæ¨ç†

```python
from mindformers.pipeline import TokenClassificationPipeline
from mindformers import AutoTokenizer, BertForTokenClassification, AutoConfig
from mindformers.dataset.labels import cluener_labels

input_data = ["è¡¨èº«åˆ»æœ‰ä»£è¡¨æ—¥å†…ç“¦é’Ÿè¡¨åŒ freresoltramareçš„â€œfoâ€å­—æ ·ã€‚"]

id2label = {label_id: label for label_id, label in enumerate(cluener_labels)}

tokenizer = AutoTokenizer.from_pretrained('tokcls_bert_base_chinese_cluener')
tokcls_cluener_config = AutoConfig.from_pretrained('tokcls_bert_base_chinese_cluener')

# This is a known issue, you need to specify batch size equal to 1 when creating bert model.
tokcls_cluener_config.batch_size = 1

model = BertForTokenClassification(tokcls_cluener_config)
tokcls_pipeline = TokenClassificationPipeline(task='token_classification',
                                              model=model,
                                              id2label=id2label,
                                              tokenizer=tokenizer,
                                              max_length=model.config.seq_length,
                                              padding="max_length")

results = tokcls_pipeline(input_data)
print(results)
# è¾“å‡º
# [[{'entity_group': 'address', 'start': 6, 'end': 8, 'score': 0.52329, 'word': 'æ—¥å†…ç“¦'},
#   {'entity_group': 'name', 'start': 12, 'end': 25, 'score': 0.83922, 'word': 'freresoltramar'}]]
```
