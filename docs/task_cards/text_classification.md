# Text Classification

> ## ğŸš¨ å¼ƒç”¨è¯´æ˜
>
> æœ¬æ–‡æ¡£å·²è¿‡æ—¶ï¼Œä¸å†è¿›è¡Œç»´æŠ¤ï¼Œå¹¶å°†åœ¨ *1.6.0* ç‰ˆæœ¬ä¸‹æ¶ï¼Œå…¶ä¸­å¯èƒ½åŒ…å«è¿‡æ—¶çš„ä¿¡æ¯æˆ–å·²è¢«æ›´æ–°çš„åŠŸèƒ½æ›¿ä»£ã€‚å»ºè®®å‚è€ƒæœ€æ–°çš„ **[å®˜æ–¹æ–‡æ¡£](https://www.mindspore.cn/mindformers/docs/zh-CN/r1.5.0/index.html)** ï¼Œä»¥è·å–å‡†ç¡®çš„ä¿¡æ¯ã€‚
>
> å¦‚æœæ‚¨ä»éœ€ä½¿ç”¨æœ¬æ–‡æ¡£ä¸­çš„å†…å®¹ï¼Œè¯·ä»”ç»†æ ¸å¯¹å…¶é€‚ç”¨æ€§ï¼Œå¹¶ç»“åˆæœ€æ–°ç‰ˆæœ¬çš„ç›¸å…³èµ„æºè¿›è¡ŒéªŒè¯ã€‚
>
> å¦‚æœ‰ä»»ä½•é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·é€šè¿‡ **[ç¤¾åŒºIssue](https://gitee.com/mindspore/mindformers/issues/new)** æäº¤åé¦ˆã€‚æ„Ÿè°¢æ‚¨çš„ç†è§£ä¸æ”¯æŒï¼

## ä»»åŠ¡æè¿°

æ–‡æœ¬åˆ†ç±»ï¼šæ¨¡å‹åœ¨åŸºäºæ–‡æœ¬å¯¹çš„å¾®è°ƒåï¼Œå¯ä»¥åœ¨ç»™å®šä»»æ„æ–‡æœ¬å¯¹ä¸å€™é€‰æ ‡ç­¾åˆ—è¡¨çš„æƒ…å†µä¸‹ï¼Œå®Œæˆå¯¹æ–‡æœ¬å¯¹å…³ç³»çš„åˆ†ç±»ï¼Œæ–‡æœ¬å¯¹çš„ä¸¤ä¸ªæ–‡æœ¬ä¹‹é—´ä»¥-åˆ†å‰²ã€‚

[ç›¸å…³è®ºæ–‡](https://arxiv.org/pdf/1810.04805.pdf) Jacob Devlin, Ming-Wei Chang, et al., BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, 2019.

## å·²æ”¯æŒæ•°æ®é›†æ€§èƒ½

| model  |             type              | datasets | Top1-accuracy |            stage            | example |
|:------:|:-----------------------------:|:--------:|:-------------:|:---------------------------:|:-------:|
|  bert  |   txtcls_bert_base_uncased    |   Mnli   |     30.9%     |          pretrain           |    -    |
| txtcls | txtcls_bert_case_uncased_mnli |   Mnli   |     84.8%     | finetune<br>eval<br>predict |    -    |

### [Mnli](https://dl.fbaipublicfiles.com/glue/data/MNLI.zip)

- æ•°æ®é›†å¤§å°ï¼š298Mï¼Œå…±431992ä¸ªæ ·æœ¬ï¼Œ3ä¸ªç±»åˆ«
    - è®­ç»ƒé›†ï¼š392702ä¸ªæ ·æœ¬
    - åŒ¹é…æµ‹è¯•é›†ï¼š9796ä¸ªæ ·æœ¬
    - éåŒ¹é…æµ‹è¯•é›†ï¼š9847ä¸ªæ ·æœ¬
    - åŒ¹é…å¼€å‘é›†ï¼š9815ä¸ªæ ·æœ¬
    - éåŒ¹é…å¼€å‘é›†ï¼š9832ä¸ªæ ·æœ¬
- æ•°æ®æ ¼å¼ï¼štsvæ–‡ä»¶

 ```bash
æ•°æ®é›†ç›®å½•æ ¼å¼
â””â”€mnli
    â”œâ”€dev
    â”œâ”€test  
    â””â”€train
 ```

- ç”¨æˆ·å¯ä»¥å‚è€ƒ[BERT](https://github.com/google-research/bert)ä»£ç ä»“ä¸­çš„run_classifier.pyæ–‡ä»¶ï¼Œè¿›è¡ŒMnliæ•°æ®é›†`TFRecord`æ ¼å¼æ–‡ä»¶çš„ç”Ÿæˆã€‚

## å¿«é€Ÿä»»åŠ¡æ¥å£

### è„šæœ¬å¯åŠ¨

> éœ€å¼€å‘è€…æå‰cloneå·¥ç¨‹ã€‚

- è¯·å‚è€ƒ[ä½¿ç”¨è„šæœ¬å¯åŠ¨](../../README.md#æ–¹å¼ä¸€ä½¿ç”¨å·²æœ‰è„šæœ¬å¯åŠ¨)

- è„šæœ¬è¿è¡Œæµ‹è¯•

```shell
# finetune
python run_mindformer.py --config ./configs/txtcls/run_txtcls_bert_base_uncased.yaml --run_mode finetune --load_checkpoint txtcls_bert_base_uncased

# evaluate
python run_mindformer.py --config ./configs/txtcls/run_txtcls_bert_base_uncased.yaml --run_mode eval --load_checkpoint txtcls_bert_base_uncased_mnli

# predict
python run_mindformer.py --config ./configs/txtcls/run_txtcls_bert_base_uncased.yaml --run_mode predict --load_checkpoint txtcls_bert_base_uncased_mnli --predict_data [TEXT]
```

### è°ƒç”¨APIå¯åŠ¨

- Traineræ¥å£å¼€å¯è®­ç»ƒ/è¯„ä¼°/æ¨ç†ï¼š

```python
import mindspore; mindspore.set_context(mode=0, device_id=0)
from mindformers import MindFormerBook
from mindformers.trainer import Trainer

# æ˜¾ç¤ºTrainerçš„æ¨¡å‹æ”¯æŒåˆ—è¡¨
MindFormerBook.show_trainer_support_model_list("text_classification")
# INFO - Trainer support model list for txt_classification task is:
# INFO -    ['txtcls_bert_base_uncased']
# INFO - -------------------------------------

# åˆå§‹åŒ–trainer
trainer = Trainer(task='text_classification',
    model='txtcls_bert_base_uncased',
    train_dataset='./mnli/train',
    eval_dataset='./mnli/eval')
# æµ‹è¯•æ•°æ®ï¼Œè¯¥input_dataæœ‰ä¸¤ä¸ªæµ‹è¯•æ¡ˆä¾‹ï¼Œå³ä¸¤ä¸ªæ–‡æœ¬å¯¹ï¼Œå•ä¸ªæ–‡æœ¬å¯¹çš„ä¸¤ä¸ªæ–‡æœ¬ä¹‹é—´ç”¨-åˆ†å‰²
input_data = ["The new rights are nice enough-Everyone really likes the newest benefits ",
              "i don't know um do you do a lot of camping-I know exactly."]

#æ–¹å¼1ï¼šä½¿ç”¨ç°æœ‰çš„é¢„è®­ç»ƒæƒé‡è¿›è¡Œfinetuneï¼Œ å¹¶ä½¿ç”¨finetuneè·å¾—çš„æƒé‡è¿›è¡Œevalå’Œæ¨ç†
trainer.train(resume_or_finetune_from_checkpoint="txtcls_bert_base_uncased",
              do_finetune=True)
trainer.evaluate(eval_checkpoint=True)
trainer.predict(predict_checkpoint=True, input_data=input_data, top_k=1)

# æ–¹å¼2ï¼š ä»obsä¸‹è½½è®­ç»ƒå¥½çš„æƒé‡å¹¶è¿›è¡Œevalå’Œæ¨ç†
trainer.evaluate()
# INFO - Top1 Accuracy=84.8%
trainer.predict(input_data=input_data, top_k=1)
# INFO - output result is [[{'label': 'neutral', 'score': 0.9714198708534241}],
#                         [{'label': 'contradiction', 'score': 0.9967639446258545}]]
```

- pipelineæ¥å£å¼€å¯å¿«é€Ÿæ¨ç†

```python
import mindspore; mindspore.set_context(mode=0, device_id=0)
from mindformers.pipeline import TextClassificationPipeline
from mindformers import AutoTokenizer, BertForMultipleChoice, AutoConfig

input_data = ["The new rights are nice enough-Everyone really likes the newest benefits ",
                "i don't know um do you do a lot of camping-I know exactly."]

tokenizer = AutoTokenizer.from_pretrained('txtcls_bert_base_uncased_mnli')
txtcls_mnli_config = AutoConfig.from_pretrained('txtcls_bert_base_uncased_mnli')

# Because batch_size parameter is required when bert model is created, and pipeline
# function deals with samples one by one, the batch_size parameter is seted one.
txtcls_mnli_config.batch_size = 1

model = BertForMultipleChoice(txtcls_mnli_config)
txtcls_pipeline = TextClassificationPipeline(task='text_classification',
                                             model=model,
                                             tokenizer=tokenizer,
                                             max_length=model.config.seq_length,
                                             padding="max_length")

results = txtcls_pipeline(input_data, top_k=1)
print(results)
# è¾“å‡º
# [[{'label': 'neutral', 'score': 0.9714198708534241}], [{'label': 'contradiction', 'score': 0.9967639446258545}]]
```
