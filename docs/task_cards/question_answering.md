# Question Answering

> ## ğŸš¨ å¼ƒç”¨è¯´æ˜
>
> æœ¬æ–‡æ¡£å·²è¿‡æ—¶ï¼Œä¸å†è¿›è¡Œç»´æŠ¤ï¼Œå¹¶å°†åœ¨ *1.6.0* ç‰ˆæœ¬ä¸‹æ¶ï¼Œå…¶ä¸­å¯èƒ½åŒ…å«è¿‡æ—¶çš„ä¿¡æ¯æˆ–å·²è¢«æ›´æ–°çš„åŠŸèƒ½æ›¿ä»£ã€‚å»ºè®®å‚è€ƒæœ€æ–°çš„ **[å®˜æ–¹æ–‡æ¡£](https://www.mindspore.cn/mindformers/docs/zh-CN/r1.5.0/index.html)** ï¼Œä»¥è·å–å‡†ç¡®çš„ä¿¡æ¯ã€‚
>
> å¦‚æœæ‚¨ä»éœ€ä½¿ç”¨æœ¬æ–‡æ¡£ä¸­çš„å†…å®¹ï¼Œè¯·ä»”ç»†æ ¸å¯¹å…¶é€‚ç”¨æ€§ï¼Œå¹¶ç»“åˆæœ€æ–°ç‰ˆæœ¬çš„ç›¸å…³èµ„æºè¿›è¡ŒéªŒè¯ã€‚
>
> å¦‚æœ‰ä»»ä½•é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·é€šè¿‡ **[ç¤¾åŒºIssue](https://gitee.com/mindspore/mindformers/issues/new)** æäº¤åé¦ˆã€‚æ„Ÿè°¢æ‚¨çš„ç†è§£ä¸æ”¯æŒï¼

## ä»»åŠ¡æè¿°

**é—®ç­”ä»»åŠ¡**ï¼šæ¨¡å‹åœ¨åŸºäºé—®ç­”æ•°æ®é›†çš„å¾®è°ƒåï¼Œè¾“å…¥ä¸ºä¸Šä¸‹æ–‡ï¼ˆcontextï¼‰å’Œé—®é¢˜ï¼ˆquestionï¼‰ï¼Œæ¨¡å‹æ ¹æ®ä¸Šä¸‹æ–‡ï¼ˆcontextï¼‰ç»™å‡ºç›¸åº”çš„å›ç­”ã€‚

**ç›¸å…³è®ºæ–‡**

- Jacob Devlin, Ming-Wei Chang, et al., [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf), 2019.
- Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, [SQuAD: 100,000+ Questions for Machine Comprehension of Text](https://arxiv.org/pdf/1606.05250.pdf), 2016.

## å·²æ”¯æŒæ•°æ®é›†æ€§èƒ½

| model |            type            |  datasets  |  EM   | F1    |            stage            | example |
|:-----:|:--------------------------:|:----------:|:-----:|-------|:---------------------------:|:-------:|
|  q'a  | qa_bert_case_uncased_squad | SQuAD v1.1 | 80.74 | 88.33 | finetune<br>eval<br>predict |    -    |

### [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/)

- ä¸‹è½½åœ°å€ï¼š[SQuAD v1.1è®­ç»ƒé›†](https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json)ï¼Œ[SQuAD v1.1éªŒè¯é›†](https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json)
- æ•°æ®é›†ï¼šè¯¥æ•°æ®é›†åŒ…å« 10 ä¸‡ä¸ªï¼ˆé—®é¢˜ï¼ŒåŸæ–‡ï¼Œç­”æ¡ˆï¼‰ä¸‰å…ƒç»„ï¼ŒåŸæ–‡æ¥è‡ªäº 536 ç¯‡ç»´åŸºç™¾ç§‘æ–‡ç« ï¼Œè€Œé—®é¢˜å’Œç­”æ¡ˆçš„æ„å»ºä¸»è¦æ˜¯é€šè¿‡ä¼—åŒ…çš„æ–¹å¼ï¼Œè®©æ ‡æ³¨äººå‘˜æå‡ºæœ€å¤š 5 ä¸ªåŸºäºæ–‡ç« å†…å®¹çš„é—®é¢˜å¹¶æä¾›æ­£ç¡®ç­”æ¡ˆï¼Œä¸”ç­”æ¡ˆå‡ºç°åœ¨åŸæ–‡ä¸­ã€‚
- æ•°æ®æ ¼å¼ï¼šjsonæ–‡ä»¶

 ```bash
æ•°æ®é›†ç›®å½•æ ¼å¼
â””â”€squad
    â”œâ”€train-v1.1.json
    â””â”€dev-v1.1.json
 ```

## å¿«é€Ÿä»»åŠ¡æ¥å£

### è„šæœ¬å¯åŠ¨

> éœ€å¼€å‘è€…æå‰cloneå·¥ç¨‹ã€‚

- è¯·å‚è€ƒ[ä½¿ç”¨è„šæœ¬å¯åŠ¨](../../README.md#æ–¹å¼ä¸€ä½¿ç”¨å·²æœ‰è„šæœ¬å¯åŠ¨)

- åœ¨è„šæœ¬æ‰§è¡Œç›®å½•åˆ›å»º `squad` æ–‡ä»¶å¤¹ï¼Œç„¶åå°†æ•°æ®é›†æ”¾å…¥å…¶ä¸­

- è„šæœ¬è¿è¡Œæµ‹è¯•

```shell
# finetune
python run_mindformer.py --config ./configs/qa/run_qa_bert_base_uncased.yaml --run_mode finetune --load_checkpoint qa_bert_base_uncased

# evaluate
python run_mindformer.py --config ./configs/qa/run_qa_bert_base_uncased.yaml --run_mode eval --load_checkpoint qa_bert_base_uncased_squad

# predict
python run_mindformer.py --config ./configs/qa/run_qa_bert_base_uncased.yaml --run_mode predict --load_checkpoint qa_bert_base_uncased_squad --predict_data [TEXT]
```

### è°ƒç”¨APIå¯åŠ¨

- Traineræ¥å£å¼€å¯è®­ç»ƒ/è¯„ä¼°/æ¨ç†ï¼š

```python
import mindspore; mindspore.set_context(mode=0, device_id=0)
from mindformers.trainer import Trainer

# åˆå§‹åŒ–trainer
trainer = Trainer(task='question_answering',
                  model='qa_bert_base_uncased',
                  train_dataset='./squad/',
                  eval_dataset='./squad/')

#æ–¹å¼1ï¼šä½¿ç”¨ç°æœ‰çš„é¢„è®­ç»ƒæƒé‡è¿›è¡Œfinetuneï¼Œ å¹¶ä½¿ç”¨finetuneè·å¾—çš„æƒé‡è¿›è¡Œevalå’Œæ¨ç†
trainer.train(resume_or_finetune_from_checkpoint="qa_bert_base_uncased",
              do_finetune=True)
trainer.evaluate(eval_checkpoint=True)
# æµ‹è¯•æ•°æ®ï¼Œæµ‹è¯•æ•°æ®åˆ†ä¸ºcontextå’Œquestionä¸¤éƒ¨åˆ†ï¼Œä¸¤è€…ä»¥ â€œ-â€ åˆ†éš”
input_data = ["My name is Wolfgang and I live in Berlin - Where do I live?"]
trainer.predict(predict_checkpoint=True, input_data=input_data)

# æ–¹å¼2ï¼š ä»obsä¸‹è½½è®­ç»ƒå¥½çš„æƒé‡å¹¶è¿›è¡Œevalå’Œæ¨ç†
trainer.evaluate()
# INFO - QA Metric = {'QA Metric': {'exact_match': 80.74739829706716, 'f1': 88.33552874684968}}
# æµ‹è¯•æ•°æ®ï¼Œæµ‹è¯•æ•°æ®åˆ†ä¸ºcontextå’Œquestionä¸¤éƒ¨åˆ†ï¼Œä¸¤è€…ä»¥ â€œ-â€ åˆ†éš”
input_data = ["My name is Wolfgang and I live in Berlin - Where do I live?"]
trainer.predict(input_data=input_data)
# INFO - output result is [{'text': 'Berlin', 'score': 0.9941, 'start': 34, 'end': 40}]
```

- pipelineæ¥å£å¼€å¯å¿«é€Ÿæ¨ç†

```python
from mindformers.pipeline import QuestionAnsweringPipeline
from mindformers import AutoTokenizer, BertForQuestionAnswering, AutoConfig

# æµ‹è¯•æ•°æ®ï¼Œæµ‹è¯•æ•°æ®åˆ†ä¸ºcontextå’Œquestionä¸¤éƒ¨åˆ†ï¼Œä¸¤è€…ä»¥ â€œ-â€ åˆ†éš”
input_data = ["My name is Wolfgang and I live in Berlin - Where do I live?"]

tokenizer = AutoTokenizer.from_pretrained('qa_bert_base_uncased_squad')
qa_squad_config = AutoConfig.from_pretrained('qa_bert_base_uncased_squad')

# This is a known issue, you need to specify batch size equal to 1 when creating bert model.
qa_squad_config.batch_size = 1

model = BertForQuestionAnswering(qa_squad_config)
qa_pipeline = QuestionAnsweringPipeline(task='question_answering',
                                        model=model,
                                        tokenizer=tokenizer)

results = qa_pipeline(input_data)
print(results)
# è¾“å‡º
# [{'text': 'Berlin', 'score': 0.9941, 'start': 34, 'end': 40}]
```
