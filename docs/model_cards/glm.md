# ChatGLM6B

## æ¨¡å‹æè¿°

ChatGLM-6B æ˜¯ä¸€ä¸ªå¼€æºçš„ã€æ”¯æŒä¸­è‹±åŒè¯­çš„å¯¹è¯è¯­è¨€æ¨¡å‹ï¼ŒåŸºäº [General Language Model (GLM)](https://github.com/THUDM/GLM) æ¶æ„ï¼Œå…·æœ‰ 62 äº¿å‚æ•°ã€‚ChatGLM-6B ä½¿ç”¨äº†å’Œ ChatGPT ç›¸ä¼¼çš„æŠ€æœ¯ï¼Œé’ˆå¯¹ä¸­æ–‡é—®ç­”å’Œå¯¹è¯è¿›è¡Œäº†ä¼˜åŒ–ã€‚ç»è¿‡çº¦ 1T æ ‡è¯†ç¬¦çš„ä¸­è‹±åŒè¯­è®­ç»ƒï¼Œè¾…ä»¥ç›‘ç£å¾®è°ƒã€åé¦ˆè‡ªåŠ©ã€äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ç­‰æŠ€æœ¯çš„åŠ æŒï¼Œ62 äº¿å‚æ•°çš„ ChatGLM-6B å·²ç»èƒ½ç”Ÿæˆç›¸å½“ç¬¦åˆäººç±»åå¥½çš„å›ç­”ï¼Œæ›´å¤šä¿¡æ¯è¯·å‚è€ƒæ¸…åçš„[åšå®¢](https://chatglm.cn/blog)ã€‚åœ¨æ­¤ä»“ä¸­ï¼Œæä¾›ChatGLM6Bçš„æ¨ç†å’Œå¾®è°ƒèƒ½åŠ›ã€‚

## ä»“åº“ä»‹ç»

`chatGLM6B` åŸºäº `mindformers` å®ç°ï¼Œä¸»è¦æ¶‰åŠçš„æ–‡ä»¶æœ‰ï¼š

1. æ¨¡å‹å…·ä½“å®ç°ï¼š`mindformers/models/glm`

    ```bash
    glm
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ attention.py            # è‡ªæ³¨æ„åŠ›
        â”œâ”€â”€ chatglm_6b_tokenizer.py # tokenizer
        â”œâ”€â”€ glm_config.py           # æ¨¡å‹é…ç½®é¡¹
        â”œâ”€â”€ glm.py                  # æ¨¡å‹å®ç°
        â””â”€â”€ layers.py               # glm å±‚å®šä¹‰
    ```

2. æ¨¡å‹é…ç½®ï¼š`configs/glm`

    ```bash
    glm
        â”œâ”€â”€ run_glm_6b_fintune.yaml     # å…¨é‡å¾®è°ƒå¯åŠ¨é…ç½®
        â”œâ”€â”€ run_glm_6b_lora.yaml        # loraä½å‚å¾®è°ƒå¯åŠ¨é…ç½®
        â”œâ”€â”€ run_glm_6b_infer.yaml       # æ¨ç†å¯åŠ¨é…ç½®
        â””â”€â”€ run_glm_6b_lora_infer.yaml  # loraæ¨¡å‹æ¨ç†å¯åŠ¨é…ç½®
    ```

## ç¯å¢ƒè¦æ±‚

- ç¡¬ä»¶ï¼šAscend 910A
- MindSporeï¼š2.0.0rc1 / 1.10.1
- MindFormersç‰ˆæœ¬ï¼šdev

æ¨ç†å¯åœ¨å•æœºå•å¡ä¸Šå®Œæˆéƒ¨ç½²

è®­ç»ƒéœ€è¦æœ€å°‘å•æœº8å¡

## ChatGLM6Bæ¨ç†

> éœ€å¼€å‘è€…æå‰pipå®‰è£…ã€‚å…·ä½“æ¥å£è¯´æ˜è¯·å‚[APIæ¥å£](https://gitee.com/mindspore/transformer/wikis/API/)

### AutoClassæ¨ç†

å¯ä»¥ä½¿ç”¨AutoClassæ¥å£ï¼Œé€šè¿‡æ¨¡å‹åç§°è·å–ç›¸åº”çš„æ¨¡å‹/tokenizerå®ä¾‹ï¼Œå¹¶è‡ªåŠ¨ä¸‹è½½å¹¶åŠ è½½æƒé‡

`from_pretrained()` æ¥å£ä¼šè‡ªåŠ¨ä»äº‘ä¸Šä¸‹è½½é¢„è®­ç»ƒçš„æ¨¡å‹ï¼Œå­˜å‚¨è·¯å¾„ï¼š`mindformers/checkpoint_download/glm`

é¦–æ¬¡è¿è¡Œpipelineæ¨ç†æ—¶éœ€è¦è¿›è¡Œæ¨¡å‹ç¼–è¯‘ï¼Œéœ€ç­‰å¾…ä¸€æ®µæ—¶é—´

```python
>>> from mindformers import AutoModel, AutoTokenizer, TextGenerationPipeline
>>> model = AutoModel.from_pretrained("glm_6b_chat")
>>> tokenizer = AutoTokenizer.from_pretrained("glm_6b")
>>> pipeline = TextGenerationPipeline(model, tokenizer, max_length=2048)
>>> pipeline("ä½ å¥½")
[{'text_generation_text': ['ä½ å¥½ ä½ å¥½ğŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6B,å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚']}]
```

> æ³¨ï¼š`AutoModel.from_pretrained()` æ¥å£å½“å‰æ”¯æŒ `glm_6b` å’Œ `glm_6b_chat` ä¸¤ç±»æ¨¡å‹ï¼Œå‰è€…ä¸ºé€šç”¨æ¨¡å‹ï¼Œåè€…å…·å¤‡æ¨ç†åŠ é€Ÿç‰¹æ€§ï¼Œä»…ç”¨äºæ¨ç†ï¼Œä¸¤è€…å…±äº«æƒé‡ï¼Œåœ¨æ¨ç†åœºæ™¯ä¸‹å»ºè®®ä½¿ç”¨åè€…ï¼Œä»¥è·å¾—æ›´å¿«çš„æ¨ç†ä½“éªŒ

### pipelineæ¨ç†

ä¹Ÿå¯ä»¥ä¸å®ä¾‹åŒ–æ„é€ æ¨¡å‹ï¼Œç›´æ¥é€šè¿‡æŒ‡å®šä»»åŠ¡æ¨¡å‹ä¸æ¨¡å‹åçš„æ–¹å¼è¿›è¡Œpipelineçš„æ„é€ 

pipelineä¸­ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨ `glm_6b_chat` æ¨¡å‹åŠ é€Ÿæ¨ç†

```python
>>> from mindformers import pipeline
>>> task_pipeline = pipeline(task='text_generation', model='glm_6b_chat', max_length=2048)
>>> task_pipeline('ä½ å¥½')
[{'text_generation_text': ['ä½ å¥½ ä½ å¥½ğŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6B,å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚']}]
```

### åŸºäºAPIæ¥å£çš„æ¨ç†

å¯ä½¿ç”¨å¦‚ä¸‹`chat_glm.py`è„šæœ¬ï¼š

```python
import time
import mindspore as ms
import numpy as np
from mindformers.models.glm import GLMConfig, GLMChatModel
from mindformers.models.glm.chatglm_6b_tokenizer import ChatGLMTokenizer
from mindformers.models.glm.glm_processor import process_response

config = GLMConfig(
    position_encoding_2d=True,
    use_past=True,
    is_npu_acceleration=True,
)

def chat_glm():
    ms.set_context(mode=ms.GRAPH_MODE, device_target="Ascend", device_id=7)
    model = GLMChatModel(config)
    ms.load_checkpoint("./checkpoint_download/glm/glm_6b.ckpt", model)
    tokenizer = ChatGLMTokenizer('./checkpoint_download/glm/ice_text.model')

    prompts = ["ä½ å¥½", "è¯·ä»‹ç»ä¸€ä¸‹åä¸º"]
    history = []
    for query in prompts:
        if not history:
            prompt = query
        else:
            prompt = ""
            for i, (old_query, response) in enumerate(history):
                prompt += "[Round {}]\né—®ï¼š{}\nç­”ï¼š{}\n".format(i, old_query, response)
            prompt += "[Round {}]\né—®ï¼š{}\nç­”ï¼š".format(len(history), query)
        inputs = tokenizer(prompt)

        start_time = time.time()
        outputs = model.generate(np.expand_dims(np.array(inputs['input_ids']).astype(np.int32), 0),
                                    max_length=config.max_decode_length, do_sample=False, top_p=0.7, top_k=1)
        end_time = time.time()
        print(f'generate speed: {outputs[0].shape[0]/(end_time-start_time):.2f} tokens/s')
        response = tokenizer.decode(outputs)
        response = process_response(response[0])
        history = history + [(query, response)]
        print(response)

if __name__ == "__main__":
    chat_glm()
```

## å¾®è°ƒ

ä¸‹é¢ä»¥ [ADGEN](https://aclanthology.org/D19-1321.pdf) (å¹¿å‘Šç”Ÿæˆ) æ•°æ®é›†ä¸ºä¾‹ä»‹ç»ä»£ç çš„ä½¿ç”¨æ–¹æ³•

### æ•°æ®å¤„ç†

ADGEN æ•°æ®é›†ä»»åŠ¡ä¸ºæ ¹æ®è¾“å…¥ï¼ˆcontentï¼‰ç”Ÿæˆä¸€æ®µå¹¿å‘Šè¯ï¼ˆsummaryï¼‰ã€‚æ•°æ®é›†å¯é€‰ç¦»çº¿ç”Ÿæˆ `Mindrecord` æˆ–è€…å®æ—¶ç”Ÿæˆä¸¤ç§æ–¹å¼

```json
{
    "content": "ç±»å‹#ä¸Šè¡£*ç‰ˆå‹#å®½æ¾*ç‰ˆå‹#æ˜¾ç˜¦*å›¾æ¡ˆ#çº¿æ¡*è¡£æ ·å¼#è¡¬è¡«*è¡£è¢–å‹#æ³¡æ³¡è¢–*è¡£æ¬¾å¼#æŠ½ç»³",
    "summary": "è¿™ä»¶è¡¬è¡«çš„æ¬¾å¼éå¸¸çš„å®½æ¾ï¼Œåˆ©è½çš„çº¿æ¡å¯ä»¥å¾ˆå¥½çš„éšè—èº«æä¸Šçš„å°ç¼ºç‚¹ï¼Œç©¿åœ¨èº«ä¸Šæœ‰ç€å¾ˆå¥½çš„æ˜¾ç˜¦æ•ˆæœã€‚é¢†å£è£…é¥°äº†ä¸€ä¸ªå¯çˆ±çš„æŠ½ç»³ï¼Œæ¼‚äº®çš„ç»³ç»“å±•ç°å‡ºäº†åè¶³çš„ä¸ªæ€§ï¼Œé…åˆæ—¶å°šçš„æ³¡æ³¡è¢–å‹ï¼Œå°½æ˜¾å¥³æ€§ç”œç¾å¯çˆ±çš„æ°”æ¯ã€‚"
}
```

ä» [Google Drive](https://drive.google.com/file/d/13_vf0xRTQsyneRKdD1bZIr93vBGOczrk/view?usp=sharing) æˆ–è€… [Tsinghua Cloud](https://cloud.tsinghua.edu.cn/f/b3f119a008264b1cabd1/?dl=1) ä¸‹è½½å¤„ç†å¥½çš„ ADGEN æ•°æ®é›†ï¼Œå°†è§£å‹åçš„ `AdvertiseGen` ä»»æ„ç›®å½•ä¸‹

#### ç¦»çº¿ç”Ÿæˆ

ä½¿ç”¨ `mindformers/tools/dataset_preprocess/glm/adgen_dataset.py` è„šæœ¬å°†æ•°æ®é›†å¤„ç†æˆmindrecordæ ¼å¼ã€‚

æ‰§è¡Œå‘½ä»¤ç”Ÿæˆè®­ç»ƒæ•°æ®é›†ï¼š

```bash
python adgen_dataset.py \
    --input_file /path/to/AdvertiseGen/train.json \
    --vocab_file /path/to/ice_text.model\
    --output_file /path/to/AdvertiseGen/train_0604_128.mindrecord \
    --max_source_length 64 \
    --max_target_length 64 \
    --mode train
```

æ‰§è¡Œå‘½ä»¤ç”Ÿæˆè¯„ä¼°æ•°æ®é›†ï¼š

```bash
python adgen_dataset.py \
    --input_file /path/to/AdvertiseGen/dev.json \
    --vocab_file /path/to/ice_text.model \
    --output_file /path/to/AdvertiseGen/eval_0604_256.mindrecord \
    --max_source_length 256 \
    --max_target_length 256 \
    --mode eval
```

#### åœ¨çº¿åŠ è½½

åœ¨çº¿åŠ è½½æ•°æ®é›†çš„æ–¹å¼ç›®å‰æ­£åœ¨å¼€å‘ä¸­ï¼Œå»ºè®®ä½¿ç”¨ç”ŸæˆMindRecordçš„æ–¹å¼å¤„ç†æ•°æ®é›†

### ç”ŸæˆHCCLæ–‡ä»¶

è¿è¡Œmindformers/tools/hccl_tools.pyç”ŸæˆRANK_TABLE_FILEçš„jsonæ–‡ä»¶ï¼›

```shell
# step1ï¼šæœºå™¨ä¸Šè¿è¡Œå¦‚ä¸‹å‘½ä»¤ï¼Œç”Ÿæˆå„è‡ªçš„RANK_TABLE_FILEçš„jsonæ–‡ä»¶
python ./mindformers/tools/hccl_tools.py --device_num "[0,8)"
```

> æ³¨ï¼šè‹¥ä½¿ç”¨ModelArtsçš„notebookç¯å¢ƒï¼Œå¯ä» `/user/config/jobstart_hccl.json` è·¯å¾„ä¸‹ç›´æ¥è·å–rank tableï¼Œæ— éœ€æ‰‹åŠ¨ç”Ÿæˆ

RANK_TABLE_FILE å•æœº8å¡å‚è€ƒæ ·ä¾‹:

```json
{
    "version": "1.0",
    "server_count": "1",
    "server_list": [
        {
            "server_id": "xx.xx.xx.xx",
            "device": [
                {"device_id": "0","device_ip": "192.1.27.6","rank_id": "0"},
                {"device_id": "1","device_ip": "192.2.27.6","rank_id": "1"},
                {"device_id": "2","device_ip": "192.3.27.6","rank_id": "2"},
                {"device_id": "3","device_ip": "192.4.27.6","rank_id": "3"},
                {"device_id": "4","device_ip": "192.1.27.7","rank_id": "4"},
                {"device_id": "5","device_ip": "192.2.27.7","rank_id": "5"},
                {"device_id": "6","device_ip": "192.3.27.7","rank_id": "6"},
                {"device_id": "7","device_ip": "192.4.27.7","rank_id": "7"}],
             "host_nic_ip": "reserve"
        }
    ],
    "status": "completed"
}
```

### å…¨å‚å¾®è°ƒ

#### run_mindformersè„šæœ¬å¯åŠ¨å…¨å‚å¾®è°ƒ

å…¨å‚å¾®è°ƒä½¿ç”¨ `configs/glm/run_glm_6b_finetune.yaml` é…ç½®æ–‡ä»¶ï¼Œé…ç½®æ–‡ä»¶ä¸­å®šä¹‰äº†å¾®è°ƒæ‰€éœ€çš„å„é…ç½®é¡¹

ä¿®æ”¹æ•°æ®é›†/æ¨¡å‹æƒé‡é…ç½®è·¯å¾„ï¼š

- æ•°æ®é›†ï¼šä¿®æ”¹ `mindformers/configs/glm/run_glm_6b_finetune.yaml` è„šæœ¬ä¸­`train_dataset` çš„ `dataset_dir` ä¸ºå‰æ–‡ç”Ÿæˆçš„æ•°æ®é›†è·¯å¾„ã€‚
- åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æƒé‡ï¼šä¿®æ”¹ `mindformers/configs/glm/run_glm_6b_finetune.yaml` è„šæœ¬ä¸­çš„ `load_checkpoint` ä¸ºé¢„è®­ç»ƒæ¨¡å‹æƒé‡è·¯å¾„ã€‚

å¯åŠ¨å…¨å‚å¾®è°ƒè„šæœ¬ï¼š

```shell
cd scripts
# Usage Help: bash run_distribute.sh [RANK_TABLE_FILE] [CONFIG_PATH] [DEVICE_RANGE] [RUN_STATUS]
bash run_distribute.sh /path/to/hccl_8p_01234567_127.0.1.1.json ../configs/glm/run_glm_6b_finetune.yaml '[0,8]' finetune
# å°†æ­¤å¤„rank_table_fileæ›¿æ¢ä¸ºå®é™…è·¯å¾„
```

å‚æ•°è¯´æ˜

```text
RANK_TABLE_FILE: ç”±mindformers/tools/hccl_tools.pyç”Ÿæˆçš„åˆ†å¸ƒå¼jsonæ–‡ä»¶
CONFIG_PATH: ä¸ºconfigsæ–‡ä»¶å¤¹ä¸‹é¢çš„glm/run_glm_6b.yamlé…ç½®æ–‡ä»¶
DEVICE_RANGE: ä¸ºå•æœºåˆ†å¸ƒå¼å¡çš„èŒƒå›´ï¼Œå¦‚ '[0,8]' ä¸º8å¡åˆ†å¸ƒå¼ï¼Œä¸åŒ…å«8æœ¬èº«
RUN_STATUS: ä¸ºä»»åŠ¡è¿è¡ŒçŠ¶æ€ï¼Œæ”¯æŒå…³é”®å­— train\finetune\eval\predict
```

> æ³¨ï¼šç”±äºGLM6Bçš„æ¨¡å‹è¾ƒå¤§ï¼Œæ— æ³•åœ¨å•å¡ä¸Šè¿è¡Œï¼Œæ­¤å¤„ä»…æä¾›åˆ†å¸ƒå¼å¯åŠ¨è„šæœ¬

è®­ç»ƒçš„logæ—¥å¿—è·¯å¾„ï¼šmindformers/output/log

checkpointå­˜å‚¨è·¯å¾„ï¼šmindformers/output/checkpoint

#### Traineré«˜é˜¶æ¥å£å¯åŠ¨å…¨å‚å¾®è°ƒ

ä¸‹é¢æä¾›ä¸€ä¸ªä½¿ç”¨é«˜é˜¶æ¥å£è¿›è¡ŒGLMæ¨¡å‹å¼€å‘çš„æ ·ä¾‹è„šæœ¬ `task.py`ï¼Œç”¨æˆ·å¯å‚ç…§ä»¥ä¸‹æ­¥éª¤ç†Ÿæ‚‰å¦‚ä½•ä½¿ç”¨é«˜é˜¶æ¥å£è¿›è¡ŒGLMæ¨¡å‹çš„è®­ç»ƒå¼€å‘

```python
import argparse

from mindformers import Trainer, TrainingArguments
from mindformers import init_context, ContextConfig, ParallelContextConfig

def context_init(use_parallel=False, optimizer_parallel=False):
    """init context for mindspore."""
    context_config = ContextConfig(mode=0, device_target="Ascend", device_id=0)
    parallel_config = None
    if use_parallel:
        parallel_config = ParallelContextConfig(parallel_mode='SEMI_AUTO_PARALLEL',
                                                gradients_mean=False,
                                                enable_parallel_optimizer=optimizer_parallel,
                                                full_batch=True)
    rank_id, device_num = init_context(use_parallel=use_parallel,
                                       context_config=context_config,
                                       parallel_config=parallel_config)

def main(use_parallel=False,
         run_mode='train',
         task='text_generation',
         model_type='glm_6b',
         checkpoint_path='./glm_6b.ckpt',
         train_dataset='./train',
         eval_dataset='./eval',
         predict_data='ä½ å¥½',
         batch_size=4,
         dp=1, mp=1, pp=1, micro_size=1, op=False):
    # ç¯å¢ƒåˆå§‹åŒ–
    context_init(use_parallel, op)
    # è®­ç»ƒè¶…å‚æ•°å®šä¹‰
    training_args = TrainingArguments(num_train_epochs=1, batch_size=batch_size, learning_rate=5e-5, warmup_steps=100, sink_mode=True, sink_size=4)
    # å®šä¹‰ä»»åŠ¡ï¼Œé¢„å…ˆå‡†å¤‡å¥½ç›¸åº”æ•°æ®é›†
    task = Trainer(task=task, model=model_type, args=training_args, train_dataset=train_dataset, eval_dataset=eval_dataset)
    task.set_parallel_config(data_parallel=dp,
                             model_parallel=mp,
                             pipeline_stage=pp,
                             optimizer_shard=op,
                             micro_batch_num=micro_size)
    if run_mode == 'train':
        # è®­ç»ƒ
        task.train()
    elif run_mode == 'finetune':
        # å¾®è°ƒ
        task.finetune(checkpoint_path)
    elif run_mode == 'eval':
        # è¯„ä¼°
        task.evaluate(checkpoint_path)
    elif run_mode == 'predict':
        # æ¨ç†ï¼Œä»…æ”¯æŒå•å¡æ¨ç†
        assert use_parallel == False, "only support predict under stand_alone mode."
        result = task.predict(input_data=predict_data)
        print(result)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--run_mode', default='train', required=True, help='set run mode for model.')
    parser.add_argument('--use_parallel', default=False, help='open parallel for model.')
    parser.add_argument('--task', default='text_generation', required=True, help='set task type.')
    parser.add_argument('--model_type', default='glm_6b', required=True, help='set model type.')
    parser.add_argument('--checkpoint_path', default=None, help='set checkpoint path.')
    parser.add_argument('--train_dataset', default=None, help='set train dataset.')
    parser.add_argument('--eval_dataset', default=None, help='set eval dataset.')
    parser.add_argument('--batch_size', default=4, help='batch size of dataset.')
    parser.add_argument('--data_parallel', default=1, type=int,help='set data parallel number. Default: None')
    parser.add_argument('--model_parallel', default=1, type=int, help='set model parallel number. Default: None')
    parser.add_argument('--pipeline_parallel', default=1, type=int, help='set pipeline parallel number. Default: None')
    parser.add_argument('--micro_size', default=1, type=int, help='set micro batch number. Default: None')
    parser.add_argument('--optimizer_parallel', default=False, type=bool, help='whether use optimizer parallel. Default: None')
    args = parser.parse_args()
    print(args)
    main(run_mode=args.run_mode,
         task=args.task,
         use_parallel=args.use_parallel,
         model_type=args.model_type,
         checkpoint_path=args.checkpoint_path,
         train_dataset=args.train_dataset,
         eval_dataset=args.eval_dataset,
         batch_size=int(args.batch_size),
         dp=args.data_parallel,
         mp=args.model_parallel,
         pp=args.pipeline_parallel,
         micro_size=args.micro_size,
         op=args.optimizer_parallel)
```

å› GLMæ¨¡å‹è¿‡å¤§ï¼Œ**æ— æ³•åœ¨å•å¡ä¸Šå¯åŠ¨è®­ç»ƒ**ï¼Œå› æ­¤éœ€è¦**é€šè¿‡åˆ†å¸ƒå¼è„šæœ¬æ‹‰èµ·å¤šå¡è®­ç»ƒä»»åŠ¡**

åœ¨æ­¤æä¾› `run_distribute_single_node.sh` å•æœºå¤šå¡æ ‡å‡†å¯åŠ¨è„šæœ¬ï¼Œç”¨æˆ·å¯ç”¨å…¶æ‹‰èµ·åˆ†å¸ƒå¼è®­ç»ƒ

```bash
#!/bin/bash
# Copyright 2023 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================

if [ $# != 4 ]
then
  echo "Usage Help: bash run_distribute_single_node.sh [EXECUTE_ORDER] [RANK_TABLE_PATH]  [DEVICE_RANGE] [RANK_SIZE] For Multiple Devices In Single Machine"
  exit 1
fi

check_real_path(){
  if [ "${1:0:1}" == "/" ]; then
    echo "$1"
  else
    echo "$(realpath -m $PWD/$1)"
  fi
}

EXECUTE_ORDER=$1
RANK_TABLE_PATH=$(check_real_path $2)
DEVICE_RANGE=$3

DEVICE_RANGE_LEN=${#DEVICE_RANGE}
DEVICE_RANGE=${DEVICE_RANGE:1:DEVICE_RANGE_LEN-2}
PREFIX=${DEVICE_RANGE%%","*}
INDEX=${#PREFIX}
START_DEVICE=${DEVICE_RANGE:0:INDEX}
END_DEVICE=${DEVICE_RANGE:INDEX+1:DEVICE_RANGE_LEN-INDEX}

if [ ! -f $RANK_TABLE_PATH ]
then
    echo "error: RANK_TABLE_FILE=$RANK_TABLE_PATH is not a file"
exit 1
fi


if [[ ! $START_DEVICE =~ ^[0-9]+$ ]]; then
    echo "error: start_device=$START_DEVICE is not a number"
exit 1
fi

if [[ ! $END_DEVICE =~ ^[0-9]+$ ]]; then
    echo "error: end_device=$END_DEVICE is not a number"
exit 1
fi

ulimit -u unlimited

export RANK_SIZE=$4
export RANK_TABLE_FILE=$RANK_TABLE_PATH

shopt -s extglob

for((i=${START_DEVICE}; i<${END_DEVICE}; i++))
do
    export DEVICE_ID=${i}
    export RANK_ID=$((i-START_DEVICE))
    mkdir -p ./output/log/rank_$RANK_ID
    echo "start training for rank $RANK_ID, device $DEVICE_ID"
    $EXECUTE_ORDER &> ./output/log/rank_$RANK_ID/mindformer.log &
done

shopt -u extglob
```

å…¨å‚å¾®è°ƒåˆ†å¸ƒå¼æ‹‰èµ·å‘½ä»¤(8å¡)ï¼š

```bash
bash run_distribute_single_node.sh "python task.py --task text_generation --model_type glm_6b --checkpoint_path ./glm_6b.ckpt --train_dataset ./train --run_mode finetune --use_parallel True --data_parallel 1 --model_parallel 8" /path/to/hccl_8p_xxx.json '[0,8]' 8
```

å‚æ•°å«ä¹‰:

- `"python task.py --task text_generation --model_type glm_6b --checkpoint_path ./glm_6b.ckpt --train_dataset ./train --run_mode finetune --use_parallel True --data_parallel 1 --model_parallel 8"`: éœ€æ‰§è¡Œçš„å‘½ä»¤ï¼Œæ­¤å¤„å®Œæ•´è¾“å…¥task.pyçš„å¯åŠ¨å‘½ä»¤

python task.py å„é¡¹å‚æ•°å«ä¹‰ï¼š

- `task`: éœ€è¿è¡Œçš„è®­ç»ƒä»»åŠ¡ï¼Œæ­¤å¤„ä¸º `text_generation` æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
- `model_type`: æ¨¡å‹ç±»å‹ï¼Œæ­¤å¤„é€‰æ‹© `glm_6b` æ¨¡å‹
- `checkpoint_path`: æƒé‡è·¯å¾„ï¼Œæ­¤å¤„æ›¿æ¢ä¸ºå®é™…éœ€åŠ è½½çš„æƒé‡è·¯å¾„
- `train_dataset`: è®­ç»ƒæ•°æ®é›†è·¯å¾„ï¼Œæ›¿æ¢ä¸ºå®é™…è·¯å¾„
- `run_mode`: å¯åŠ¨æ¨¡å¼ï¼Œtrainâ€”â€”è®­ç»ƒï¼Œfinetuneâ€”â€”å¾®è°ƒï¼Œevalâ€”â€”è¯„ä¼°ï¼Œpredictâ€”â€”æ¨ç†ï¼Œæ­¤å¤„é€‰æ‹© `finetune`
- `use_parallel`: æ˜¯å¦ä½¿ç”¨å¤šå¡å¹¶è¡Œè®­ç»ƒï¼Œæ­¤å¤„ä¸º `True`
- `data_parallel`: æ•°æ®å¹¶è¡Œæ•°ï¼Œæ­¤å¤„ä¸º1è¡¨ç¤ºä¸å¼€å¯
- `model_parallel`: æ¨¡å‹å¹¶è¡Œæ•°ï¼Œæ­¤å¤„ä¸º8è¡¨ç¤º8å¡å¹¶è¡Œ

bash è„šæœ¬å…¶ä½™å‚æ•°ï¼š

- `/path/to/hccl_4p_xxx.json`: rank table fileè·¯å¾„ï¼Œæ›¿æ¢ä¸ºä¹‹å‰å‡†å¤‡çš„rank table fileçš„å®é™…è·¯å¾„
- `'[0,8]'`: å ç”¨çš„å¡èŒƒå›´ï¼Œ0åŒ…å«ï¼Œ8ä¸åŒ…å«ï¼Œè¡¨ç¤ºä½¿ç”¨ `0~7` 8å¼ å¡å¹¶è¡Œè®­ç»ƒ
- `8`: rank sizeï¼Œä¸€å…±ä½¿ç”¨äº†å¤šå°‘å¼ å¡ï¼Œæ­¤å¤„ä¸º8

è®­ç»ƒçš„logæ—¥å¿—è·¯å¾„ï¼šmindformers/output/log

checkpointå­˜å‚¨è·¯å¾„ï¼šmindformers/output/checkpoint

### LoRAä½å‚å¾®è°ƒ

å…¨å‚å¾®è°ƒèƒ½å¤Ÿåœ¨å¾®è°ƒæ•°æ®é›†ä¸Šå–å¾—è‰¯å¥½æ•ˆæœï¼Œä½†å­˜åœ¨é—å¿˜é¢„è®­ç»ƒçŸ¥è¯†çš„ç°è±¡
å› æ­¤æ¨èä½¿ç”¨ä½å‚å¾®è°ƒç®—æ³•ï¼Œå†»ç»“åŸæ¨¡å‹æƒé‡ï¼Œä»…åœ¨å°è§„æ¨¡å‚æ•°é‡ä¸Šè¿›è¡Œè®­ç»ƒï¼Œåœ¨å¾®è°ƒæ•°æ®é›†ä¸Šå–å¾—è‰¯å¥½æ•ˆæœçš„åŒæ—¶ï¼Œç¼“è§£æ¨¡å‹é—å¿˜ç°è±¡

#### run_mindformersè„šæœ¬å¯åŠ¨LoRAä½å‚å¾®è°ƒ

ä½¿ç”¨LoRAç®—æ³•è¿›è¡Œä½å‚å¾®è°ƒæ—¶ï¼Œä½¿ç”¨ `configs/glm/run_glm_6b_lora.yaml` é…ç½®æ–‡ä»¶ï¼Œè¯¥é…ç½®æ–‡ä»¶åŒ…å«äº†loraä½å‚å¾®è°ƒç®—æ³•æ‰€éœ€çš„é…ç½®é¡¹

ä¿®æ”¹æ•°æ®é›†/æ¨¡å‹æƒé‡é…ç½®è·¯å¾„ï¼š

- æ•°æ®é›†ï¼šä¿®æ”¹ `mindformers/configs/glm/run_glm_6b_lora.yaml` è„šæœ¬ä¸­`train_dataset` çš„ `dataset_dir` ä¸ºå‰æ–‡ç”Ÿæˆçš„æ•°æ®é›†è·¯å¾„ã€‚
- åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æƒé‡ï¼šä¿®æ”¹ `mindformers/configs/glm/run_glm_6b_lora.yaml` è„šæœ¬ä¸­çš„ `load_checkpoint` ä¸ºé¢„è®­ç»ƒæ¨¡å‹æƒé‡è·¯å¾„ã€‚

å¯åŠ¨LoRAä½å‚å¾®è°ƒè„šæœ¬(4å¡)ï¼š

> æ³¨ï¼šå› ä½å‚å¾®è°ƒæ‰€éœ€å†…å­˜å‡å°‘ï¼Œæ­¤å¤„ç”¨4å¡å¹¶è¡Œå³å¯è®­ç»ƒï¼Œéœ€é‡æ–°ç”Ÿæˆ4å¡è®­ç»ƒæ‰€éœ€çš„rank table file

```shell
cd scripts
# Usage Help: bash run_distribute.sh [RANK_TABLE_FILE] [CONFIG_PATH] [DEVICE_RANGE] [RUN_STATUS]
bash run_distribute.sh /path/to/hccl_4p_0123_xxx.json ../configs/glm/run_glm_6b_lora.yaml '[0,4]' finetune
# å°†æ­¤å¤„rank_table_fileæ›¿æ¢ä¸ºå®é™…è·¯å¾„
```

å‚æ•°è¯´æ˜ï¼š

å¯¹æ¯”å…¨å‚å¾®è°ƒå¯åŠ¨æ–¹å¼ï¼Œä»…å°† `CONFIG_PATH` é¡¹ä¿®æ”¹ä¸ºconfigsæ–‡ä»¶å¤¹ä¸‹é¢çš„ `glm/run_glm_6b_lora.yaml` é…ç½®æ–‡ä»¶ï¼Œè¡¨ç¤ºä½¿ç”¨è¯¥æ¥å£è¿›è¡Œ

è®­ç»ƒçš„logæ—¥å¿—è·¯å¾„ï¼šmindformers/output/log

checkpointå­˜å‚¨è·¯å¾„ï¼šmindformers/output/checkpoint

#### Traineré«˜é˜¶æ¥å£å¯åŠ¨LoRAä½å‚å¾®è°ƒ

å¯å¤ç”¨å…¨å‚å¾®è°ƒéƒ¨åˆ†æ‰€æä¾›çš„ `task.py` å’Œ `run_distribute_single_node.sh` è„šæœ¬

4å¡åˆ†å¸ƒå¼å¯åŠ¨å‘½ä»¤ï¼š

```bash
bash run_distribute_single_node.sh "python task.py --task text_generation --model_type glm_6b_lora --checkpoint_path ./glm_6b.ckpt --train_dataset ./train --run_mode finetune --use_parallel True --data_parallel 1 --model_parallel 4" /path/to/hccl_4p_xxx.json '[0,4]' 4
```

å‚æ•°è¯´æ˜ï¼šå¯¹æ¯”å…¨å‚å¾®è°ƒå¯åŠ¨ï¼Œä»…æ”¹åŠ¨ä»¥ä¸‹å‡ ç‚¹ï¼š

- `model_type`: æŒ‡å®šæ¨¡å‹ç±»å‹ä¸º `glm_6b_lora`ï¼Œè¡¨ç¤ºä½¿ç”¨ä½å‚å¾®è°ƒç®—æ³•
- `model_parallel`: 4å¡å¯åŠ¨ï¼Œæ¨¡å‹å¹¶è¡Œæ•°æ”¹ä¸º4
- `/path/to/hccl_4p_xxx.json`: ä½¿ç”¨4å¡çš„rank_table_file
- `'[0,4]' 4`: ä½¿ç”¨0~3å…±4å¡

è®­ç»ƒçš„logæ—¥å¿—è·¯å¾„ï¼šmindformers/output/log

checkpointå­˜å‚¨è·¯å¾„ï¼šmindformers/output/checkpoint

## è¯„ä¼°

### æ¨¡å‹æƒé‡æ–‡ä»¶åˆä¸€

å¾®è°ƒæ‰€å¾—åˆ°çš„æƒé‡æ–‡ä»¶ä¸ºæ ¹æ®æ¨¡å‹åˆ‡åˆ†ç­–ç•¥åˆ‡åˆ†åçš„æƒé‡ï¼Œæˆ‘ä»¬éœ€è¦æ‰‹åŠ¨å°†åˆ‡åˆ†æƒé‡åˆä¸€ï¼Œä»¥ç”¨äºè¯„ä¼°å’Œæ¨ç†

1. è·å–æ¨¡å‹åˆ‡åˆ†ç­–ç•¥æ–‡ä»¶ï¼š
   åœ¨æ‰§è¡Œå…¨å‚å¾®è°ƒè„šæœ¬æ—¶ï¼Œæ¨¡å‹å®Œæˆç¼–è¯‘åï¼Œå°†ä¼šåœ¨è¿è¡Œè·¯å¾„ä¸‹ï¼Œç”Ÿæˆåä¸º `ckpt_strategy.ckpt` çš„åˆ‡åˆ†ç­–ç•¥æ–‡ä»¶ï¼Œå°†å…¶ä¿å­˜

2. MindSporeæä¾›äº†æ ¹æ®åˆ‡åˆ†ç­–ç•¥è½¬æ¢æ¨¡å‹æƒé‡åˆ‡åˆ†çš„æ¥å£ï¼Œ[mindspore.transform_checkpoints](https://www.mindspore.cn/docs/zh-CN/r2.0/api_python/mindspore/mindspore.transform_checkpoints.html)ï¼Œæ‰§è¡Œä»¥ä¸‹pythonè„šæœ¬ï¼Œå°†8ä»½æ¨¡å‹æ–‡ä»¶åˆæˆä¸€ä»½

    ```python
    from mindspore import transform_checkpoints
    transform_checkpoints(
        src_checkpoints_dir="./output/checkpoint/", # åŸåˆ‡åˆ†æƒé‡æ–‡ä»¶å¤¹
        dst_checkpoints_dir="./target_checkpoint/", # ç›®æ ‡è·¯å¾„
        ckpt_prefix="glm-6b", # .ckptæ–‡ä»¶å‰ç¼€å
        src_strategy_file="ckpt_stragery.ckpt", # æ­¥éª¤1ä¸­çš„åˆ‡åˆ†ç­–ç•¥æ–‡ä»¶è·¯å¾„
        dst_strategy_file=None # Noneè¡¨ç¤ºä¸åˆ‡åˆ†ï¼Œæƒé‡åˆä¸€
    )
    ```

> æ³¨ï¼š`transform_checkpoints` æ¥å£å½“å‰ä»…mindspore 2.0ä»¥ä¸Šç‰ˆæœ¬æ”¯æŒï¼Œå¦‚å½“å‰ç¡¬ä»¶ç¯å¢ƒåªæ”¯æŒ2.0ä»¥ä¸‹ç‰ˆæœ¬ï¼Œå¯ä»¥æ–°å»ºcondaç¯å¢ƒå®‰è£…mindspore 2.0çš„cpuç‰ˆæœ¬ä»¥æ‰§è¡Œè¯¥è„šæœ¬
>
> æ­¤å¤–ï¼Œé2.0ç‰ˆæœ¬çš„mindsporeï¼Œåœ¨ä½å‚å¾®è°ƒæ—¶ï¼Œç”Ÿæˆçš„åˆ‡åˆ†ç­–ç•¥æ–‡ä»¶å°†ä¸åŒ…å«è¢«å†»ç»“çš„æƒé‡ï¼Œå¯¼è‡´æƒé‡æ–‡ä»¶åˆå¹¶å¤±è´¥ï¼›æ­¤æ—¶ï¼Œéœ€å°† `mindformers/models/glm/glm.py` æ–‡ä»¶ä¸­æœ‰å…³LoRAå†»ç»“æƒé‡çš„ä»£ç æ³¨é‡Šåï¼Œé‡æ–°è¿è¡Œå¾®è°ƒè„šæœ¬ï¼Œè·å–åˆ°æ­£ç¡®çš„åˆ‡åˆ†ç­–ç•¥æ–‡ä»¶ååœæ­¢è®­ç»ƒè¿›ç¨‹ï¼›ç›¸å…³ä»£ç å¦‚ä¸‹

```python
@MindFormerRegister.register(MindFormerModuleType.MODELS)
class GLMForPreTrainingWithLora(GLMForPreTraining):
    """GLM Model for pretraining with LoRA

Args:
    config (GLMConfig): The config of network.
"""

def __init__(self, config: GLMConfig = None, pet=None, **kwargs):
    _ = kwargs
    super().__init__(config)
    # get Pet tuning model.
    self.pet = pet
    self.pet.pet_config.reg_rules = r'.*query_key_value*'
    self.transformer = LoraAdapter.get_pet_model(self.transformer, self.pet.pet_config)
    # freeze pretrained model
    PetAdapter.freeze_pretrained_model(self, self.pet.pet_type)     # æ³¨é‡Šæ­¤è¡Œä»¥ç”Ÿæˆæ–°çš„ç­–ç•¥æ–‡ä»¶
```

### ä½¿ç”¨å…¨å‚å¾®è°ƒæƒé‡

#### run_mindformerså¯åŠ¨eval

ä½¿ç”¨å…¨å‚å¾®è°ƒæƒé‡æ—¶ï¼Œå¯åŠ¨å¦‚ä¸‹shellè„šæœ¬ï¼Œæ‰§è¡Œå•å¡è¯„ä¼°

é…ç½®æ–‡ä»¶é€‰æ‹© `configs/glm/run_glm_6b_infer.yaml` glmæ¨¡å‹æ¨ç†é…ç½®ï¼Œæ­¤é…ç½®ä¸‹è¯„ä¼°é€Ÿåº¦æ›´å¿«

```bash
python run_mindformer.py --config configs/glm/run_glm_6b_infer.yaml --run_mode eval --load_checkpoint /path/to/glm_6b.ckpt --eval_dataset_dir /path/to/data/AdvertiseGen/adgen_dev.mindrecord --device_id 0
```

å„é¡¹å‚æ•°ï¼š

- `config`: æŒ‡å®šç”¨äºè¯„ä¼°çš„é…ç½®æ–‡ä»¶åç§°ï¼Œæ­¤å¤„ä¸º`configs/glm/run_glm_6b_infer.yaml`
- `run_mode`: æŒ‡å®šæ‰§è¡Œæ¨¡å¼ï¼Œæ­¤ä¸º`eval`ï¼Œè¡¨ç¤ºä¸ºè¯„ä¼°æ¨¡å¼
- `load_checkpoint`: æŒ‡å®šè¦åŠ è½½çš„checkpointè·¯å¾„ï¼Œæ­¤å¤„ä¸º`/path/to/glm_6b.ckpt`ï¼Œæ›¿æ¢ä¸ºéœ€åŠ è½½çš„æƒé‡çš„çœŸå®è·¯å¾„
- `eval_dataset_dir`: è¯„ä¼°æ•°æ®é›†çš„è·¯å¾„
- `device_id`: æŒ‡å®šè¦ä½¿ç”¨çš„è®¾å¤‡ç¼–å·ï¼ˆä»0å¼€å§‹ï¼‰

è¯„ä¼°å®Œæˆåä¼šæ‰“å°è¯„ä¼°æŒ‡æ ‡ `bleu-4`ã€`rouge-1`ã€`rouge-2`ã€`rouge-l`

> æ³¨ï¼šç”±äºé»˜è®¤è¯„ä¼°æŒ‡æ ‡çš„è·å–æ–¹å¼ä¸ºç”Ÿæˆå®Œæ•´æ–‡æœ¬åä¸é¢„æœŸæ–‡æœ¬åšæ¯”è¾ƒï¼Œè¯„ä¼°é€Ÿåº¦å°†å—é™äºæ¨¡å‹å¤§å°ä¸æ–‡æœ¬ç”Ÿæˆé€Ÿåº¦ï¼Œè¯„ä¼°æµç¨‹å¯èƒ½è¾ƒä¸ºç¼“æ…¢

#### Traineré«˜é˜¶æ¥å£å¯åŠ¨eval

ä»ç„¶å¯å¤ç”¨ `task.py` è„šæœ¬ï¼Œå¯åŠ¨å‘½ä»¤ï¼š

```bash
python task.py --task text_generation --model_type glm_6b_chat --checkpoint_path /path/to/glm_6b.ckpt --eval_dataset /path/to/data/AdvertiseGen/adgen_dev.mindrecord --run_mode eval --batch_size 1
```

> æ³¨ï¼šå½“å‰è¯„ä¼°æ—¶ï¼Œbatch_sizeéœ€ä¸º1ï¼Œå¦åˆ™è¯„ä¼°é€Ÿåº¦ä¸‹é™ä¸¥é‡

### ä½¿ç”¨LoRAä½å‚å¾®è°ƒæƒé‡

#### run_mindformerså¯åŠ¨lora eval

ä½¿ç”¨LoRAä½å‚å¾®è°ƒæƒé‡æ—¶ï¼Œå¯åŠ¨å¦‚ä¸‹shellè„šæœ¬ï¼Œæ‰§è¡Œå•å¡è¯„ä¼°

é…ç½®æ–‡ä»¶é€‰æ‹© `configs/glm/run_glm_6b_lora_infer.yaml` glm_loraæ¨¡å‹æ¨ç†é…ç½®ï¼Œæ­¤é…ç½®å¯ç”¨äºloraæ¨¡å‹ï¼Œå¹¶ä¸”è¯„ä¼°é€Ÿåº¦æ›´å¿«

```bash
python run_mindformer.py --config configs/glm/run_glm_6b_lora_infer.yaml --run_mode eval --load_checkpoint /path/to/glm_6b_lora.ckpt --eval_dataset_dir /path/to/data/AdvertiseGen/adgen_dev.mindrecord --device_id 0
```

å„é¡¹å‚æ•°åŒä¸Šï¼Œè·¯å¾„éœ€æ›¿æ¢ä¸ºå®é™…è·¯å¾„

#### Traineré«˜é˜¶æ¥å£å¯åŠ¨lora eval

ä»ç„¶å¯å¤ç”¨ `task.py` è„šæœ¬ï¼Œå¯åŠ¨å‘½ä»¤ï¼š

```bash
python task.py --task text_generation --model_type glm_6b_lora_chat --checkpoint_path /path/to/glm_6b_lora.ckpt --eval_dataset /path/to/data/AdvertiseGen/adgen_dev.mindrecord --run_mode eval --batch_size 1
```

> æ³¨ï¼šå½“å‰è¯„ä¼°æ—¶ï¼Œbatch_sizeéœ€ä¸º1ï¼Œå¦åˆ™è¯„ä¼°é€Ÿåº¦ä¸‹é™ä¸¥é‡

## æ¨¡å‹æƒé‡è½¬åŒ–

æœ¬ä»“åº“ä¸­çš„`glm`æ¥è‡ªäºHuggingFaceçš„[chatglm-6b](https://huggingface.co/THUDM/chatglm-6b)ï¼ŒåŸºäºä¸‹è¿°çš„æ­¥éª¤è·å–ï¼š

1. å…‹éš†chatglm-6bä»£ç ä»“ï¼Œä¸‹è½½åˆ†å¸ƒå¼çš„æ¨¡å‹æ–‡ä»¶ã€‚

   ```shell
   git lfs install
   git clone https://huggingface.co/THUDM/chatglm-6b
   ```

2. æ‰§è¡Œ python è„šæœ¬ï¼Œåˆå¹¶æ¨¡å‹æƒé‡ã€‚

   ```python
   from transformers import AutoModel
   import torch as pt

   pt_ckpt_path="Your chatglm-6b path"
   model = AutoModel.from_pretrained(pt_ckpt_path, trust_remote_code=True).half()
   pt_pth_path = "pt_glm_6b.pth"
   pt.save(model.state_dict(), pt_pth_path)
   ```

3. æ‰§è¡Œè½¬æ¢è„šæœ¬ï¼Œå¾—åˆ°è½¬æ¢åçš„è¾“å‡ºæ–‡ä»¶`ms_glm_6b.ckpt`ã€‚

   ```shell
   python mindformers/models/glm/convert_weight.py --pt_ckpt_path "replace your ptroch pth path" --ms_ckpt_path ./ms_glm_6b.ckpt
   ```
