# ChatGLM

## æ¨¡å‹æè¿°

ChatGLM-6B æ˜¯ä¸€ä¸ªå¼€æºçš„ã€æ”¯æŒä¸­è‹±åŒè¯­çš„å¯¹è¯è¯­è¨€æ¨¡å‹ï¼ŒåŸºäº [General Language Model (GLM)](https://github.com/THUDM/GLM) æ¶æ„ï¼Œå…·æœ‰ 62 äº¿å‚æ•°ã€‚ChatGLM-6B ä½¿ç”¨äº†å’Œ ChatGPT ç›¸ä¼¼çš„æŠ€æœ¯ï¼Œé’ˆå¯¹ä¸­æ–‡é—®ç­”å’Œå¯¹è¯è¿›è¡Œäº†ä¼˜åŒ–ã€‚ç»è¿‡çº¦ 1T æ ‡è¯†ç¬¦çš„ä¸­è‹±åŒè¯­è®­ç»ƒï¼Œè¾…ä»¥ç›‘ç£å¾®è°ƒã€åé¦ˆè‡ªåŠ©ã€äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ç­‰æŠ€æœ¯çš„åŠ æŒï¼Œ62 äº¿å‚æ•°çš„ ChatGLM-6B å·²ç»èƒ½ç”Ÿæˆç›¸å½“ç¬¦åˆäººç±»åå¥½çš„å›ç­”ï¼Œæ›´å¤šä¿¡æ¯è¯·å‚è€ƒæ¸…åçš„[åšå®¢](https://chatglm.cn/blog)ã€‚åœ¨æ­¤ä»“ä¸­ï¼Œæä¾›ChatGLM6Bçš„æ¨ç†å’Œå¾®è°ƒèƒ½åŠ›ã€‚

## ä»“åº“ä»‹ç»

`chatGLM6B` åŸºäº `mindformers` å®ç°ï¼Œä¸»è¦æ¶‰åŠçš„æ–‡ä»¶æœ‰ï¼š

1. æ¨¡å‹å…·ä½“å®ç°ï¼š`mindformers/models/glm`

    ```bash
    glm
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ attention.py            # è‡ªæ³¨æ„åŠ›
        â”œâ”€â”€ chatglm_6b_tokenizer.py # tokenizer
        â”œâ”€â”€ glm_config.py           # æ¨¡å‹é…ç½®é¡¹
        â”œâ”€â”€ glm.py                  # æ¨¡å‹å®ç°
        â””â”€â”€ layers.py               # glm å±‚å®šä¹‰
    ```

2. æ¨¡å‹é…ç½®ï¼š`configs/glm`

    ```bash
    glm
        â”œâ”€â”€ run_glm_6b_fintune.yaml     # å…¨é‡å¾®è°ƒå¯åŠ¨é…ç½®
        â”œâ”€â”€ run_glm_6b_lora.yaml        # loraä½å‚å¾®è°ƒå¯åŠ¨é…ç½®
        â”œâ”€â”€ run_glm_6b_infer.yaml       # æ¨ç†å¯åŠ¨é…ç½®
        â””â”€â”€ run_glm_6b_lora_infer.yaml  # loraæ¨¡å‹æ¨ç†å¯åŠ¨é…ç½®
    ```

## ç¯å¢ƒè¦æ±‚

- ç¡¬ä»¶ï¼šAtlas 800
- MindSporeï¼š2.0.0rc1 / 1.10.1
- MindFormersç‰ˆæœ¬ï¼šdev

æ¨ç†å¯åœ¨å•æœºå•å¡ä¸Šå®Œæˆéƒ¨ç½²

å…¨é‡å¾®è°ƒè®­ç»ƒéœ€è¦æœ€å°‘å•æœº8å¡ï¼ŒLoraå¾®è°ƒè®­ç»ƒæœ€å°‘éœ€è¦1å¡

## ChatGLM6Bæ¨ç†

> éœ€å¼€å‘è€…æå‰pipå®‰è£…ã€‚å…·ä½“æ¥å£è¯´æ˜è¯·å‚[APIæ¥å£](https://gitee.com/mindspore/transformer/wikis/API/)

### AutoClassæ¨ç†

å¯ä»¥ä½¿ç”¨AutoClassæ¥å£ï¼Œé€šè¿‡æ¨¡å‹åç§°è·å–ç›¸åº”çš„æ¨¡å‹/tokenizerå®ä¾‹ï¼Œå¹¶è‡ªåŠ¨ä¸‹è½½å¹¶åŠ è½½æƒé‡

`from_pretrained()` æ¥å£ä¼šè‡ªåŠ¨ä»äº‘ä¸Šä¸‹è½½é¢„è®­ç»ƒçš„æ¨¡å‹ï¼Œå­˜å‚¨è·¯å¾„ï¼š`mindformers/checkpoint_download/glm`

é¦–æ¬¡è¿è¡Œpipelineæ¨ç†æ—¶éœ€è¦è¿›è¡Œæ¨¡å‹ç¼–è¯‘ï¼Œéœ€ç­‰å¾…ä¸€æ®µæ—¶é—´

```python
>>> import mindspore; mindspore.set_context(mode=0, device_id=0)
>>> from mindformers import AutoModel, AutoTokenizer, TextGenerationPipeline
>>> model = AutoModel.from_pretrained("glm_6b_chat")
>>> tokenizer = AutoTokenizer.from_pretrained("glm_6b")
>>> pipeline = TextGenerationPipeline(model, tokenizer, max_length=2048)
>>> pipeline("ä½ å¥½")
[{'text_generation_text': ['ä½ å¥½ ä½ å¥½ğŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6B,å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚']}]
```

> æ³¨ï¼š`AutoModel.from_pretrained()` æ¥å£å½“å‰æ”¯æŒ `glm_6b` å’Œ `glm_6b_chat` ä¸¤ç±»æ¨¡å‹ï¼Œå‰è€…ä¸ºé€šç”¨æ¨¡å‹ï¼Œåè€…å…·å¤‡æ¨ç†åŠ é€Ÿç‰¹æ€§ï¼Œä»…ç”¨äºæ¨ç†ï¼Œä¸¤è€…å…±äº«æƒé‡ï¼Œåœ¨æ¨ç†åœºæ™¯ä¸‹å»ºè®®ä½¿ç”¨åè€…ï¼Œä»¥è·å¾—æ›´å¿«çš„æ¨ç†ä½“éªŒ

### pipelineæ¨ç†

ä¹Ÿå¯ä»¥ä¸å®ä¾‹åŒ–æ„é€ æ¨¡å‹ï¼Œç›´æ¥é€šè¿‡æŒ‡å®šä»»åŠ¡æ¨¡å‹ä¸æ¨¡å‹åçš„æ–¹å¼è¿›è¡Œpipelineçš„æ„é€ 

pipelineä¸­ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨ `glm_6b_chat` æ¨¡å‹åŠ é€Ÿæ¨ç†

```python
>>> import mindspore; mindspore.set_context(mode=0, device_id=0)
>>> from mindformers import pipeline
>>> task_pipeline = pipeline(task='text_generation', model='glm_6b_chat', max_length=2048)
>>> task_pipeline('ä½ å¥½')
[{'text_generation_text': ['ä½ å¥½ ä½ å¥½ğŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6B,å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚']}]
```

### åŸºäºAPIæ¥å£çš„æ¨ç†

å¯ä½¿ç”¨å¦‚ä¸‹`chat_glm.py`è„šæœ¬ï¼š

```python
import time
import mindspore as ms
import numpy as np
from mindformers.models.glm import GLMConfig, GLMChatModel
from mindformers.models.glm.chatglm_6b_tokenizer import ChatGLMTokenizer
from mindformers.models.glm.glm_processor import process_response

config = GLMConfig(
    position_encoding_2d=True,
    use_past=True,
    is_sample_acceleration=True,
)

def chat_glm():
    ms.set_context(mode=ms.GRAPH_MODE, device_target="Ascend", device_id=7)
    model = GLMChatModel(config)
    ms.load_checkpoint("./checkpoint_download/glm/glm_6b.ckpt", model)
    tokenizer = ChatGLMTokenizer('./checkpoint_download/glm/ice_text.model')

    prompts = ["ä½ å¥½", "è¯·ä»‹ç»ä¸€ä¸‹åä¸º", "ç”¨pythonå†™ä¸€ä¸ªå¿«æ’"]
    history = []
    for query in prompts:
        input_ids = tokenizer(query)['input_ids']

        start_time = time.time()
        outputs = model.generate(input_ids, max_length=config.max_decode_length, do_sample=False)
        end_time = time.time()
        print(f'generate speed: {outputs[0].shape[0]/(end_time-start_time):.2f} tokens/s')

        response = tokenizer.decode(outputs)
        response = process_response(response[0])
        print(response)


if __name__ == "__main__":
    chat_glm()
```

## å¾®è°ƒ

ä¸‹é¢ä»¥ [ADGEN](https://aclanthology.org/D19-1321.pdf) (å¹¿å‘Šç”Ÿæˆ) æ•°æ®é›†ä¸ºä¾‹ä»‹ç»ä»£ç çš„ä½¿ç”¨æ–¹æ³•

### æ•°æ®å¤„ç†ï¼ˆåœ¨çº¿åŠ è½½ä¸ç¦»çº¿ç”ŸæˆäºŒé€‰ä¸€ï¼Œä¼˜å…ˆæ¨èåœ¨çº¿åŠ è½½æ–¹å¼ï¼‰

ADGEN æ•°æ®é›†ä»»åŠ¡ä¸ºæ ¹æ®è¾“å…¥ï¼ˆcontentï¼‰ç”Ÿæˆä¸€æ®µå¹¿å‘Šè¯ï¼ˆsummaryï¼‰ã€‚æ•°æ®é›†å¯é€‰ç¦»çº¿ç”Ÿæˆ `Mindrecord` æˆ–è€…å®æ—¶ç”Ÿæˆä¸¤ç§æ–¹å¼ï¼Œä¸¤ç§æ–¹å¼é€‰å…¶ä¸€å³å¯ã€‚

```json
{
    "content": "ç±»å‹#ä¸Šè¡£*ç‰ˆå‹#å®½æ¾*ç‰ˆå‹#æ˜¾ç˜¦*å›¾æ¡ˆ#çº¿æ¡*è¡£æ ·å¼#è¡¬è¡«*è¡£è¢–å‹#æ³¡æ³¡è¢–*è¡£æ¬¾å¼#æŠ½ç»³",
    "summary": "è¿™ä»¶è¡¬è¡«çš„æ¬¾å¼éå¸¸çš„å®½æ¾ï¼Œåˆ©è½çš„çº¿æ¡å¯ä»¥å¾ˆå¥½çš„éšè—èº«æä¸Šçš„å°ç¼ºç‚¹ï¼Œç©¿åœ¨èº«ä¸Šæœ‰ç€å¾ˆå¥½çš„æ˜¾ç˜¦æ•ˆæœã€‚é¢†å£è£…é¥°äº†ä¸€ä¸ªå¯çˆ±çš„æŠ½ç»³ï¼Œæ¼‚äº®çš„ç»³ç»“å±•ç°å‡ºäº†åè¶³çš„ä¸ªæ€§ï¼Œé…åˆæ—¶å°šçš„æ³¡æ³¡è¢–å‹ï¼Œå°½æ˜¾å¥³æ€§ç”œç¾å¯çˆ±çš„æ°”æ¯ã€‚"
}
```

ä» [Google Drive](https://drive.google.com/file/d/13_vf0xRTQsyneRKdD1bZIr93vBGOczrk/view?usp=sharing) æˆ–è€… [Tsinghua Cloud](https://cloud.tsinghua.edu.cn/f/b3f119a008264b1cabd1/?dl=1) ä¸‹è½½å¤„ç†å¥½çš„ ADGEN æ•°æ®é›†ï¼Œå°†è§£å‹åçš„ `AdvertiseGen` ä»»æ„ç›®å½•ä¸‹

#### 1. åœ¨çº¿åŠ è½½

å°†ä»»åŠ¡é…ç½®æ–‡ä»¶ `configs/glm/run_glm_6b_*.yaml` ä¸­çš„ `==== dataset config ====` éƒ¨åˆ†ä¸­çš„ `dataset_dir` æŒ‡å‘ `*.json` æ–‡ä»¶ï¼Œ`vocab_file` æŒ‡å‘è¯è¡¨æ–‡ä»¶ï¼Œ**è·³è¿‡** â€œ2. ç¦»çº¿ç”Ÿæˆâ€ æ­¥éª¤ã€‚

#### 2. ç¦»çº¿ç”Ÿæˆ

å°†ä»»åŠ¡é…ç½®æ–‡ä»¶ `configs/glm/run_glm_6b_*.yaml` ä¸­çš„ `==== dataset config ====` éƒ¨åˆ†æ›¿æ¢æˆï¼š

```yaml
train_dataset: &train_dataset
  data_loader:
    type: MindDataset
    dataset_dir: ""
    shuffle: True
  input_columns: ["input_ids", "labels", "position_ids", "attention_mask"]
  num_parallel_workers: 8
  python_multiprocessing: False
  drop_remainder: True
  batch_size: 1
  repeat: 1
  numa_enable: False
  prefetch_size: 1
  seed: 0

train_dataset_task:
  type: CausalLanguageModelDataset
  dataset_config: *train_dataset

eval_dataset: &eval_dataset
  data_loader:
    type: MindDataset
    dataset_dir: ""
    shuffle: True
  input_columns: ["input_ids", "labels"]
  num_parallel_workers: 8
  python_multiprocessing: False
  drop_remainder: True
  batch_size: 1
  repeat: 1
  numa_enable: False
  prefetch_size: 1
  seed: 0

eval_dataset_task:
  type: CausalLanguageModelDataset
  dataset_config: *eval_dataset
```

ä½¿ç”¨ `mindformers/tools/dataset_preprocess/glm/adgen_dataset.py` è„šæœ¬å°†æ•°æ®é›†å¤„ç†æˆmindrecordæ ¼å¼ã€‚

æ‰§è¡Œå‘½ä»¤ç”Ÿæˆè®­ç»ƒæ•°æ®é›†ï¼š

```bash
python adgen_dataset.py \
    --input_file /path/to/AdvertiseGen/train.json \
    --vocab_file /path/to/ice_text.model\
    --output_file /path/to/AdvertiseGen/train_0604_128.mindrecord \
    --max_source_length 64 \
    --max_target_length 64 \
    --mode train
```

æ‰§è¡Œå‘½ä»¤ç”Ÿæˆè¯„ä¼°æ•°æ®é›†ï¼š

```bash
python adgen_dataset.py \
    --input_file /path/to/AdvertiseGen/dev.json \
    --vocab_file /path/to/ice_text.model \
    --output_file /path/to/AdvertiseGen/eval_0604_256.mindrecord \
    --max_source_length 256 \
    --max_target_length 256 \
    --mode eval
```

### ç”ŸæˆHCCLæ–‡ä»¶

è¿è¡Œmindformers/tools/hccl_tools.pyç”ŸæˆRANK_TABLE_FILEçš„jsonæ–‡ä»¶ï¼›

```shell
# step1ï¼šæœºå™¨ä¸Šè¿è¡Œå¦‚ä¸‹å‘½ä»¤ï¼Œç”Ÿæˆå„è‡ªçš„RANK_TABLE_FILEçš„jsonæ–‡ä»¶
python ./mindformers/tools/hccl_tools.py --device_num "[0,8)"
```

> æ³¨ï¼šè‹¥ä½¿ç”¨ModelArtsçš„notebookç¯å¢ƒï¼Œå¯ä» `/user/config/jobstart_hccl.json` è·¯å¾„ä¸‹ç›´æ¥è·å–rank tableï¼Œæ— éœ€æ‰‹åŠ¨ç”Ÿæˆ

RANK_TABLE_FILE å•æœº8å¡å‚è€ƒæ ·ä¾‹:

```json
{
    "version": "1.0",
    "server_count": "1",
    "server_list": [
        {
            "server_id": "xx.xx.xx.xx",
            "device": [
                {"device_id": "0","device_ip": "192.1.27.6","rank_id": "0"},
                {"device_id": "1","device_ip": "192.2.27.6","rank_id": "1"},
                {"device_id": "2","device_ip": "192.3.27.6","rank_id": "2"},
                {"device_id": "3","device_ip": "192.4.27.6","rank_id": "3"},
                {"device_id": "4","device_ip": "192.1.27.7","rank_id": "4"},
                {"device_id": "5","device_ip": "192.2.27.7","rank_id": "5"},
                {"device_id": "6","device_ip": "192.3.27.7","rank_id": "6"},
                {"device_id": "7","device_ip": "192.4.27.7","rank_id": "7"}],
             "host_nic_ip": "reserve"
        }
    ],
    "status": "completed"
}
```

### å…¨å‚å¾®è°ƒ

#### run_mindformersè„šæœ¬å¯åŠ¨å…¨å‚å¾®è°ƒ

å…¨å‚å¾®è°ƒä½¿ç”¨ `configs/glm/run_glm_6b_finetune.yaml` é…ç½®æ–‡ä»¶ï¼Œé…ç½®æ–‡ä»¶ä¸­å®šä¹‰äº†å¾®è°ƒæ‰€éœ€çš„å„é…ç½®é¡¹

ä¿®æ”¹æ•°æ®é›†/æ¨¡å‹æƒé‡é…ç½®è·¯å¾„ï¼š

- æ•°æ®é›†ï¼šä¿®æ”¹ `mindformers/configs/glm/run_glm_6b_finetune.yaml` è„šæœ¬ä¸­`train_dataset` çš„ `dataset_dir` ä¸ºå‰æ–‡ç”Ÿæˆçš„æ•°æ®é›†è·¯å¾„ã€‚
- åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æƒé‡ï¼šä¿®æ”¹ `mindformers/configs/glm/run_glm_6b_finetune.yaml` è„šæœ¬ä¸­çš„ `load_checkpoint` ä¸ºé¢„è®­ç»ƒæ¨¡å‹æƒé‡è·¯å¾„ã€‚

å¯åŠ¨å…¨å‚å¾®è°ƒè„šæœ¬ï¼š

```shell
cd scripts
# Usage Help: bash run_distribute.sh [RANK_TABLE_FILE] [CONFIG_PATH] [DEVICE_RANGE] [RUN_STATUS]
bash run_distribute.sh /path/to/hccl_8p_01234567_127.0.1.1.json ../configs/glm/run_glm_6b_finetune.yaml '[0,8]' finetune
# å°†æ­¤å¤„rank_table_fileæ›¿æ¢ä¸ºå®é™…è·¯å¾„
```

å‚æ•°è¯´æ˜

```text
RANK_TABLE_FILE: ç”±mindformers/tools/hccl_tools.pyç”Ÿæˆçš„åˆ†å¸ƒå¼jsonæ–‡ä»¶
CONFIG_PATH: ä¸ºconfigsæ–‡ä»¶å¤¹ä¸‹é¢çš„glm/run_glm_6b.yamlé…ç½®æ–‡ä»¶
DEVICE_RANGE: ä¸ºå•æœºåˆ†å¸ƒå¼å¡çš„èŒƒå›´ï¼Œå¦‚ '[0,8]' ä¸º8å¡åˆ†å¸ƒå¼ï¼Œä¸åŒ…å«8æœ¬èº«
RUN_STATUS: ä¸ºä»»åŠ¡è¿è¡ŒçŠ¶æ€ï¼Œæ”¯æŒå…³é”®å­— train\finetune\eval\predict
```

> æ³¨ï¼šç”±äºGLM6Bçš„æ¨¡å‹è¾ƒå¤§ï¼Œæ— æ³•åœ¨å•å¡ä¸Šè¿è¡Œï¼Œæ­¤å¤„ä»…æä¾›åˆ†å¸ƒå¼å¯åŠ¨è„šæœ¬

è®­ç»ƒçš„logæ—¥å¿—è·¯å¾„ï¼šmindformers/output/log

checkpointå­˜å‚¨è·¯å¾„ï¼šmindformers/output/checkpoint

#### Traineré«˜é˜¶æ¥å£å¯åŠ¨å…¨å‚å¾®è°ƒ

ä¸‹é¢æä¾›ä¸€ä¸ªä½¿ç”¨é«˜é˜¶æ¥å£è¿›è¡ŒGLMæ¨¡å‹å¼€å‘çš„æ ·ä¾‹è„šæœ¬ `task.py`ï¼Œç”¨æˆ·å¯å‚ç…§ä»¥ä¸‹æ­¥éª¤ç†Ÿæ‚‰å¦‚ä½•ä½¿ç”¨é«˜é˜¶æ¥å£è¿›è¡ŒGLMæ¨¡å‹çš„è®­ç»ƒå¼€å‘

```python
import argparse

from mindformers import Trainer, TrainingArguments
from mindformers import init_context, ContextConfig, ParallelContextConfig

def context_init(use_parallel=False, optimizer_parallel=False):
    """init context for mindspore."""
    context_config = ContextConfig(mode=0, device_target="Ascend", device_id=0)
    parallel_config = None
    if use_parallel:
        parallel_config = ParallelContextConfig(parallel_mode='SEMI_AUTO_PARALLEL',
                                                gradients_mean=False,
                                                enable_parallel_optimizer=optimizer_parallel,
                                                full_batch=True)
    rank_id, device_num = init_context(use_parallel=use_parallel,
                                       context_config=context_config,
                                       parallel_config=parallel_config)

def main(use_parallel=False,
         run_mode='train',
         task='text_generation',
         model_type='glm_6b',
         checkpoint_path='./glm_6b.ckpt',
         train_dataset='./train',
         eval_dataset='./eval',
         predict_data='ä½ å¥½',
         batch_size=4,
         dp=1, mp=1, pp=1, micro_size=1, op=False):
    if use_parallel.lower() == "true":
        use_parallel = True
    else:
        use_parallel = False
    # ç¯å¢ƒåˆå§‹åŒ–
    context_init(use_parallel, op)
    # è®­ç»ƒè¶…å‚æ•°å®šä¹‰
    training_args = TrainingArguments(num_train_epochs=1, batch_size=batch_size, learning_rate=5e-5, warmup_steps=100, sink_mode=True, sink_size=4)
    # å®šä¹‰ä»»åŠ¡ï¼Œé¢„å…ˆå‡†å¤‡å¥½ç›¸åº”æ•°æ®é›†
    task = Trainer(task=task, model=model_type, args=training_args, train_dataset=train_dataset, eval_dataset=eval_dataset)
    task.set_parallel_config(data_parallel=dp,
                             model_parallel=mp,
                             pipeline_stage=pp,
                             micro_batch_num=micro_size)
    if run_mode == 'train':
        # è®­ç»ƒ
        task.train()
    elif run_mode == 'finetune':
        # å¾®è°ƒ
        task.finetune(checkpoint_path)
    elif run_mode == 'eval':
        # è¯„ä¼°
        task.evaluate(checkpoint_path)
    elif run_mode == 'predict':
        # æ¨ç†ï¼Œä»…æ”¯æŒå•å¡æ¨ç†
        assert use_parallel == False, "only support predict under stand_alone mode."
        result = task.predict(input_data=predict_data)
        print(result)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--run_mode', default='train', required=True, help='set run mode for model.')
    parser.add_argument('--use_parallel', default=False, help='open parallel for model.')
    parser.add_argument('--task', default='text_generation', required=True, help='set task type.')
    parser.add_argument('--model_type', default='glm_6b', required=True, help='set model type.')
    parser.add_argument('--checkpoint_path', default=None, help='set checkpoint path.')
    parser.add_argument('--train_dataset', default=None, help='set train dataset.')
    parser.add_argument('--eval_dataset', default=None, help='set eval dataset.')
    parser.add_argument('--batch_size', default=4, help='batch size of dataset.')
    parser.add_argument('--data_parallel', default=1, type=int,help='set data parallel number. Default: None')
    parser.add_argument('--model_parallel', default=1, type=int, help='set model parallel number. Default: None')
    parser.add_argument('--pipeline_parallel', default=1, type=int, help='set pipeline parallel number. Default: None')
    parser.add_argument('--micro_size', default=1, type=int, help='set micro batch number. Default: None')
    parser.add_argument('--optimizer_parallel', default=False, type=bool, help='whether use optimizer parallel. Default: None')
    args = parser.parse_args()
    print(args)
    main(run_mode=args.run_mode,
         task=args.task,
         use_parallel=args.use_parallel,
         model_type=args.model_type,
         checkpoint_path=args.checkpoint_path,
         train_dataset=args.train_dataset,
         eval_dataset=args.eval_dataset,
         batch_size=int(args.batch_size),
         dp=args.data_parallel,
         mp=args.model_parallel,
         pp=args.pipeline_parallel,
         micro_size=args.micro_size,
         op=args.optimizer_parallel)
```

å› GLMæ¨¡å‹è¿‡å¤§ï¼Œ**æ— æ³•åœ¨å•å¡ä¸Šå¯åŠ¨è®­ç»ƒ**ï¼Œå› æ­¤éœ€è¦**é€šè¿‡åˆ†å¸ƒå¼è„šæœ¬æ‹‰èµ·å¤šå¡è®­ç»ƒä»»åŠ¡**

åœ¨æ­¤æä¾› `run_distribute_single_node.sh` å•æœºå¤šå¡æ ‡å‡†å¯åŠ¨è„šæœ¬ï¼Œç”¨æˆ·å¯ç”¨å…¶æ‹‰èµ·åˆ†å¸ƒå¼è®­ç»ƒ

```bash
#!/bin/bash
# Copyright 2023 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================

if [ $# != 4 ]
then
  echo "Usage Help: bash run_distribute_single_node.sh [EXECUTE_ORDER] [RANK_TABLE_PATH]  [DEVICE_RANGE] [RANK_SIZE] For Multiple Devices In Single Machine"
  exit 1
fi

check_real_path(){
  if [ "${1:0:1}" == "/" ]; then
    echo "$1"
  else
    echo "$(realpath -m $PWD/$1)"
  fi
}

EXECUTE_ORDER=$1
RANK_TABLE_PATH=$(check_real_path $2)
DEVICE_RANGE=$3

DEVICE_RANGE_LEN=${#DEVICE_RANGE}
DEVICE_RANGE=${DEVICE_RANGE:1:DEVICE_RANGE_LEN-2}
PREFIX=${DEVICE_RANGE%%","*}
INDEX=${#PREFIX}
START_DEVICE=${DEVICE_RANGE:0:INDEX}
END_DEVICE=${DEVICE_RANGE:INDEX+1:DEVICE_RANGE_LEN-INDEX}

if [ ! -f $RANK_TABLE_PATH ]
then
    echo "error: RANK_TABLE_FILE=$RANK_TABLE_PATH is not a file"
exit 1
fi


if [[ ! $START_DEVICE =~ ^[0-9]+$ ]]; then
    echo "error: start_device=$START_DEVICE is not a number"
exit 1
fi

if [[ ! $END_DEVICE =~ ^[0-9]+$ ]]; then
    echo "error: end_device=$END_DEVICE is not a number"
exit 1
fi

ulimit -u unlimited

export RANK_SIZE=$4
export RANK_TABLE_FILE=$RANK_TABLE_PATH

shopt -s extglob

for((i=${START_DEVICE}; i<${END_DEVICE}; i++))
do
    export DEVICE_ID=${i}
    export RANK_ID=$((i-START_DEVICE))
    mkdir -p ./output/log/rank_$RANK_ID
    echo "start training for rank $RANK_ID, device $DEVICE_ID"
    $EXECUTE_ORDER &> ./output/log/rank_$RANK_ID/mindformer.log &
done

shopt -u extglob
```

å…¨å‚å¾®è°ƒåˆ†å¸ƒå¼æ‹‰èµ·å‘½ä»¤(8å¡)ï¼š

```bash
bash run_distribute_single_node.sh "python task.py --task text_generation --model_type glm_6b --checkpoint_path ./glm_6b.ckpt --train_dataset ./train --run_mode finetune --use_parallel True --data_parallel 1 --model_parallel 8" /path/to/hccl_8p_xxx.json '[0,8]' 8
```

å‚æ•°å«ä¹‰:

- `"python task.py --task text_generation --model_type glm_6b --checkpoint_path ./glm_6b.ckpt --train_dataset ./train --run_mode finetune --use_parallel True --data_parallel 1 --model_parallel 8"`: éœ€æ‰§è¡Œçš„å‘½ä»¤ï¼Œæ­¤å¤„å®Œæ•´è¾“å…¥task.pyçš„å¯åŠ¨å‘½ä»¤

python task.py å„é¡¹å‚æ•°å«ä¹‰ï¼š

- `task`: éœ€è¿è¡Œçš„è®­ç»ƒä»»åŠ¡ï¼Œæ­¤å¤„ä¸º `text_generation` æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
- `model_type`: æ¨¡å‹ç±»å‹ï¼Œæ­¤å¤„é€‰æ‹© `glm_6b` æ¨¡å‹
- `checkpoint_path`: æƒé‡è·¯å¾„ï¼Œæ­¤å¤„æ›¿æ¢ä¸ºå®é™…éœ€åŠ è½½çš„æƒé‡è·¯å¾„
- `train_dataset`: è®­ç»ƒæ•°æ®é›†è·¯å¾„ï¼Œæ›¿æ¢ä¸ºå®é™…è·¯å¾„
- `run_mode`: å¯åŠ¨æ¨¡å¼ï¼Œtrainâ€”â€”è®­ç»ƒï¼Œfinetuneâ€”â€”å¾®è°ƒï¼Œevalâ€”â€”è¯„ä¼°ï¼Œpredictâ€”â€”æ¨ç†ï¼Œæ­¤å¤„é€‰æ‹© `finetune`
- `use_parallel`: æ˜¯å¦ä½¿ç”¨å¤šå¡å¹¶è¡Œè®­ç»ƒï¼Œæ­¤å¤„ä¸º `True`
- `data_parallel`: æ•°æ®å¹¶è¡Œæ•°ï¼Œæ­¤å¤„ä¸º1è¡¨ç¤ºä¸å¼€å¯
- `model_parallel`: æ¨¡å‹å¹¶è¡Œæ•°ï¼Œæ­¤å¤„ä¸º8è¡¨ç¤º8å¡å¹¶è¡Œ

bash è„šæœ¬å…¶ä½™å‚æ•°ï¼š

- `/path/to/hccl_4p_xxx.json`: rank table fileè·¯å¾„ï¼Œæ›¿æ¢ä¸ºä¹‹å‰å‡†å¤‡çš„rank table fileçš„å®é™…è·¯å¾„
- `'[0,8]'`: å ç”¨çš„å¡èŒƒå›´ï¼Œ0åŒ…å«ï¼Œ8ä¸åŒ…å«ï¼Œè¡¨ç¤ºä½¿ç”¨ `0~7` 8å¼ å¡å¹¶è¡Œè®­ç»ƒ
- `8`: rank sizeï¼Œä¸€å…±ä½¿ç”¨äº†å¤šå°‘å¼ å¡ï¼Œæ­¤å¤„ä¸º8

è®­ç»ƒçš„logæ—¥å¿—è·¯å¾„ï¼šmindformers/output/log

checkpointå­˜å‚¨è·¯å¾„ï¼šmindformers/output/checkpoint

### LoRAä½å‚å¾®è°ƒ

å…¨å‚å¾®è°ƒèƒ½å¤Ÿåœ¨å¾®è°ƒæ•°æ®é›†ä¸Šå–å¾—è‰¯å¥½æ•ˆæœï¼Œä½†å­˜åœ¨é—å¿˜é¢„è®­ç»ƒçŸ¥è¯†çš„ç°è±¡
å› æ­¤æ¨èä½¿ç”¨ä½å‚å¾®è°ƒç®—æ³•ï¼Œå†»ç»“åŸæ¨¡å‹æƒé‡ï¼Œä»…åœ¨å°è§„æ¨¡å‚æ•°é‡ä¸Šè¿›è¡Œè®­ç»ƒï¼Œåœ¨å¾®è°ƒæ•°æ®é›†ä¸Šå–å¾—è‰¯å¥½æ•ˆæœçš„åŒæ—¶ï¼Œç¼“è§£æ¨¡å‹é—å¿˜ç°è±¡

#### run_mindformersè„šæœ¬å¯åŠ¨LoRAä½å‚å¾®è°ƒ

ä½¿ç”¨LoRAç®—æ³•è¿›è¡Œä½å‚å¾®è°ƒæ—¶ï¼Œä½¿ç”¨ `configs/glm/run_glm_6b_lora.yaml` é…ç½®æ–‡ä»¶ï¼Œè¯¥é…ç½®æ–‡ä»¶åŒ…å«äº†loraä½å‚å¾®è°ƒç®—æ³•æ‰€éœ€çš„é…ç½®é¡¹

ä¿®æ”¹æ•°æ®é›†/æ¨¡å‹æƒé‡é…ç½®è·¯å¾„ï¼š

- æ•°æ®é›†ï¼šä¿®æ”¹ `mindformers/configs/glm/run_glm_6b_lora.yaml` è„šæœ¬ä¸­`train_dataset` çš„ `dataset_dir` ä¸ºå‰æ–‡ç”Ÿæˆçš„æ•°æ®é›†è·¯å¾„ã€‚
- åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æƒé‡ï¼šä¿®æ”¹ `mindformers/configs/glm/run_glm_6b_lora.yaml` è„šæœ¬ä¸­çš„ `load_checkpoint` ä¸ºé¢„è®­ç»ƒæ¨¡å‹æƒé‡è·¯å¾„ã€‚

#### å¯åŠ¨LoRAä½å‚å¾®è°ƒè„šæœ¬(1å¡)ï¼š

æ‰§è¡Œå‘½ä»¤ï¼š

```shell
cd scripts
# Usage Help: bash run_standalone.sh [CONFIG_PATH] [DEVICE_ID] [RUN_STATUS]
bash run_standalone.sh ../configs/glm/run_glm_6b_lora.yaml 0 finetune
```

è®­ç»ƒçš„logæ—¥å¿—è·¯å¾„ï¼šmindformers/scripts/mf_standalone/

checkpointå­˜å‚¨è·¯å¾„ï¼šmindformers/scripts/mf_standalone/output/checkpoint

#### å¯åŠ¨LoRAä½å‚å¾®è°ƒè„šæœ¬(4å¡)ï¼š

> æ³¨ï¼šå¦‚æœéœ€è¦è¿›è¡Œå¤šå¡è®­ç»ƒï¼Œåˆ™éœ€è¦å¯¹`glm/run_glm_6b_lora.yaml`é…ç½®æ–‡ä»¶å¯¹åº”å‚æ•°è¿›è¡Œä¿®æ”¹ï¼Œä»¥4å¡ä¸ºä¾‹ï¼Œéœ€è¦é‡æ–°ç”Ÿæˆ4å¡çš„HCCLæ–‡ä»¶ï¼š

```shell
data_parallel: 4
```

```shell
cd scripts
# Usage Help: bash run_distribute.sh [RANK_TABLE_FILE] [CONFIG_PATH] [DEVICE_RANGE] [RUN_STATUS]
bash run_distribute.sh /path/to/hccl_4_0123_xxx.json ../configs/glm/run_glm_6b_lora.yaml '[0,4]' finetune
# å°†æ­¤å¤„rank_table_fileæ›¿æ¢ä¸ºå®é™…è·¯å¾„
```

å‚æ•°è¯´æ˜ï¼š

å¯¹æ¯”å…¨å‚å¾®è°ƒå¯åŠ¨æ–¹å¼ï¼Œä»…å°† `CONFIG_PATH` é¡¹ä¿®æ”¹ä¸ºconfigsæ–‡ä»¶å¤¹ä¸‹é¢çš„ `glm/run_glm_6b_lora.yaml` é…ç½®æ–‡ä»¶ï¼Œè¡¨ç¤ºä½¿ç”¨è¯¥æ¥å£è¿›è¡Œ

è®­ç»ƒçš„logæ—¥å¿—è·¯å¾„ï¼šmindformers/output/log

checkpointå­˜å‚¨è·¯å¾„ï¼šmindformers/output/checkpoint

#### Traineré«˜é˜¶æ¥å£å¯åŠ¨LoRAä½å‚å¾®è°ƒ

å¯å¤ç”¨å…¨å‚å¾®è°ƒéƒ¨åˆ†æ‰€æä¾›çš„ `task.py` å’Œ `run_distribute_single_node.sh` è„šæœ¬

4å¡åˆ†å¸ƒå¼å¯åŠ¨å‘½ä»¤ï¼š

```bash
bash run_distribute_single_node.sh "python task.py --task text_generation --model_type glm_6b_lora --checkpoint_path ./glm_6b.ckpt --train_dataset ./train --run_mode finetune --use_parallel True --data_parallel 4 --model_parallel 1" /path/to/hccl_4p_xxx.json '[0,4]' 4
```

å‚æ•°è¯´æ˜ï¼šå¯¹æ¯”å…¨å‚å¾®è°ƒå¯åŠ¨ï¼Œä»…æ”¹åŠ¨ä»¥ä¸‹å‡ ç‚¹ï¼š

- `model_type`: æŒ‡å®šæ¨¡å‹ç±»å‹ä¸º `glm_6b_lora`ï¼Œè¡¨ç¤ºä½¿ç”¨ä½å‚å¾®è°ƒç®—æ³•
- `data_parallel`: 4å¡å¯åŠ¨ï¼Œæ•°æ®å¹¶è¡Œæ”¹ä¸º4
- `/path/to/hccl_4p_xxx.json`: ä½¿ç”¨4å¡çš„rank_table_file
- `'[0,4]' 4`: ä½¿ç”¨0~3å…±4å¡

è®­ç»ƒçš„logæ—¥å¿—è·¯å¾„ï¼šmindformers/output/log

checkpointå­˜å‚¨è·¯å¾„ï¼šmindformers/output/checkpoint

1å¡å¯åŠ¨å‘½ä»¤ï¼š

```shell
python task.py --task text_generation --model_type glm_6b_lora --checkpoint_path ./glm_6b.ckpt --train_dataset ./train --run_mode finetune --use_parallel False --data_parallel 1 --model_parallel 1
```

### å¤šæœºå¤šå¡å¾®è°ƒè®­ç»ƒ

å¤šæœºå¤šå¡å¯åŠ¨
é¦–å…ˆåœ¨æ¯å°æœºå™¨ä¸Šè¿è¡Œmindformers/tools/hccl_tools.pyç”ŸæˆRANK_TABLE_FILEçš„jsonæ–‡ä»¶ï¼›

å°†ä¸åŒæœºå™¨ä¸Šç”Ÿæˆçš„RANK_TABLE_FILEæ–‡ä»¶ä¸­çš„server_liståˆå¹¶ï¼Œserver_countè®¾ä¸ºæœºå™¨æ•°ï¼Œrank_idé¡ºåºå¢åŠ ï¼Œå¹¶ä¿è¯ä¸åŒæœºå™¨ä¸Šçš„RANK_TABLE_FILEç›¸åŒï¼›

åœ¨å¤šæœºä¸ŠåŒæ—¶æ‹‰èµ·ä»»åŠ¡ï¼Œæ‹‰èµ·æ–¹å¼ä¸º

cd scripts
bash run_distribute.sh RANK_TABLE_FILE CONFIG_PATH DEVICE_RANGE RUN_MODE RANK_SIZE

#### å‚æ•°è¯´æ˜

- RANK_TABLE_FILE: ç”±mindformers/tools/hccl_tools.pyç”Ÿæˆçš„åˆ†å¸ƒå¼jsonæ–‡ä»¶
- CONFIG_PATH: ä¸ºconfigsæ–‡ä»¶å¤¹ä¸‹é¢çš„gpt2/run_gpt2*.yamlé…ç½®æ–‡ä»¶
- DEVICE_RANGE: ä¸ºå•æœºåˆ†å¸ƒå¼å¡çš„èŒƒå›´, å¦‚[0,8]ä¸º8å¡åˆ†å¸ƒå¼ï¼Œä¸åŒ…å«8æœ¬èº«
- RUN_MODE: ä¸ºä»»åŠ¡è¿è¡ŒçŠ¶æ€ï¼Œæ”¯æŒå…³é”®å­— train é¢„è®­ç»ƒã€predictï¼ˆæ–‡æœ¬ç”Ÿæˆé¢„æµ‹ï¼‰
- RANK_SIZE: æ€»è¿è¡Œå¡æ•°

#### 4æœº32å¡å‚è€ƒRANK_TABLE_FILEæ ·ä¾‹

```text
{
  "version": "1.0",
  "server_count": "4",
  "server_list": [
    {
      "server_id": "10.155.111.140",
      "device": [
        {"device_id": "0","device_ip": "192.1.27.6","rank_id": "0"},
        {"device_id": "1","device_ip": "192.2.27.6","rank_id": "1"},
        {"device_id": "2","device_ip": "192.3.27.6","rank_id": "2"},
        {"device_id": "3","device_ip": "192.4.27.6","rank_id": "3"},
        {"device_id": "4","device_ip": "192.1.27.7","rank_id": "4"},
        {"device_id": "5","device_ip": "192.2.27.7","rank_id": "5"},
        {"device_id": "6","device_ip": "192.3.27.7","rank_id": "6"},
        {"device_id": "7","device_ip": "192.4.27.7","rank_id": "7"}],
      "host_nic_ip": "reserve"
    },
    {
      "server_id": "10.155.111.141",
      "device": [
        {"device_id": "0","device_ip": "192.1.27.8","rank_id": "8"},
        {"device_id": "1","device_ip": "192.2.27.8","rank_id": "9"},
        {"device_id": "2","device_ip": "192.3.27.8","rank_id": "10"},
        {"device_id": "3","device_ip": "192.4.27.8","rank_id": "11"},
        {"device_id": "4","device_ip": "192.1.27.9","rank_id": "12"},
        {"device_id": "5","device_ip": "192.2.27.9","rank_id": "13"},
        {"device_id": "6","device_ip": "192.3.27.9","rank_id": "14"},
        {"device_id": "7","device_ip": "192.4.27.9","rank_id": "15"}],
      "host_nic_ip": "reserve"
    },
    {
      "server_id": "10.155.111.142",
      "device": [
        {"device_id": "0","device_ip": "192.1.27.10","rank_id": "16"},
        {"device_id": "1","device_ip": "192.2.27.10","rank_id": "17"},
        {"device_id": "2","device_ip": "192.3.27.10","rank_id": "18"},
        {"device_id": "3","device_ip": "192.4.27.10","rank_id": "19"},
        {"device_id": "4","device_ip": "192.1.27.11","rank_id": "20"},
        {"device_id": "5","device_ip": "192.2.27.11","rank_id": "21"},
        {"device_id": "6","device_ip": "192.3.27.11","rank_id": "22"},
        {"device_id": "7","device_ip": "192.4.27.11","rank_id": "23"}],
      "host_nic_ip": "reserve"
    },
    {
      "server_id": "10.155.111.143",
      "device": [
        {"device_id": "0","device_ip": "192.1.27.12","rank_id": "24"},
        {"device_id": "1","device_ip": "192.2.27.12","rank_id": "25"},
        {"device_id": "2","device_ip": "192.3.27.12","rank_id": "26"},
        {"device_id": "3","device_ip": "192.4.27.12","rank_id": "27"},
        {"device_id": "4","device_ip": "192.1.27.13","rank_id": "28"},
        {"device_id": "5","device_ip": "192.2.27.13","rank_id": "29"},
        {"device_id": "6","device_ip": "192.3.27.13","rank_id": "30"},
        {"device_id": "7","device_ip": "192.4.27.13","rank_id": "31"}],
      "host_nic_ip": "reserve"
    }
  ],
  "status": "completed"
}
```

#### ä»»åŠ¡æ‹‰èµ·å‘½ä»¤ç¤ºä¾‹

```shell
# ç¬¬ä¸€å°æœºå™¨
bash run_distribute.sh {RANK_TABLE_FILE path of the first device} ../configs/glm/run_glm_6b_lora.yaml [0,8] train 32
# ç¬¬äºŒå°æœºå™¨
bash run_distribute.sh {RANK_TABLE_FILE path of the first device} ../configs/glm/run_glm_6b_lora.yaml [8,16] train 32
# ç¬¬ä¸‰å°æœºå™¨
bash run_distribute.sh {RANK_TABLE_FILE path of the first device} ../configs/glm/run_glm_6b_lora.yaml [16,24] train 32
# ç¬¬å››å°æœºå™¨
bash run_distribute.sh {RANK_TABLE_FILE path of the first device} ../configs/glm/run_glm_6b_lora.yaml [24,32] train 32
```

### å¾®è°ƒåæ¨ç†

#### æ¨ç†æ ·ä¾‹è„šæœ¬

ä¸‹é¢æä¾›ä¸€ä¸ªæ¨¡å‹æ¨ç†æ ·ä¾‹è„šæœ¬ `infer.py`

```python
import time
import mindspore as ms
import numpy as np
import argparse
from mindformers.models.glm import GLMConfig, GLMChatModel
from mindformers.models.glm.chatglm_6b_tokenizer import ChatGLMTokenizer
from mindformers.models.glm.glm_processor import process_response
from mindformers.pet.pet_config import LoraConfig
from mindformers.pet import get_pet_model

parser = argparse.ArgumentParser()
parser.add_argument('--seq_length', default=1024, type=int, help='Which device to run service.')
parser.add_argument('--device_id', default=0, type=int, help='Which device to run service.')
parser.add_argument('--checkpoint_path', type=str, default='/path/chatglm6b.ckpt', help='Checkpoint file to load on.')
parser.add_argument('--vocab_path', type=str, default='/path/ice_text.model', help='Vocab file to load on.')
parser.add_argument('--is_lora', type=str, default='false',help='Whether is lora model.')

args = parser.parse_args()

if args.is_lora.lower() == "true":
    is_lora = True
else:
    is_lora = False

config = GLMConfig(
    position_encoding_2d=True,
    use_past=True,
    is_sample_acceleration=True,
)

pet_config = LoraConfig(
    lora_rank=8,
    lora_alpha=32,
    lora_dropout=0.1,
    target_modules = '.*query_key_value*'
)


def chat_glm():
    ms.set_context(mode=ms.GRAPH_MODE, device_target="Ascend", device_id=args.device_id)
    model = GLMChatModel(config)
    if is_lora:
       config.pet_config = pet_config
       model = get_pet_model(model, pet_config)
    ms.load_checkpoint(args.checkpoint_path, model)
    tokenizer = ChatGLMTokenizer(args.vocab_path)

    inputs = ["ä½ å¥½",
              "è¯·ä»‹ç»ä¸€ä¸‹åä¸º",
              "ç”¨Pythonå†™ä¸€ä¸ªå¿«æ’",
              "ç±»å‹#ä¸Šè¡£*æè´¨#ç‰›ä»”å¸ƒ*é¢œè‰²#ç™½è‰²*é£æ ¼#ç®€çº¦*å›¾æ¡ˆ#åˆºç»£*è¡£æ ·å¼#å¤–å¥—*è¡£æ¬¾å¼#ç ´æ´"]

    for query in inputs:
        input_ids = tokenizer(query)['input_ids']

        start_time = time.time()
        outputs = model.generate(input_ids, max_length=config.max_decode_length, do_sample=False)
        end_time = time.time()
        print(f'generate speed: {outputs[0].shape[0]/(end_time-start_time):.2f} tokens/s')

        response = tokenizer.decode(outputs)
        response = process_response(response[0])
        print(response)


if __name__ == "__main__":
    chat_glm()
```

#### è¿è¡Œå‘½ä»¤

```shell
python infer.py --seq_length 1024 --device_id 0  --checkpoint_path /path/chatglm6b.ckpt --vocab_path /path/ice_text.model --is_lora True
```

å‚æ•°è¯´æ˜ï¼š

- `seq_length`: ç”¨äºæŒ‡å®šæ¨ç†è¾“å…¥é•¿åº¦
- `device_id`: æŒ‡å®šæ¨ç†åœ¨é‚£å¼ è®¾å¤‡è¿è¡Œ
- `checkpoint_path`: æŒ‡å®šè®­ç»ƒå‡ºæ¥çš„æ¨¡å‹æ–‡ä»¶è·¯å¾„ç”¨äºæ¨ç†
- `vocab_path`: æ¨¡å‹è¯è¡¨
- `is_lora`: ç”¨äºåŒºåˆ†æ˜¯å¦æ˜¯loraæ¨¡å‹ï¼Œè®¾ç½®ä¸ºtrueè¡¨ç¤ºä¸ºloraå¾®è°ƒè®­ç»ƒæ¨¡å‹

## è¯„ä¼°

### æ¨¡å‹æƒé‡æ–‡ä»¶åˆä¸€

å¾®è°ƒæ‰€å¾—åˆ°çš„æƒé‡æ–‡ä»¶ä¸ºæ ¹æ®æ¨¡å‹åˆ‡åˆ†ç­–ç•¥åˆ‡åˆ†åçš„æƒé‡ï¼Œæˆ‘ä»¬éœ€è¦æ‰‹åŠ¨å°†åˆ‡åˆ†æƒé‡åˆä¸€ï¼Œä»¥ç”¨äºè¯„ä¼°å’Œæ¨ç†

1. è·å–æ¨¡å‹åˆ‡åˆ†ç­–ç•¥æ–‡ä»¶ï¼š
   åœ¨æ‰§è¡Œå…¨å‚å¾®è°ƒè„šæœ¬æ—¶ï¼Œæ¨¡å‹å®Œæˆç¼–è¯‘åï¼Œå°†ä¼šåœ¨è¿è¡Œè·¯å¾„ä¸‹ï¼Œç”Ÿæˆåä¸º `ckpt_strategy.ckpt` çš„åˆ‡åˆ†ç­–ç•¥æ–‡ä»¶ï¼Œè¯¥æ–‡ä»¶å°†ç”¨äºç¬¬äºŒæ­¥æ¨¡å‹åˆæˆ

2. MindSporeæä¾›äº†æ ¹æ®åˆ‡åˆ†ç­–ç•¥è½¬æ¢æ¨¡å‹æƒé‡åˆ‡åˆ†çš„æ¥å£ï¼Œ[mindspore.transform_checkpoints](https://www.mindspore.cn/docs/zh-CN/r2.0/api_python/mindspore/mindspore.transform_checkpoints.html)ï¼Œæ‰§è¡Œä»¥ä¸‹pythonè„šæœ¬ï¼Œå°†8ä»½æ¨¡å‹æ–‡ä»¶åˆæˆä¸€ä»½

    ```python
    from mindspore import transform_checkpoints
    transform_checkpoints(
        src_checkpoints_dir="./output/checkpoint/", # åŸåˆ‡åˆ†æƒé‡æ–‡ä»¶å¤¹
        dst_checkpoints_dir="./target_checkpoint/", # ç›®æ ‡è·¯å¾„
        ckpt_prefix="glm-6b", # .ckptæ–‡ä»¶å‰ç¼€å
        src_strategy_file="ckpt_stragery.ckpt", # æ­¥éª¤1ä¸­çš„åˆ‡åˆ†ç­–ç•¥æ–‡ä»¶è·¯å¾„
        dst_strategy_file=None # Noneè¡¨ç¤ºä¸åˆ‡åˆ†ï¼Œæƒé‡åˆä¸€
    )
    ```

> æ³¨ï¼š`transform_checkpoints` æ¥å£å½“å‰ä»…mindspore 2.0ä»¥ä¸Šç‰ˆæœ¬æ”¯æŒï¼Œå¦‚å½“å‰ç¡¬ä»¶ç¯å¢ƒåªæ”¯æŒ2.0ä»¥ä¸‹ç‰ˆæœ¬ï¼Œå¯ä»¥æ–°å»ºcondaç¯å¢ƒå®‰è£…mindspore 2.0çš„cpuç‰ˆæœ¬ä»¥æ‰§è¡Œè¯¥è„šæœ¬

### ä½¿ç”¨å…¨å‚å¾®è°ƒæƒé‡

#### run_mindformerså¯åŠ¨eval

ä½¿ç”¨å…¨å‚å¾®è°ƒæƒé‡æ—¶ï¼Œå¯åŠ¨å¦‚ä¸‹shellè„šæœ¬ï¼Œæ‰§è¡Œå•å¡è¯„ä¼°

é…ç½®æ–‡ä»¶é€‰æ‹© `configs/glm/run_glm_6b_infer.yaml` glmæ¨¡å‹æ¨ç†é…ç½®ï¼Œæ­¤é…ç½®ä¸‹è¯„ä¼°é€Ÿåº¦æ›´å¿«

```bash
python run_mindformer.py --config configs/glm/run_glm_6b_infer.yaml --run_mode eval --load_checkpoint /path/to/glm_6b.ckpt --eval_dataset_dir /path/to/data/AdvertiseGen/ --device_id 0
```

> æ³¨ï¼šä½¿ç”¨ç¦»çº¿ç”Ÿæˆæ•°æ®æ–¹å¼æ—¶ï¼Œå°† `eval_dataset_dir` ä¸€é¡¹æŒ‡å‘`.mindrecord`æ–‡ä»¶ï¼Œå¦‚ `/path/to/data/AdvertiseGen/adgen_dev.mindrecord`ã€‚

å„é¡¹å‚æ•°ï¼š

- `config`: æŒ‡å®šç”¨äºè¯„ä¼°çš„é…ç½®æ–‡ä»¶åç§°ï¼Œæ­¤å¤„ä¸º`configs/glm/run_glm_6b_infer.yaml`
- `run_mode`: æŒ‡å®šæ‰§è¡Œæ¨¡å¼ï¼Œæ­¤ä¸º`eval`ï¼Œè¡¨ç¤ºä¸ºè¯„ä¼°æ¨¡å¼
- `load_checkpoint`: æŒ‡å®šè¦åŠ è½½çš„checkpointè·¯å¾„ï¼Œæ­¤å¤„ä¸º`/path/to/glm_6b.ckpt`ï¼Œæ›¿æ¢ä¸ºéœ€åŠ è½½çš„æƒé‡çš„çœŸå®è·¯å¾„
- `eval_dataset_dir`: è¯„ä¼°æ•°æ®é›†çš„è·¯å¾„
- `device_id`: æŒ‡å®šè¦ä½¿ç”¨çš„è®¾å¤‡ç¼–å·ï¼ˆä»0å¼€å§‹ï¼‰

è¯„ä¼°å®Œæˆåä¼šæ‰“å°è¯„ä¼°æŒ‡æ ‡ `bleu-4`ã€`rouge-1`ã€`rouge-2`ã€`rouge-l`

> æ³¨ï¼šç”±äºé»˜è®¤è¯„ä¼°æŒ‡æ ‡çš„è·å–æ–¹å¼ä¸ºç”Ÿæˆå®Œæ•´æ–‡æœ¬åä¸é¢„æœŸæ–‡æœ¬åšæ¯”è¾ƒï¼Œè¯„ä¼°é€Ÿåº¦å°†å—é™äºæ¨¡å‹å¤§å°ä¸æ–‡æœ¬ç”Ÿæˆé€Ÿåº¦ï¼Œè¯„ä¼°æµç¨‹å¯èƒ½è¾ƒä¸ºç¼“æ…¢

#### Traineré«˜é˜¶æ¥å£å¯åŠ¨eval

ä»ç„¶å¯å¤ç”¨ `task.py` è„šæœ¬ï¼Œå¯åŠ¨å‘½ä»¤ï¼š

```bash
python task.py --task text_generation --model_type glm_6b_chat --checkpoint_path /path/to/glm_6b.ckpt --eval_dataset /path/to/data/AdvertiseGen/ --run_mode eval --batch_size 1
```

> 1. å½“å‰è¯„ä¼°æ—¶ï¼Œbatch_sizeéœ€ä¸º1ï¼Œå¦åˆ™è¯„ä¼°é€Ÿåº¦ä¸‹é™ä¸¥é‡
> 2. ä½¿ç”¨ç¦»çº¿ç”Ÿæˆæ•°æ®æ–¹å¼æ—¶ï¼Œå°† `eval_dataset` ä¸€é¡¹æŒ‡å‘`.mindrecord`æ–‡ä»¶ï¼Œå¦‚ `/path/to/data/AdvertiseGen/adgen_dev.mindrecord`ã€‚

### ä½¿ç”¨LoRAä½å‚å¾®è°ƒæƒé‡

#### run_mindformerså¯åŠ¨lora eval

ä½¿ç”¨LoRAä½å‚å¾®è°ƒæƒé‡æ—¶ï¼Œå¯åŠ¨å¦‚ä¸‹shellè„šæœ¬ï¼Œæ‰§è¡Œå•å¡è¯„ä¼°

é…ç½®æ–‡ä»¶é€‰æ‹© `configs/glm/run_glm_6b_lora_infer.yaml` glm_loraæ¨¡å‹æ¨ç†é…ç½®ï¼Œæ­¤é…ç½®å¯ç”¨äºloraæ¨¡å‹ï¼Œå¹¶ä¸”è¯„ä¼°é€Ÿåº¦æ›´å¿«

```bash
python run_mindformer.py --config configs/glm/run_glm_6b_lora_infer.yaml --run_mode eval --load_checkpoint /path/to/glm_6b_lora.ckpt --eval_dataset_dir /path/to/data/AdvertiseGen/ --device_id 0
```

å„é¡¹å‚æ•°åŒä¸Šï¼Œè·¯å¾„éœ€æ›¿æ¢ä¸ºå®é™…è·¯å¾„

> ä½¿ç”¨ç¦»çº¿ç”Ÿæˆæ•°æ®æ–¹å¼æ—¶ï¼Œå°† `eval_dataset_dir` ä¸€é¡¹æŒ‡å‘`.mindrecord`æ–‡ä»¶ï¼Œå¦‚ `/path/to/data/AdvertiseGen/adgen_dev.mindrecord`ã€‚

#### Traineré«˜é˜¶æ¥å£å¯åŠ¨lora eval

ä»ç„¶å¯å¤ç”¨ `task.py` è„šæœ¬ï¼Œå¯åŠ¨å‘½ä»¤ï¼š

```bash
python task.py --task text_generation --model_type glm_6b_lora_chat --checkpoint_path /path/to/glm_6b_lora.ckpt --eval_dataset /path/to/data/AdvertiseGen/ --run_mode eval --batch_size 1
```

> 1. å½“å‰è¯„ä¼°æ—¶ï¼Œbatch_sizeéœ€ä¸º1ï¼Œå¦åˆ™è¯„ä¼°é€Ÿåº¦ä¸‹é™ä¸¥é‡
> 2. ä½¿ç”¨ç¦»çº¿ç”Ÿæˆæ•°æ®æ–¹å¼æ—¶ï¼Œå°† `eval_dataset_dir` ä¸€é¡¹æŒ‡å‘`.mindrecord`æ–‡ä»¶ï¼Œå¦‚ `/path/to/data/AdvertiseGen/adgen_dev.mindrecord`ã€‚

## æ¨¡å‹æƒé‡è½¬åŒ–

æœ¬ä»“åº“ä¸­çš„`glm`æ¥è‡ªäºHuggingFaceçš„[chatglm-6b](https://huggingface.co/THUDM/chatglm-6b)ï¼ŒåŸºäºä¸‹è¿°çš„æ­¥éª¤è·å–ï¼š

1. å…‹éš†chatglm-6bä»£ç ä»“ï¼Œä¸‹è½½åˆ†å¸ƒå¼çš„æ¨¡å‹æ–‡ä»¶ã€‚

   ```shell
   git lfs install
   git clone https://huggingface.co/THUDM/chatglm-6b
   ```

2. æ‰§è¡Œè½¬æ¢è„šæœ¬ï¼Œå¾—åˆ°è½¬æ¢åçš„è¾“å‡ºæ–‡ä»¶`ms_glm_6b.ckpt`ã€‚

   ```shell
   python mindformers/models/glm/convert_weight.py --pt_ckpt_path "replace your ptroch pth path" --ms_ckpt_path ./ms_glm_6b.ckpt
   ```

   ```shell
   # å‚æ•°è¯´æ˜
   pt_ckpt_path: huggingfaceæƒé‡ä¿å­˜ç›®å½•ä¸‹çš„ä»»æ„æƒé‡binæ–‡ä»¶,æ ¹æ®è¯¥æ–‡ä»¶è·¯å¾„è¯»å–ç›®å½•ä¸‹å…¨éƒ¨æƒé‡
   ms_ckpt_path: æƒé‡ä¿å­˜æ–‡ä»¶åï¼Œå¯ä»¥æŒ‡å®šè‡ªå®šä¹‰ä¿å­˜è·¯å¾„
   ```

