# ChatGLM6B

## æ¨¡å‹æè¿°

ChatGLM-6B æ˜¯ä¸€ä¸ªå¼€æºçš„ã€æ”¯æŒä¸­è‹±åŒè¯­çš„å¯¹è¯è¯­è¨€æ¨¡å‹ï¼ŒåŸºäº [General Language Model (GLM)](https://github.com/THUDM/GLM) æ¶æ„ï¼Œå…·æœ‰ 62 äº¿å‚æ•°ã€‚ChatGLM-6B ä½¿ç”¨äº†å’Œ ChatGPT ç›¸ä¼¼çš„æŠ€æœ¯ï¼Œé’ˆå¯¹ä¸­æ–‡é—®ç­”å’Œå¯¹è¯è¿›è¡Œäº†ä¼˜åŒ–ã€‚ç»è¿‡çº¦ 1T æ ‡è¯†ç¬¦çš„ä¸­è‹±åŒè¯­è®­ç»ƒï¼Œè¾…ä»¥ç›‘ç£å¾®è°ƒã€åé¦ˆè‡ªåŠ©ã€äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ç­‰æŠ€æœ¯çš„åŠ æŒï¼Œ62 äº¿å‚æ•°çš„ ChatGLM-6B å·²ç»èƒ½ç”Ÿæˆç›¸å½“ç¬¦åˆäººç±»åå¥½çš„å›ç­”ï¼Œæ›´å¤šä¿¡æ¯è¯·å‚è€ƒæ¸…åçš„[åšå®¢](https://chatglm.cn/blog)ã€‚åœ¨æ­¤ä»“ä¸­ï¼Œæä¾›ChatGLM6Bçš„æ¨ç†å’Œå¾®è°ƒèƒ½åŠ›ã€‚

## ä»“åº“ä»‹ç»

`chatGLM6B` åŸºäº `mindformers` å®ç°ï¼Œä¸»è¦æ¶‰åŠçš„æ–‡ä»¶æœ‰ï¼š

1. æ¨¡å‹å…·ä½“å®ç°ï¼š`mindformers/models/glm`

    ```bash
    glm
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ attention.py            # è‡ªæ³¨æ„åŠ›
        â”œâ”€â”€ chatglm_6b_tokenizer.py # tokenizer
        â”œâ”€â”€ glm_config.py           # æ¨¡å‹é…ç½®é¡¹
        â”œâ”€â”€ glm.py                  # æ¨¡å‹å®ç°
        â””â”€â”€ layers.py               # glm å±‚å®šä¹‰
    ```

2. æ¨¡å‹é…ç½®ï¼š`configs/glm`

    ```bash
    glm
        â”œâ”€â”€ run_glm_6b_fintune.yaml     # å…¨é‡å¾®è°ƒå¯åŠ¨é…ç½®
        â”œâ”€â”€ run_glm_6b_lora.yaml        # ä½å‚å¾®è°ƒå¯åŠ¨é…ç½®
        â””â”€â”€ run_glm_6b_infer.yaml       # æ¨ç†å¯åŠ¨é…ç½®
    ```

## ç¯å¢ƒè¦æ±‚

- ç¡¬ä»¶ï¼šAscend 910A
- MindSporeï¼š2.0.0rc1 / 1.10.1
- MindFormersç‰ˆæœ¬ï¼šdev

æ¨ç†å¯åœ¨å•æœºå•å¡ä¸Šå®Œæˆéƒ¨ç½²

è®­ç»ƒéœ€è¦æœ€å°‘å•æœº8å¡

## ChatGLM6Bæ¨ç†

> éœ€å¼€å‘è€…æå‰pipå®‰è£…ã€‚å…·ä½“æ¥å£è¯´æ˜è¯·å‚[APIæ¥å£](https://gitee.com/mindspore/transformer/wikis/API/)

### AutoClassæ¨ç†

å¯ä»¥ä½¿ç”¨AutoClassæ¥å£ï¼Œé€šè¿‡æ¨¡å‹åç§°è·å–ç›¸åº”çš„æ¨¡å‹/tokenizerå®ä¾‹ï¼Œå¹¶è‡ªåŠ¨ä¸‹è½½å¹¶åŠ è½½æƒé‡

`from_pretrained()` æ¥å£ä¼šè‡ªåŠ¨ä»äº‘ä¸Šä¸‹è½½é¢„è®­ç»ƒçš„æ¨¡å‹ï¼Œå­˜å‚¨è·¯å¾„ï¼š`mindformers/checkpoint_download/glm`

é¦–æ¬¡è¿è¡Œpipelineæ¨ç†æ—¶éœ€è¦è¿›è¡Œæ¨¡å‹ç¼–è¯‘ï¼Œéœ€ç­‰å¾…ä¸€æ®µæ—¶é—´

```python
>>> from mindformers import AutoModel, AutoTokenizer, TextGenerationPipeline
>>> model = AutoModel.from_pretrained("glm_6b_chat")
>>> tokenizer = AutoTokenizer.from_pretrained("glm_6b")
>>> pipeline = TextGenerationPipeline(model, tokenizer, max_length=2048)
>>> pipeline("ä½ å¥½", top_p=0.7)
[{'text_generation_text': ['ä½ å¥½ ä½ å¥½ğŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6B,å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚']}]
```

> æ³¨ï¼š`AutoModel.from_pretrained()` æ¥å£å½“å‰æ”¯æŒ `glm_6b` å’Œ `glm_6b_chat` ä¸¤ç±»æ¨¡å‹ï¼Œå‰è€…ä¸ºé€šç”¨æ¨¡å‹ï¼Œåè€…å…·å¤‡æ¨ç†åŠ é€Ÿç‰¹æ€§ï¼Œä¸¤è€…å…±äº«æƒé‡ï¼Œåœ¨æ¨ç†åœºæ™¯ä¸‹å»ºè®®ä½¿ç”¨åè€…

### pipelineæ¨ç†

ä¹Ÿå¯ä»¥ä¸å®ä¾‹åŒ–æ„é€ æ¨¡å‹ï¼Œç›´æ¥é€šè¿‡æŒ‡å®šä»»åŠ¡æ¨¡å‹ä¸æ¨¡å‹åçš„æ–¹å¼è¿›è¡Œpipelineçš„æ„é€ 

pipelineä¸­ï¼Œ`glm_6b` é»˜è®¤ä½¿ç”¨æ¨ç†åŠ é€Ÿæ¨¡å‹

```python
>>> from mindformers import pipeline
>>> task_pipeline = pipeline(task='text_generation', model='glm_6b', max_length=2048)
>>> task_pipeline('ä½ å¥½', top_p=0.7)
[{'text_generation_text': ['ä½ å¥½ ä½ å¥½ğŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6B,å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚']}]
```

### åŸºäºAPIæ¥å£çš„æ¨ç†

å¯ä½¿ç”¨å¦‚ä¸‹`chat_glm.py`è„šæœ¬ï¼š

```python
import time
import mindspore as ms
import numpy as np
from mindformers.models.glm import GLMConfig, GLMChatModel
from mindformers.models.glm.chatglm_6b_tokenizer import ChatGLMTokenizer
from mindformers.models.glm.glm_processor import process_response

config = GLMConfig(
    position_encoding_2d=True,
    phase="predict",
    use_past=True,
    is_npu_acceleration=True,
)

def chat_glm():
    ms.set_context(mode=ms.GRAPH_MODE, device_target="Ascend", device_id=7)
    model = GLMChatModel(config)
    ms.load_checkpoint("./checkpoint_download/glm/glm_6b.ckpt", model)
    tokenizer = ChatGLMTokenizer('./checkpoint_download/glm/ice_text.model')

    prompts = ["ä½ å¥½", "è¯·ä»‹ç»ä¸€ä¸‹åä¸º"]
    history = []
    for query in prompts:
        if not history:
            prompt = query
        else:
            prompt = ""
            for i, (old_query, response) in enumerate(history):
                prompt += "[Round {}]\né—®ï¼š{}\nç­”ï¼š{}\n".format(i, old_query, response)
            prompt += "[Round {}]\né—®ï¼š{}\nç­”ï¼š".format(len(history), query)
        inputs = tokenizer(prompt)

        start_time = time.time()
        outputs = model.generate(np.expand_dims(np.array(inputs['input_ids']).astype(np.int32), 0),
                                    max_length=config.max_decode_length, do_sample=False, top_p=0.7, top_k=1)
        end_time = time.time()
        print(f'generate speed: {outputs[0].shape[0]/(end_time-start_time):.2f} tokens/s')
        response = tokenizer.decode(outputs)
        response = process_response(response[0])
        history = history + [(query, response)]
        print(response)

if __name__ == "__main__":
    chat_glm()
```

## å¾®è°ƒ

ä¸‹é¢ä»¥ [ADGEN](https://aclanthology.org/D19-1321.pdf) (å¹¿å‘Šç”Ÿæˆ) æ•°æ®é›†ä¸ºä¾‹ä»‹ç»ä»£ç çš„ä½¿ç”¨æ–¹æ³•

### æ•°æ®å¤„ç†

ADGEN æ•°æ®é›†ä»»åŠ¡ä¸ºæ ¹æ®è¾“å…¥ï¼ˆcontentï¼‰ç”Ÿæˆä¸€æ®µå¹¿å‘Šè¯ï¼ˆsummaryï¼‰ã€‚

```json
{
    "content": "ç±»å‹#ä¸Šè¡£*ç‰ˆå‹#å®½æ¾*ç‰ˆå‹#æ˜¾ç˜¦*å›¾æ¡ˆ#çº¿æ¡*è¡£æ ·å¼#è¡¬è¡«*è¡£è¢–å‹#æ³¡æ³¡è¢–*è¡£æ¬¾å¼#æŠ½ç»³",
    "summary": "è¿™ä»¶è¡¬è¡«çš„æ¬¾å¼éå¸¸çš„å®½æ¾ï¼Œåˆ©è½çš„çº¿æ¡å¯ä»¥å¾ˆå¥½çš„éšè—èº«æä¸Šçš„å°ç¼ºç‚¹ï¼Œç©¿åœ¨èº«ä¸Šæœ‰ç€å¾ˆå¥½çš„æ˜¾ç˜¦æ•ˆæœã€‚é¢†å£è£…é¥°äº†ä¸€ä¸ªå¯çˆ±çš„æŠ½ç»³ï¼Œæ¼‚äº®çš„ç»³ç»“å±•ç°å‡ºäº†åè¶³çš„ä¸ªæ€§ï¼Œé…åˆæ—¶å°šçš„æ³¡æ³¡è¢–å‹ï¼Œå°½æ˜¾å¥³æ€§ç”œç¾å¯çˆ±çš„æ°”æ¯ã€‚"
}
```

ä» [Google Drive](https://drive.google.com/file/d/13_vf0xRTQsyneRKdD1bZIr93vBGOczrk/view?usp=sharing) æˆ–è€… [Tsinghua Cloud](https://cloud.tsinghua.edu.cn/f/b3f119a008264b1cabd1/?dl=1) ä¸‹è½½å¤„ç†å¥½çš„ ADGEN æ•°æ®é›†ï¼Œå°†è§£å‹åçš„ `AdvertiseGen` ä»»æ„ç›®å½•ä¸‹

ä½¿ç”¨ `mindformers/mindformers/dataset/glm_data_process/adgen_dataset.py` è„šæœ¬å°†æ•°æ®é›†å¤„ç†æˆmindrecordæ ¼å¼

æ‰§è¡Œå‘½ä»¤ç”Ÿæˆè®­ç»ƒæ•°æ®é›†ï¼š

```bash
python adgen_dataset.py \
--input_file /data3/l00806781/dataset/AdvertiseGen/train.json \
--vocab_file /root/l00806781/mindformers_glm/ice_text.model\
--output_file /data3/lzd/dataset/AdvertiseGen/train_0604_128.mindrecord \
--max_source_length 64 \
--max_target_length 64 \
--mode train
```

æ‰§è¡Œå‘½ä»¤ç”Ÿæˆè¯„ä¼°æ•°æ®é›†ï¼š

```bash
python adgen_dataset.py \
--input_file /data3/l00806781/dataset/AdvertiseGen/dev.json \
--vocab_file /root/l00806781/mindformers_glm/ice_text.model \
--output_file /data3/lzd/dataset/AdvertiseGen/eval_0604_256.mindrecord \
--max_source_length 256 \
--max_target_length 256 \
--mode eval
```

### ç”ŸæˆHCCLæ–‡ä»¶

è¿è¡Œmindformers/tools/hccl_tools.pyç”ŸæˆRANK_TABLE_FILEçš„jsonæ–‡ä»¶ï¼›

```shell
# step1ï¼šæœºå™¨ä¸Šè¿è¡Œå¦‚ä¸‹å‘½ä»¤ï¼Œç”Ÿæˆå„è‡ªçš„RANK_TABLE_FILEçš„jsonæ–‡ä»¶
python ./mindformers/tools/hccl_tools.py --device_num "[0,8)"
```

> æ³¨ï¼šè‹¥ä½¿ç”¨ModelArtsçš„notebookç¯å¢ƒï¼Œå¯ä» `/user/config/jobstart_hccl.json` è·¯å¾„ä¸‹ç›´æ¥è·å–rank tableï¼Œæ— éœ€æ‰‹åŠ¨ç”Ÿæˆ

RANK_TABLE_FILE å•æœº8å¡å‚è€ƒæ ·ä¾‹:

```json
{
    "version": "1.0",
    "server_count": "1",
    "server_list": [
        {
            "server_id": "xx.xx.xx.xx",
            "device": [
                {"device_id": "0","device_ip": "192.1.27.6","rank_id": "0"},
                {"device_id": "1","device_ip": "192.2.27.6","rank_id": "1"},
                {"device_id": "2","device_ip": "192.3.27.6","rank_id": "2"},
                {"device_id": "3","device_ip": "192.4.27.6","rank_id": "3"},
                {"device_id": "4","device_ip": "192.1.27.7","rank_id": "4"},
                {"device_id": "5","device_ip": "192.2.27.7","rank_id": "5"},
                {"device_id": "6","device_ip": "192.3.27.7","rank_id": "6"},
                {"device_id": "7","device_ip": "192.4.27.7","rank_id": "7"}],
             "host_nic_ip": "reserve"
        }
    ],
    "status": "completed"
}
```

### è®­ç»ƒå¯åŠ¨å‘½ä»¤è¯´æ˜

#### ä¿®æ”¹é…ç½®æ–‡ä»¶

- æ•°æ®é›†ï¼šä¿®æ”¹ `mindformers/configs/glm/glm/run_glm_6b_finetune.yaml` è„šæœ¬ä¸­`train_dataset` çš„ `dataset_dir` ä¸ºå‰æ–‡ç”Ÿæˆçš„æ•°æ®é›†è·¯å¾„ã€‚
- åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æƒé‡ï¼šä¿®æ”¹ `mindformers/configs/glm/glm/run_glm_6b_finetune.yaml` è„šæœ¬ä¸­çš„ `load_checkpoint` ä¸ºé¢„è®­ç»ƒæ¨¡å‹æƒé‡è·¯å¾„ã€‚

#### å¯åŠ¨å…¨å‚å¾®è°ƒè„šæœ¬

```shell
cd scripts
# Usage Help: bash run_distribute.sh [RANK_TABLE_FILE] [CONFIG_PATH] [DEVICE_RANGE] [RUN_STATUS]
bash run_distribute.sh /path/to/hccl_8p_01234567_127.0.1.1.json ../configs/glm/run_glm_6b_finetune.yaml '[0,8]' finetune
# å°†æ­¤å¤„rank_table_fileæ›¿æ¢ä¸ºå®é™…è·¯å¾„
```

å‚æ•°è¯´æ˜

```text
RANK_TABLE_FILE: ç”±mindformers/tools/hccl_tools.pyç”Ÿæˆçš„åˆ†å¸ƒå¼jsonæ–‡ä»¶
CONFIG_PATH: ä¸ºconfigsæ–‡ä»¶å¤¹ä¸‹é¢çš„glm/run_glm_6b.yamlé…ç½®æ–‡ä»¶
DEVICE_RANGE: ä¸ºå•æœºåˆ†å¸ƒå¼å¡çš„èŒƒå›´ï¼Œå¦‚ '[0,8]' ä¸º8å¡åˆ†å¸ƒå¼ï¼Œä¸åŒ…å«8æœ¬èº«
RUN_STATUS: ä¸ºä»»åŠ¡è¿è¡ŒçŠ¶æ€ï¼Œæ”¯æŒå…³é”®å­— train\finetune\eval\predict
```

> æ³¨ï¼šç”±äºGLM6Bçš„æ¨¡å‹è¾ƒå¤§ï¼Œæ— æ³•åœ¨å•å¡ä¸Šè¿è¡Œï¼Œæ­¤å¤„ä»…æä¾›åˆ†å¸ƒå¼å¯åŠ¨è„šæœ¬

è®­ç»ƒçš„logæ—¥å¿—è·¯å¾„ï¼šmindformers/output/log

checkpointå­˜å‚¨è·¯å¾„ï¼šmindformers/output/checkpoint

## è¯„ä¼°

### æ¨¡å‹æ–‡ä»¶åˆä¸€

finetuneæ‰€å¾—åˆ°çš„æƒé‡æ–‡ä»¶ä¸ºæ ¹æ®æ¨¡å‹åˆ‡åˆ†ç­–ç•¥åˆ‡åˆ†åçš„æƒé‡ï¼Œæˆ‘ä»¬éœ€è¦æ‰‹åŠ¨å°†åˆ‡åˆ†æƒé‡åˆä¸€ï¼Œä»¥ç”¨äºè¯„ä¼°å’Œæ¨ç†

1. è·å–æ¨¡å‹åˆ‡åˆ†ç­–ç•¥æ–‡ä»¶ï¼š
   åœ¨æ‰§è¡Œå…¨å‚å¾®è°ƒè„šæœ¬æ—¶ï¼Œæ¨¡å‹å®Œæˆç¼–è¯‘åï¼Œå°†ä¼šåœ¨ `mindformers/scripts/mf_parallelx` æ–‡ä»¶å¤¹ä¸‹ï¼Œç”Ÿæˆåä¸º `ckpt_strategy.ckpt` çš„åˆ‡åˆ†ç­–ç•¥æ–‡ä»¶ï¼Œå°†å…¶ä¿å­˜

2. MindSporeæä¾›äº†æ ¹æ®åˆ‡åˆ†ç­–ç•¥è½¬æ¢æ¨¡å‹æƒé‡åˆ‡åˆ†çš„æ¥å£ï¼Œ[mindspore.transform_checkpoints](https://www.mindspore.cn/docs/zh-CN/r2.0/api_python/mindspore/mindspore.transform_checkpoints.html)ï¼Œæ‰§è¡Œä»¥ä¸‹pythonè„šæœ¬ï¼Œå°†8ä»½æ¨¡å‹æ–‡ä»¶åˆæˆä¸€ä»½

    ```python
    from mindspore import transform_checkpoints
    transform_checkpoints(
        src_checkpoints_dir="./output/checkpoint/", # åŸåˆ‡åˆ†æƒé‡æ–‡ä»¶å¤¹
        dst_checkpoints_dir="./target_checkpoint/", # ç›®æ ‡è·¯å¾„
        ckpt_prefix="glm-6b_rank_xx", # .ckptæ–‡ä»¶å‰ç¼€å
        src_strategy_file="ckpt_stragery.ckpt" # æ­¥éª¤1ä¸­çš„åˆ‡åˆ†ç­–ç•¥æ–‡ä»¶è·¯å¾„
        dst_strategy_file=None # Noneè¡¨ç¤ºä¸åˆ‡åˆ†ï¼Œæƒé‡åˆä¸€
    )
    ```

### å¯åŠ¨ eval è„šæœ¬

å¯åŠ¨å¦‚ä¸‹shellè„šæœ¬ï¼Œæ‰§è¡Œå•å¡è¯„ä¼°

```bash
python run_mindformer.py --config configs/glm/run_glm_6b_finetune.yaml --run_mode eval --load_checkpoint checkpoint_download/glm/glm_6b.ckpt --eval_dataset_dir /./data/AdvertiseGen/adgen_dev.mindrecord --device_id 7
```

å„é¡¹å‚æ•°ï¼š

- `--config`: æŒ‡å®šç”¨äºè¯„ä¼°çš„é…ç½®æ–‡ä»¶åç§°ï¼Œæ­¤å¤„ä¸º`configs/glm/run_glm_6b_finetune.yaml`
- `run_mode`: æŒ‡å®šæ‰§è¡Œæ¨¡å¼ï¼Œæ­¤ä¸º`eval`ï¼Œè¡¨ç¤ºä¸ºè¯„ä¼°æ¨¡å¼
- `load_checkpoint`: æŒ‡å®šè¦åŠ è½½çš„checkpointè·¯å¾„ï¼Œæ­¤å¤„ä¸º`checkpoint_download/glm/glm_6b.ckpt`ï¼Œå¯æ›¿æ¢ä¸ºéœ€åŠ è½½çš„æƒé‡çš„çœŸå®è·¯å¾„
- `eval_dataset_dir`: è¯„ä¼°æ•°æ®é›†çš„è·¯å¾„
- `device_id`: æŒ‡å®šè¦ä½¿ç”¨çš„è®¾å¤‡ç¼–å·ï¼ˆä»0å¼€å§‹ï¼‰

è¯„ä¼°å®Œæˆåä¼šæ‰“å°è¯„ä¼°æŒ‡æ ‡ `bleu-4`ã€`rouge-1`ã€`rouge-2`ã€`rouge-l`

> æ³¨ï¼šç”±äºé»˜è®¤è¯„ä¼°æŒ‡æ ‡çš„è·å–æ–¹å¼ä¸ºç”Ÿæˆå®Œæ•´æ–‡æœ¬åä¸é¢„æœŸæ–‡æœ¬åšæ¯”è¾ƒï¼Œè¯„ä¼°é€Ÿåº¦å°†å—é™äºæ–‡æœ¬ç”Ÿæˆé€Ÿåº¦ï¼Œæ¯”è¾ƒç¼“æ…¢

## æ¨¡å‹æƒé‡è½¬åŒ–

æœ¬ä»“åº“ä¸­çš„`glm`æ¥è‡ªäºHuggingFaceçš„[chatglm-6b](https://huggingface.co/THUDM/chatglm-6b)ï¼ŒåŸºäºä¸‹è¿°çš„æ­¥éª¤è·å–ï¼š

1. å…‹éš†chatglm-6bä»£ç ä»“ï¼Œä¸‹è½½åˆ†å¸ƒå¼çš„æ¨¡å‹æ–‡ä»¶ã€‚

   ```shell
   git lfs install
   git clone https://huggingface.co/THUDM/chatglm-6b
   ```

2. æ‰§è¡Œä¸‹åˆ—pythonè„šæœ¬å°†8ä»½åˆ†å¸ƒå¼çš„pytorchæ¨¡å‹æ–‡ä»¶ä¿å­˜æˆ1ä»½ã€‚

   ```python
   from transformers import AutoModel
   import torch as pt

   pt_ckpt_path="Your chatglm-6b path"
   model = AutoModel.from_pretrained(pt_ckpt_path, trust_remote_code=True).half()
   pt_pth_path = "pt_glm_6b.pth"
   pt.save(model.state_dict(), pt_pth_path)
   ```

3. æ‰§è¡Œè½¬æ¢è„šæœ¬ï¼Œå¾—åˆ°è½¬æ¢åçš„è¾“å‡ºæ–‡ä»¶`ms_glm_6b.ckpt`ã€‚

   ```shell
   python mindformers/models/glm/convert_weight.py --pt_ckpt_path "replace your ptroch pth path" --ms_ckpt_path ./ms_glm_6b.ckpt
   ```
