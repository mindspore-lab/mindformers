# CogVLM2-Video

> ## ğŸš¨ å¼ƒç”¨è¯´æ˜
>
> æœ¬æ¨¡å‹å·²è¿‡æ—¶ï¼Œä¸å†è¿›è¡Œç»´æŠ¤ï¼Œå¹¶å°†åœ¨ *1.6.0* ç‰ˆæœ¬ä¸‹æ¶ã€‚å¦‚éœ€ä½¿ç”¨æ­¤æ¨¡å‹ï¼Œå»ºè®®æ ¹æ®å®˜æ–¹æ–‡æ¡£ä¸­çš„ **[æ¨¡å‹åº“](https://www.mindspore.cn/mindformers/docs/zh-CN/r1.5.0/start/models.html)** é€‰æ‹©åˆé€‚çš„ç‰ˆæœ¬è¿›è¡Œä½¿ç”¨ã€‚
>
> å¦‚æœ‰ä»»ä½•é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·é€šè¿‡ **[ç¤¾åŒºIssue](https://gitee.com/mindspore/mindformers/issues/new)** æäº¤åé¦ˆã€‚æ„Ÿè°¢æ‚¨çš„ç†è§£ä¸æ”¯æŒï¼

## æ¨¡å‹æè¿°

CogVLM2 æ˜¯æ™ºè°±å¼€å‘çš„å¤šæ¨¡æ€ç†è§£ç³»åˆ—å¤§æ¨¡å‹ï¼Œè¯¥ç³»åˆ—ä¸­åŒ…å«äº†å›¾æ–‡ç†è§£ä»¥åŠè§†é¢‘ç†è§£å¤§æ¨¡å‹ã€‚**CogVLM2-Video-13B** ä½œä¸ºè§†é¢‘ç†è§£å¤§æ¨¡å‹ï¼Œåœ¨å¤šä¸ªè§†é¢‘é—®ç­”ä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€ä¼˜çš„æ€§èƒ½ï¼Œå®ƒå¯ä»¥å¿«é€Ÿå®Œæˆå¯¹è¾“å…¥è§†é¢‘çš„ç†è§£å¹¶æ ¹æ®è¾“å…¥æ–‡æœ¬ä½œå‡ºå›ç­”ã€‚ç›®å‰è¯¥æ¨¡å‹æ”¯æŒ**2Kåºåˆ—é•¿åº¦**ã€**224Ã—224åˆ†è¾¨ç‡çš„è§†é¢‘ç†è§£**ä»¥åŠ**ä¸­è‹±æ–‡å›ç­”**ç­‰åŠŸèƒ½ã€‚

```text
@misc{wang2023cogvlm,
      title={CogVLM: Visual Expert for Pretrained Language Models},
      author={Weihan Wang and Qingsong Lv and Wenmeng Yu and Wenyi Hong and Ji Qi and Yan Wang and Junhui Ji and Zhuoyi Yang and Lei Zhao and Xixuan Song and Jiazheng Xu and Bin Xu and Juanzi Li and Yuxiao Dong and Ming Ding and Jie Tang},
      year={2023},
      eprint={2311.03079},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```

## æ¨¡å‹æ–‡ä»¶

`CogVLM2-Video`åŸºäº`mindformers`å®ç°ï¼Œä¸»è¦æ¶‰åŠçš„æ–‡ä»¶æœ‰ï¼š

1. æ¨¡å‹å…·ä½“å®ç°ï¼š

   ```text
   mindformers/models/cogvlm2
       â”œâ”€â”€ __init__.py
       â”œâ”€â”€ cogvlm2.py                # æ¨¡å‹å®ç°
       â”œâ”€â”€ cogvlm2_config.py         # æ¨¡å‹é…ç½®é¡¹
       â”œâ”€â”€ cogvlm2_llm.py            # cogvlm2 è¯­è¨€æ¨¡å‹å®ç°
       â”œâ”€â”€ cogvlm2_processor.py      # cogvlm2 æ•°æ®é¢„å¤„ç†
       â”œâ”€â”€ cogvlm2_tokenizer.py      # cogvlm2 tokenizer
       â””â”€â”€ convert_weight.py         # æƒé‡è½¬æ¢è„šæœ¬
   ```

2. æ¨¡å‹é…ç½®ï¼š

   ```text
   configs/cogvlm2
       â”œâ”€â”€ finetune_cogvlm2_video_llama3_chat_13b_lora.yaml  # cogvlm2-video-13bæ¨¡å‹LoRAå¾®è°ƒå¯åŠ¨é…ç½®
       â””â”€â”€ predict_cogvlm2_video_llama3_chat_13b.yaml        # cogvlm2-video-13bæ¨¡å‹æ¨ç†å¯åŠ¨é…ç½®
   ```

## ç¯å¢ƒåŠæ•°æ®å‡†å¤‡

### å®‰è£…ç¯å¢ƒ

MindFormersè½¯ç¡¬ä»¶é…å¥—å…³ç³»ä»¥åŠå®‰è£…å‚è€ƒ[ç¯å¢ƒå®‰è£…æŒ‡å—](../../README_CN.md#æºç ç¼–è¯‘å®‰è£…)å’Œ[ç‰ˆæœ¬åŒ¹é…å…³ç³»](../../README_CN.md#ç‰ˆæœ¬åŒ¹é…å…³ç³»)ã€‚

### æ•°æ®åŠæƒé‡å‡†å¤‡

#### æ•°æ®é›†ä¸‹è½½

MindFormersæä¾›`RWF2000`ä½œä¸º[å¾®è°ƒ](#å¾®è°ƒ)æ•°æ®é›†ï¼Œç”¨æˆ·å¯é€šè¿‡å¦‚ä¸‹é“¾æ¥è¿›è¡Œä¸‹è½½ã€‚

| æ•°æ®é›†åç§°   |       é€‚ç”¨æ¨¡å‹        |   é€‚ç”¨é˜¶æ®µ   |                            ä¸‹è½½é“¾æ¥                             |
|:--------|:-----------------:|:--------:|:-----------------------------------------------------------:|
| RWF2000 | CogVLM2-Video-13B | Finetune | [Link](https://www.kaggle.com/datasets/vulamnguyen/rwf2000) |

æ•°æ®é¢„å¤„ç†ä¸­æ‰€ç”¨çš„`tokenizer.model`å¯ä»¥å‚è€ƒ[æ¨¡å‹æƒé‡ä¸‹è½½](#æ¨¡å‹æƒé‡ä¸‹è½½)è¿›è¡Œä¸‹è½½ã€‚

- **RWF2000 æ•°æ®é¢„å¤„ç†**

  æ‰§è¡Œæ•°æ®é¢„å¤„ç†è„šæœ¬`mindformers/tools/dataset_preprocess/cogvlm2/rwf2000_process.py`åˆ¶ä½œæ•°æ®é›†ã€‚

  ```shell
  cd mindformers/tools/dataset_preprocess/cogvlm2
  python rwf2000_process.py \
   --data_dir /path/RWF-2000/ \
   --output_file /path/RWF-2000/train.json

  # å‚æ•°è¯´æ˜
  data_dir:   ä¸‹è½½åä¿å­˜æ•°æ®çš„æ–‡ä»¶å¤¹è·¯å¾„, æ–‡ä»¶å¤¹å†…åŒ…å«'train'å’Œ'val'æ–‡ä»¶å¤¹
  output_dir: ç”Ÿæˆæ•°æ®é›†æ ‡ç­¾æ–‡ä»¶è·¯å¾„
  ```

#### æ¨¡å‹æƒé‡ä¸‹è½½

MindFormersæä¾›HuggingFaceå®˜æ–¹æƒé‡ä¸‹è½½é“¾æ¥ï¼Œç”¨æˆ·å¯ä¸‹è½½æƒé‡å¹¶ç»è¿‡[æ¨¡å‹æƒé‡è½¬æ¢](#æ¨¡å‹æƒé‡è½¬æ¢)åè¿›è¡Œä½¿ç”¨ã€‚

è¯è¡¨ä¸‹è½½é“¾æ¥ï¼š[tokenizer.model](https://huggingface.co/meta-llama/Meta-Llama-3-8B)

> è¯¥tokenizerä¸llama3æ¨¡å‹ç›¸åŒï¼Œè¯·è‡ªè¡Œç”³è¯·huggingfaceä¸Šllama3ä½¿ç”¨æƒé™è¿›è¡Œä¸‹è½½ã€‚

| æ¨¡å‹åç§°                   | MindSporeæƒé‡ |                              HuggingFaceæƒé‡                               |
|:-----------------------|:-----------:|:------------------------------------------------------------------------:|
| CogVLM2-Video-Chat-13B |      -      | [Link](https://huggingface.co/THUDM/cogvlm2-video-llama3-chat/tree/main) |

#### æ¨¡å‹æƒé‡è½¬æ¢

æ‰§è¡Œ`convert_weight.py`è½¬æ¢è„šæœ¬ï¼Œå°†HuggingFaceçš„æƒé‡è½¬æ¢ä¸ºå®Œæ•´çš„ckptæƒé‡ã€‚

```shell
pip install transformers torch
python convert_weight.py --model cogvlm2 --input_path TORCH_CKPT_DIR --output_path {path}/MS_CKPT_NAME --dtype 'fp32'

# å‚æ•°è¯´æ˜
model:       æ¨¡å‹åç§°
input_path:  ä¸‹è½½HuggingFaceæƒé‡çš„æ–‡ä»¶å¤¹è·¯å¾„
output_path: è½¬æ¢åçš„MindSporeæƒé‡æ–‡ä»¶ä¿å­˜è·¯å¾„
dtype:       è½¬æ¢åçš„MindSporeæƒé‡å‚æ•°ç±»å‹
```

## å¾®è°ƒ

### LoRAå¾®è°ƒ

MindFormersæ”¯æŒå¯¹`CogVLM2-Video-Chat-13B`è¿›è¡ŒLoRAå¾®è°ƒï¼Œå¾®è°ƒæ•°æ®é›†å¯å‚è€ƒ[æ•°æ®é›†ä¸‹è½½](#æ•°æ®é›†ä¸‹è½½)éƒ¨åˆ†è·å–ã€‚

1. å°†HuggingFaceæƒé‡è½¬æ¢ä¸ºå¯åŠ è½½çš„LoRAæƒé‡

   ```shell
   pip install transformers torch
   cd mindformers/models/cogvlm2
   python convert_weight.py --torch_ckpt_dir TORCH_CKPT_DIR --mindspore_ckpt_path {path}/MS_CKPT_NAME --sft 'lora'

   # å‚æ•°è¯´æ˜
   input_path:  ä¸‹è½½HuggingFaceæƒé‡çš„æ–‡ä»¶å¤¹è·¯å¾„
   output_path: è½¬æ¢åçš„MindSporeæƒé‡æ–‡ä»¶ä¿å­˜è·¯å¾„
   sft:         è½¬æ¢å¾®è°ƒæƒé‡ç±»å‹, 'lora'è¡¨ç¤ºå°†åŸå§‹æƒé‡è½¬æ¢ä¸ºå¯åŠ è½½çš„LoRAæƒé‡
   ```

2. ä¿®æ”¹æ¨¡å‹é…ç½®æ–‡ä»¶`configs/cogvlm2/finetune_cogvlm2_video_llama3_chat_13b_lora.yaml`

   ```yaml
   train_dataset:
     data_loader:
       annotation_file: "/{path}/RWF-2000/train.json"  # é¢„å¤„ç†åçš„æ•°æ®é›†æ–‡ä»¶è·¯å¾„
       shuffle: True                                   # å¼€å¯æ•°æ®é›†éšæœºé‡‡æ ·
     tokenizer:
       vocab_file: "/{path}/tokenizer.model"           # æŒ‡å®štokenizeræ–‡ä»¶è·¯å¾„
   ```

3. å¯åŠ¨å¾®è°ƒè„šæœ¬

   ```shell
   python run_mindformer.py \
    --config configs/cogvlm2/finetune_cogvlm2_video_llama3_chat_13b_lora.yaml \
    --run_mode finetune \
    --load_checkpoint /{path}/cogvlm2-video-llama3-chat_lora.ckpt

   # å‚æ•°è¯´æ˜
   config:          æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„
   run_mode:        æ¨¡å‹æ‰§è¡Œæ¨¡å¼, 'finetune'è¡¨ç¤ºå¾®è°ƒ
   load_checkpoint: æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„
   ```

## æ¨ç†

MindFormersæä¾›`CogVLM2-Video-Chat-13B`çš„æ¨ç†ç¤ºä¾‹ï¼Œæ”¯æŒå•å¡æ¨ç†ã€å¤šå¡æ¨ç†ã€‚

### å•å¡æ¨ç†

1. ä¿®æ”¹æ¨¡å‹é…ç½®æ–‡ä»¶`configs/cogvlm2/predict_cogvlm2_video_llama3_chat_13b.yaml`

   ```yaml
   model:
     model_config:
       use_past: True                         # å¼€å¯å¢é‡æ¨ç†
       is_dynamic: True                       # å¼€å¯åŠ¨æ€shape

   processor:
     tokenizer:
       vocab_file: "/{path}/tokenizer.model"  # æŒ‡å®štokenizeræ–‡ä»¶è·¯å¾„
   ```

2. å¯åŠ¨æ¨ç†è„šæœ¬

   ```shell
   python run_mindformer.py \
    --config configs/cogvlm2/predict_cogvlm2_video_llama3_chat_13b.yaml \
    --run_mode predict \
    --predict_data "/path/video.mp4" "Please describe this video." \
    --modal_type "video" "text" \
    --load_checkpoint /{path}/cogvlm2-video-llama3-chat.ckpt

   # å‚æ•°è¯´æ˜
   config:          æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„
   run_mode:        æ¨¡å‹æ‰§è¡Œæ¨¡å¼, 'predict'è¡¨ç¤ºæ¨ç†
   predict_data:    æ¨¡å‹æ¨ç†è¾“å…¥, ç¬¬ä¸€ä¸ªè¾“å…¥æ˜¯è§†é¢‘æ–‡ä»¶è·¯å¾„, ç¬¬äºŒä¸ªè¾“å…¥æ˜¯prompt
   modal_type:      æ¨¡å‹æ¨ç†è¾“å…¥çš„æ¨¡æ€ç±»å‹, å†…å®¹é¡ºåºå¯¹åº”predict_dataä¸­è¾“å…¥çš„æ¨¡æ€ç±»å‹ï¼Œæ”¯æŒ "video"ï¼Œ"text"
   load_checkpoint: æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„
   ```

   æ¨ç†ç»“æœç¤ºä¾‹ï¼š

   ```text
   inputs: "run.mp4" "Please describe this video."
   outputs: "The video features a series of close-up shots of a person's feet running on a sidewalk.
   The footage is captured in a slow-motion style, with each frame highlighting the feet' movement and the texture of the shoes..."
   ```

### å¤šå¡æ¨ç†

ä»¥`CogVLM2-Video-Chat-13B`2å¡æ¨ç†ä¸ºä¾‹ã€‚

1. ä¿®æ”¹æ¨¡å‹é…ç½®æ–‡ä»¶`configs/cogvlm2/predict_cogvlm2_video_llama3_chat_13b.yaml`

   ```yaml
   auto_trans_ckpt: True                      # å¼€å¯æƒé‡è‡ªåŠ¨è½¬æ¢
   use_parallel: True
   parallel_config:
     model_parallel: 2                        # å¯æ ¹æ®ä½¿ç”¨deviceæ•°è¿›è¡Œä¿®æ”¹

   model:
     model_config:
       use_past: True                         # å¼€å¯å¢é‡æ¨ç†
       is_dynamic: True                       # å¼€å¯åŠ¨æ€shape

   processor:
     tokenizer:
       vocab_file: "/{path}/tokenizer.model"  # æŒ‡å®štokenizeræ–‡ä»¶è·¯å¾„
   ```

2. å¯åŠ¨æ¨ç†è„šæœ¬

   ```shell
   bash scripts/msrun_launcher.sh "run_mindformer.py \
    --config configs/cogvlm2/predict_cogvlm2_video_llama3_chat_13b.yaml \
    --run_mode predict \
    --predict_data \"/path/video.mp4\" \"Please describe this video.\" \
    --modal_type video text \
    --load_checkpoint /{path}/cogvlm2-video-llama3-chat.ckpt" 2
   ```

   æ¨ç†ç»“æœç¤ºä¾‹ï¼š

   ```text
   inputs: "run.mp4" "Please describe this video."
   outputs: "The video features a series of close-up shots of a person's feet running on a sidewalk.
   The footage is captured in a slow-motion style, with each frame highlighting the feet' movement and the texture of the shoes..."
   ```
