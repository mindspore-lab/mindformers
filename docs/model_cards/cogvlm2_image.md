# CogVLM2-Image

> ## ğŸš¨ å¼ƒç”¨è¯´æ˜
>
> æœ¬æ¨¡å‹å·²è¿‡æ—¶ï¼Œä¸å†è¿›è¡Œç»´æŠ¤ï¼Œå¹¶å°†åœ¨ *1.6.0* ä¹‹åçš„ç‰ˆæœ¬ä¸‹æ¶ã€‚å¦‚éœ€ä½¿ç”¨æ­¤æ¨¡å‹ï¼Œå»ºè®®æ ¹æ®å®˜æ–¹æ–‡æ¡£ä¸­çš„ **[æ¨¡å‹åº“](https://www.mindspore.cn/mindformers/docs/zh-CN/r1.5.0/start/models.html)** é€‰æ‹©åˆé€‚çš„ç‰ˆæœ¬è¿›è¡Œä½¿ç”¨ã€‚
>
> å¦‚æœ‰ä»»ä½•é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·é€šè¿‡ **[ç¤¾åŒºIssue](https://gitee.com/mindspore/mindformers/issues/new)** æäº¤åé¦ˆã€‚æ„Ÿè°¢æ‚¨çš„ç†è§£ä¸æ”¯æŒï¼

## æ¨¡å‹æè¿°

CogVLM2 æ˜¯æ™ºè°±å¼€å‘çš„å¤šæ¨¡æ€ç†è§£ç³»åˆ—å¤§æ¨¡å‹ï¼Œè¯¥ç³»åˆ—ä¸­åŒ…å«äº†å›¾æ–‡ç†è§£ä»¥åŠè§†é¢‘ç†è§£å¤§æ¨¡å‹ã€‚**cogvlm2-llama3-chat-19B** ä½œä¸ºå›¾ç‰‡ç†è§£å¤§æ¨¡å‹ï¼Œåœ¨è¯¸å¦‚ TextVQAã€DocVQA ç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚ç›®å‰è¯¥æ¨¡å‹æ”¯æŒ**8Kåºåˆ—é•¿åº¦**ã€**æ”¯æŒæœ€é«˜ 1344 * 1344 çš„å›¾åƒåˆ†è¾¨ç‡**ä»¥åŠ**æä¾›æ”¯æŒä¸­è‹±æ–‡çš„å¼€æºæ¨¡å‹ç‰ˆæœ¬**ç­‰åŠŸèƒ½ã€‚

```text
@misc{wang2023cogvlm,
      title={CogVLM: Visual Expert for Pretrained Language Models},
      author={Weihan Wang and Qingsong Lv and Wenmeng Yu and Wenyi Hong and Ji Qi and Yan Wang and Junhui Ji and Zhuoyi Yang and Lei Zhao and Xixuan Song and Jiazheng Xu and Bin Xu and Juanzi Li and Yuxiao Dong and Ming Ding and Jie Tang},
      year={2023},
      eprint={2311.03079},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```

## æ¨¡å‹æ–‡ä»¶

`CogVLM2-Image`åŸºäº`mindformers`å®ç°ï¼Œä¸»è¦æ¶‰åŠçš„æ–‡ä»¶æœ‰ï¼š

1. æ¨¡å‹å…·ä½“å®ç°ï¼š

   ```text
   mindformers/models/cogvlm2
       â”œâ”€â”€ __init__.py
       â”œâ”€â”€ cogvlm2.py                # æ¨¡å‹å®ç°
       â”œâ”€â”€ cogvlm2_config.py         # æ¨¡å‹é…ç½®é¡¹
       â”œâ”€â”€ cogvlm2image_llm.py       # cogvlm2 è¯­è¨€æ¨¡å‹å®ç°
       â”œâ”€â”€ cogvlm2image_processor.py # cogvlm2 æ•°æ®é¢„å¤„ç†
       â””â”€â”€ cogvlm2_tokenizer.py      # cogvlm2 tokenizer
   ```

2. æ¨¡å‹é…ç½®ï¼š

   ```text
   configs/cogvlm2
       â””â”€â”€ predict_cogvlm2_image_llama3_chat_19b.yaml  # æ¨¡å‹æ¨ç†å¯åŠ¨é…ç½®
   ```

## ç¯å¢ƒåŠæ•°æ®å‡†å¤‡

### å®‰è£…ç¯å¢ƒ

MindFormersè½¯ç¡¬ä»¶é…å¥—å…³ç³»ä»¥åŠå®‰è£…å‚è€ƒ[ç¯å¢ƒå®‰è£…æŒ‡å—](../../README.md#æºç ç¼–è¯‘å®‰è£…)å’Œ[ç‰ˆæœ¬åŒ¹é…å…³ç³»](../../README.md#ç‰ˆæœ¬åŒ¹é…å…³ç³»)ã€‚

### æ•°æ®åŠæƒé‡å‡†å¤‡

#### æ¨¡å‹æƒé‡ä¸‹è½½

MindFormersæä¾›HuggingFaceå®˜æ–¹æƒé‡ä¸‹è½½é“¾æ¥ï¼Œç”¨æˆ·å¯ä¸‹è½½æƒé‡å¹¶ç»è¿‡[æ¨¡å‹æƒé‡è½¬æ¢](#æ¨¡å‹æƒé‡è½¬æ¢)åè¿›è¡Œä½¿ç”¨ã€‚

è¯è¡¨ä¸‹è½½é“¾æ¥ï¼š[tokenizer.model](https://huggingface.co/meta-llama/Meta-Llama-3-8B)

> è¯¥tokenizerä¸llama3æ¨¡å‹ç›¸åŒï¼Œè¯·è‡ªè¡Œç”³è¯·huggingfaceä¸Šllama3ä½¿ç”¨æƒé™è¿›è¡Œä¸‹è½½ã€‚

| æ¨¡å‹åç§°                    | MindSporeæƒé‡ |                        HuggingFaceæƒé‡                         |
|:------------------------|:-----------:|:------------------------------------------------------------:|
| cogvlm2-llama3-chat-19B |      -      | [Link](https://huggingface.co/THUDM/cogvlm2-llama3-chat-19B) |

#### æ¨¡å‹æƒé‡è½¬æ¢

æ‰§è¡Œ`convert_weight.py`è½¬æ¢è„šæœ¬ï¼Œå°†HuggingFaceçš„æƒé‡è½¬æ¢ä¸ºå®Œæ•´çš„ckptæƒé‡ã€‚

```shell
pip install transformers torch
python convert_weight.py --modal image --model cogvlm2 --input_path TORCH_CKPT_DIR --output_path {path}/MS_CKPT_NAME --dtype 'fp16'

# å‚æ•°è¯´æ˜
modal:       æ¨¡å‹æ¨¡æ€, è¯¥æ¨¡å‹è¾“å…¥'image'
model:       æ¨¡å‹åç§°
input_path:  ä¸‹è½½HuggingFaceæƒé‡çš„æ–‡ä»¶å¤¹è·¯å¾„
output_path: è½¬æ¢åçš„MindSporeæƒé‡æ–‡ä»¶ä¿å­˜è·¯å¾„
dtype:       è½¬æ¢åçš„MindSporeæƒé‡å‚æ•°ç±»å‹
```

## æ¨ç†

MindFormersæä¾›`cogvlm2-llama3-chat-19B`çš„æ¨ç†ç¤ºä¾‹ï¼Œæ”¯æŒå•å¡æ¨ç†ã€å¤šå¡æ¨ç†ã€‚

### å•å¡æ¨ç†

1. ä¿®æ”¹æ¨¡å‹é…ç½®æ–‡ä»¶`configs/cogvlm2/predict_cogvlm2_image_llama3_chat_19b.yaml`

   ```yaml
   model:
     model_config:
       use_past: True                         # å¼€å¯å¢é‡æ¨ç†
       is_dynamic: False                      # å…³é—­åŠ¨æ€shape

     tokenizer:
       vocab_file: "/{path}/tokenizer.model"  # æŒ‡å®štokenizeræ–‡ä»¶è·¯å¾„
   ```

2. å¯åŠ¨æ¨ç†è„šæœ¬

   ```shell
   python run_mindformer.py \
    --config configs/cogvlm2/predict_cogvlm2_image_llama3_chat_19b.yaml \
    --run_mode predict \
    --predict_data "/path/image.jpg" "Please describe this image." \
    --modal_type image text \
    --load_checkpoint /{path}/cogvlm2-image-llama3-chat.ckpt

   # å‚æ•°è¯´æ˜
   config:          æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„
   run_mode:        æ¨¡å‹æ‰§è¡Œæ¨¡å¼, 'predict'è¡¨ç¤ºæ¨ç†
   predict_data:    æ¨¡å‹æ¨ç†è¾“å…¥, ç¬¬ä¸€ä¸ªè¾“å…¥æ˜¯å›¾ç‰‡è·¯å¾„, ç¬¬äºŒä¸ªè¾“å…¥æ˜¯æ–‡æœ¬
   modal_type:      æ¨¡å‹æ¨ç†è¾“å…¥å¯¹åº”æ¨¡æ€, å›¾ç‰‡è·¯å¾„å¯¹åº”'image', æ–‡æœ¬å¯¹åº”'text'
   load_checkpoint: æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„
   ```

### å¤šå¡æ¨ç†

1. ä¿®æ”¹æ¨¡å‹é…ç½®æ–‡ä»¶`configs/cogvlm2/predict_cogvlm2_image_llama3_chat_19b.yaml`

   ```yaml
   auto_trans_ckpt: True                      # å¼€å¯æƒé‡è‡ªåŠ¨è½¬æ¢
   use_parallel: True
   parallel_config:
     model_parallel: 2                        # å¯æ ¹æ®ä½¿ç”¨deviceæ•°è¿›è¡Œä¿®æ”¹

   model:
     model_config:
       use_past: True                         # å¼€å¯å¢é‡æ¨ç†
       is_dynamic: False                      # å…³é—­åŠ¨æ€shape

     tokenizer:
       vocab_file: "/{path}/tokenizer.model"  # æŒ‡å®štokenizeræ–‡ä»¶è·¯å¾„
   ```

2. å¯åŠ¨æ¨ç†è„šæœ¬

   ```shell
   bash scripts/msrun_launcher.sh "run_mindformer.py \
    --config configs/cogvlm2/predict_cogvlm2_image_llama3_chat_19b.yaml \
    --run_mode predict \
    --predict_data \"/path/image.jpg\" \"Please describe this image.\" \
    --modal_type image text \
    --load_checkpoint /{path}/cogvlm2-image-llama3-chat.ckpt" 2
   ```

****