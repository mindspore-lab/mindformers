# Llama 2

> ## ğŸš¨ å¼ƒç”¨è¯´æ˜
>
> æœ¬æ¨¡å‹å·²è¿‡æ—¶ï¼Œä¸å†è¿›è¡Œç»´æŠ¤ï¼Œå¹¶å°†åœ¨ *1.5.0* ä¹‹åçš„ç‰ˆæœ¬ä¸‹æ¶ã€‚å¦‚éœ€ä½¿ç”¨æ­¤æ¨¡å‹ï¼Œå»ºè®®æ ¹æ®å®˜æ–¹æ–‡æ¡£ä¸­çš„ **[æ¨¡å‹åº“](https://www.mindspore.cn/mindformers/docs/zh-CN/dev/start/models.html)** é€‰æ‹©åˆé€‚çš„ç‰ˆæœ¬è¿›è¡Œä½¿ç”¨ã€‚
>
> å¦‚æœ‰ä»»ä½•é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·é€šè¿‡ **[ç¤¾åŒºIssue](https://gitee.com/mindspore/mindformers/issues/new)** æäº¤åé¦ˆã€‚æ„Ÿè°¢æ‚¨çš„ç†è§£ä¸æ”¯æŒï¼

## æ¨¡å‹æè¿°

Llama 2ï¼Œæ˜¯MetaåŸºäºLLaMA 1çš„æ›´æ–°ç‰ˆæœ¬ï¼ŒåŸºäºæ–°çš„å…¬å¼€å¯ç”¨æ•°æ®æ··åˆè¿›è¡Œè®­ç»ƒï¼ŒåŒæ—¶å°†é¢„è®­ç»ƒè¯­æ–™åº“çš„å¤§å°å¢åŠ äº†40%ï¼Œæœ€åå°†æ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦ç¿»å€ï¼ˆç”±2048æé«˜åˆ°4096ï¼‰ï¼Œå¹¶é‡‡ç”¨äº†åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›æœºåˆ¶ã€‚Llama 2æ¨¡å‹æ˜¯ç±»GPTæ¨¡å‹ï¼Œæ˜¯ä¸€ä¸ªç”Ÿæˆå¼çš„è¯­è¨€æ¨¡å‹ï¼Œä¸»è¦æ˜¯ç”¨äºé¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ã€‚Llama 2æŒ‰ç…§å‚æ•°é‡ï¼Œç›®å‰æœ‰ä¸‰ä¸ªç‰ˆæœ¬ï¼šLlama 2-7Bï¼ˆ7Bï¼‰ã€Llama 2-13Bï¼ˆ13Bï¼‰ã€Llama 2-70Bï¼ˆ70Bï¼‰ï¼Œæœ¬ä»“åº“å·²å…¨éƒ¨æ”¯æŒä¸‰ç‰ˆæƒé‡ï¼Œæƒé‡æ–‡ä»¶æ¥æºäºMetaLLama2ã€‚Llama 2 çš„7Bå’Œ13B æ¨¡å‹ç»“æ„ä¸LLaMA 1ä¸€è‡´ï¼Œ70B åˆ™åŠ å…¥åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆGQAï¼‰ã€‚

[Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)

``` text
@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}
```

## æ¨¡å‹æ€§èƒ½

ä»¥ä¸‹æ¨¡å‹æ€§èƒ½å‡ç”±Atlas 800T A2ç¡¬ä»¶ç¯å¢ƒä¸‹æµ‹è¯•å¾—å‡ºã€‚

| Config                                                                   |      Task       | Datasets | SeqLength | DataType |  Phase   |   Performance   |
|:-------------------------------------------------------------------------|:---------------:|:--------:|:---------:|:--------:|:--------:|:---------------:|
| [llama2_7b](../../configs/llama2/pretrain_llama2_7b_bf16.yaml)           | text_generation |  alpaca  |   4096    | bfloat16 | Finetune | 4160 tokens/s/p |
| [llama2_7b](../../configs/llama2/finetune_llama2_7b.yaml)                | text_generation |  alpaca  |   4096    | float16  | Finetune | 3484 tokens/s/p |
| [llama2_13b](../../configs/llama2/finetune_llama2_13b_bf16.yaml)         | text_generation |  alpaca  |   4096    | bfloat16 | Finetune | 1691 tokens/s/p |
| [llama2_13b_lora](../../configs/llama2/lora_llama2_13b.yaml)             | text_generation |  alpaca  |   4096    | float16  |   LoRA   | 2193 tokens/s/p |
| [llama2_70b_32p](../../configs/llama2/finetune_llama2_70b_bf16_32p.yaml) | text_generation |  alpaca  |   4096    | bfloat16 | Finetune | 337 tokens/s/p  |
| [llama2_7b](../../configs/llama2/predict_llama2_7b.yaml)                 | text_generation |    -     |   4096    |    -     | Predict  |  332 tokens/s   |
| [llama2_13b](../../configs/llama2/predict_llama2_13b.yaml)               | text_generation |    -     |   4096    |    -     | Predict  |  420 tokens/s   |
| [llama2_70b](../../configs/llama2/predict_llama2_70b.yaml)               | text_generation |    -     |   4096    |    -     | Predict  |  522 tokens/s   |

ä»¥ä¸‹æ¨¡å‹æ€§èƒ½å‡ç”±Atlas 900 A2 PoDcç¡¬ä»¶ç¯å¢ƒä¸‹æµ‹è¯•å¾—å‡ºã€‚

| Config                                                                   |      Task       | Datasets | SeqLength | DataType |  Phase   |   Performance   |
|:-------------------------------------------------------------------------|:---------------:|:--------:|:---------:|:--------:|:--------:|:---------------:|
| [llama2_13b](../../configs/llama2/finetune_llama2_13b_bf16.yaml)         | text_generation |  alpaca  |   4096    | bfloat16 | Finetune | 1945 tokens/s/p |
| [llama2_13b](../../configs/llama2/finetune_llama2_13b.yaml)              | text_generation |  alpaca  |   4096    | float16  | Finetune | 1911 tokens/s/p |
| [llama2_70b_32p](../../configs/llama2/finetune_llama2_70b_bf16_32p.yaml) | text_generation |  alpaca  |   4096    | bfloat16 | Finetune | 404 tokens/s/p  |
| [llama2_70b_64p](../../configs/llama2/finetune_llama2_70b_bf16_64p.yaml) | text_generation |  alpaca  |   4096    | bfloat16 | Finetune | 405 tokens/s/p  |

## æ¨¡å‹æ–‡ä»¶

`Llama 2`åŸºäº`mindformers`å®ç°ï¼Œä¸»è¦æ¶‰åŠçš„æ–‡ä»¶æœ‰ï¼š

1. æ¨¡å‹å…·ä½“å®ç°ï¼š

   ```bash
   mindformers/models/llama
       â”œâ”€â”€ __init__.py
       â”œâ”€â”€ llama.py                  # æ¨¡å‹å®ç°
       â”œâ”€â”€ llama_config.py           # æ¨¡å‹é…ç½®é¡¹
       â”œâ”€â”€ llama_layer.py            # llamaç½‘ç»œå±‚å®šä¹‰
       â”œâ”€â”€ llama_processor.py        # llamaé¢„å¤„ç†
       â”œâ”€â”€ llama_tokenizer.py        # tokenizer
       â””â”€â”€ llama_transformer.py      # transformerå±‚å®ç°
   ```

2. æ¨¡å‹é…ç½®ï¼š

   ```bash
   configs/llama2
       â”œâ”€â”€ predict_llama2_7b.yaml          # 7bæ¨¡å‹æ¨ç†å¯åŠ¨é…ç½®
       â”œâ”€â”€ predict_llama2_13b.yaml         # 13bæ¨¡å‹æ¨ç†å¯åŠ¨é…ç½®
       â”œâ”€â”€ predict_llama2_70b.yaml         # 70bæ¨¡å‹æ¨ç†å¯åŠ¨é…ç½®
       â”œâ”€â”€ pretrain_llama2_7b.yaml         # 7bæ¨¡å‹é¢„è®­ç»ƒå¯åŠ¨é…ç½®
       â”œâ”€â”€ pretrain_llama2_13b.yaml        # 13bæ¨¡å‹é¢„è®­ç»ƒå¯åŠ¨é…ç½®
       â”œâ”€â”€ pretrain_llama2_70b.yaml        # 70bæ¨¡å‹é¢„è®­ç»ƒå¯åŠ¨é…ç½®
       â”œâ”€â”€ finetune_llama2_7b.yaml         # 7bæ¨¡å‹å…¨é‡å¾®è°ƒå¯åŠ¨é…ç½®
       â”œâ”€â”€ finetune_llama2_13b.yaml        # 13bæ¨¡å‹å…¨é‡å¾®è°ƒå¯åŠ¨é…ç½®
       â””â”€â”€ finetune_llama2_70b.yaml        # 70bæ¨¡å‹å…¨é‡å¾®è°ƒå¯åŠ¨é…ç½®
   ```

3. æ•°æ®é¢„å¤„ç†è„šæœ¬ï¼š

   ```bash
   mindformers/tools/dataset_preprocess/llama
       â”œâ”€â”€ alpaca_converter.py     # åŸºäºfschatçš„alpacaæ•°æ®é›†æ ¼å¼è½¬æ¢è„šæœ¬
       â”œâ”€â”€ llama_preprocess.py     # llamaæ¨¡å‹çš„mindrecordæ•°æ®å¤„ç†è„šæœ¬
       â””â”€â”€ squad_data_process.py   # squadæ•°æ®é›†æ ¼å¼è½¬æ¢è„šæœ¬
   ```

## ç¯å¢ƒåŠæ•°æ®å‡†å¤‡

### å®‰è£…ç¯å¢ƒ

MindFormersè½¯ç¡¬ä»¶é…å¥—å…³ç³»ä»¥åŠå®‰è£…å‚è€ƒ[ç¯å¢ƒå®‰è£…æŒ‡å—](../../README.md#æºç ç¼–è¯‘å®‰è£…)å’Œ[ç‰ˆæœ¬åŒ¹é…å…³ç³»](../../README.md#ç‰ˆæœ¬åŒ¹é…å…³ç³»)ã€‚

> æ³¨ï¼šAtlas 800T A2èŠ¯ç‰‡æ”¯æŒ7b,13bå•æœºå•å¡æ¨ç†ï¼Œ70bæ¨ç†è‡³å°‘ä½¿ç”¨8å¡ï¼Œå…¨å‚å¾®è°ƒè‡³å°‘éœ€è¦4æœº32å¡ï¼Œæ¨èä½¿ç”¨8æœº64å¡ã€‚

### æ•°æ®åŠæƒé‡å‡†å¤‡

#### æ•°æ®é›†ä¸‹è½½

MindFormersæä¾›**Wikitext2**ä½œä¸º[é¢„è®­ç»ƒ](#é¢„è®­ç»ƒ)æ•°æ®é›†å’ŒPPLè¯„æµ‹æ•°æ®é›†ï¼Œ**alpaca**ä½œä¸º[å¾®è°ƒ](#å¾®è°ƒ)æ•°æ®é›†ï¼Œ**SQuAD1.1**ä¸ºé˜…è¯»ç†è§£è¯„æµ‹æ•°æ®é›†ã€‚

| æ•°æ®é›†åç§°     |                    é€‚ç”¨æ¨¡å‹                     |          é€‚ç”¨é˜¶æ®µ           |                                        ä¸‹è½½é“¾æ¥                                        |
|:----------|:-------------------------------------------:|:-----------------------:|:----------------------------------------------------------------------------------:|
| Wikitext2 | llama2-7b <br/> llama2-13b <br/> llama2-70b | Pretrain <br/> Evaluate | [Link](https://www.mindspore.cn/mindformers/docs/zh-CN/dev/faq/func_related.html)   |
| alpaca    | llama2-7b <br/> llama2-13b <br/> llama2-70b |        Finetune         |  [Link](https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json)   |
| SQuAD 1.1 | llama2-7b <br/> llama2-13b <br/> llama2-70b |        Evaluate         |                    [Link](https://data.deepai.org/squad1.1.zip)                    |

æ•°æ®é¢„å¤„ç†ä¸­æ‰€ç”¨çš„`tokenizer.model`å¯ä»¥å‚è€ƒ[æ¨¡å‹æƒé‡ä¸‹è½½](#æ¨¡å‹æƒé‡ä¸‹è½½)è¿›è¡Œä¸‹è½½ã€‚

- **Wikitext2 æ•°æ®é¢„å¤„ç†â€”é¢„è®­ç»ƒ**

  ä½¿ç”¨`mindformers/tools/dataset_preprocess/llama/llama_preprocess.py`å¯¹ä¸‹è½½åçš„æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼Œå¹¶ç”ŸæˆMindrecordæ•°æ®ã€‚

  ```shell
  python mindformers/tools/dataset_preprocess/llama/llama_preprocess.py \
    --dataset_type wiki \
    --input_glob /{path}/wiki.train.tokens \
    --model_file /{path}/tokenizer.model \
    --seq_length 4096 \
    --output_file /{path}/wiki4096.mindrecord

  # å‚æ•°è¯´æ˜
  dataset_type: é¢„å¤„ç†æ•°æ®ç±»å‹
  input_glob:   è¾“å…¥ä¸‹è½½åwiki.train.tokensçš„æ–‡ä»¶è·¯å¾„
  model_file:   æ¨¡å‹tokenizer.modelæ–‡ä»¶è·¯å¾„
  seq_length:   è¾“å‡ºæ•°æ®çš„åºåˆ—é•¿åº¦
  output_file:  è¾“å‡ºæ–‡ä»¶çš„ä¿å­˜è·¯å¾„
  ```

  > æ³¨ï¼š`bos`, `eos`, `pad`ç­‰ç‰¹æ®Š`ids`è¦å’Œ`yaml`é…ç½®æ–‡ä»¶ä¸­`model_config`éƒ¨åˆ†ä¿æŒä¸€è‡´ï¼Œé»˜è®¤`bos_token_id=1`, `eos_token_id=2`, `pad_token_id=0`ã€‚
å¦‚æœæœ‰æ‰€ä¿®æ”¹ï¼Œé…ç½®æ–‡ä»¶ä¸­å¯¹åº”è®¾ç½®ä¹Ÿéœ€è¦ä¿®æ”¹ï¼Œé€šå¸¸é¢„è®­ç»ƒæ•°æ®ä¸åŒ…å«`pad_token`ï¼Œå› æ­¤å»ºè®®è®¾ç½®`pad_token_id=-1`ã€‚

- **Wikitext2 æ•°æ®é¢„å¤„ç†â€”è¯„æµ‹**

  ä½¿ç”¨`mindformers/tools/dataset_preprocess/llama/llama_preprocess.py`å¯¹ä¸‹è½½åçš„æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼Œå¹¶ç”ŸæˆMindrecordæ•°æ®ã€‚

  ```shell
   python mindformers/tools/dataset_preprocess/llama/llama_preprocess.py \
    --dataset_type wiki \
    --input_glob  /{path}/wiki.valid.tokens \
    --model_file /{path}/tokenizer.model \
    --seq_length 4095 \
    --output_file /{path}/wiki4096.mindrecord

    # å‚æ•°è¯´æ˜
  dataset_type: é¢„å¤„ç†æ•°æ®ç±»å‹
  input_glob:   è¾“å…¥ä¸‹è½½åwiki.valid.tokensçš„æ–‡ä»¶è·¯å¾„
  model_file:   æ¨¡å‹tokenizer.modelæ–‡ä»¶è·¯å¾„
  seq_length:   è¾“å‡ºæ•°æ®çš„åºåˆ—é•¿åº¦
  output_file:  è¾“å‡ºæ–‡ä»¶çš„ä¿å­˜è·¯å¾„
  ```

- **alpaca æ•°æ®é¢„å¤„ç†**

  1. æ‰§è¡Œ`mindformers/tools/dataset_preprocess/llama/alpaca_converter.py`ï¼Œä½¿ç”¨fastchatå·¥å…·æ·»åŠ promptsæ¨¡æ¿ï¼Œå°†åŸå§‹æ•°æ®é›†è½¬æ¢ä¸ºå¤šè½®å¯¹è¯æ ¼å¼ã€‚

     ```shell
     python mindformers/tools/dataset_preprocess/llama/alpaca_converter.py \
       --data_path /{path}/alpaca_data.json \
       --output_path /{path}/alpaca-data-conversation.json

     # å‚æ•°è¯´æ˜
     data_path:   è¾“å…¥ä¸‹è½½çš„æ–‡ä»¶è·¯å¾„
     output_path: è¾“å‡ºæ–‡ä»¶çš„ä¿å­˜è·¯å¾„
     ```

  2. æ‰§è¡Œ`mindformers/tools/dataset_preprocess/llama/llama_preprocess.py`ï¼Œç”ŸæˆMindrecordæ•°æ®ï¼Œå°†å¸¦æœ‰promptæ¨¡æ¿çš„æ•°æ®è½¬æ¢ä¸ºmindrecordæ ¼å¼ã€‚

     ```shell
     # æ­¤å·¥å…·ä¾èµ–fschatå·¥å…·åŒ…è§£æpromptæ¨¡æ¿, è¯·æå‰å®‰è£…fschat >= 0.2.13 python = 3.9
     python mindformers/tools/dataset_preprocess/llama/llama_preprocess.py \
       --dataset_type qa \
       --input_glob /{path}/alpaca-data-conversation.json \
       --model_file /{path}/tokenizer.model \
       --seq_length 4096 \
       --output_file /{path}/alpaca-fastchat4096.mindrecord

     # å‚æ•°è¯´æ˜
     dataset_type: é¢„å¤„ç†æ•°æ®ç±»å‹
     input_glob:   è½¬æ¢åçš„alpacaçš„æ–‡ä»¶è·¯å¾„
     model_file:   æ¨¡å‹tokenizer.modelæ–‡ä»¶è·¯å¾„
     seq_length:   è¾“å‡ºæ•°æ®çš„åºåˆ—é•¿åº¦
     output_file:  è¾“å‡ºæ–‡ä»¶çš„ä¿å­˜è·¯å¾„
     ```

- **SQuAD 1.1 æ•°æ®é¢„å¤„ç†**

  æ‰§è¡Œ`mindformers/tools/dataset_preprocess/llama/squad_data_process.py`ç”ŸæˆMindrecordæ•°æ®

  ```shell
  python mindformers/tools/dataset_preprocess/llama/squad_data_process.py \
    --input_file /{path}/squad/dev-v1.1.json \
    --output_file /{path}/squad2048.mindrecord \
    --mode eval \
    --max_length 2048 \
    --tokenizer_type "llama2_7b"
  ```

#### æ¨¡å‹æƒé‡ä¸‹è½½

MindFormersæä¾›å·²ç»è½¬æ¢å®Œæˆçš„é¢„è®­ç»ƒæƒé‡ã€è¯è¡¨æ–‡ä»¶ç”¨äºé¢„è®­ç»ƒã€å¾®è°ƒå’Œæ¨ç†ï¼Œç”¨æˆ·ä¹Ÿå¯ä»¥ä¸‹è½½HuggingFaceå®˜æ–¹æƒé‡ç»è¿‡[æ¨¡å‹æƒé‡è½¬æ¢](#æ¨¡å‹æƒé‡è½¬æ¢)åè¿›è¡Œä½¿ç”¨ã€‚

è¯è¡¨ä¸‹è½½é“¾æ¥ï¼š[tokenizer.model](https://ascend-repo-modelzoo.obs.cn-east-2.myhuaweicloud.com/MindFormers/llama2/tokenizer.model)

| æ¨¡å‹åç§°            |                                                 MindSporeæƒé‡                                                  |                      HuggingFaceæƒé‡                       |
|:----------------|:------------------------------------------------------------------------------------------------------------:|:--------------------------------------------------------:|
| llama2-7b       |    [Link](https://ascend-repo-modelzoo.obs.cn-east-2.myhuaweicloud.com/MindFormers/llama2/llama2_7b.ckpt)    | [Link](https://huggingface.co/meta-llama/Llama-2-7b-hf)  |
| llama2-13b      | [Link](https://ascend-repo-modelzoo.obs.cn-east-2.myhuaweicloud.com/MindFormers/llama2/llama2-13b-fp16.ckpt) | [Link](https://huggingface.co/meta-llama/Llama-2-13b-hf) |
| llama2-70b      |                                                      /                                                       | [Link](https://huggingface.co/meta-llama/Llama-2-70b-hf) |

> æ³¨ï¼šLlama2çš„æ‰€æœ‰æƒé‡éƒ½éœ€è¦é€šè¿‡å‘Meta[æäº¤ç”³è¯·](https://ai.meta.com/resources/models-and-libraries/llama-downloads)æ¥è·å–ï¼Œå¦‚æœ‰éœ€è¦è¯·è‡ªè¡Œç”³è¯·ã€‚

#### æ¨¡å‹æƒé‡è½¬æ¢

æ‰§è¡Œ`convert_weight.py`è½¬æ¢è„šæœ¬ï¼Œå°†HuggingFaceçš„æƒé‡è½¬æ¢ä¸ºå®Œæ•´çš„ckptæƒé‡ã€‚

```shell
python convert_weight.py --model llama --input_path TORCH_CKPT_DIR --output_path {path}/MS_CKPT_NAME

# å‚æ•°è¯´æ˜
model:       æ¨¡å‹åç§°
input_path:  ä¸‹è½½HuggingFaceæƒé‡çš„æ–‡ä»¶å¤¹è·¯å¾„
output_path: è½¬æ¢åçš„MindSporeæƒé‡æ–‡ä»¶ä¿å­˜è·¯å¾„
```

## é¢„è®­ç»ƒ

MindFormersæä¾›`llama2-7b`å•æœºå¤šå¡ä»¥åŠ`llama2_13b`å¤šæœºå¤šå¡çš„é¢„è®­ç»ƒç¤ºä¾‹ï¼Œè¿‡ç¨‹ä¸­ä½¿ç”¨`Wikitext2`æ•°æ®é›†å¯¹æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œæ•°æ®é›†å¯ä»¥å‚è€ƒ[æ•°æ®é›†ä¸‹è½½](#æ•°æ®é›†ä¸‹è½½)è·å¾—ã€‚

### å•æœºè®­ç»ƒ

ä»¥Llama2-7bä¸ºä¾‹ï¼Œæ‰§è¡Œmsrunå¯åŠ¨è„šæœ¬ï¼Œè¿›è¡Œ8å¡åˆ†å¸ƒå¼è®­ç»ƒã€‚

```shell
bash scripts/msrun_launcher.sh "run_mindformer.py \
 --config configs/llama2/pretrain_llama2_7b.yaml \
 --train_dataset_dir /{path}/wiki4096.mindrecord \
 --use_parallel True \
 --run_mode train" 8
```

åœ¨`llama2_70b`é¢„è®­ç»ƒä¸­ï¼Œå¯ä»¥é€šè¿‡å¦‚ä¸‹æ–¹å¼æå‡æ¨¡å‹æ€§èƒ½ï¼š

1. ä¿®æ”¹é…ç½®æ–‡ä»¶ä¸­`qkv_concat=True`, `micro_batch_num=256`
2. åˆ›å»º`parallel_speed_up.jsonæ–‡ä»¶`ï¼Œæ–‡ä»¶å†…å®¹å¦‚ä¸‹

   ```json
   {
     "recompute_comm_overlap": false,
     "matmul_grad_comm_overlap": true,
     "enable_task_opt": false,
     "enable_grad_comm_opt": false,
     "enable_opt_shard_comm_opt": false,
     "enable_concat_eliminate_opt": false,
     "enable_begin_end_inline_opt": false,
     "compute_communicate_fusion_level": 0
   }
   ```

   åŒæ—¶åœ¨é…ç½®æ–‡ä»¶`context`éƒ¨åˆ†æ·»åŠ `ascend_config`

   ```yaml
   context:
     ascend_config:
       parallel_speed_up_json_path: "/{path}/parallel_speed_up.json"
   ```

> å¦‚æœæŠ¥é”™æç¤ºæ˜¾å­˜ä¸è¶³ï¼Œå¯ä»¥é€šè¿‡`export HCCL_BUFFSIZE=100`å°†å¯¹åº”ç¯å¢ƒå˜é‡ä¸‹è°ƒè‡³100ã€‚

`ymal`é…ç½®æ–‡ä»¶ä¸­å„å‚æ•°å«ä¹‰è¯¦è§[Configé…ç½®è¯´æ˜](../../configs/README.md)ï¼Œ`parallel_speed_up`å„å‚æ•°å«ä¹‰è¯¦è§[parallel_speed_upè¯´æ˜](https://www.mindspore.cn/docs/zh-CN/r2.3.0/api_python/mindspore/mindspore.set_context.html#mindspore.set_context)ã€‚

### å¤šæœºè®­ç»ƒ

ä»¥Llama2-13bä¸ºä¾‹ï¼Œæ‰§è¡Œ2æœº16å¡é¢„è®­ç»ƒã€‚

1. æ ¹æ®ä½¿ç”¨èŠ‚ç‚¹æ•°ç­‰ä¿¡æ¯ï¼Œä¿®æ”¹ç›¸åº”çš„é…ç½®æ–‡ä»¶`configs/llama2/pretrain_llama2_13b.yaml`

   ```yaml
   parallel_config:
     data_parallel: 2
     model_parallel: 4
     pipeline_stage: 2
     micro_batch_num: 16
     vocab_emb_dp: True
     gradient_aggregation_group: 4
   ```

   > æ³¨ï¼šå¦‚ä½¿ç”¨èŠ‚ç‚¹æ•°å’Œå¡æ•°æ”¹å˜éœ€è¦ä¿®æ”¹`data_parallel`, `model_parallel`, `pipeline_stage`æ»¡è¶³å®é™…è¿è¡Œçš„å¡æ•° `device_num=data_parallelÃ—model_parallelÃ—pipeline_stage`ï¼Œ
åŒæ—¶æ»¡è¶³`micro_batch_num >= pipeline_stage`ã€‚

2. æ‰§è¡Œmsrunå¯åŠ¨è„šæœ¬

   å¤šæœºå¤šå¡æ‰§è¡Œè„šæœ¬è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒéœ€è¦åˆ†åˆ«åœ¨ä¸åŒèŠ‚ç‚¹è¿è¡Œè„šæœ¬ï¼Œå¹¶å°†å‚æ•°`MASTER_ADDR`è®¾ç½®ä¸ºä¸»èŠ‚ç‚¹çš„ipåœ°å€ï¼Œæ‰€æœ‰èŠ‚ç‚¹è®¾ç½®çš„ipåœ°å€ç›¸åŒï¼Œä¸åŒèŠ‚ç‚¹ä¹‹é—´ä»…å‚æ•°`NODE_RANK`ä¸åŒï¼Œå„ä¸ªå‚æ•°ä½ç½®å«ä¹‰å‚è§msrunå¿«é€Ÿå¯åŠ¨ã€‚

   ```shell
   # èŠ‚ç‚¹0ä½œä¸ºä¸»èŠ‚ç‚¹, {ip_addr}å¤„å¡«å†™èŠ‚ç‚¹0å®é™…ip, æ€»å…±16å¡ä¸”æ¯ä¸ªèŠ‚ç‚¹8å¡
   bash scripts/msrun_launcher.sh "run_mindformer.py \
     --config {CONFIG_PATH} \
     --train_dataset_dir /{path}/wiki4096.mindrecord \
     --use_parallel True \
     --run_mode {train}" \
     16 8 {ip_addr} 8118 0 output/msrun_log False 300

   # èŠ‚ç‚¹1ï¼Œ{ip_addr}å¤„å¡«å†™èŠ‚ç‚¹0å®é™…ipï¼ŒèŠ‚ç‚¹0ä¸èŠ‚ç‚¹1å¯åŠ¨å‘½ä»¤ä»…å‚æ•°NODE_RANKä¸åŒ
   bash scripts/msrun_launcher.sh "run_mindformer.py \
     --config {CONFIG_PATH} \
     --train_dataset_dir /{path}/wiki4096.mindrecord \
     --use_parallel True \
     --run_mode {train}" \
     16 8 {ip_addr} 8118 1 output/msrun_log False 300
   ```

3. å¯¹äºllama2-70bæ¨¡å‹ï¼Œå†è®­ç»ƒä¹‹å‰è¯·å®šä¹‰ä»¥ä¸‹ç¯å¢ƒå˜é‡ã€‚

   ```shell
   export MS_DEV_SIDE_EFFECT_LOAD_ELIM=3 # ä¼˜åŒ–æ˜¾å­˜
   ```

## å¾®è°ƒ

MindFormersæä¾›`Llama2-7b`çš„å¾®è°ƒç¤ºä¾‹ï¼Œè¿‡ç¨‹ä¸­ä½¿ç”¨`alpaca`æ•°æ®é›†å¯¹æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œæ•°æ®é›†å¯ä»¥å‚è€ƒ[æ•°æ®é›†ä¸‹è½½](#æ•°æ®é›†ä¸‹è½½)è·å¾—ã€‚

### å…¨å‚å¾®è°ƒ

#### å•æœºè®­ç»ƒ

ä»¥Llama2-7bä¸ºä¾‹ï¼Œæ‰§è¡Œmsrunå¯åŠ¨è„šæœ¬ï¼Œè¿›è¡Œ8å¡åˆ†å¸ƒå¼è®­ç»ƒã€‚

```shell
bash scripts/msrun_launcher.sh "run_mindformer.py \
 --config configs/llama2/finetune_llama2_7b.yaml \
 --load_checkpoint /{path}/llama2_7b.ckpt \
 --train_dataset_dir /{path}/alpaca-fastchat4096.mindrecord \
 --use_parallel True \
 --run_mode finetune" 8
```

#### å¤šæœºè®­ç»ƒ

å¤šæœºå¤šå¡å¾®è°ƒä»»åŠ¡å¯åŠ¨é¢„è®­ç»ƒç±»ä¼¼ï¼Œå¯å‚è€ƒ[é¢„è®­ç»ƒç« èŠ‚](#é¢„è®­ç»ƒ)å¹¶å¯¹å¯åŠ¨å‘½ä»¤è¿›è¡Œå¦‚ä¸‹ä¿®æ”¹ï¼š

1. å¢åŠ è„šæœ¬å…¥å‚`--load_checkpoint /{path}/llama2_7b.ckpt`åŠ è½½é¢„è®­ç»ƒæƒé‡
2. è®¾ç½®å¯åŠ¨è„šæœ¬ä¸­çš„`--train_dataset_dir /{path}/alpaca-fastchat4096.mindrecord`åŠ è½½å¾®è°ƒæ•°æ®é›†
3. è®¾ç½®å¯åŠ¨è„šæœ¬ä¸­çš„`--run_mode finetune`

### LoRAå¾®è°ƒ

LoRAä½å‚å¾®è°ƒç®—æ³•ï¼Œå¯ä»¥å†»ç»“åŸæ¨¡å‹æƒé‡ï¼Œä»…åœ¨å°è§„æ¨¡å‚æ•°é‡ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä½¿å¤§æ¨¡å‹åœ¨å°‘é‡èµ„æºçš„æƒ…å†µä¸‹ä¹Ÿèƒ½è®­ç»ƒã€‚

MindFormersæä¾›`Llama2-7b`çš„LoRAå¾®è°ƒç¤ºä¾‹ï¼Œå¾®è°ƒè¿‡ç¨‹ä¸­ä½¿ç”¨çš„æ•°æ®é›†å¯ä»¥å‚è€ƒ[æ•°æ®é›†ä¸‹è½½](#æ•°æ®é›†ä¸‹è½½)è·å¾—ã€‚

ä»¥Llama2-7bä¸ºä¾‹ï¼Œæ‰§è¡Œmsrunå¯åŠ¨è„šæœ¬ï¼Œè¿›è¡Œ8å¡åˆ†å¸ƒå¼å¾®è°ƒã€‚

```shell
bash scripts/msrun_launcher.sh "run_mindformer.py \
 --config configs/llama2/lora_llama2_7b.yaml \
 --train_dataset_dir /{path}/alpaca-fastchat4096.mindrecord \
 --load_checkpoint /{path}/llama2_7b.ckpt \
 --auto_trans_ckpt False \
 --use_parallel True \
 --run_mode finetune" 8
```

å¦‚æœåŠ è½½åˆ†å¸ƒå¼æƒé‡ï¼ŒåŠ è½½æƒé‡è·¯å¾„åº”è®¾ç½®ä¸ºrank_0çš„ä¸Šä¸€å±‚è·¯å¾„ï¼ŒåŒæ—¶å¼€å¯æƒé‡è‡ªåŠ¨è½¬æ¢åŠŸèƒ½`--auto_trans_ckpt True`ï¼š

```shell
bash scripts/msrun_launcher.sh "run_mindformer.py \
 --config configs/llama2/lora_llama2_7b.yaml \
 --train_dataset_dir /{path}/alpaca-fastchat4096.mindrecord \
 --load_checkpoint /{path}/rank_0/ \
 --auto_trans_ckpt True \
 --use_parallel True \
 --run_mode finetune" 8
```

### PrefixTuningå¾®è°ƒ

PrefixTuningä½å‚å¾®è°ƒç®—æ³•ï¼Œå¯ä»¥å†»ç»“åŸæ¨¡å‹æƒé‡ï¼Œä»…åœ¨kvå‘é‡å‰æ·»åŠ å¯è®­ç»ƒå‰ç¼€å‘é‡è¿›è¡Œè®­ç»ƒï¼Œä½¿å¤§æ¨¡å‹åœ¨å°‘é‡èµ„æºçš„æƒ…å†µä¸‹ä¹Ÿèƒ½è®­ç»ƒã€‚

MindFormersæä¾›`Llama2-7b`çš„PrefixTuningå¾®è°ƒç¤ºä¾‹ï¼Œå¾®è°ƒè¿‡ç¨‹ä¸­ä½¿ç”¨çš„æ•°æ®é›†å¯ä»¥å‚è€ƒ[æ•°æ®é›†ä¸‹è½½](#æ•°æ®é›†ä¸‹è½½)è·å¾—ã€‚

ä»¥Llama2-7bä¸ºä¾‹ï¼Œæ‰§è¡Œmsrunå¯åŠ¨è„šæœ¬ï¼Œè¿›è¡Œ8å¡åˆ†å¸ƒå¼å¾®è°ƒã€‚

> æ³¨ï¼šPrefixTuningå¾®è°ƒä½¿ç”¨æ•°æ®é›†`seq_length=512`ï¼Œæ•°æ®é¢„å¤„ç†æ—¶åº”æŒ‰è¯¥åºåˆ—é•¿åº¦å¯¹æ•°æ®è¿›è¡Œå¤„ç†ã€‚

```shell
bash scripts/msrun_launcher.sh "run_mindformer.py \
 --config configs/llama2/finetune_llama2_7b_prefixtuning.yaml \
 --train_dataset_dir /{path}/alpaca-fastchat512.mindrecord \
 --load_checkpoint /{path}/llama2_7b.ckpt \
 --auto_trans_ckpt False \
 --use_parallel True \
 --run_mode finetune" 8
```

å¦‚æœåŠ è½½åˆ†å¸ƒå¼æƒé‡ï¼ŒåŠ è½½æƒé‡è·¯å¾„åº”è®¾ç½®ä¸ºrank_0çš„ä¸Šä¸€å±‚è·¯å¾„ï¼ŒåŒæ—¶å¼€å¯æƒé‡è‡ªåŠ¨è½¬æ¢åŠŸèƒ½`--auto_trans_ckpt True`ï¼š

```shell
bash scripts/msrun_launcher.sh "run_mindformer.py \
 --config configs/llama2/finetune_llama2_7b_prefixtuning.yaml \
 --train_dataset_dir /{path}/alpaca-fastchat512.mindrecord \
 --load_checkpoint /{path}/rank_0/ \
 --auto_trans_ckpt True \
 --use_parallel True \
 --run_mode finetune" 8
```

### åˆ†å¸ƒå¼è®­ç»ƒæƒé‡åˆå¹¶

åˆ†å¸ƒå¼è®­ç»ƒï¼ˆå¾®è°ƒï¼‰åæ‰€å¾—åˆ°çš„æƒé‡æ–‡ä»¶ä¸ºæ ¹æ®ç­–ç•¥åˆ‡åˆ†åçš„æƒé‡ï¼Œå¯ä»¥æ‰‹åŠ¨å°†åˆ‡åˆ†æƒé‡åˆä¸€ï¼Œä»¥ç”¨äºè¯„ä¼°å’Œæ¨ç†ã€‚

MindFormersæä¾›è‡ªåŠ¨æƒé‡è½¬æ¢å’Œç¦»çº¿æƒé‡è½¬æ¢åŠŸèƒ½ï¼Œå¯å‚è€ƒ[è‡ªåŠ¨è½¬æ¢æ¡ˆä¾‹](../feature_cards/Transform_Ckpt.md#è‡ªåŠ¨è½¬æ¢æ¡ˆä¾‹)å’Œ[ç¦»çº¿æƒé‡è½¬æ¢](../feature_cards/Transform_Ckpt.md#ç¦»çº¿æƒé‡è½¬æ¢)è¿›è¡Œåˆ†å¸ƒå¼æ¨¡å‹æƒé‡è½¬æ¢ã€‚

## æ¨ç†

MindFormersæä¾›`Llama2-7b`çš„å¿«é€Ÿæ¨ç†è„šæœ¬ï¼Œè„šæœ¬ä¸»è¦é€šè¿‡generateé«˜é˜¶æ¥å£å®ç°ï¼Œæ”¯æŒå•å¡ã€å¤šå¡ä»¥åŠå¤šbatchæ¨ç†ã€‚

```shell
# è„šæœ¬ä½¿ç”¨
bash scripts/examples/llama2/run_llama2_predict.sh PARALLEL CONFIG_PATH CKPT_PATH DEVICE_NUM

# å‚æ•°è¯´æ˜
PARALLEL:    æ˜¯å¦ä½¿ç”¨å¤šå¡æ¨ç†, 'single'è¡¨ç¤ºå•å¡æ¨ç†, 'parallel'è¡¨ç¤ºå¤šå¡æ¨ç†
CONFIG_PATH: æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„
CKPT_PATH:   æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„
DEVICE_NUM:  ä½¿ç”¨å¡æ•°, ä»…å¼€å¯å¤šå¡æ¨ç†æ—¶ç”Ÿæ•ˆ
```

### å•å¡æ¨ç†

ä»¥`Llama2-7b`å•å¡æ¨ç†ä¸ºä¾‹ã€‚

```shell
bash scripts/examples/llama2/run_llama2_predict.sh single \
 configs/llama2/predict_llama2_7b.yaml \
 path/to/llama2_7b.ckpt

# å¤šbatchè¾“å‡º
# <s>I love Beijing, because it is a city that is constantly changing. I have been living here for 10 years ...
# <s>LlaMa is a large-scale, open-source, multimodal, multilingual, multitask, and multimodal pretrained ...
# <s>Huawei is a company that has been around for a long time. ...
```

### å¤šå¡æ¨ç†

ä»¥`Llama2-7b`2å¡æ¨ç†ä¸ºä¾‹ã€‚

```shell
bash scripts/examples/llama2/run_llama2_predict.sh parallel \
 configs/llama2/predict_llama2_7b.yaml \
 path/to/llama2_7b.ckpt 2

# å¤šbatchè¾“å‡º
# <s>I love Beijing, because it is a city that is constantly changing. I have been living here for 10 years ...
# <s>LlaMa is a large-scale, open-source, multimodal, multilingual, multitask, and multimodal pretrained ...
# <s>Huawei is a company that has been around for a long time. ...
```

## è¯„æµ‹

ä»¥Llama2_7bä¸ºä¾‹ï¼ŒLlama 2å½“å‰æ”¯æŒä½¿ç”¨based modelï¼ˆåˆå§‹æƒé‡ï¼‰è¿›è¡Œè¯„æµ‹ä»»åŠ¡å¦‚ä¸‹ï¼š

| ä»»åŠ¡ç±»å‹ |    è¯„æµ‹æŒ‡æ ‡    |    æ•°æ®é›†    |
|:----:|:----------:|:---------:|
| æ–‡æœ¬ç”Ÿæˆ | Perplexity | WikiText2 |
| é˜…è¯»ç†è§£ |   Em/F1    | SQuAD 1.1 |

è¯„æµ‹æ—¶åœ¨`vocab_file`é…ç½®ä¸­åŠ å…¥ç›¸åº”`tokenizer.model`çš„è·¯å¾„ï¼Œè‹¥ä½¿ç”¨Atlas 800T A2è¿›è¡Œè¯„æµ‹ï¼Œåˆ™è¿˜éœ€åœ¨é…ç½®ä¸­åŠ å…¥`ascend_config`é…ç½®ã€‚

```yaml
# context config
context:
  ascend_config:
    precision_mode: "must_keep_origin_dtype"

# tokenizer
processor:
  return_tensors: ms
  tokenizer:
    vocab_file: "path/to/tokenizer.model"
```

### æ–‡æœ¬ç”Ÿæˆ

1. è·å–æ•°æ®é›†

   æ–‡æœ¬ç”Ÿæˆä»»åŠ¡è¯„æµ‹ä½¿ç”¨**WikiText2**æ•°æ®é›†ï¼Œå¯é€šè¿‡[æ•°æ®é›†ä¸‹è½½](#æ•°æ®é›†ä¸‹è½½)å¾—åˆ°ï¼Œå¹¶è¿›è¡Œç›¸åº”çš„é¢„å¤„ç†ã€‚

2. ä¿®æ”¹æ¨¡å‹é…ç½®æ–‡ä»¶`configs/llama2/pretrain_llama2_7b_bf16.yaml`

   ```yaml
   metric:
     type: PerplexityMetric
   ```

3. æ‰§è¡Œè¯„æµ‹å‘½ä»¤ï¼ŒæŒ‡æ ‡ä¸ºPPL

   ```shell
   python run_mindformer.py \
     --config configs/llama2/pretrain_llama2_7b_bf16.yaml \
     --eval_dataset_dir /{path}/wiki4096.mindrecord \
     --run_mode eval \
     --load_checkpoint /{path}/llama2_7b.ckpt \
     --epochs 1 \
     --use_parallel False \
     --device_id 0

   # PerplexityMetric = {'PerplexityMetric': {'loss': 2.1142693907022476, 'PPL': 6.58}}
   ```

### é˜…è¯»ç†è§£

1. è·å–æ•°æ®é›†

   é˜…è¯»ç†è§£ä»»åŠ¡è¯„æµ‹ä½¿ç”¨**SQuAD 1.1**æ•°æ®é›†ï¼Œå¯é€šè¿‡[æ•°æ®é›†ä¸‹è½½](#æ•°æ®é›†ä¸‹è½½)å¾—åˆ°ï¼Œå¹¶è¿›è¡Œç›¸åº”çš„é¢„å¤„ç†ã€‚**SQuAD 1.1**ä¸­åŒ…å«é’ˆå¯¹500+æ–‡ç« çš„10ä¸‡+é—®ç­”å¯¹ï¼Œæ˜¯ä¸€ä¸ªé˜…è¯»ç†è§£æ•°æ®é›†ï¼Œç”±ç»´åŸºç™¾ç§‘æ–‡ç« ä¸Šæå‡ºçš„é—®é¢˜ç»„æˆï¼Œå…¶ä¸­æ¯ä¸ªé—®é¢˜çš„ç­”æ¡ˆéƒ½æ˜¯ç›¸åº”æ–‡ç« ä¸­çš„ä¸€æ®µæ–‡æœ¬ã€‚

2. ä¿®æ”¹æ¨¡å‹é…ç½®æ–‡ä»¶`configs/llama2/predict_llama2_7b.yaml`

   ```yaml
   # eval dataset
   eval_dataset:
     data_loader:
       type: MindDataset
       dataset_dir: "/{path}/squad2048.mindrecord"  # å¤„ç†åçš„è¯„æµ‹æ•°æ®é›†è·¯å¾„
       shuffle: False
     input_columns: ["input_ids", "labels"]

   # metric
   metric:
     type: EmF1Metric

   # model config
   model:
     model_config:
       type: LlamaConfig
       batch_size: 1
       seq_length: 2048
       max_decode_length: 700
       max_new_tokens: 20
   ```

3. æ‰§è¡Œè¯„æµ‹å‘½ä»¤ï¼ŒæŒ‡æ ‡ä¸º`Em/F1`

   ```shell
   python run_mindformer.py \
     --config configs/llama2/predict_llama2_7b.yaml \
     --eval_dataset_dir /{path}/squad2048.mindrecord \
     --run_mode eval \
     --load_checkpoint /{path}/llama2_7b.ckpt \
     --epochs 1 \
     --batch_size 1 \
     --use_parallel False \
     --device_id 0

   # F1 score: 60.5, Em score: 39.6, total_count: 2067
   ```

### åˆ†å¸ƒå¼è¯„æµ‹

å¯¹äºè¾ƒå¤§æ¨¡å‹æ¯”å¦‚`llama2_70b`ï¼Œæ¨¡å‹æ— æ³•å®Œå…¨å¯¼å…¥åˆ°å•å¡ä¸­è¿›è¡Œè¯„æµ‹ï¼Œå°±éœ€è¦è¿›è¡Œåˆ†å¸ƒå¼è¯„æµ‹ã€‚

ä»¥`llama2_70b`åœ¨**SQuAD 1.1**æ•°æ®é›†ä¸Šè¿›è¡Œæµ‹è¯„ä¸ºä¾‹ã€‚

1. åˆ‡åˆ†æ¨¡å‹æƒé‡

   å¯å‚è€ƒ[æƒé‡åˆ‡åˆ†ä¸åˆå¹¶](../feature_cards/Transform_Ckpt.md#ç¦»çº¿è½¬æ¢æ¡ˆä¾‹ä¸€å®Œæ•´æƒé‡è½¬æ¢ä¸ºåˆ†å¸ƒå¼æƒé‡)ä¸­çš„æ¨ç†æ¡ˆä¾‹ä¸‰è¿›è¡Œå®Œæ•´æƒé‡åˆ‡åˆ†ä»¥ç”¨äºåˆ†å¸ƒå¼è¯„æµ‹ã€‚

   ä¿®æ”¹æƒé‡æ–‡ä»¶å¤¹ç›®å½•ç»“æ„å¦‚ä¸‹ï¼Œå°†æ¨¡å‹æƒé‡æ”¾å…¥`rank_0`çš„æ–‡ä»¶å¤¹ä¸­ã€‚

   ```text
   path/to/checkpoint_dir
       â”œâ”€â”€rank_0
       â”‚Â Â â”œâ”€â”€model.ckpt
   ```

2. ä¿®æ”¹æ¨¡å‹é…ç½®æ–‡ä»¶

   ```yaml
   load_checkpoint: 'path/to/checkpoint_dir'
   auto_trans_ckpt: True
   use_parallel: True
   parallel_config:
     data_parallel: 1
     model_parallel: 8  # ä¿®æ”¹ä¸ºä½¿ç”¨å¡æ•°ï¼Œ 70bæ¨èè®¾ç½®ä¸º8å¡æ¨ç†
     pipeline_stage: 1
     use_seq_parallel: False

   # metric
   metric:
     type: EmF1Metric

   eval_dataset:
     data_loader:
       type: MindDataset
       dataset_dir: "{path}/squad2048.mindrecord"
   ```

3. æ‰§è¡Œè¯„æµ‹å‘½ä»¤

   ```shell
   bash scripts/msrun_launcher.sh "run_mindformer.py \
     --config configs/llama2/predict_llama2_70b.yaml \
     --run_mode eval \
     --use_parallel True" 8
   ```
