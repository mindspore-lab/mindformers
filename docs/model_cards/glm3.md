# ChatGLM3

## æ¨¡å‹æè¿°

ChatGLM3 æ˜¯æ™ºè°±AIå’Œæ¸…åå¤§å­¦ KEG å®éªŒå®¤è”åˆå‘å¸ƒçš„æ–°ä¸€ä»£å¯¹è¯é¢„è®­ç»ƒæ¨¡å‹ã€‚ChatGLM3-6B æ˜¯ ChatGLM3 ç³»åˆ—ä¸­çš„å¼€æºæ¨¡å‹ï¼Œåœ¨ä¿ç•™äº†å‰ä¸¤ä»£æ¨¡å‹å¯¹è¯æµç•…ã€éƒ¨ç½²é—¨æ§›ä½ç­‰ä¼—å¤šä¼˜ç§€ç‰¹æ€§çš„åŸºç¡€ä¸Šï¼ŒChatGLM3-6B å¼•å…¥äº†å¦‚ä¸‹ç‰¹æ€§ï¼š**æ›´å¼ºå¤§çš„åŸºç¡€æ¨¡å‹**ï¼Œ**æ›´å®Œæ•´çš„åŠŸèƒ½æ”¯æŒ**ï¼Œ**æ›´å…¨é¢çš„å¼€æºåºåˆ—**

```text
@article{zeng2022glm,
  title={Glm-130b: An open bilingual pre-trained model},
  author={Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and others},
  journal={arXiv preprint arXiv:2210.02414},
  year={2022}
}
```

## ä»“åº“ä»‹ç»

`chatGLM3-6B` åŸºäº `mindformers` å®ç°ï¼Œä¸»è¦æ¶‰åŠçš„æ–‡ä»¶æœ‰ï¼š

1. æ¨¡å‹å…·ä½“å®ç°ï¼š`mindformers/models/glm3`

    ```text
    glm3
        â”œâ”€â”€ __init__.py
        â””â”€â”€ glm3_tokenizer.py        # tokenizer
    ```

  glm3çš„æ¨¡å‹ç»“æ„å’ŒconfigåŒglm2

2. æ¨¡å‹é…ç½®ï¼š`configs/glm3`

    ```bash
    glm3
        â”œâ”€â”€ export_glm3_6b.yaml                # å¯¼å‡ºmindiré…ç½®
        â”œâ”€â”€ run_glm3_6b_finetune_2k_910b.yaml  # Atlas 800T A2æœ€ä½³æ€§èƒ½å…¨é‡å¾®è°ƒå¯åŠ¨é…ç½®
        â””â”€â”€ run_glm3_6b.yaml                   # æ¨ç†ç”¨é…ç½®
    ```

## å‰æœŸå‡†å¤‡

### ç”ŸæˆRANK_TABLE_FILE

è¿è¡Œmindformers/tools/hccl_tools.pyç”ŸæˆRANK_TABLE_FILEçš„jsonæ–‡ä»¶

```bash
# è¿è¡Œå¦‚ä¸‹å‘½ä»¤ï¼Œç”Ÿæˆå½“å‰æœºå™¨çš„RANK_TABLE_FILEçš„jsonæ–‡ä»¶
python ./mindformers/tools/hccl_tools.py --device_num "[0,8)"
```

**æ³¨ï¼šè‹¥ä½¿ç”¨ModelArtsçš„notebookç¯å¢ƒï¼Œå¯ä» `/user/config/jobstart_hccl.json` è·¯å¾„ä¸‹ç›´æ¥è·å–rank tableï¼Œæ— éœ€æ‰‹åŠ¨ç”Ÿæˆ**

RANK_TABLE_FILE å•æœº8å¡å‚è€ƒæ ·ä¾‹:

```json
{
    "version": "1.0",
    "server_count": "1",
    "server_list": [
        {
            "server_id": "xx.xx.xx.xx",
            "device": [
                {"device_id": "0","device_ip": "192.1.27.6","rank_id": "0"},
                {"device_id": "1","device_ip": "192.2.27.6","rank_id": "1"},
                {"device_id": "2","device_ip": "192.3.27.6","rank_id": "2"},
                {"device_id": "3","device_ip": "192.4.27.6","rank_id": "3"},
                {"device_id": "4","device_ip": "192.1.27.7","rank_id": "4"},
                {"device_id": "5","device_ip": "192.2.27.7","rank_id": "5"},
                {"device_id": "6","device_ip": "192.3.27.7","rank_id": "6"},
                {"device_id": "7","device_ip": "192.4.27.7","rank_id": "7"}],
             "host_nic_ip": "reserve"
        }
    ],
    "status": "completed"
}
```

### å¤šæœºRANK_TABLE_FILEåˆå¹¶

- step 1. é¦–å…ˆæ ¹æ®ä¸Šç« èŠ‚å†…å®¹ï¼Œåœ¨æ¯ä¸ªæœºå™¨ä¸Šç”Ÿæˆå„è‡ªçš„`RANK_TABLE_FILE`æ–‡ä»¶ï¼Œç„¶åå°†ä¸åŒæœºå™¨ä¸Šç”Ÿæˆçš„`RANK_TABLE_FILE`æ–‡ä»¶å…¨éƒ¨æ‹·è´åˆ°åŒä¸€å°æœºå™¨ä¸Šã€‚

```bash
# è¿è¡Œå¦‚ä¸‹å‘½ä»¤ï¼Œç”Ÿæˆå½“å‰æœºå™¨çš„RANK_TABLE_FILEçš„jsonæ–‡ä»¶
python ./mindformers/tools/hccl_tools.py --device_num "[0,8)" --server_ip xx.xx.xx.xx
```

**æ³¨ï¼šéœ€è¦æ ¹æ®æœºå™¨çš„ipåœ°å€æŒ‡å®š --server_ipï¼Œé¿å…ç”±äºä¸åŒæœºå™¨server_ipä¸åŒï¼Œå¯¼è‡´å¤šèŠ‚ç‚¹é—´é€šä¿¡å¤±è´¥ã€‚**

- step 2. è¿è¡Œmindformers/tools/merge_hccl.pyå°†ä¸åŒæœºå™¨ä¸Šç”Ÿæˆçš„`RANK_TABLE_FILE`æ–‡ä»¶åˆå¹¶

```bash
# è¿è¡Œå¦‚ä¸‹å‘½ä»¤ï¼Œåˆå¹¶æ¯ä¸ªæœºå™¨ä¸Šçš„RANK_TABLE_FILEçš„jsonæ–‡ä»¶ã€‚
python ./mindformers/tools/merge_hccl.py hccl*.json
```

- step 3. å°†åˆå¹¶åçš„`RANK_TABLE_FILE`æ–‡ä»¶æ‹·è´åˆ°æ‰€æœ‰æœºå™¨ä¸­ï¼Œä¿è¯ä¸åŒæœºå™¨ä¸Šçš„`RANK_TABLE_FILE`ç›¸åŒã€‚

RANK_TABLE_FILE åŒæœº16å¡å‚è€ƒæ ·ä¾‹:

```json
{
    "version": "1.0",
    "server_count": "2",
    "server_list": [
        {
            "server_id": "xx.xx.xx.xx",
            "device": [
                {
                    "device_id": "0", "device_ip": "192.168.0.0", "rank_id": "0"
                },
                {
                    "device_id": "1", "device_ip": "192.168.1.0", "rank_id": "1"
                },
                {
                    "device_id": "2", "device_ip": "192.168.2.0", "rank_id": "2"
                },
                {
                    "device_id": "3", "device_ip": "192.168.3.0", "rank_id": "3"
                },
                {
                    "device_id": "4", "device_ip": "192.168.0.1", "rank_id": "4"
                },
                {
                    "device_id": "5", "device_ip": "192.168.1.1", "rank_id": "5"
                },
                {
                    "device_id": "6", "device_ip": "192.168.2.1", "rank_id": "6"
                },
                {
                    "device_id": "7", "device_ip": "192.168.3.1", "rank_id": "7"
                }
            ],
            "host_nic_ip": "reserve"
        },
        {
            "server_id": "xx.xx.xx.xx",
            "device": [
                {
                    "device_id": "0", "device_ip": "192.168.0.1", "rank_id": "8"
                },
                {
                    "device_id": "1", "device_ip": "192.168.1.1", "rank_id": "9"
                },
                {
                    "device_id": "2", "device_ip": "192.168.2.1", "rank_id": "10"
                },
                {
                    "device_id": "3", "device_ip": "192.168.3.1", "rank_id": "11"
                },
                {
                    "device_id": "4", "device_ip": "192.168.0.2", "rank_id": "12"
                },
                {
                    "device_id": "5", "device_ip": "192.168.1.2", "rank_id": "13"
                },
                {
                    "device_id": "6", "device_ip": "192.168.2.2", "rank_id": "14"
                },
                {
                    "device_id": "7", "device_ip": "192.168.3.2", "rank_id": "15"
                }
            ],
            "host_nic_ip": "reserve"
        }
    ],
    "status": "completed"
}
```

### æ¨¡å‹æƒé‡ä¸‹è½½ä¸è½¬æ¢

å¼€å‘è€…å¯ä»¥ä¸‹è½½è·å–å®˜æ–¹æƒé‡åï¼Œé€šè¿‡ä¸‹é¢æä¾›çš„**æƒé‡è½¬æ¢è„šæœ¬**ï¼Œå°†å®˜æ–¹æƒé‡è½¬æ¢ä¸ºMindSporeæƒé‡ï¼›æˆ–ç›´æ¥ä½¿ç”¨MindFormersæä¾›çš„**å·²è½¬æ¢æƒé‡**

1. ä½¿ç”¨å®˜æ–¹æƒé‡è¿›è¡Œè½¬æ¢

   å…‹éš†glm3-6bä»£ç ä»“ï¼Œä¸‹è½½åˆ†å¸ƒå¼çš„æ¨¡å‹æ–‡ä»¶ã€‚

   ```shell
   git lfs install
   git clone https://huggingface.co/THUDM/chatglm3-6b
   ```

   æ‰§è¡Œ python è„šæœ¬ï¼Œåˆå¹¶æ¨¡å‹æƒé‡ã€‚

   ```python
   from transformers import AutoTokenizer, AutoModel
   import torch

   tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True)
   model = AutoModel.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True)

   with open("pt_model_arch.txt", "w") as fp:
       print(model, file=fp, flush=True)
   with open("pt_ckpt.txt", "w") as fp:
       for name, param in model.named_parameters():
           fp.write(f"{name} {param.shape} {param.dtype}\n")
   torch.save(model.state_dict(), "glm3_6b.pth")
   ```

   æ‰§è¡Œè½¬æ¢è„šæœ¬ï¼Œå¾—åˆ°è½¬æ¢åçš„è¾“å‡ºæ–‡ä»¶`glm3_6b.ckpt`ã€‚

   ```python
   import mindspore as ms
   import torch as pt
   from tqdm import tqdm

   pt_ckpt_path = "glm3_6b.pth"
   pt_param = pt.load(pt_ckpt_path)

   type_map = {"torch.float16": "ms.float16",
               "torch.float32": "ms.float32"}
   ms_param = []
   with open("check_pt_ckpt.txt", "w") as fp:
       for k, v in tqdm(pt_param.items()):
           if "word_embeddings.weight" in k:
               k = k.replace("word_embeddings.weight", "embedding_table")
           fp.write(f"{k} {v.shape} {v.dtype}\n")
           ms_param.append({"name": k, "data": ms.Tensor(v.numpy())})

   ms.save_checkpoint(ms_param, "glm3_6b.ckpt")
   ```

2. è·å–MindFormersæä¾›çš„å·²è½¬æ¢æƒé‡

   å¯é€šè¿‡from_pretrainedæ¥å£ä¸‹è½½ï¼Œä¹Ÿå¯ç›´æ¥ä»ä¸‹é¢çš„é“¾æ¥è·å–

   [glm3_6bæƒé‡](https://ascend-repo-modelzoo.obs.cn-east-2.myhuaweicloud.com/XFormer_for_mindspore/glm3/glm3_6b.ckpt)

   [tokenizeræ–‡ä»¶](https://ascend-repo-modelzoo.obs.cn-east-2.myhuaweicloud.com/XFormer_for_mindspore/glm3/tokenizer.model)

### [åˆ†å¸ƒå¼è®­ç»ƒ/å¾®è°ƒæƒé‡åˆå¹¶](../feature_cards/Transform_Ckpt.md)

åˆ†å¸ƒå¼è®­ç»ƒ/å¾®è°ƒåæ‰€å¾—åˆ°çš„æƒé‡æ–‡ä»¶ä¸ºæ ¹æ®ç­–ç•¥åˆ‡åˆ†åçš„æƒé‡ï¼Œéœ€è¦æ‰‹åŠ¨å°†åˆ‡åˆ†æƒé‡åˆä¸€ï¼Œä»¥ç”¨äºè¯„ä¼°å’Œæ¨ç†ã€‚

æ¶‰åŠåˆ°ckptçš„å•å¡ï¼Œå¤šå¡è½¬æ¢ï¼Œè¯¦ç»†æ•™ç¨‹è¯·å‚è€ƒç‰¹æ€§æ–‡æ¡£æ¨¡å‹[æƒé‡åˆ‡åˆ†ä¸åˆå¹¶](../feature_cards/Transform_Ckpt.md)

- step 1. è·å–æ¨¡å‹åˆ‡åˆ†ç­–ç•¥æ–‡ä»¶ï¼š

åœ¨æ‰§è¡Œå¾®è°ƒè„šæœ¬æ—¶ï¼Œæ¨¡å‹å®Œæˆç¼–è¯‘åï¼Œå°†ä¼šåœ¨`output/strategy`è·¯å¾„ä¸‹ç”Ÿæˆå„å¡çš„åˆ‡åˆ†ç­–ç•¥æ–‡ä»¶ï¼Œç”¨äºæƒé‡åˆå¹¶ã€‚

> æ³¨ï¼šloraå¾®è°ƒæ—¶éœ€è¦ç¡®è®¤é…ç½®æ–‡ä»¶`parallel context config`ä¸­`only_trainable_params`è®¾ä¸º`False`ï¼Œä»¥è·å–æ‰€æœ‰å‚æ•°å®Œæ•´ç­–ç•¥ã€‚

- step 2. è¿è¡Œ`mindformers/tools/transform_ckpt.py`è„šæœ¬è¿›è¡Œå¤šå¡æƒé‡åˆå¹¶ï¼š

```shell
python transform_ckpt.py \
--src_ckpt_strategy {path}/output/strategy/ \
--src_ckpt_dir {path}/output/checkpoint/ \
--dst_ckpt_dir {path}/target_checkpoint/ \
--prefix glm3_6b
```

```text
# å‚æ•°è¯´æ˜
src_ckpt_strategy: æ­¥éª¤1ä¸­çš„åˆ‡åˆ†ç­–ç•¥æ–‡ä»¶è·¯å¾„
src_ckpt_dir: åŸåˆ‡åˆ†æƒé‡æ–‡ä»¶å¤¹
dst_ckpt_dir: ç›®æ ‡è·¯å¾„
prefix: ckptæ–‡ä»¶å‰ç¼€å
```

> æ³¨ï¼š`transform_checkpoints` æ¥å£å½“å‰ä»…mindspore 2.0ä»¥ä¸Šç‰ˆæœ¬æ”¯æŒï¼Œå¦‚å½“å‰ç¡¬ä»¶ç¯å¢ƒåªæ”¯æŒ2.0ä»¥ä¸‹ç‰ˆæœ¬ï¼Œå¯ä»¥æ–°å»ºcondaç¯å¢ƒå®‰è£…mindspore 2.0çš„cpuç‰ˆæœ¬ä»¥æ‰§è¡Œè¯¥è„šæœ¬

## åŸºäºAPIçš„å¿«é€Ÿä½¿ç”¨

### åŸºäºAutoClassçš„å¿«é€Ÿä½¿ç”¨

å¯ä»¥ä½¿ç”¨AutoClassæ¥å£ï¼Œé€šè¿‡æ¨¡å‹åç§°è·å–ç›¸åº”çš„model/preprocess/tokenizerç­‰å®ä¾‹ï¼Œå¹¶è‡ªåŠ¨ä¸‹è½½å¹¶åŠ è½½æƒé‡

`from_pretrained()` æ¥å£ä¼šè‡ªåŠ¨ä»äº‘ä¸Šä¸‹è½½é¢„è®­ç»ƒçš„æ¨¡å‹ï¼Œå­˜å‚¨è·¯å¾„ï¼š`./checkpoint_download/glm3`

```python
import mindspore as ms
from mindformers import AutoConfig, AutoModel, AutoTokenizer, AutoProcessor

# æŒ‡å®šå›¾æ¨¡å¼ï¼ŒæŒ‡å®šä½¿ç”¨è®­ç»ƒå¡id
ms.set_context(mode=ms.GRAPH_MODE, device_target="Ascend", device_id=0)

# ä»¥ä¸‹ä¸¤ç§tokenizerå®ä¾‹åŒ–æ–¹å¼é€‰å…¶ä¸€å³å¯
# 1. åœ¨çº¿åŠ è½½æ–¹å¼
tokenizer = AutoTokenizer.from_pretrained("glm3_6b")
# 2. æœ¬åœ°åŠ è½½æ–¹å¼
# tokenizer = AutoProcessor.from_pretrained("/path/to/your.yaml").tokenizer

# ä»¥ä¸‹ä¸¤ç§modelçš„å®ä¾‹åŒ–æ–¹å¼é€‰å…¶ä¸€å³å¯
# 1. ç›´æ¥æ ¹æ®é»˜è®¤é…ç½®å®ä¾‹åŒ–
# model = AutoModel.from_pretrained('glm3_6b')
# 2. è‡ªå®šä¹‰ä¿®æ”¹é…ç½®åå®ä¾‹åŒ–
config = AutoConfig.from_pretrained('glm3_6b')
config.use_past = True                  # æ­¤å¤„ä¿®æ”¹é»˜è®¤é…ç½®ï¼Œå¼€å¯å¢é‡æ¨ç†èƒ½å¤ŸåŠ é€Ÿæ¨ç†æ€§èƒ½
config.seq_length = 2048                 # æ ¹æ®éœ€æ±‚è‡ªå®šä¹‰ä¿®æ”¹å…¶ä½™æ¨¡å‹é…ç½®
config.checkpoint_name_or_path = "/path/to/your.ckpt"
model = AutoModel.from_config(config)   # ä»è‡ªå®šä¹‰é…ç½®é¡¹ä¸­å®ä¾‹åŒ–æ¨¡å‹

role="user"

inputs_list=["ä½ å¥½", "è¯·ä»‹ç»ä¸€ä¸‹åä¸º", "æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠ", "å†™ä¸€ä¸ªå¿«æ’ç®—æ³•"]

for input_item in inputs_list:
    history=[]
    inputs = tokenizer.build_chat_input(input_item, history=history, role=role)
    inputs = inputs['input_ids']
    # é¦–æ¬¡è°ƒç”¨model.generate()è¿›è¡Œæ¨ç†å°†åŒ…å«å›¾ç¼–è¯‘æ—¶é—´ï¼Œæ¨ç†æ€§èƒ½æ˜¾ç¤ºä¸å‡†ç¡®ï¼Œå¤šæ¬¡é‡å¤è°ƒç”¨ä»¥è·å–å‡†ç¡®çš„æ¨ç†æ€§èƒ½
    outputs = model.generate(inputs, do_sample=False, top_k=1, max_length=config.seq_length)
    for i, output in enumerate(outputs):
        output = output[len(inputs[i]):]
        response = tokenizer.decode(output)
        print(response)
# answer 1:
# ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM3-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚

# answer 2:
# åä¸ºæ˜¯ä¸€å®¶æ€»éƒ¨ä½äºä¸­å›½æ·±åœ³çš„å¤šå…ƒåŒ–ç§‘æŠ€å…¬å¸,æˆç«‹äº1987å¹´,æ˜¯å…¨çƒæœ€å¤§çš„ç”µä¿¡è®¾å¤‡åˆ¶é€ å•†ä¹‹ä¸€ã€‚è¯¥å…¬å¸ä¹Ÿåœ¨æ™ºèƒ½æ‰‹æœºã€ç”µè„‘ã€å¹³æ¿ç”µè„‘ã€äº‘è®¡ç®—ç­‰é¢†åŸŸå¼€å±•ä¸šåŠ¡,å…¶äº§å“å’ŒæœåŠ¡è¦†ç›–å…¨çƒ170å¤šä¸ªå›½å®¶å’Œåœ°åŒºã€‚

# åä¸ºçš„ä¸»è¦ä¸šåŠ¡åŒ…æ‹¬ç”µä¿¡ç½‘ç»œè®¾å¤‡ã€æ™ºèƒ½æ‰‹æœºã€ç”µè„‘å’Œæ¶ˆè´¹ç”µå­äº§å“ã€‚å…¬å¸åœ¨å…¨çƒèŒƒå›´å†…æœ‰è¶…è¿‡190,000åå‘˜å·¥,å…¶ä¸­çº¦ä¸€åŠä»¥ä¸Šä»äº‹ç ”å‘å·¥ä½œã€‚åä¸ºä»¥å…¶é«˜å“è´¨çš„äº§å“å’ŒæœåŠ¡èµ¢å¾—äº†å…¨çƒå®¢æˆ·çš„ä¿¡ä»»å’Œå¥½è¯„,ä¹Ÿæ›¾å› å…¶é¢†å…ˆæŠ€æœ¯å’Œåˆ›æ–°ç²¾ç¥è€Œè·å¾—å¤šé¡¹å›½é™…å¥–é¡¹å’Œè®¤å¯ã€‚

# ç„¶è€Œ,åä¸ºä¹Ÿé¢ä¸´ç€æ¥è‡ªä¸€äº›å›½å®¶æ”¿åºœçš„å®‰å…¨é—®é¢˜å’Œæ”¿æ²»å‹åŠ›,å…¶ä¸­åŒ…æ‹¬ç¾å›½æ”¿åºœå¯¹å…¶äº§å“çš„ç¦ä»¤å’Œé™åˆ¶ã€‚åä¸ºä¸€ç›´åšç§°è‡ªå·±çš„äº§å“æ˜¯å®‰å…¨çš„,å¹¶é‡‡å–äº†ä¸€ç³»åˆ—æªæ–½æ¥ç¡®ä¿å…¶äº§å“çš„å®‰å…¨æ€§å’Œé€æ˜åº¦ã€‚

# answer 3:
#  æ™šä¸Šç¡ä¸ç€å¯ä»¥å°è¯•ä»¥ä¸‹æ–¹æ³•:

# 1. å°è¯•æ”¾æ¾èº«å¿ƒ,æ¯”å¦‚æ·±å‘¼å¸ã€å†¥æƒ³ã€ç‘œä¼½ç­‰ã€‚

# 2. é¿å…é¥®ç”¨å’–å•¡ã€èŒ¶ã€å¯ä¹ç­‰åˆºæ¿€æ€§é¥®æ–™ã€‚

# 3. é¿å…è¿‡åº¦å…´å¥‹,æ¯”å¦‚çœ‹æƒŠæ‚šç”µå½±ã€ç©åˆºæ¿€æ¸¸æˆç­‰ã€‚

# 4. ä¿æŒè§„å¾‹çš„ä½œæ¯æ—¶é—´,å°½é‡æ¯å¤©æŒ‰æ—¶ä¸ŠåºŠç¡è§‰ã€æŒ‰æ—¶èµ·åºŠã€‚

# 5. ç¡å‰é€‚å½“è¿åŠ¨,æ¯”å¦‚æ•£æ­¥ã€æ…¢è·‘ç­‰ã€‚

# 6. ç¡å‰å¯ä»¥å–ä¸€æ¯æ¸©ç‰›å¥¶æˆ–è€…ä¸€äº›åŠ©çœ çš„é£Ÿå“ã€‚

# 7. å¦‚æœé•¿æ—¶é—´ç¡ä¸ç€å¯ä»¥è€ƒè™‘å’¨è¯¢åŒ»ç”Ÿæˆ–å¿ƒç†å’¨è¯¢å¸ˆã€‚

# answer 4:
# å¿«é€Ÿæ’åºï¼ˆQuick Sortï¼‰æ˜¯ä¸€ç§å¸¸ç”¨çš„æ’åºç®—æ³•ï¼Œå…¶åŸºæœ¬æ€æƒ³æ˜¯é€šè¿‡ä¸€è¶Ÿæ’åºå°†å¾…æ’åºçš„æ•°æ®åˆ†å‰²æˆç‹¬ç«‹çš„ä¸¤éƒ¨åˆ†ï¼Œå…¶ä¸­ä¸€éƒ¨åˆ†çš„æ‰€æœ‰æ•°æ®éƒ½æ¯”å¦ä¸€éƒ¨åˆ†çš„æ‰€æœ‰æ•°æ®è¦å°ï¼Œç„¶åå†æŒ‰æ­¤æ–¹æ³•å¯¹è¿™ä¸¤éƒ¨åˆ†æ•°æ®åˆ†åˆ«è¿›è¡Œå¿«é€Ÿæ’åºï¼Œæ•´ä¸ªæ’åºè¿‡ç¨‹å¯ä»¥é€’å½’è¿›è¡Œï¼Œä»¥æ­¤è¾¾åˆ°æ•´ä¸ªæ•°æ®å˜æˆæœ‰åºåºåˆ—ã€‚

# ä¸‹é¢æ˜¯ä¸€ä¸ªç”¨Pythonå®ç°çš„å¿«é€Ÿæ’åºç®—æ³•ï¼š

# ```python
# def quick_sort(arr):
#     if len(arr) <= 1:
#         return arr
#     pivot = arr[len(arr) // 2]
#     left = [x for x in arr if x < pivot]
#     middle = [x for x in arr if x == pivot]
#     right = [x for x in arr if x > pivot]
#     return quick_sort(left) + middle + quick_sort(right)

# arr = [3,6,8,10,1,2,1]
# print(quick_sort(arr))
# ```

# åœ¨è¿™ä¸ªå®ç°ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆåˆ¤æ–­è¾“å…¥æ•°ç»„çš„é•¿åº¦æ˜¯å¦å°äºç­‰äº1ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™ç›´æ¥è¿”å›æ•°ç»„ï¼Œå› ä¸ºé•¿åº¦ä¸º1çš„æ•°ç»„æœ¬èº«å°±æ˜¯æœ‰åºçš„ã€‚å¦åˆ™ï¼Œæˆ‘ä»¬é€‰æ‹©æ•°ç»„ä¸­é—´çš„å…ƒç´ ä½œä¸ºåŸºå‡†å€¼ï¼ˆpivotï¼‰ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†æ•°ç»„ä¸­çš„å…ƒç´ åˆ†æˆä¸‰éƒ¨åˆ†ï¼šå°äºåŸºå‡†å€¼çš„å…ƒç´ ï¼ˆleftï¼‰ã€ç­‰äºåŸºå‡†å€¼çš„å…ƒç´ ï¼ˆmiddleï¼‰å’Œå¤§äºåŸºå‡†å€¼çš„å…ƒç´ ï¼ˆrightï¼‰ã€‚æ¥ç€ï¼Œæˆ‘ä»¬åˆ†åˆ«å¯¹leftå’Œrightå­æ•°ç»„è¿›è¡Œé€’å½’è°ƒç”¨quick_sortå‡½æ•°è¿›è¡Œæ’åºï¼Œå¹¶å°†æ’åºåçš„ç»“æœä¸middleå­æ•°ç»„è¿æ¥èµ·æ¥ï¼Œå¾—åˆ°æœ€ç»ˆçš„æ’åºç»“æœã€‚
```

å¦‚æœéœ€è¦åŠ è½½æœ¬åœ°è¯è¡¨ï¼Œè¯·ä¿®æ”¹é…ç½®æ–‡ä»¶ä¸­ä»¥ä¸‹é¡¹ï¼š

  ```yaml
  processor:
    tokenizer:
      vocab_file: "/path/to/tokenizer.model"
  ```

## å¾®è°ƒ

ä¸‹é¢ä»¥ [ADGEN](https://aclanthology.org/D19-1321.pdf) (å¹¿å‘Šç”Ÿæˆ) æ•°æ®é›†ä¸ºä¾‹ä»‹ç»ä»£ç çš„ä½¿ç”¨æ–¹æ³•

### æ•°æ®é›†å‡†å¤‡

ADGEN æ•°æ®é›†ä»»åŠ¡ä¸ºæ ¹æ®è¾“å…¥ï¼ˆcontentï¼‰ç”Ÿæˆä¸€æ®µå¹¿å‘Šè¯ï¼ˆsummaryï¼‰ã€‚

```json
{
    "content": "ç±»å‹#ä¸Šè¡£*ç‰ˆå‹#å®½æ¾*ç‰ˆå‹#æ˜¾ç˜¦*å›¾æ¡ˆ#çº¿æ¡*è¡£æ ·å¼#è¡¬è¡«*è¡£è¢–å‹#æ³¡æ³¡è¢–*è¡£æ¬¾å¼#æŠ½ç»³",
    "summary": "è¿™ä»¶è¡¬è¡«çš„æ¬¾å¼éå¸¸çš„å®½æ¾ï¼Œåˆ©è½çš„çº¿æ¡å¯ä»¥å¾ˆå¥½çš„éšè—èº«æä¸Šçš„å°ç¼ºç‚¹ï¼Œç©¿åœ¨èº«ä¸Šæœ‰ç€å¾ˆå¥½çš„æ˜¾ç˜¦æ•ˆæœã€‚é¢†å£è£…é¥°äº†ä¸€ä¸ªå¯çˆ±çš„æŠ½ç»³ï¼Œæ¼‚äº®çš„ç»³ç»“å±•ç°å‡ºäº†åè¶³çš„ä¸ªæ€§ï¼Œé…åˆæ—¶å°šçš„æ³¡æ³¡è¢–å‹ï¼Œå°½æ˜¾å¥³æ€§ç”œç¾å¯çˆ±çš„æ°”æ¯ã€‚"
}
```

ä» [Google Drive](https://drive.google.com/file/d/13_vf0xRTQsyneRKdD1bZIr93vBGOczrk/view?usp=sharing) æˆ–è€… [Tsinghua Cloud](https://cloud.tsinghua.edu.cn/f/b3f119a008264b1cabd1/?dl=1) ä¸‹è½½å¤„ç†å¥½çš„ ADGEN æ•°æ®é›†ï¼Œç›®å½•ç»“æ„ä¸º

```text
AdvertiseGen
  â”œâ”€â”€ train.json
  â””â”€â”€ dev.json
```

å°†ä»»åŠ¡é…ç½®æ–‡ä»¶ `configs/glm3/run_glm3_6b_*.yaml` ä¸­çš„ `==== dataset config ====` éƒ¨åˆ†æ›¿æ¢æˆï¼š

```yaml
train_dataset: &train_dataset
  data_loader:
    type: ADGenDataLoader
    dataset_dir: "/path/to/AdvertiseGen/train.json"
    shuffle: True
    phase: "train"
    version: 3
    origin_columns: ["content", "summary"]
  tokenizer:
    type: ChatGLM3Tokenizer
    vocab_file: "/path/to/tokenizer.model"
  input_columns: ["input_ids", "labels"]
  max_source_length: 64
  max_target_length: 128
  ignore_pad_token_for_loss: True
  num_parallel_workers: 8
  python_multiprocessing: False
  drop_remainder: True
  batch_size: 1
  repeat: 1
  numa_enable: False
  prefetch_size: 1
  seed: 0

train_dataset_task:
  type: KeyWordGenDataset
  dataset_config: *train_dataset

eval_dataset: &eval_dataset
  data_loader:
    type: ADGenDataLoader
    dataset_dir: "/path/to/AdvertiseGen/dev.json"
    shuffle: False
    phase: "eval"
    version: 3
    origin_columns: ["content", "summary"]
  tokenizer:
    type: ChatGLM3Tokenizer
    vocab_file: "/path/to/tokenizer.model"
  max_source_length: 256
  max_target_length: 256
  ignore_pad_token_for_loss: True
  input_columns: ["input_ids", "labels"]
  num_parallel_workers: 8
  python_multiprocessing: False
  drop_remainder: True
  batch_size: 1
  repeat: 1
  numa_enable: False
  prefetch_size: 1
  seed: 0

eval_dataset_task:
  type: KeyWordGenDataset
  dataset_config: *eval_dataset
```

> æ³¨æ„ï¼šå¾®è°ƒæ—¶çš„æ¨¡å‹`seq_length`éœ€è¦ç­‰äºå¾®è°ƒæ•°æ®é›†çš„`max_source_length + max_target_length + 1`ã€‚
> yamlæ–‡ä»¶ä¸­é»˜è®¤çš„`seq_length: 193`ä»¥åŠ`max_source_length: 64`å’Œ`max_target_length: 128`é€‚ç”¨äºADGENæ•°æ®é›†

### å…¨å‚å¾®è°ƒ

å…¨å‚å¾®è°ƒä½¿ç”¨ `configs/glm3/run_glm3_6b_finetune*.yaml` é…ç½®æ–‡ä»¶ï¼Œé…ç½®æ–‡ä»¶ä¸­å®šä¹‰äº†å¾®è°ƒæ‰€éœ€çš„å„é…ç½®é¡¹

ä¿®æ”¹æ•°æ®é›†/æ¨¡å‹æƒé‡é…ç½®è·¯å¾„ï¼š

- æ•°æ®é›†ï¼šä¿®æ”¹ `configs/glm3/run_glm3_6b_finetune*.yaml` è„šæœ¬ä¸­`train_dataset` çš„ `dataset_dir` ä¸ºå‰æ–‡ç”Ÿæˆçš„æ•°æ®é›†è·¯å¾„ã€‚
- åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æƒé‡ï¼šä¿®æ”¹ `configs/glm3/run_glm3_6b_finetune*.yaml` è„šæœ¬ä¸­çš„ `load_checkpoint` ä¸ºé¢„è®­ç»ƒæ¨¡å‹æƒé‡è·¯å¾„ã€‚

å½“å‰æ¨¡å‹å·²æ”¯æŒä½¿ç”¨**Flash Attentionç®—æ³•**è¿›è¡Œå…¨å‚å¾®è°ƒï¼Œè¯·å‚è€ƒ [Flash Attentionä½¿ç”¨æ–‡æ¡£](../feature_cards/Training_Algorithms.md#flash-attention)

#### å•å¡å¾®è°ƒ

ç”±äºglm3_6bæ¨¡å‹è¾ƒå¤§ï¼Œå…¨é‡å¾®è°ƒä¸æ”¯æŒå•å¡è¿è¡Œ

#### å¤šå¡å¾®è°ƒ

- å•æœºå¤šå¡

å¤šå¡è¿è¡Œéœ€è¦RANK_FILE_TABLEï¼Œè¯·å‚è€ƒå‰æœŸå‡†å¤‡â€”â€”[ç”ŸæˆRANK_TABLE_FILE](#ç”Ÿæˆranktablefile)

```shell
cd scripts
# Usage Help: bash run_distribute.sh [RANK_TABLE_FILE] [CONFIG_PATH] [DEVICE_RANGE] [RUN_STATUS]
bash run_distribute.sh /path/to/hccl_8p_01234567_127.0.1.1.json ../configs/glm3/run_glm3_6b_finetune*.yaml '[0,8]' finetune
# å°†æ­¤å¤„rank_table_fileæ›¿æ¢ä¸ºå®é™…è·¯å¾„
```

å‚æ•°è¯´æ˜

```text
RANK_TABLE_FILE: ç”±mindformers/tools/hccl_tools.pyç”Ÿæˆçš„åˆ†å¸ƒå¼jsonæ–‡ä»¶
CONFIG_PATH: ä¸ºconfigsæ–‡ä»¶å¤¹ä¸‹é¢çš„glm3/run_glm3_6b_finetune*.yamlé…ç½®æ–‡ä»¶
DEVICE_RANGE: ä¸ºå•æœºåˆ†å¸ƒå¼å¡çš„èŒƒå›´ï¼Œå¦‚ '[0,8]' ä¸º8å¡åˆ†å¸ƒå¼ï¼Œä¸åŒ…å«8æœ¬èº«
RUN_STATUS: ä¸ºä»»åŠ¡è¿è¡ŒçŠ¶æ€ï¼Œæ”¯æŒå…³é”®å­— train\finetune\eval\predict
```

è®­ç»ƒçš„logæ—¥å¿—è·¯å¾„ï¼šmindformers/output/log

checkpointå­˜å‚¨è·¯å¾„ï¼šmindformers/output/checkpoint

- å¤šæœºå¤šå¡

å¤šæœºå¤šå¡è¿è¡Œéœ€è¦åˆå¹¶ä¸åŒæœºå™¨çš„RANK_FILE_TABLEï¼Œå‚è€ƒå‰æœŸå‡†å¤‡â€”â€”[å¤šæœºRANK_TABLE_FILEåˆå¹¶](#å¤šæœºranktablefileåˆå¹¶)

åœ¨æ¯å°æœºå™¨ä¸Šå¯åŠ¨`bash run_distribute.sh`ã€‚

```bash
server_count=12
device_num=8*$server_count
# launch ranks in the 0th server
cd scripts
bash run_distribute.sh $RANK_TABLE_FILE path/to/config.yaml [0,8] finetune $device_num

# launch ranks in the 1-11 server via ssh
for idx in {1..11}
do
    let rank_start=8*$idx
    let rank_end=$rank_start+8
    ssh ${IP_LIST[$idx]} "cd scripts; bash run_distribute.sh $RANK_TABLE_FILE path/to/config.yaml [$rank_start,$rank_end] finetune $device_num"
done
```

å…¶ä¸­

- `RANK_TABLE_FILE`ä¸ºä¸Šä¸€æ­¥æ±‡æ€»å¹¶åˆ†å‘çš„æ€»rank tableæ–‡ä»¶ï¼›
- `IP_LIST`ä¸º12å°æœåŠ¡å™¨çš„IPåœ°å€ã€‚å¦‚192.168.0.[0-11]

```bash
IP_LIST=("192.168.0.0", "192.168.0.1", ..., "192.168.0.11")
```

## æ¨ç†

### åŸºäºgenerateçš„æ¨ç†

ä¸‹é¢æä¾›ä¸€ä¸ªæ¨¡å‹æ¨ç†æ ·ä¾‹è„šæœ¬ `infer.py`

```python
import mindspore as ms
from mindformers import AutoConfig, AutoModel, AutoTokenizer, AutoProcessor

# æŒ‡å®šå›¾æ¨¡å¼ï¼ŒæŒ‡å®šä½¿ç”¨è®­ç»ƒå¡id
ms.set_context(mode=ms.GRAPH_MODE, device_target="Ascend", device_id=0)

# ä»¥ä¸‹ä¸¤ç§tokenizerå®ä¾‹åŒ–æ–¹å¼é€‰å…¶ä¸€å³å¯
# 1. åœ¨çº¿åŠ è½½æ–¹å¼
tokenizer = AutoTokenizer.from_pretrained("glm3_6b")
# 2. æœ¬åœ°åŠ è½½æ–¹å¼
# tokenizer = AutoProcessor.from_pretrained("/path/to/your.yaml").tokenizer

# ä»¥ä¸‹ä¸¤ç§modelçš„å®ä¾‹åŒ–æ–¹å¼é€‰å…¶ä¸€å³å¯
# 1. ç›´æ¥æ ¹æ®é»˜è®¤é…ç½®å®ä¾‹åŒ–
# model = AutoModel.from_pretrained('glm3_6b')
# 2. è‡ªå®šä¹‰ä¿®æ”¹é…ç½®åå®ä¾‹åŒ–
config = AutoConfig.from_pretrained('glm3_6b')
config.use_past = True                  # æ­¤å¤„ä¿®æ”¹é»˜è®¤é…ç½®ï¼Œå¼€å¯å¢é‡æ¨ç†èƒ½å¤ŸåŠ é€Ÿæ¨ç†æ€§èƒ½
config.seq_length = 2048                      # æ ¹æ®éœ€æ±‚è‡ªå®šä¹‰ä¿®æ”¹å…¶ä½™æ¨¡å‹é…ç½®
config.checkpoint_name_or_path = "/path/to/your.ckpt"
model = AutoModel.from_config(config)   # ä»è‡ªå®šä¹‰é…ç½®é¡¹ä¸­å®ä¾‹åŒ–æ¨¡å‹

role="user"

inputs_list=["ä½ å¥½", "è¯·ä»‹ç»ä¸€ä¸‹åä¸º"]

for input_item in inputs_list:
    history=[]
    inputs = tokenizer.build_chat_input(input_item, history=history, role=role)
    inputs = inputs['input_ids']
    # é¦–æ¬¡è°ƒç”¨model.generate()è¿›è¡Œæ¨ç†å°†åŒ…å«å›¾ç¼–è¯‘æ—¶é—´ï¼Œæ¨ç†æ€§èƒ½æ˜¾ç¤ºä¸å‡†ç¡®ï¼Œå¤šæ¬¡é‡å¤è°ƒç”¨ä»¥è·å–å‡†ç¡®çš„æ¨ç†æ€§èƒ½
    outputs = model.generate(inputs, do_sample=False, top_k=1, max_length=config.seq_length)
    response = tokenizer.decode(outputs)
    for i, output in enumerate(outputs):
        output = output[len(inputs[i]):]
        response = tokenizer.decode(output)
        print(response)
    # answer 1:
    # ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM3-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚

    # answer 2:
    # åä¸ºæ˜¯ä¸€å®¶æ€»éƒ¨ä½äºä¸­å›½æ·±åœ³çš„å¤šå…ƒåŒ–ç§‘æŠ€å…¬å¸,æˆç«‹äº1987å¹´,æ˜¯å…¨çƒæœ€å¤§çš„ç”µä¿¡è®¾å¤‡åˆ¶é€ å•†ä¹‹ä¸€ã€‚è¯¥å…¬å¸ä¹Ÿåœ¨æ™ºèƒ½æ‰‹æœºã€ç”µè„‘ã€å¹³æ¿ç”µè„‘ã€äº‘è®¡ç®—ç­‰é¢†åŸŸå¼€å±•ä¸šåŠ¡,å…¶äº§å“å’ŒæœåŠ¡è¦†ç›–å…¨çƒ170å¤šä¸ªå›½å®¶å’Œåœ°åŒºã€‚

    # åä¸ºçš„ä¸»è¦ä¸šåŠ¡åŒ…æ‹¬ç”µä¿¡ç½‘ç»œè®¾å¤‡ã€æ™ºèƒ½æ‰‹æœºã€ç”µè„‘å’Œæ¶ˆè´¹ç”µå­äº§å“ã€‚å…¬å¸åœ¨å…¨çƒèŒƒå›´å†…æœ‰è¶…è¿‡190,000åå‘˜å·¥,å…¶ä¸­çº¦ä¸€åŠä»¥ä¸Šä»äº‹ç ”å‘å·¥ä½œã€‚åä¸ºä»¥å…¶é«˜å“è´¨çš„äº§å“å’ŒæœåŠ¡èµ¢å¾—äº†å…¨çƒå®¢æˆ·çš„ä¿¡ä»»å’Œå¥½è¯„,ä¹Ÿæ›¾å› å…¶é¢†å…ˆæŠ€æœ¯å’Œåˆ›æ–°ç²¾ç¥è€Œè·å¾—å¤šé¡¹å›½é™…å¥–é¡¹å’Œè®¤å¯ã€‚

    # ç„¶è€Œ,åä¸ºä¹Ÿé¢ä¸´ç€æ¥è‡ªä¸€äº›å›½å®¶æ”¿åºœçš„å®‰å…¨é—®é¢˜å’Œæ”¿æ²»å‹åŠ›,å…¶ä¸­åŒ…æ‹¬ç¾å›½æ”¿åºœå¯¹å…¶äº§å“çš„ç¦ä»¤å’Œé™åˆ¶ã€‚åä¸ºä¸€ç›´åšç§°è‡ªå·±çš„äº§å“æ˜¯å®‰å…¨çš„,å¹¶é‡‡å–äº†ä¸€ç³»åˆ—æªæ–½æ¥ç¡®ä¿å…¶äº§å“çš„å®‰å…¨æ€§å’Œé€æ˜åº¦ã€‚

```

å¦‚æœéœ€è¦åŠ è½½æœ¬åœ°è¯è¡¨ï¼Œè¯·ä¿®æ”¹é…ç½®æ–‡ä»¶ä¸­ä»¥ä¸‹é¡¹ï¼š

  ```yaml
  processor:
    tokenizer:
      vocab_file: "/path/to/tokenizer.model"
  ```

### åŸºäºgenerateçš„å¤šè§’è‰²æ¨ç†

ä¸‹é¢æä¾›ä¸€ä¸ªæ¨¡å‹æ¨ç†æ ·ä¾‹ã€‚

```python
from copy import deepcopy

import mindspore as ms
from mindformers import AutoConfig, AutoModel, AutoTokenizer, AutoProcessor


def process_response(output, history):
    content = ""
    history = deepcopy(history)
    for response in output.split("<|assistant|>"):
        metadata, content = response.split("\n", maxsplit=1)
        if not metadata.strip():
            content = content.strip()
            history.append({"role": "assistant", "metadata": metadata, "content": content})
            content = content.replace("[[è®­ç»ƒæ—¶é—´]]", "2023å¹´")
        else:
            history.append({"role": "assistant", "metadata": metadata, "content": content})
            if history[0]["role"] == "system" and "tools" in history[0]:
                content = "\n".join(content.split("\n")[1:-1])
                def tool_call(**kwargs):
                    return kwargs
                parameters = eval(content)
                content = {"name": metadata.strip(), "parameters": parameters}
            else:
                content = {"name": metadata.strip(), "content": content}
    return content, history


# æŒ‡å®šå›¾æ¨¡å¼ï¼ŒæŒ‡å®šä½¿ç”¨è®­ç»ƒå¡id
ms.set_context(mode=ms.GRAPH_MODE, device_target="Ascend", device_id=0)

# ä»¥ä¸‹ä¸¤ç§tokenizerå®ä¾‹åŒ–æ–¹å¼é€‰å…¶ä¸€å³å¯
# 1. åœ¨çº¿åŠ è½½æ–¹å¼
tokenizer = AutoTokenizer.from_pretrained("glm3_6b")
# 2. æœ¬åœ°åŠ è½½æ–¹å¼
# tokenizer = AutoProcessor.from_pretrained("/path/to/your.yaml").tokenizer

# ä»¥ä¸‹ä¸¤ç§modelçš„å®ä¾‹åŒ–æ–¹å¼é€‰å…¶ä¸€å³å¯
# 1. ç›´æ¥æ ¹æ®é»˜è®¤é…ç½®å®ä¾‹åŒ–
# model = AutoModel.from_pretrained('glm3_6b')
# 2. è‡ªå®šä¹‰ä¿®æ”¹é…ç½®åå®ä¾‹åŒ–
config = AutoConfig.from_pretrained('glm3_6b')
config.use_past = True                  # æ­¤å¤„ä¿®æ”¹é»˜è®¤é…ç½®ï¼Œå¼€å¯å¢é‡æ¨ç†èƒ½å¤ŸåŠ é€Ÿæ¨ç†æ€§èƒ½
config.seq_length = 8192                      # æ ¹æ®éœ€æ±‚è‡ªå®šä¹‰ä¿®æ”¹å…¶ä½™æ¨¡å‹é…ç½®
config.checkpoint_name_or_path = "/path/to/your.ckpt"
model = AutoModel.from_config(config)   # ä»è‡ªå®šä¹‰é…ç½®é¡¹ä¸­å®ä¾‹åŒ–æ¨¡å‹

kwargs={}
gen_kwargs = {"max_length": config.seq_length,"num_beams": 1, "do_sample": False, "top_p": 1,"top_k": 1,
              "temperature": 1,**kwargs}

role="system"
text = "å‡è®¾ä½ ç°åœ¨æ˜¯ä¸€ä¸ªå¯¼æ¸¸ï¼Œè¯·å°½å¯èƒ½è´´è¿‘è¿™ä¸ªè§’è‰²å›ç­”é—®é¢˜ã€‚"
history = []
inputs = tokenizer.build_chat_input(text, history=history, role=role)
inputs = inputs['input_ids']
history.append({'role':role, 'content':text}) # ç¬¬ä¸€ä¸ªè¾“å…¥

outputs = model.generate(inputs, **gen_kwargs)
outputs =outputs[0][len(inputs[0]):-1]
response = tokenizer.decode(outputs)
print(response, flush=True)
# æ‚¨å¥½ï¼Œæˆ‘æ˜¯æ‚¨çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œä¹Ÿå¯ä»¥æ˜¯ä½ çš„å¯¼æ¸¸ã€‚è¯·é—®æœ‰ä»€ä¹ˆé—®é¢˜æˆ‘å¯ä»¥å¸®æ‚¨è§£ç­”å‘¢ï¼Ÿ
response, history = process_response(response, history)
print('history:', flush=True)
print(history, flush=True)

role="user"
text="æˆ‘æ‰“ç®—1æœˆä»½å»æµ·å—ç©ï¼Œå¯ä»¥ä»‹ç»ä¸€ä¸‹æµ·å—æœ‰å“ªäº›å¥½ç©çš„ï¼Œå¥½åƒçš„ä¹ˆï¼Ÿ"
inputs = tokenizer.build_chat_input(text, history=history, role=role)
inputs = inputs['input_ids']
history.append({'role':role, 'content':text}) # ç¬¬äºŒä¸ªè¾“å…¥
outputs = model.generate(inputs, **gen_kwargs) #, eos_token_id=eos_token_id)
outputs =outputs[0][len(inputs[0]):-1]
response = tokenizer.decode(outputs)
print(response, flush=True)
# å½“ç„¶å¯ä»¥ï¼æµ·å—æ˜¯ä¸€ä¸ªé£æ™¯ä¼˜ç¾ã€æ°”å€™å®œäººçš„çƒ­å¸¦æµ·æ´‹çœä»½ï¼Œæ‹¥æœ‰ä¸°å¯Œçš„æ—…æ¸¸èµ„æºå’Œç¾é£Ÿã€‚ä»¥ä¸‹æ˜¯ä¸€äº›æ‚¨å¯èƒ½ä¼šæ„Ÿå…´è¶£çš„æ™¯ç‚¹å’Œç¾é£Ÿï¼š

# 1. æ™¯ç‚¹ï¼š
# - æµ·å—å²›ï¼šè¿™æ˜¯æµ·å—æœ€è‘—åçš„æ™¯ç‚¹ä¹‹ä¸€ï¼Œæ‹¥æœ‰ç¾ä¸½çš„æ²™æ»©å’Œçƒ­å¸¦é›¨æ—ã€‚
# - äºšé¾™æ¹¾ï¼šè¿™æ˜¯æµ·å—æœ€è‘—åçš„æµ·æ»©ä¹‹ä¸€ï¼Œæ‹¥æœ‰æŸ”è½¯çš„æ²™æ»©å’Œæ¸…æ¾ˆçš„æµ·æ°´ã€‚
# - å—å±±å¯ºï¼šè¿™æ˜¯æµ·å—æœ€è‘—åçš„ä½›æ•™å¯ºåº™ä¹‹ä¸€ï¼Œæ‹¥æœ‰ç²¾ç¾çš„å»ºç­‘å’Œæ‚ ä¹…çš„å†å²ã€‚
# - åšé³Œäºšæ´²è®ºå›æ°¸ä¹…ä¼šå€ï¼šè¿™æ˜¯ä¸­å›½æœ€è‘—åçš„å›½é™…ä¼šè®®ä¸­å¿ƒï¼Œä¹Ÿæ˜¯äºšæ´²åœ°åŒºæœ€é‡è¦çš„æ”¿æ²»ã€ç»æµã€æ–‡åŒ–è®ºå›ä¹‹ä¸€ã€‚

# 2. ç¾é£Ÿï¼š
# - æµ·å—é¸¡é¥­ï¼šè¿™æ˜¯æµ·å—æœ€è‘—åçš„ç¾é£Ÿä¹‹ä¸€ï¼Œä»¥é¸¡è‚‰ã€ç±³é¥­å’Œæ¤°æ±ä¸ºä¸»è¦ææ–™ï¼Œå‘³é“é²œç¾ã€‚
# - æµ·é²œï¼šæµ·å—çš„æµ·é²œéå¸¸æ–°é²œï¼Œæ‚¨å¯ä»¥åœ¨å½“åœ°çš„æµ·é²œå¸‚åœºæˆ–é¤å…å“å°åˆ°å„ç§æµ·é²œç¾é£Ÿï¼Œå¦‚æ¸…è’¸æµ·é²œã€çº¢çƒ§æµ·é²œç­‰ã€‚
# - æ¤°å­é¥­ï¼šè¿™æ˜¯æµ·å—æœ€è‘—åçš„ä¼ ç»Ÿç¾é£Ÿä¹‹ä¸€ï¼Œä»¥æ¤°å­è‚‰ã€ç³¯ç±³å’Œæ¤°å­æ±ä¸ºä¸»è¦ææ–™ï¼Œå‘³é“é¦™ç”œã€‚
# - æµ·å—ç²‰ï¼šè¿™æ˜¯æµ·å—æœ€è‘—åçš„ä¼ ç»Ÿå°åƒä¹‹ä¸€ï¼Œä»¥ç±³ç²‰ã€çŒªè‚‰ã€èŠ±ç”Ÿã€è”¬èœç­‰ä¸ºä¸»è¦ææ–™ï¼Œå‘³é“é²œç¾ã€‚

# å¸Œæœ›è¿™äº›ä¿¡æ¯å¯¹æ‚¨æœ‰æ‰€å¸®åŠ©ï¼Œå¦‚æœæ‚¨è¿˜æœ‰å…¶ä»–é—®é¢˜ï¼Œè¯·éšæ—¶é—®æˆ‘ã€‚
response, history = process_response(response, history)

role="user"
text="å“ªé‡Œé€‚åˆå†²æµªå’Œæ½œæ°´å‘¢ï¼Ÿ"
inputs = tokenizer.build_chat_input(text, history=history, role=role)

inputs = inputs['input_ids']
history.append({'role':role, 'content':text}) # ç¬¬ä¸‰ä¸ªè¾“å…¥

outputs = model.generate(inputs, **gen_kwargs)
outputs =outputs[0][len(inputs[0]):-1]
response = tokenizer.decode(outputs)
print(response, flush=True)
# åœ¨æµ·å—ï¼Œå†²æµªå’Œæ½œæ°´çš„å¥½å»å¤„æœ‰å¾ˆå¤šã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å»ºè®®ï¼š

# 1. å†²æµªï¼š
# - èºæ­Œæµ·ï¼šä½äºæµ·å—å²›è¥¿æµ·å²¸ï¼Œæ˜¯å†²æµªçˆ±å¥½è€…çš„å¤©å ‚ã€‚è¿™é‡Œçš„æµ·æµªé€‚ä¸­ï¼Œæ²™æ»©æ¼‚äº®ï¼Œéå¸¸é€‚åˆå†²æµªã€‚
# - ä¸‰äºšï¼šä½äºæµ·å—å²›å—ç«¯ï¼Œæ˜¯æµ·å—æœ€è‘—åçš„å†²æµªèƒœåœ°ä¹‹ä¸€ã€‚è¿™é‡Œçš„æ²™æ»©ç»†è…»ï¼Œæµ·æµªè¾ƒå¤§ï¼Œéå¸¸é€‚åˆå†²æµªã€‚

# 2. æ½œæ°´ï¼š
# - èœˆæ”¯æ´²å²›ï¼šä½äºæµ·å—å²›ä¸œæµ·å²¸ï¼Œæ˜¯æµ·å—æœ€è‘—åçš„æ½œæ°´èƒœåœ°ä¹‹ä¸€ã€‚è¿™é‡Œçš„æ½œæ°´æ¡ä»¶è¾ƒå¥½ï¼Œèƒ½è§åº¦è¾ƒé«˜ï¼Œæ°´ä¸‹ç”Ÿç‰©ä¸°å¯Œï¼Œéå¸¸é€‚åˆæ½œæ°´ã€‚
# - è¥¿æ²™ç¾¤å²›ï¼šä½äºæµ·å—å²›ä¸œå—æ–¹å‘ï¼Œæ˜¯æµ·å—å¦ä¸€ä¸ªè‘—åçš„æ½œæ°´èƒœåœ°ã€‚è¿™é‡Œçš„æ½œæ°´æ¡ä»¶éå¸¸å¥½ï¼Œæ°´ä¸‹ç”Ÿç‰©ä¸°å¯Œï¼Œé€‚åˆå„ç§çº§åˆ«çš„æ½œæ°´çˆ±å¥½è€…ã€‚

# å½“ç„¶ï¼Œå†²æµªå’Œæ½œæ°´éƒ½éœ€è¦ä¸€å®šçš„æŠ€èƒ½å’Œç»éªŒï¼Œå¦‚æœæ‚¨æ˜¯åˆå­¦è€…ï¼Œå»ºè®®åœ¨ä¸“ä¸šäººå£«çš„æŒ‡å¯¼ä¸‹è¿›è¡Œã€‚å¸Œæœ›è¿™äº›ä¿¡æ¯å¯¹æ‚¨æœ‰æ‰€å¸®åŠ©ï¼Œå¦‚æœæ‚¨è¿˜æœ‰å…¶ä»–é—®é¢˜ï¼Œè¯·éšæ—¶é—®æˆ‘ã€‚
response, history = process_response(response, history)

role="user"
text="å¯ä»¥å¸®æˆ‘åšä¸€ä»½æ—…æ¸¸æ”»ç•¥å—ï¼Ÿ"
inputs = tokenizer.build_chat_input(text, history=history, role=role)
inputs = inputs['input_ids']
history.append({'role':role, 'content':text}) # ç¬¬å››ä¸ªè¾“å…¥
outputs = model.generate(inputs, **gen_kwargs)
outputs =outputs[0][len(inputs[0]):-1]
response = tokenizer.decode(outputs)
print(response, flush=True)
#  å½“ç„¶å¯ä»¥ï¼ä»¥ä¸‹æ˜¯ä¸€ä»½ç®€è¦çš„æµ·å—æ—…æ¸¸æ”»ç•¥ï¼Œä¾›æ‚¨å‚è€ƒï¼š

# ä¸€ã€è¡Œç¨‹å®‰æ’ï¼š
# å»ºè®®è¡Œç¨‹å®‰æ’ä¸º7å¤©6å¤œï¼Œå…·ä½“å¦‚ä¸‹ï¼š

# ç¬¬1å¤©ï¼šæŠµè¾¾ä¸‰äºšï¼Œå…¥ä½é…’åº—ï¼Œé€‚åº”ä¸€ä¸‹å½“åœ°çš„æ°”å€™å’Œç¯å¢ƒã€‚

# ç¬¬2å¤©ï¼šæ¸¸è§ˆäºšé¾™æ¹¾ï¼Œäº«å—é˜³å…‰å’Œæ²™æ»©ï¼Œæ™šä¸Šå¯ä»¥å“å°å½“åœ°çš„ç¾é£Ÿã€‚

# ç¬¬3å¤©ï¼šæ¸¸è§ˆå—å±±å¯ºï¼Œæ„Ÿå—ä½›æ•™æ–‡åŒ–çš„é­…åŠ›ï¼Œæ™šä¸Šå¯ä»¥å‰å¾€ä¸‰äºšå¸‚åŒºé€›è¡—è´­ç‰©ã€‚

# ç¬¬4å¤©ï¼šå‰å¾€èœˆæ”¯æ´²å²›ï¼Œäº«å—æ½œæ°´å’Œå†²æµªçš„ä¹è¶£ï¼Œæ™šä¸Šå¯ä»¥åœ¨å²›ä¸Šä½å®¿ã€‚

# ç¬¬5å¤©ï¼šç»§ç»­åœ¨èœˆæ”¯æ´²å²›æ¸¸ç©ï¼Œæ¢ç´¢æ›´å¤šçš„æ½œæ°´ç‚¹å’Œå†²æµªåœºæ‰€ï¼Œæ™šä¸Šå¯ä»¥åœ¨å²›ä¸Šä½å®¿ã€‚

# ç¬¬6å¤©ï¼šå‰å¾€è¥¿æ²™ç¾¤å²›ï¼Œè¿›è¡Œä¸€å¤©ä¸€å¤œçš„æ½œæ°´ä¹‹æ—…ï¼Œæ™šä¸Šè¿”å›ä¸‰äºšã€‚

# ç¬¬7å¤©ï¼šè¿”å›ä¸‰äºšï¼Œç»“æŸè¡Œç¨‹ï¼Œç¦»å¼€æµ·å—ã€‚

# äºŒã€æ³¨æ„äº‹é¡¹ï¼š

# 1. æµ·å—å²›çš„æ°”å€™æ¯”è¾ƒçƒ­ï¼Œå»ºè®®æ‚¨ç©¿ç€è½»ä¾¿çš„è¡£ç‰©ï¼Œå¹¶æ³¨æ„é˜²æ™’ã€‚
# 2. æµ·å—å²›çš„æµ·é²œç¾é£Ÿä¸°å¯Œï¼Œä½†è¯·æ³¨æ„é£Ÿç”¨å®‰å…¨ï¼Œé¿å…é£Ÿç‰©ä¸­æ¯’ã€‚
# 3. åœ¨æµ·æ»©ä¸Šè¦æ³¨æ„å®‰å…¨ï¼Œé¿å…åœ¨æ— äººçš„æµ·æ»©æ¸¸æ³³ï¼Œæ³¨æ„é˜²æ™’å’Œé˜²æ°´ã€‚
# 4. æ½œæ°´å’Œå†²æµªéœ€è¦ä¸€å®šçš„æŠ€èƒ½å’Œç»éªŒï¼Œå»ºè®®åœ¨ä¸“ä¸šäººå£«çš„æŒ‡å¯¼ä¸‹è¿›è¡Œã€‚

# å¸Œæœ›è¿™ä»½æ”»ç•¥å¯¹æ‚¨æœ‰æ‰€å¸®åŠ©ï¼Œå¦‚æœæ‚¨è¿˜æœ‰å…¶ä»–é—®é¢˜ï¼Œè¯·éšæ—¶é—®æˆ‘ã€‚
response, history = process_response(response, history)

```

å¦‚æœéœ€è¦åŠ è½½æœ¬åœ°è¯è¡¨ï¼Œè¯·ä¿®æ”¹é…ç½®æ–‡ä»¶ä¸­ä»¥ä¸‹é¡¹ï¼š

  ```yaml
  processor:
    tokenizer:
      vocab_file: "/path/to/tokenizer.model"
  ```

## Mindspore-Lite æ¨ç†

### åŸºæœ¬ä»‹ç»

ã€€ã€€MindFormers å®šä½æ‰“é€ è®­ç»ƒ->å¾®è°ƒ->éƒ¨ç½²çš„ç«¯åˆ°ç«¯å¤§æ¨¡å‹å·¥å…·å¥—ä»¶ï¼Œä¸ºäº†æ›´å¥½æ€§èƒ½åœ°éƒ¨ç½²å·²ç»å¾®è°ƒè®­ç»ƒå¥½çš„å¤§æ¨¡å‹ï¼Œæˆ‘ä»¬åˆ©ç”¨MindSporeæ‰“é€ çš„æ¨ç†å¼•æ“ [MindSpore_lite](https://gitee.com/link?target=https%3A%2F%2Fwww.mindspore.cn%2Flite)ï¼Œä¸ºç”¨æˆ·æä¾›äº†å¼€ç®±å³ç”¨çš„æ¨ç†éƒ¨ç½²æ–¹æ¡ˆï¼Œä¸ºç”¨æˆ·æä¾›ç«¯åˆ°ç«¯çš„å¤§æ¨¡å‹è§£å†³æ–¹æ¡ˆï¼Œå¸®åŠ©ç”¨æˆ·ä½¿èƒ½å¤§æ¨¡å‹ä¸šåŠ¡ã€‚

ã€€ã€€Lite æ¨ç†å¤§è‡´åˆ†ä¸¤æ­¥ï¼šæƒé‡è½¬æ¢å¯¼å‡º MindIR -> Lite æ¨ç†ï¼Œæ¥ä¸‹æ¥åˆ†åˆ«æè¿°ä¸Šè¿°ä¸¤ä¸ªè¿‡ç¨‹ã€‚

   æœ¬ç« èŠ‚æä¾›ChatGLM3-6Båœ¨MindSpore Liteä¸Šè¿›è¡Œæ¨ç†çš„åŸºæœ¬ä½¿ç”¨æµç¨‹ï¼Œæ›´å¤šè¯¦ç»†çš„ç‰¹æ€§ä»‹ç»å¯ä»¥å‚è€ƒ[Mindspore Liteç‰¹æ€§æ–‡æ¡£](../../docs/feature_cards/Inference.md)

### å•å¡å¯¼å‡ºä¸PAæ¨ç†

#### Step1. MindIR å¯¼å‡º

1. ä¿®æ”¹æ¨¡å‹ç›¸å…³çš„é…ç½®æ–‡ä»¶ configs/glm3/export_glm3_6b_pa.yamlï¼Œå…¶ä¸­éœ€è¦å…³æ³¨è¿™å‡ é¡¹ï¼š

```yaml
# model config
model:
  model_config:
    seq_length: 2048
    checkpoint_name_or_path: "/path/to/your.ckpt"
    use_past: True              # å¼€å¯å¢é‡æ¨ç†
    is_dynamic: True            # ä½¿ç”¨PAæ¨ç†æ—¶è®¾ç½®ä¸ºTrueï¼Œé™æ€shapeæ¨ç†è®¾ä¸ºFalse
    use_paged_attention: True   # ä½¿ç”¨PAæ¨ç†æ—¶è®¾ç½®ä¸ºTrue
    block_size: 16             # PAæ¨ç†çš„å‚æ•°è®¾ç½®
    num_blocks: 224             # PAæ¨ç†çš„å‚æ•°è®¾ç½®
```

2. æ‰§è¡Œrun_mindformer.pyï¼Œå®Œæˆæ¨¡å‹è½¬æ¢

æ‰§è¡Œrun_mindformer.pyï¼Œå®ŒæˆMindIRå¯¼å‡ºï¼Œå¾—åˆ°å…¨é‡minder_full_checkpoint/rank_0_graph.mindirå’Œå¢é‡minder_inc_checkpoint/rank_0_graph.mindirä¸¤ä¸ªMindIRå›¾

```bash
python run_mindformer.py
--config configs/glm3/export_glm3_6b_pa.yaml
--run_mode export
--use_parallel False
--batch_size 1
--device_id 0
```

#### Step2. æ‰§è¡ŒMS Liteæ¨ç†

æ–°å»ºæ¨ç†é…ç½®æ–‡ä»¶ï¼ŒChatGLM3-6Båœ¨Atlas 800T A2ä¸Šæ¨èçš„GEé…ç½®å¦‚ä¸‹ï¼š

1. å…¨é‡å’Œå¢é‡çš„GEé…ç½®ä¸åŒï¼Œå¦‚ä¸‹æ‰€ç¤º

- å…¨é‡mindiræ¨¡å‹PAæ¨ç†é…ç½®ï¼ˆ910b_ge_prefill_pa.cfgï¼‰

```ini
[ascend_context]
plugin_custom_ops=All
provider=ge
[ge_session_options]
ge.exec.formatMode=1
ge.exec.precision_mode=must_keep_origin_dtype
ge.externalWeight=1
ge.exec.atomicCleanPolicy=1
ge.deterministic=1   # æ³¨é‡Šæ­¤è¡Œï¼Œå¯ä»¥æå‡æ¨ç†é€Ÿåº¦
[ge_graph_options]
ge.inputShape=batch_valid_length:1;tokens:1,2048;slot_mapping:2048
[graph_kernel_param]
opt_level=2
disable_cluster_ops=MatMul,Reshape
enable_cce_lib=true
enable_cluster_ops_only="paged_attention"
enable_expand_ops_only="paged_attention"
disable_cce_lib_ops=MatMul
```

- å¢é‡mindiræ¨¡å‹PAæ¨ç†é…ç½®ï¼ˆ910b_ge_inc_pa.cfgï¼‰

```ini
[ascend_context]
plugin_custom_ops=All
provider=ge
[ge_session_options]
ge.exec.formatMode=1
ge.exec.precision_mode=must_keep_origin_dtype
ge.externalWeight=1
ge.exec.atomicCleanPolicy=1
ge.deterministic=1   # æ³¨é‡Šæ­¤è¡Œï¼Œå¯ä»¥æå‡æ¨ç†é€Ÿåº¦
[ge_graph_options]
ge.inputShape=batch_valid_length:-1;block_tables:-1,128;slot_mapping:-1;tokens:-1,1
ge.dynamicDims=1,1,1,1;2,2,2,2;4,4,4,4
ge.dynamicNodeType=1
[graph_kernel_param]
opt_level=2
disable_cluster_ops=MatMul,Reshape
enable_cce_lib=true
enable_cluster_ops_only="paged_attention"
enable_expand_ops_only="paged_attention"
disable_cce_lib_ops=MatMul
```

2. æ‰§è¡Œrun_infer_main.pyè„šæœ¬ï¼Œä¿®æ”¹ç›¸å…³é…ç½®å¯åŠ¨æ¨ç†ï¼š

- PAæ¨ç†æ‰§è¡Œå‘½ä»¤å¦‚ä¸‹ï¼š

```bash
python run_infer_main.py
--batch_size 1
--device_id 0
--model_name glm3
--prefill_model_path /path/to/mindir_full_checkpoint/rank_0_graph.mindir
--increment_model_path /path/to/mindir_inc_checkpoint/rank_0_graph.mindir
--tokenizer_path /path/to/glm3_6b/tokenizer.model
--config_path "configs/glm3/910b_ge_prefill_pa.cfg,configs/glm3/910b_ge_inc_pa.cfg"
--seq_length 2048
--max_length 2048
--dynamic False
--paged_attention True
--pa_block_size 16
--pa_num_blocks 224

# å‚æ•°è¯´æ˜
batch_size: æ¨ç†å¤šbatchè®¾ç½®
device_id: è®¾å¤‡ç‰©ç†ID
model_name: æ¨¡å‹åç§°
prefill_model_path: å…¨é‡å›¾è·¯å¾„
increment_model_path: å¢é‡å›¾è·¯å¾„
tokenizer_path: æ¨¡å‹tokenizerè·¯å¾„
config_path: GEé…ç½®æ–‡ä»¶è·¯å¾„
seq_length: æ¨ç†åºåˆ—é•¿åº¦
max_length: èƒ½å¤Ÿç”Ÿæˆçš„æœ€å¤§è¯­å¥é•¿åº¦
dynamic: æ˜¯å¦é‡‡ç”¨åŒåŠ¨æ€æ¨ç†,æ‰§è¡ŒPAæ¨ç†æ—¶è®¾ç½®ä¸ºFalse
paged_attention: æ˜¯å¦æ‰§è¡ŒPAæ¨ç†
pa_block_size: PAæ¨ç†çš„å‚æ•°
pa_num_blocks: PAæ¨ç†çš„å‚æ•°
```

#### æ¨¡å‹æ¨ç†ç»“æœ

ã€€ã€€ç­‰å¾…æ¨¡å‹è½½å…¥ã€ç¼–è¯‘åï¼Œå‡ºç°ï¼š

```bash
Please enter your predict data:
```

ã€€ã€€è¾“å…¥ï¼š

```bash
ä½ å¥½ã€‚

```

ã€€ã€€è¾“å‡ºï¼š

```bash
['[gMASK]sop<|user|> \n ä½ å¥½<|assistant|> \n ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM3-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚']
```

### å¤šbatchæ¨ç†æµç¨‹ï¼ˆä»¥batch_size=4ä¸ºä¾‹ï¼‰

#### Step1. MindIR å¯¼å‡º

1. ä¿®æ”¹æ¨¡å‹ç›¸å…³çš„é…ç½®æ–‡ä»¶ configs/glm3/export_glm3_6b_pa.yamlï¼Œå…¶ä¸­éœ€è¦å…³æ³¨è¿™å‡ é¡¹ï¼š

```yaml
# model config
model:
  model_config:
    type: ChatGLM2Config
    batch_size: 4         # æ­¤å¤„è®¾ç½®batch sizeå€¼
```

2. æ‰§è¡Œrun_mindformer.pyï¼Œå®Œæˆæ¨¡å‹è½¬æ¢

æ‰§è¡Œrun_mindformer.pyï¼Œå®ŒæˆMindIRå¯¼å‡ºï¼Œå¾—åˆ°å…¨é‡minder_full_checkpoint/rank_0_graph.mindirå’Œå¢é‡minder_inc_checkpoint/rank_0_graph.mindirä¸¤ä¸ªMindIRå›¾

```bash
python run_mindformer.py \
--config configs/glm3/export_glm3_6b_pa.yaml \
--run_mode export \
--use_parallel False \
--batch_size 4 \
--device_id 0
```

#### Step2. æ‰§è¡ŒMS Liteæ¨ç†

æ–°å»ºæ¨ç†é…ç½®æ–‡ä»¶ï¼ŒChatGLM3-6Båœ¨Atlas 800T A2ä¸Šæ¨èçš„GEé…ç½®å¦‚ä¸‹ï¼š

1. å…¨é‡å’Œå¢é‡çš„GEé…ç½®ä¸åŒï¼Œå¦‚ä¸‹æ‰€ç¤º

- å…¨é‡mindiræ¨¡å‹PAæ¨ç†é…ç½®ï¼ˆ910b_ge_prefill_pa.cfgï¼‰ï¼šslot_mappingçš„å€¼ç­‰äºbatch_size*seq_len=4*2048

```ini
[ascend_context]
plugin_custom_ops=All
provider=ge
[ge_session_options]
ge.exec.formatMode=1
ge.exec.precision_mode=must_keep_origin_dtype
ge.externalWeight=1
ge.exec.atomicCleanPolicy=1
ge.deterministic=1   # æ³¨é‡Šæ­¤è¡Œï¼Œå¯ä»¥æå‡æ¨ç†é€Ÿåº¦
[ge_graph_options]
ge.inputShape=batch_valid_length:4;tokens:4,2048;slot_mapping:8192
[graph_kernel_param]
opt_level=2
disable_cluster_ops=MatMul,Reshape
enable_cce_lib=true
enable_cluster_ops_only="paged_attention"
enable_expand_ops_only="paged_attention"
disable_cce_lib_ops=MatMul
```

- å¢é‡mindiræ¨¡å‹PAæ¨ç†é…ç½®ï¼ˆ910b_ge_inc_pa.cfgï¼‰

```ini
[ascend_context]
plugin_custom_ops=All
provider=ge
[ge_session_options]
ge.exec.formatMode=1
ge.exec.precision_mode=must_keep_origin_dtype
ge.externalWeight=1
ge.exec.atomicCleanPolicy=1
ge.deterministic=1   # æ³¨é‡Šæ­¤è¡Œï¼Œå¯ä»¥æå‡æ¨ç†é€Ÿåº¦
[ge_graph_options]
ge.inputShape=batch_valid_length:-1;block_tables:-1,128;slot_mapping:-1;tokens:-1,1
ge.dynamicDims=1,1,1,1;2,2,2,2;4,4,4,4
ge.dynamicNodeType=1
[graph_kernel_param]
opt_level=2
disable_cluster_ops=MatMul,Reshape
enable_cce_lib=true
enable_cluster_ops_only="paged_attention"
enable_expand_ops_only="paged_attention"
disable_cce_lib_ops=MatMul
```

2. æ‰§è¡Œrun_infer_main.pyè„šæœ¬ï¼Œä¿®æ”¹ç›¸å…³é…ç½®å¯åŠ¨æ¨ç†ï¼š

- PAæ¨ç†æ‰§è¡Œå‘½ä»¤å¦‚ä¸‹ï¼š

```bash
python run_infer_main.py \
--batch_size 4 \
--device_id 0 \
--model_name glm3 \
--prefill_model_path /path/to/mindir_full_checkpoint/rank_0_graph.mindir \
--increment_model_path /path/to/mindir_inc_checkpoint/rank_0_graph.mindir \
--tokenizer_path /path/to/glm3_6b/tokenizer.model \
--config_path "configs/glm3/910b_ge_prefill_pa.cfg,configs/glm3/910b_ge_inc_pa.cfg" \
--seq_length 2048 \
--max_length 2048 \
--dynamic False \
--paged_attention True \
--pa_block_size 16 \
--pa_num_blocks 224

# å‚æ•°è¯´æ˜
batch_size: æ¨ç†å¤šbatchè®¾ç½®
```

**æ³¨ï¼š** ä¸ºé€‚é…batch_size=4ï¼Œéœ€è¦ä¿®æ”¹run_infer_main.pyéƒ¨åˆ†ä»£ç ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
def infer_main(args_):
    ...
    if args_.distributed:
        ...
    else:
        while True:
        user_input = input("Please enter your predict data: \n")
        if user_input == "exit":
            print("Task is over.")
            sys.exit()
        user_input = [user_input] * args_.batch_size   # æ­¤å¤„æ–°å¢ä¸€è¡Œä»£ç ï¼Œç”¨äºå¤šbatchæ¨ç†
        ...
```

### CEVALå¼€æºæ•°æ®é›†è¯„æµ‹

#### è¯„æµ‹ç»“æœ

|                                 | Average Accuary |
|---------------------------------|-----------------|
| Atlas 800T A2 + Mindspore (PAæ¨ç†) | 51.41           |
| A100 + Pytorch                  | 51.43           |

**æ³¨ï¼š** è¯„æµ‹ç»“æœæ˜¯åŸºäºå¼€æºçš„é¢„è®­ç»ƒæ¨¡å‹