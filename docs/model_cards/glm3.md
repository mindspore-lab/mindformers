# ChatGLM3

## æ¨¡å‹æè¿°

ChatGLM3 æ˜¯æ™ºè°±AIå’Œæ¸…åå¤§å­¦ KEG å®éªŒå®¤è”åˆå‘å¸ƒçš„æ–°ä¸€ä»£å¯¹è¯é¢„è®­ç»ƒæ¨¡å‹ã€‚ChatGLM3-6B æ˜¯ ChatGLM3 ç³»åˆ—ä¸­çš„å¼€æºæ¨¡å‹ï¼Œåœ¨ä¿ç•™äº†å‰ä¸¤ä»£æ¨¡å‹å¯¹è¯æµç•…ã€éƒ¨ç½²é—¨æ§›ä½ç­‰ä¼—å¤šä¼˜ç§€ç‰¹æ€§çš„åŸºç¡€ä¸Šï¼ŒChatGLM3-6B å¼•å…¥äº†å¦‚ä¸‹ç‰¹æ€§ï¼š**æ›´å¼ºå¤§çš„åŸºç¡€æ¨¡å‹**ï¼Œ**æ›´å®Œæ•´çš„åŠŸèƒ½æ”¯æŒ**ï¼Œ**æ›´å…¨é¢çš„å¼€æºåºåˆ—**

```text
@article{zeng2022glm,
  title={Glm-130b: An open bilingual pre-trained model},
  author={Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and others},
  journal={arXiv preprint arXiv:2210.02414},
  year={2022}
}
```

## ä»“åº“ä»‹ç»

`chatGLM3-6B` åŸºäº `mindformers` å®ç°ï¼Œä¸»è¦æ¶‰åŠçš„æ–‡ä»¶æœ‰ï¼š

1. æ¨¡å‹å…·ä½“å®ç°ï¼š

    ```text
    mindformers/models/glm3
    â”œâ”€â”€ __init__.py
    â””â”€â”€ glm3_tokenizer.py        # tokenizer
    ```

  glm3çš„æ¨¡å‹ç»“æ„å’ŒconfigåŒglm2

2. æ¨¡å‹é…ç½®ï¼š

    ```bash
    configs/glm3
    â”œâ”€â”€ predict_glm3_6b.yaml                              # åœ¨çº¿æ¨ç†é…ç½®æ–‡ä»¶
    â”œâ”€â”€ run_glm3_6b_finetune_2k_800T_A2_64G.yaml          # Atlas 800T A2 æœ€ä½³æ€§èƒ½å…¨é‡å¾®è°ƒå¯åŠ¨é…ç½®
    â”œâ”€â”€ run_glm3_6b_finetune_800T_A2_64G.yaml             # Atlas 800T A2 ADGEN å…¨é‡å¾®è°ƒå¯åŠ¨é…ç½®
    â”œâ”€â”€ run_glm3_6b_multiturn_finetune_800T_A2_64G.yaml   # Atlas 800T A2 å¤šè½®å¯¹è¯å…¨é‡å¾®è°ƒå¯åŠ¨é…ç½®
    â””â”€â”€ run_glm3_6b.yaml                                  # ChatGLM3é…ç½®æ¨¡æ¿
    ```

## å‰æœŸå‡†å¤‡

### ç¯å¢ƒè¦æ±‚

**MindFormerså®‰è£…**ä»¥åŠ**è½¯ç¡¬ä»¶é…å¥—å…³ç³»**å‚è€ƒ[MindFormerså®‰è£…](../../README.md#äºŒMindFormerså®‰è£…)å’Œ[ç‰ˆæœ¬åŒ¹é…å…³ç³»](../../README.md#ä¸‰ç‰ˆæœ¬åŒ¹é…å…³ç³»)ã€‚

### æ¨¡å‹æƒé‡ä¸‹è½½ä¸è½¬æ¢

å¼€å‘è€…å¯ä»¥ä¸‹è½½è·å–å®˜æ–¹æƒé‡åï¼Œé€šè¿‡ä¸‹é¢æä¾›çš„**æƒé‡è½¬æ¢è„šæœ¬**ï¼Œå°†å®˜æ–¹æƒé‡è½¬æ¢ä¸ºMindSporeæƒé‡ï¼›æˆ–ç›´æ¥ä½¿ç”¨MindFormersæä¾›çš„**å·²è½¬æ¢æƒé‡**

1. ä½¿ç”¨å®˜æ–¹æƒé‡è¿›è¡Œè½¬æ¢

   å…‹éš†glm3-6bä»£ç ä»“ï¼Œä¸‹è½½åˆ†å¸ƒå¼çš„æ¨¡å‹æ–‡ä»¶ã€‚

   ```shell
   git lfs install
   git clone https://huggingface.co/THUDM/chatglm3-6b
   ```

   æ‰§è¡Œ python è„šæœ¬ï¼Œåˆå¹¶æ¨¡å‹æƒé‡ã€‚

   ```python
   from transformers import AutoTokenizer, AutoModel
   import torch

   tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True)
   model = AutoModel.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True)

   with open("pt_model_arch.txt", "w") as fp:
       print(model, file=fp, flush=True)
   with open("pt_ckpt.txt", "w") as fp:
       for name, param in model.named_parameters():
           fp.write(f"{name} {param.shape} {param.dtype}\n")
   torch.save(model.state_dict(), "glm3_6b.pth")
   ```

   æ‰§è¡Œè½¬æ¢è„šæœ¬ï¼Œå¾—åˆ°è½¬æ¢åçš„è¾“å‡ºæ–‡ä»¶`glm3_6b.ckpt`ã€‚

   ```python
   import mindspore as ms
   import torch as pt
   from tqdm import tqdm

   pt_ckpt_path = "glm3_6b.pth"
   pt_param = pt.load(pt_ckpt_path)

   type_map = {"torch.float16": "ms.float16",
               "torch.float32": "ms.float32"}
   ms_param = []
   with open("check_pt_ckpt.txt", "w") as fp:
       for k, v in tqdm(pt_param.items()):
           if "word_embeddings.weight" in k:
               k = k.replace("word_embeddings.weight", "embedding_table")
           fp.write(f"{k} {v.shape} {v.dtype}\n")
           ms_param.append({"name": k, "data": ms.Tensor(v.numpy())})

   ms.save_checkpoint(ms_param, "glm3_6b.ckpt")
   ```

2. è·å–MindFormersæä¾›çš„å·²è½¬æ¢æƒé‡

   å¯é€šè¿‡from_pretrainedæ¥å£ä¸‹è½½ï¼Œä¹Ÿå¯ç›´æ¥ä»ä¸‹é¢çš„é“¾æ¥è·å–

   [glm3_6bæƒé‡](https://ascend-repo-modelzoo.obs.cn-east-2.myhuaweicloud.com/XFormer_for_mindspore/glm3/glm3_6b.ckpt)

   [tokenizeræ–‡ä»¶](https://ascend-repo-modelzoo.obs.cn-east-2.myhuaweicloud.com/XFormer_for_mindspore/glm3/tokenizer.model)

### [åˆ†å¸ƒå¼è®­ç»ƒ/å¾®è°ƒæƒé‡åˆå¹¶](../feature_cards/Transform_Ckpt.md)

åˆ†å¸ƒå¼è®­ç»ƒ/å¾®è°ƒåæ‰€å¾—åˆ°çš„æƒé‡æ–‡ä»¶ä¸ºæ ¹æ®ç­–ç•¥åˆ‡åˆ†åçš„æƒé‡ï¼Œéœ€è¦æ‰‹åŠ¨å°†åˆ‡åˆ†æƒé‡åˆä¸€ï¼Œä»¥ç”¨äºè¯„ä¼°å’Œæ¨ç†ã€‚

æ¶‰åŠåˆ°ckptçš„å•å¡ï¼Œå¤šå¡è½¬æ¢ï¼Œè¯¦ç»†æ•™ç¨‹è¯·å‚è€ƒç‰¹æ€§æ–‡æ¡£æ¨¡å‹[æƒé‡åˆ‡åˆ†ä¸åˆå¹¶](../feature_cards/Transform_Ckpt.md)

- step 1. è·å–æ¨¡å‹åˆ‡åˆ†ç­–ç•¥æ–‡ä»¶ï¼š

åœ¨æ‰§è¡Œå¾®è°ƒè„šæœ¬æ—¶ï¼Œæ¨¡å‹å®Œæˆç¼–è¯‘åï¼Œå°†ä¼šåœ¨`output/strategy`è·¯å¾„ä¸‹ç”Ÿæˆå„å¡çš„åˆ‡åˆ†ç­–ç•¥æ–‡ä»¶ï¼Œç”¨äºæƒé‡åˆå¹¶ã€‚

> æ³¨ï¼šloraå¾®è°ƒæ—¶éœ€è¦ç¡®è®¤é…ç½®æ–‡ä»¶`parallel context config`ä¸­`only_trainable_params`è®¾ä¸º`False`ï¼Œä»¥è·å–æ‰€æœ‰å‚æ•°å®Œæ•´ç­–ç•¥ã€‚

- step 2. è¿è¡Œ`mindformers/tools/transform_ckpt.py`è„šæœ¬è¿›è¡Œå¤šå¡æƒé‡åˆå¹¶ï¼š

```shell
python transform_ckpt.py \
--src_ckpt_strategy {path}/output/strategy/ \
--src_ckpt_dir {path}/output/checkpoint/ \
--dst_ckpt_dir {path}/target_checkpoint/ \
--prefix glm3_6b
```

```text
# å‚æ•°è¯´æ˜
src_ckpt_strategy: æ­¥éª¤1ä¸­çš„åˆ‡åˆ†ç­–ç•¥æ–‡ä»¶è·¯å¾„
src_ckpt_dir: åŸåˆ‡åˆ†æƒé‡æ–‡ä»¶å¤¹
dst_ckpt_dir: ç›®æ ‡è·¯å¾„
prefix: ckptæ–‡ä»¶å‰ç¼€å
```

> æ³¨ï¼š`transform_checkpoints` æ¥å£å½“å‰ä»…mindspore 2.0ä»¥ä¸Šç‰ˆæœ¬æ”¯æŒï¼Œå¦‚å½“å‰ç¡¬ä»¶ç¯å¢ƒåªæ”¯æŒ2.0ä»¥ä¸‹ç‰ˆæœ¬ï¼Œå¯ä»¥æ–°å»ºcondaç¯å¢ƒå®‰è£…mindspore 2.0çš„cpuç‰ˆæœ¬ä»¥æ‰§è¡Œè¯¥è„šæœ¬

## å¾®è°ƒ

ä¸‹é¢ä»¥ [ADGEN](https://aclanthology.org/D19-1321.pdf) (å¹¿å‘Šç”Ÿæˆ) æ•°æ®é›†ä¸ºä¾‹ä»‹ç»ä»£ç çš„ä½¿ç”¨æ–¹æ³•

### æ•°æ®é›†å‡†å¤‡

#### è¾“å…¥è¾“å‡ºæ ¼å¼æ•°æ®é›†

ADGEN æ•°æ®é›†ä»»åŠ¡ä¸ºæ ¹æ®è¾“å…¥ï¼ˆcontentï¼‰ç”Ÿæˆä¸€æ®µå¹¿å‘Šè¯ï¼ˆsummaryï¼‰ã€‚

```yaml
{"content": "ç±»å‹#ä¸Šè¡£*ç‰ˆå‹#å®½æ¾*ç‰ˆå‹#æ˜¾ç˜¦*å›¾æ¡ˆ#çº¿æ¡*è¡£æ ·å¼#è¡¬è¡«*è¡£è¢–å‹#æ³¡æ³¡è¢–*è¡£æ¬¾å¼#æŠ½ç»³", "summary": "è¿™ä»¶è¡¬è¡«çš„æ¬¾å¼éå¸¸çš„å®½æ¾ï¼Œåˆ©è½çš„çº¿æ¡å¯ä»¥å¾ˆå¥½çš„éšè—èº«æä¸Šçš„å°ç¼ºç‚¹ï¼Œç©¿åœ¨èº«ä¸Šæœ‰ç€å¾ˆå¥½çš„æ˜¾ç˜¦æ•ˆæœã€‚é¢†å£è£…é¥°äº†ä¸€ä¸ªå¯çˆ±çš„æŠ½ç»³ï¼Œæ¼‚äº®çš„ç»³ç»“å±•ç°å‡ºäº†åè¶³çš„ä¸ªæ€§ï¼Œé…åˆæ—¶å°šçš„æ³¡æ³¡è¢–å‹ï¼Œå°½æ˜¾å¥³æ€§ç”œç¾å¯çˆ±çš„æ°”æ¯ã€‚"}
```

å‚ç…§ [ChatGLMä»“åº“](https://github.com/THUDM/ChatGLM-6B/tree/main/ptuning) çš„æŒ‡å¼•ä¸‹è½½å¤„ç†å¥½çš„ ADGEN æ•°æ®é›†ï¼Œç›®å½•ç»“æ„ä¸º

```text
AdvertiseGen
  â”œâ”€â”€ train.json
  â””â”€â”€ dev.json
```

ä¿®æ”¹é…ç½®æ–‡ä»¶ `configs/glm3/run_glm3_6b_finetune*.yaml` ä¸­çš„ä»¥ä¸‹é¡¹ï¼š

```yaml
train_dataset: &train_dataset
    dataset_dir: "/path/to/AdvertiseGen/train.json"
    origin_columns: ["content", "summary"]
  tokenizer:
    vocab_file: "/path/to/tokenizer.model"
  input_columns: ["input_ids", "labels"]
  max_source_length: 64
  max_target_length: 127

eval_dataset: &eval_dataset
  data_loader:
    dataset_dir: "/path/to/AdvertiseGen/dev.json"
    origin_columns: ["content", "summary"]
  tokenizer:
    vocab_file: "/path/to/tokenizer.model"
  max_source_length: 256
  max_target_length: 256
```

**æ³¨æ„**ï¼šå¾®è°ƒæ—¶çš„æ¨¡å‹`seq_length`éœ€è¦ç­‰äºå¾®è°ƒæ•°æ®é›†çš„`max_source_length + max_target_length + 1`ã€‚
yamlæ–‡ä»¶ä¸­é»˜è®¤çš„`seq_length: 192`ä»¥åŠ`max_source_length: 64`å’Œ`max_target_length: 127`é€‚ç”¨äºADGENæ•°æ®é›†ï¼Œ
å…¶ä»–æ•°æ®é›†çš„`seq_length`è®¾ç½®ï¼Œå¯ä»¥éå†å¹¶å°†æ•°æ®é›†è½¬æ¢ä¸ºtoken_idï¼Œå–token_idæœ€å¤§é•¿åº¦ï¼Œ`seq_length`å¤ªå¤§å½±å“è®­ç»ƒæ€§èƒ½ï¼Œ
å¤ªå°å½±å“è®­ç»ƒç²¾åº¦ï¼Œéœ€è¦åšå‡ºæƒè¡¡ã€‚

#### å¤šè½®å¯¹è¯æ ¼å¼æ•°æ®é›†

é¦–å…ˆï¼Œå…‹éš† [ToolAlpaca æ•°æ®é›†](https://github.com/tangqiaoyu/ToolAlpaca)ï¼Œå¹¶ä¸‹è½½å¤„ç†è„šæœ¬ [format_tool_alpaca.py](https://github.com/THUDM/ChatGLM3/blob/7cd5bc78bd6232d02764b60b33874bb2d63a0df0/finetune_chatmodel_demo/scripts/format_tool_alpaca.py)ï¼Œç„¶åæ‰§è¡Œè„šæœ¬æ‰§è¡Œè„šæœ¬ï¼š

```shell
python mindformers/tools/format_tool_alpaca.py --path ToolAlpaca/data/train_data.json
```

è„šæœ¬ä¼šåœ¨æ‰§è¡Œç›®å½•ä¸‹ç”Ÿæˆ formatted_data/tool_alpaca.jsonl

ä¹Ÿå¯ä»¥åœ¨è¿™é‡Œä¸‹è½½å¤„ç†å¥½çš„æ•°æ®é›†ï¼š

[tool_alpaca.jsonl](https://ascend-repo-modelzoo.obs.cn-east-2.myhuaweicloud.com/XFormer_for_mindspore/glm3/tool_alpaca.jsonl)

å¾®è°ƒæ—¶é€‰æ‹©é…ç½®æ–‡ä»¶ï¼š`configs/glm3/run_glm3_6b_multiturn_finetune*.yaml`

### å…¨å‚å¾®è°ƒ

å…¨å‚å¾®è°ƒä½¿ç”¨ `configs/glm3/run_glm3_6b_finetune*.yaml` é…ç½®æ–‡ä»¶ï¼Œé…ç½®æ–‡ä»¶ä¸­å®šä¹‰äº†å¾®è°ƒæ‰€éœ€çš„å„é…ç½®é¡¹

ä¿®æ”¹æ•°æ®é›†/æ¨¡å‹æƒé‡é…ç½®è·¯å¾„ï¼š

- æ•°æ®é›†ï¼šä¿®æ”¹ `configs/glm3/run_glm3_6b_finetune*.yaml` è„šæœ¬ä¸­`train_dataset` çš„ `dataset_dir` ä¸ºå‰æ–‡ç”Ÿæˆçš„æ•°æ®é›†è·¯å¾„ã€‚
- åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æƒé‡ï¼šä¿®æ”¹ `configs/glm3/run_glm3_6b_finetune*.yaml` è„šæœ¬ä¸­çš„ `load_checkpoint` ä¸ºé¢„è®­ç»ƒæ¨¡å‹æƒé‡è·¯å¾„ã€‚

å½“å‰æ¨¡å‹å·²æ”¯æŒä½¿ç”¨**Flash Attentionç®—æ³•**è¿›è¡Œå…¨å‚å¾®è°ƒï¼Œè¯·å‚è€ƒ [Flash Attentionä½¿ç”¨æ–‡æ¡£](../feature_cards/Training_Algorithms.md#flash-attention)

#### å•å¡å¾®è°ƒ

ç”±äºglm3_6bæ¨¡å‹è¾ƒå¤§ï¼Œå…¨é‡å¾®è°ƒä¸æ”¯æŒå•å¡è¿è¡Œ

#### å¤šå¡å¾®è°ƒ

- å•æœºå¤šå¡

æ¨èä½¿ç”¨msrunæ–¹å¼å¯åŠ¨

```bash
cd {mindformersæ ¹ç›®å½•}
bash scripts/msrun_launcher.sh "run_mindformer.py \
    --config configs/glm3/run_glm3_6b_finetune_2k_800T_A2_64G.yaml \
    --run_mode finetune"
```

## æ¨ç†

### åŸºæœ¬ä»‹ç»

ã€€ã€€MindFormers å®šä½æ‰“é€ è®­ç»ƒ->å¾®è°ƒ->éƒ¨ç½²çš„ç«¯åˆ°ç«¯å¤§æ¨¡å‹å·¥å…·å¥—ä»¶ï¼Œä¸ºäº†æ›´å¥½æ€§èƒ½åœ°éƒ¨ç½²å·²ç»å¾®è°ƒè®­ç»ƒå¥½çš„å¤§æ¨¡å‹ï¼Œæˆ‘ä»¬åˆ©ç”¨MindSporeæ‰“é€ äº†å…¨æ–°çš„è®­æ¨ä¸€ä½“é«˜æ€§èƒ½æ¨ç†å¼•æ“ï¼Œä¿è¯è®­ç»ƒä¸æ¨ç†ä½¿ç”¨åŒä¸€å¥—è„šæœ¬ï¼Œä¸ºç”¨æˆ·æä¾›äº†å¼€ç®±å³ç”¨çš„æ¨ç†éƒ¨ç½²æ–¹æ¡ˆï¼Œä¸ºç”¨æˆ·æä¾›ç«¯åˆ°ç«¯çš„å¤§æ¨¡å‹è§£å†³æ–¹æ¡ˆï¼Œå¸®åŠ©ç”¨æˆ·ä½¿èƒ½å¤§æ¨¡å‹ä¸šåŠ¡ã€‚

ã€€ã€€MindSpore å¤§æ¨¡å‹æ¨ç†å‡çº§è®­æ¨ä¸€ä½“æ¶æ„ï¼Œå®ç°è„šæœ¬ã€åˆ†å¸ƒå¼ç­–ç•¥å’Œè¿è¡Œæ—¶çš„ç»Ÿä¸€ï¼Œé€šè¿‡èåˆå¤§ç®—å­é™ä½æ¨ç†æ—¶å»¶ï¼Œæœ‰æ•ˆæå‡ç½‘ç»œååé‡ã€‚

### åŸºäºgenerateçš„æ¨ç†

ä¸‹é¢æä¾›ä¸€ä¸ªæ¨¡å‹æ¨ç†æ ·ä¾‹è„šæœ¬ `infer.py`

```python
import mindspore as ms
from mindformers import AutoConfig, AutoModel, ChatGLM3Tokenizer

# æŒ‡å®šå›¾æ¨¡å¼ï¼ŒæŒ‡å®šä½¿ç”¨è®­ç»ƒå¡id
ms.set_context(mode=ms.GRAPH_MODE, device_target="Ascend", device_id=0)

# æœ¬åœ°åŠ è½½æ–¹å¼ï¼Œé…ç½®ä¸ºæƒé‡ä¸‹è½½ç« èŠ‚çš„tokenizeræ–‡ä»¶
tokenizer = ChatGLM3Tokenizer("/path/to/tokenizer.model")

# è‡ªå®šä¹‰ä¿®æ”¹é…ç½®åå®ä¾‹åŒ–ï¼Œé…ç½®ä¸ºyamlæ–‡ä»¶è·¯å¾„ï¼Œç¤ºä¾‹yamlæ–‡ä»¶ä¸ºconfigs/glm3/predict_glm3_6b.yaml
config = AutoConfig.from_pretrained("/path/to/your.yaml")
model = AutoModel.from_config(config)   # ä»è‡ªå®šä¹‰é…ç½®é¡¹ä¸­å®ä¾‹åŒ–æ¨¡å‹

role="user"

inputs_list=["ä½ å¥½", "è¯·ä»‹ç»ä¸€ä¸‹åä¸º"]

for input_item in inputs_list:
    history=[]
    inputs = tokenizer.build_chat_input(input_item, history=history, role=role)
    inputs = inputs['input_ids']
    # é¦–æ¬¡è°ƒç”¨model.generate()è¿›è¡Œæ¨ç†å°†åŒ…å«å›¾ç¼–è¯‘æ—¶é—´ï¼Œæ¨ç†æ€§èƒ½æ˜¾ç¤ºä¸å‡†ç¡®ï¼Œå¤šæ¬¡é‡å¤è°ƒç”¨ä»¥è·å–å‡†ç¡®çš„æ¨ç†æ€§èƒ½
    outputs = model.generate(inputs, do_sample=False, top_k=1, max_length=config.seq_length)
    response = tokenizer.decode(outputs)
    for i, output in enumerate(outputs):
        output = output[len(inputs[i]):]
        response = tokenizer.decode(output)
        print(response)
    # answer 1:
    # ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM3-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚

    # answer 2:
    # åä¸ºæ˜¯ä¸€å®¶æ€»éƒ¨ä½äºä¸­å›½æ·±åœ³çš„å¤šå…ƒåŒ–ç§‘æŠ€å…¬å¸,æˆç«‹äº1987å¹´,æ˜¯å…¨çƒæœ€å¤§çš„ç”µä¿¡è®¾å¤‡åˆ¶é€ å•†ä¹‹ä¸€ã€‚è¯¥å…¬å¸ä¹Ÿåœ¨æ™ºèƒ½æ‰‹æœºã€ç”µè„‘ã€å¹³æ¿ç”µè„‘ã€äº‘è®¡ç®—ç­‰é¢†åŸŸå¼€å±•ä¸šåŠ¡,å…¶äº§å“å’ŒæœåŠ¡è¦†ç›–å…¨çƒ170å¤šä¸ªå›½å®¶å’Œåœ°åŒºã€‚

    # åä¸ºçš„ä¸»è¦ä¸šåŠ¡åŒ…æ‹¬ç”µä¿¡ç½‘ç»œè®¾å¤‡ã€æ™ºèƒ½æ‰‹æœºã€ç”µè„‘å’Œæ¶ˆè´¹ç”µå­äº§å“ã€‚å…¬å¸åœ¨å…¨çƒèŒƒå›´å†…æœ‰è¶…è¿‡190,000åå‘˜å·¥,å…¶ä¸­çº¦ä¸€åŠä»¥ä¸Šä»äº‹ç ”å‘å·¥ä½œã€‚åä¸ºä»¥å…¶é«˜å“è´¨çš„äº§å“å’ŒæœåŠ¡èµ¢å¾—äº†å…¨çƒå®¢æˆ·çš„ä¿¡ä»»å’Œå¥½è¯„,ä¹Ÿæ›¾å› å…¶é¢†å…ˆæŠ€æœ¯å’Œåˆ›æ–°ç²¾ç¥è€Œè·å¾—å¤šé¡¹å›½é™…å¥–é¡¹å’Œè®¤å¯ã€‚

    # ç„¶è€Œ,åä¸ºä¹Ÿé¢ä¸´ç€æ¥è‡ªä¸€äº›å›½å®¶æ”¿åºœçš„å®‰å…¨é—®é¢˜å’Œæ”¿æ²»å‹åŠ›,å…¶ä¸­åŒ…æ‹¬ç¾å›½æ”¿åºœå¯¹å…¶äº§å“çš„ç¦ä»¤å’Œé™åˆ¶ã€‚åä¸ºä¸€ç›´åšç§°è‡ªå·±çš„äº§å“æ˜¯å®‰å…¨çš„,å¹¶é‡‡å–äº†ä¸€ç³»åˆ—æªæ–½æ¥ç¡®ä¿å…¶äº§å“çš„å®‰å…¨æ€§å’Œé€æ˜åº¦ã€‚

```

å¦‚æœéœ€è¦è¿›è¡Œæ¨ç†ï¼Œè¯·ä¿®æ”¹é…ç½®æ–‡ä»¶ä¸­(æ¨ç†è„šæœ¬ä¸­/path/to/your.yaml)çš„checkpoint_name_or_pathé¡¹ï¼Œé…ç½®ä¸ºæƒé‡ä¸‹è½½ç« èŠ‚ä¸‹è½½çš„æƒé‡æ–‡ä»¶ï¼š

  ```yaml
  model:
    model_config:
      checkpoint_name_or_path: "/path/to/glm3_6b.ckpt"
  ```

### åŸºäºgenerateçš„å¤šè§’è‰²æ¨ç†

ä¸‹é¢æä¾›ä¸€ä¸ªæ¨¡å‹æ¨ç†æ ·ä¾‹ã€‚

```python
from copy import deepcopy

import mindspore as ms
from mindformers import AutoConfig, AutoModel, ChatGLM3Tokenizer


def process_response(output, history):
    content = ""
    history = deepcopy(history)
    for response in output.split("<|assistant|>"):
        metadata, content = response.split("\n", maxsplit=1)
        if not metadata.strip():
            content = content.strip()
            history.append({"role": "assistant", "metadata": metadata, "content": content})
            content = content.replace("[[è®­ç»ƒæ—¶é—´]]", "2023å¹´")
        else:
            history.append({"role": "assistant", "metadata": metadata, "content": content})
            if history[0]["role"] == "system" and "tools" in history[0]:
                content = "\n".join(content.split("\n")[1:-1])
                def tool_call(**kwargs):
                    return kwargs
                parameters = eval(content)
                content = {"name": metadata.strip(), "parameters": parameters}
            else:
                content = {"name": metadata.strip(), "content": content}
    return content, history


# æŒ‡å®šå›¾æ¨¡å¼ï¼ŒæŒ‡å®šä½¿ç”¨è®­ç»ƒå¡id
ms.set_context(mode=ms.GRAPH_MODE, device_target="Ascend", device_id=0)

# æœ¬åœ°åŠ è½½æ–¹å¼ï¼Œé…ç½®ä¸ºæƒé‡ä¸‹è½½ç« èŠ‚çš„tokenizeræ–‡ä»¶
tokenizer = ChatGLM3Tokenizer("/path/to/tokenizer.model")

# è‡ªå®šä¹‰ä¿®æ”¹é…ç½®åå®ä¾‹åŒ–ï¼Œé…ç½®ä¸ºyamlæ–‡ä»¶è·¯å¾„ï¼Œç¤ºä¾‹yamlæ–‡ä»¶ä¸ºconfigs/glm3/predict_glm3_6b.yaml
config = AutoConfig.from_pretrained("/path/to/your.yaml")
model = AutoModel.from_config(config)   # ä»è‡ªå®šä¹‰é…ç½®é¡¹ä¸­å®ä¾‹åŒ–æ¨¡å‹

kwargs={}
gen_kwargs = {"max_length": config.seq_length,"num_beams": 1, "do_sample": False, "top_p": 1,"top_k": 1,
              "temperature": 1,**kwargs}

role="system"
text = "å‡è®¾ä½ ç°åœ¨æ˜¯ä¸€ä¸ªå¯¼æ¸¸ï¼Œè¯·å°½å¯èƒ½è´´è¿‘è¿™ä¸ªè§’è‰²å›ç­”é—®é¢˜ã€‚"
history = []
inputs = tokenizer.build_chat_input(text, history=history, role=role)
inputs = inputs['input_ids']
history.append({'role':role, 'content':text}) # ç¬¬ä¸€ä¸ªè¾“å…¥

outputs = model.generate(inputs, **gen_kwargs)
outputs =outputs[0][len(inputs[0]):-1]
response = tokenizer.decode(outputs)
print(response, flush=True)
# æ‚¨å¥½ï¼Œæˆ‘æ˜¯æ‚¨çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œä¹Ÿå¯ä»¥æ˜¯ä½ çš„å¯¼æ¸¸ã€‚è¯·é—®æœ‰ä»€ä¹ˆé—®é¢˜æˆ‘å¯ä»¥å¸®æ‚¨è§£ç­”å‘¢ï¼Ÿ
response, history = process_response(response, history)
print('history:', flush=True)
print(history, flush=True)

role="user"
text="æˆ‘æ‰“ç®—1æœˆä»½å»æµ·å—ç©ï¼Œå¯ä»¥ä»‹ç»ä¸€ä¸‹æµ·å—æœ‰å“ªäº›å¥½ç©çš„ï¼Œå¥½åƒçš„ä¹ˆï¼Ÿ"
inputs = tokenizer.build_chat_input(text, history=history, role=role)
inputs = inputs['input_ids']
history.append({'role':role, 'content':text}) # ç¬¬äºŒä¸ªè¾“å…¥
outputs = model.generate(inputs, **gen_kwargs) #, eos_token_id=eos_token_id)
outputs =outputs[0][len(inputs[0]):-1]
response = tokenizer.decode(outputs)
print(response, flush=True)
#  å½“ç„¶å¯ä»¥ï¼æµ·å—æ˜¯ä¸€ä¸ªé£æ™¯ä¼˜ç¾ã€æ°”å€™å®œäººçš„çƒ­å¸¦æµ·æ´‹çœä»½ï¼Œæ‹¥æœ‰ä¸°å¯Œçš„æ—…æ¸¸èµ„æºå’Œç¾é£Ÿã€‚ä»¥ä¸‹æ˜¯ä¸€äº›æ‚¨å¯èƒ½ä¼šæ„Ÿå…´è¶£çš„æ™¯ç‚¹å’Œç¾é£Ÿï¼š

# 1. æ™¯ç‚¹ï¼š
# - æµ·å—å²›ï¼šè¿™æ˜¯æµ·å—æœ€è‘—åçš„æ™¯ç‚¹ä¹‹ä¸€ï¼Œæ‹¥æœ‰ç¾ä¸½çš„æ²™æ»©å’Œçƒ­å¸¦é›¨æ—ã€‚
# - äºšé¾™æ¹¾ï¼šè¿™æ˜¯æµ·å—æœ€è‘—åçš„æµ·æ»©ä¹‹ä¸€ï¼Œæ‹¥æœ‰æŸ”è½¯çš„æ²™æ»©å’Œæ¸…æ¾ˆçš„æµ·æ°´ã€‚
# - å—å±±å¯ºï¼šè¿™æ˜¯æµ·å—æœ€è‘—åçš„ä½›æ•™å¯ºåº™ä¹‹ä¸€ï¼Œæ‹¥æœ‰ç²¾ç¾çš„å»ºç­‘å’Œæ‚ ä¹…çš„å†å²ã€‚
# - åšé³Œäºšæ´²è®ºå›æ°¸ä¹…ä¼šå€ï¼šè¿™æ˜¯ä¸­å›½æœ€è‘—åçš„å›½é™…ä¼šè®®ä¸­å¿ƒï¼Œä½äºæµ·å—å²›ä¸œæµ·å²¸ã€‚

# 2. ç¾é£Ÿï¼š
# - æµ·å—é¸¡é¥­ï¼šè¿™æ˜¯æµ·å—æœ€è‘—åçš„ç¾é£Ÿä¹‹ä¸€ï¼Œç”±é¸¡è‚‰ã€ç±³é¥­å’Œæ¤°æ±ç»„æˆã€‚
# - æµ·é²œï¼šæµ·å—å²›å‘¨å›´çš„æµ·åŸŸæ‹¥æœ‰ä¸°å¯Œçš„æµ·é²œèµ„æºï¼ŒåŒ…æ‹¬èƒèŸ¹ã€é¾™è™¾ã€é±¼ç±»ç­‰ã€‚
# - æ¤°å­é¥­ï¼šè¿™æ˜¯æµ·å—æœ€è‘—åçš„ä¼ ç»Ÿç¾é£Ÿä¹‹ä¸€ï¼Œç”±æ¤°å­è‚‰å’Œç³¯ç±³ç»„æˆã€‚
# - æµ·å—ç²‰ï¼šè¿™æ˜¯æµ·å—çš„ä¸€ç§ä¼ ç»Ÿå°åƒï¼Œç”±ç±³ç²‰å’Œå„ç§è‚‰ç±»ã€æµ·é²œã€è”¬èœç»„æˆã€‚

# å¸Œæœ›è¿™äº›ä¿¡æ¯å¯¹æ‚¨æœ‰æ‰€å¸®åŠ©ï¼Œå¦‚æœæ‚¨è¿˜æœ‰å…¶ä»–é—®é¢˜ï¼Œè¯·éšæ—¶é—®æˆ‘ã€‚
response, history = process_response(response, history)

role="user"
text="å“ªé‡Œé€‚åˆå†²æµªå’Œæ½œæ°´å‘¢ï¼Ÿ"
inputs = tokenizer.build_chat_input(text, history=history, role=role)

inputs = inputs['input_ids']
history.append({'role':role, 'content':text}) # ç¬¬ä¸‰ä¸ªè¾“å…¥

outputs = model.generate(inputs, **gen_kwargs)
outputs =outputs[0][len(inputs[0]):-1]
response = tokenizer.decode(outputs)
print(response, flush=True)
#  åœ¨æµ·å—å²›ï¼Œå†²æµªå’Œæ½œæ°´çš„å¥½å»å¤„æœ‰å¾ˆå¤šã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å»ºè®®ï¼š

# 1. å†²æµªï¼š
# - èºæ­Œæµ·ï¼šä½äºæµ·å—å²›è¥¿å—éƒ¨ï¼Œæ˜¯å†²æµªçˆ±å¥½è€…çš„å¤©å ‚ã€‚è¿™é‡Œçš„æµ·æµªé€‚ä¸­ï¼Œæ²™æ»©æ¼‚äº®ï¼Œéå¸¸é€‚åˆå†²æµªã€‚
# - ä¸‰äºšï¼šä½äºæµ·å—å²›å—ç«¯ï¼Œæ‹¥æœ‰è®¸å¤šä¼˜è´¨çš„å†²æµªåœºåœ°ï¼Œå¦‚èœˆæ”¯æ´²å²›ã€å¤§å°æ´å¤©ç­‰ã€‚

# 2. æ½œæ°´ï¼š
# - èœˆæ”¯æ´²å²›ï¼šä½äºæµ·å—å²›ä¸œå—éƒ¨ï¼Œæ˜¯æ½œæ°´çˆ±å¥½è€…çš„å¤©å ‚ã€‚è¿™é‡Œçš„æ½œæ°´æ¡ä»¶ä¼˜è¶Šï¼Œæ‹¥æœ‰ä¸°å¯Œçš„çŠç‘šç¤å’Œæµ·æ´‹ç”Ÿç‰©ã€‚
# - èºæ­Œæµ·ï¼šä½äºæµ·å—å²›è¥¿å—éƒ¨ï¼Œè¿™é‡Œçš„æµ·æ°´æ¸…æ¾ˆï¼Œæ½œæ°´ visibility å¾ˆé«˜ï¼Œéå¸¸é€‚åˆæ½œæ°´ã€‚

# å½“ç„¶ï¼Œè¿˜æœ‰å…¶ä»–ä¸€äº›æ™¯ç‚¹ä¹Ÿé€‚åˆå†²æµªå’Œæ½œæ°´ï¼Œå¦‚æµ·å—å²›çš„ä¸œæµ·å²¸ã€è¥¿æµ·å²¸ç­‰ã€‚å…·ä½“é€‰æ‹©å–å†³äºæ‚¨çš„å…´è¶£å’Œç»éªŒã€‚å¸Œæœ›è¿™äº›å»ºè®®å¯¹æ‚¨æœ‰æ‰€å¸®åŠ©ï¼
response, history = process_response(response, history)

role="user"
text="å¯ä»¥å¸®æˆ‘åšä¸€ä»½æ—…æ¸¸æ”»ç•¥å—ï¼Ÿ"
inputs = tokenizer.build_chat_input(text, history=history, role=role)
inputs = inputs['input_ids']
history.append({'role':role, 'content':text}) # ç¬¬å››ä¸ªè¾“å…¥
outputs = model.generate(inputs, **gen_kwargs)
outputs =outputs[0][len(inputs[0]):-1]
response = tokenizer.decode(outputs)
print(response, flush=True)
#  å½“ç„¶å¯ä»¥ï¼ä»¥ä¸‹æ˜¯ä¸€ä»½ç®€è¦çš„æµ·å—æ—…æ¸¸æ”»ç•¥ï¼Œä¾›æ‚¨å‚è€ƒï¼š

# ä¸€ã€è¡Œç¨‹å®‰æ’ï¼š
# å»ºè®®è¡Œç¨‹ä¸º 4-5 å¤©ï¼Œå…·ä½“è¡Œç¨‹å¯ä»¥æ ¹æ®æ‚¨çš„éœ€æ±‚å’Œæ—¶é—´è¿›è¡Œè°ƒæ•´ã€‚

# 1. ç¬¬ä¸€å¤©ï¼šæŠµè¾¾ä¸‰äºšï¼Œå…¥ä½é…’åº—ï¼Œé€‚åº”ä¸€ä¸‹å½“åœ°çš„æ°”å€™å’Œç¯å¢ƒã€‚
# 2. ç¬¬äºŒå¤©ï¼šæ¸¸è§ˆäºšé¾™æ¹¾ï¼Œäº«å—é˜³å…‰å’Œæ²™æ»©ï¼Œæ™šä¸Šå“å°å½“åœ°çš„ç¾é£Ÿã€‚
# 3. ç¬¬ä¸‰å¤©ï¼šæ¸¸è§ˆå—å±±å¯ºï¼Œæ„Ÿå—ä½›æ•™æ–‡åŒ–çš„é­…åŠ›ï¼Œæ™šä¸Šå¯ä»¥åœ¨ä¸‰äºšå¸‚åŒºå“å°å½“åœ°çš„ç¾é£Ÿã€‚
# 4. ç¬¬å››å¤©ï¼šå‰å¾€èœˆæ”¯æ´²å²›ï¼Œäº«å—æ½œæ°´å’Œå†²æµªçš„ä¹è¶£ï¼Œæ™šä¸Šè¿”å›ä¸‰äºšã€‚
# 5. ç¬¬äº”å¤©ï¼šå‰å¾€åšé³Œäºšæ´²è®ºå›æ°¸ä¹…ä¼šå€ï¼Œæ¸¸è§ˆåšé³Œ townï¼Œæ™šä¸Šè¿”å›ä¸‰äºšï¼Œç»“æŸè¡Œç¨‹ã€‚

# äºŒã€æ™¯ç‚¹æ¨èï¼š
# 1. æµ·å—å²›ï¼šè¿™æ˜¯æµ·å—æœ€è‘—åçš„æ™¯ç‚¹ä¹‹ä¸€ï¼Œæ‹¥æœ‰ç¾ä¸½çš„æ²™æ»©å’Œçƒ­å¸¦é›¨æ—ã€‚
# 2. äºšé¾™æ¹¾ï¼šè¿™æ˜¯æµ·å—æœ€è‘—åçš„æµ·æ»©ä¹‹ä¸€ï¼Œæ‹¥æœ‰æŸ”è½¯çš„æ²™æ»©å’Œæ¸…æ¾ˆçš„æµ·æ°´ã€‚
# 3. å—å±±å¯ºï¼šè¿™æ˜¯æµ·å—æœ€è‘—åçš„ä½›æ•™å¯ºåº™ä¹‹ä¸€ï¼Œæ‹¥æœ‰ç²¾ç¾çš„å»ºç­‘å’Œæ‚ ä¹…çš„å†å²ã€‚
# 4. åšé³Œäºšæ´²è®ºå›æ°¸ä¹…ä¼šå€ï¼šè¿™æ˜¯ä¸­å›½æœ€è‘—åçš„å›½é™…ä¼šè®®ä¸­å¿ƒï¼Œä½äºæµ·å—å²›ä¸œæµ·å²¸ã€‚

# ä¸‰ã€ç¾é£Ÿæ¨èï¼š
# 1. æµ·å—é¸¡é¥­ï¼šè¿™æ˜¯æµ·å—æœ€è‘—åçš„ç¾é£Ÿä¹‹ä¸€ï¼Œç”±é¸¡è‚‰ã€ç±³é¥­å’Œæ¤°æ±ç»„æˆã€‚
# 2. æµ·é²œï¼šæµ·å—å²›å‘¨å›´çš„æµ·åŸŸæ‹¥æœ‰ä¸°å¯Œçš„æµ·é²œèµ„æºï¼ŒåŒ…æ‹¬èƒèŸ¹ã€é¾™è™¾ã€é±¼ç±»ç­‰ã€‚
# 3. æ¤°å­é¥­ï¼šè¿™æ˜¯æµ·å—æœ€è‘—åçš„ä¼ ç»Ÿç¾é£Ÿä¹‹ä¸€ï¼Œç”±æ¤°å­è‚‰å’Œç³¯ç±³ç»„æˆã€‚
# 4. æµ·å—ç²‰ï¼šè¿™æ˜¯æµ·å—çš„ä¸€ç§ä¼ ç»Ÿå°åƒï¼Œç”±ç±³ç²‰å’Œå„ç§è‚‰ç±»ã€æµ·é²œã€è”¬èœç»„æˆã€‚

# å››ã€ä½å®¿æ¨èï¼š
# å»ºè®®é€‰æ‹©ä¸‰äºšå¸‚åŒºçš„é…’åº—ï¼Œè¿™æ ·å¯ä»¥æ–¹ä¾¿æ‚¨æ¸¸è§ˆå¸‚åŒºå’Œå“å°å½“åœ°çš„ç¾é£Ÿã€‚

# å¸Œæœ›è¿™ä»½æ”»ç•¥å¯¹æ‚¨æœ‰æ‰€å¸®åŠ©ï¼Œå¦‚æœæ‚¨è¿˜æœ‰å…¶ä»–é—®é¢˜ï¼Œè¯·éšæ—¶é—®æˆ‘ã€‚
response, history = process_response(response, history)

```

å¦‚æœéœ€è¦è¿›è¡Œæ¨ç†ï¼Œè¯·ä¿®æ”¹é…ç½®æ–‡ä»¶ä¸­(æ¨ç†è„šæœ¬ä¸­/path/to/your.yaml)çš„checkpoint_name_or_pathé¡¹ï¼Œé…ç½®ä¸ºæƒé‡ä¸‹è½½ç« èŠ‚ä¸‹è½½çš„æƒé‡æ–‡ä»¶ï¼š

  ```yaml
  model:
    model_config:
      checkpoint_name_or_path: "/path/to/glm3_6b.ckpt"
  ```

## Q & A

### Q1: ç½‘ç»œè®­ç»ƒ loss ä¸ä¸‹é™ã€ç½‘ç»œè®­ç»ƒæº¢å‡ºã€`overflow_cond=True` æ€ä¹ˆåŠï¼Ÿ

A1: æ‰§è¡Œè®­ç»ƒå‰è®¾ç½®ç¯å¢ƒå˜é‡ï¼š

```shell
export MS_ASCEND_CHECK_OVERFLOW_MODE="INFNAN_MODE"
```

é‡æ–°å¯åŠ¨è®­ç»ƒã€‚

### Q2: æ¨ç†é€Ÿåº¦éå¸¸æ…¢ã€Mindsporeåªèƒ½è·‘åœ¨CPUä¸Šã€æŠ¥é”™ä¸­å«æœ‰ `te`ã€`tbe`ã€`tvm`ç­‰å­—æ ·ï¼Ÿ

A2: ä¸€èˆ¬æ˜¯ Mindspore + Ascend ç¯å¢ƒå®‰è£…é—®é¢˜ï¼Œç¡®è®¤ç¯å¢ƒå®‰è£…è¿‡ç¨‹å‚ç…§
[å®‰è£…æŒ‡å—](https://www.mindspore.cn/install/#%E6%89%8B%E5%8A%A8%E5%AE%89%E8%A3%85)å¹¶ä¸”æˆåŠŸè®¾ç½®äº†ç¯å¢ƒå˜é‡ã€‚æ‰§è¡Œï¼š

```shell
python -c "import mindspore;mindspore.set_context(device_target='Ascend');mindspore.run_check()"
```

å‡å¦‚æ‰§è¡Œè¾“å‡ºï¼š

```text
MindSpore version: ç‰ˆæœ¬å·
The result of multiplication calculation is correct, MindSpore has been installed on platform [Ascend] successfully!
```

å¹¶ä¸”æ²¡æœ‰æŠ¥é”™ï¼Œåˆ™è¯´æ˜æˆåŠŸå®‰è£…äº†ç¯å¢ƒã€‚

æˆ–è®¸ä½ æƒ³é—®ï¼Œæœ‰æ²¡æœ‰æ›´æ–¹ä¾¿çš„ç¯å¢ƒå®‰è£…æ–¹å¼ï¼Ÿæ­å–œä½ ï¼Œæœ‰çš„ï¼Œæˆ‘ä»¬è¿˜æä¾›ç°æˆçš„
[dockeré•œåƒ](http://mirrors.cn-central-221.ovaijisuan.com/mirrors.html)ï¼Œå¯ä»¥ä¾æ®éœ€æ±‚è‡ªè¡Œå–ç”¨ã€‚

### Q3: Sync stream Failedã€exec graph xxx failedï¼Ÿ

A3:è¿™ç±»æŠ¥é”™è¾ƒä¸ºå®½æ³›ï¼Œå¯ä»¥æ‰“å¼€æ˜‡è…¾hostæ—¥å¿—è¿›ä¸€æ­¥å®šä½ã€‚

```shell
export ASCEND_GLOBAL_EVENT_ENABLE=0
export ASCEND_GLOBAL_LOG_LEVEL=2
export ASCEND_SLOG_PRINT_TO_STDOUT=1
```

æ‰“å¼€æ˜‡è…¾hostæ—¥å¿—åæ¨¡å‹æ€§èƒ½å°†æ˜æ˜¾ä¸‹é™ï¼Œå®šä½é—®é¢˜ç»“æŸåéœ€è¦å–æ¶ˆæ˜‡è…¾æ—¥å¿—ï¼š

```shell
unset ASCEND_GLOBAL_EVENT_ENABLE ASCEND_GLOBAL_LOG_LEVEL ASCEND_SLOG_PRINT_TO_STDOUT
```

### Q4: the strategy is xxxxxx, shape xxxx cannot be divisible by value x

A4: æ£€æŸ¥æ¨¡å‹å¥é•¿æ˜¯å¦æ»¡è¶³ `max_source_length + max_target_length + 1 = seq_length` çš„è¦æ±‚ã€‚

### ä»ç„¶æœ‰ç–‘é—®ï¼Ÿæ¬¢è¿å‘æˆ‘ä»¬æå‡ºissueï¼Œæˆ‘ä»¬å°†å°½å¿«ä¸ºæ‚¨è§£å†³

æé—®æ—¶éº»çƒ¦æä¾›ä»¥ä¸‹ä¿¡æ¯ï¼š

1. æ‰§è¡Œå‘½ä»¤
2. è¿è¡Œç¯å¢ƒï¼ŒåŒ…æ‹¬ç¡¬ä»¶ç‰ˆæœ¬ã€CANNç‰ˆæœ¬ã€Mindsporeç‰ˆæœ¬ã€Mindformersç‰ˆæœ¬
3. æŠ¥é”™å®Œæ•´æ—¥å¿—
