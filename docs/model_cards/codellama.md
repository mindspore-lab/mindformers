# Code Llama

> ## ğŸš¨ å¼ƒç”¨è¯´æ˜
>
> æœ¬æ¨¡å‹å·²è¿‡æ—¶ï¼Œä¸å†è¿›è¡Œç»´æŠ¤ï¼Œå¹¶å°†åœ¨ *1.6.0* ç‰ˆæœ¬ä¸‹æ¶ã€‚å¦‚éœ€ä½¿ç”¨æ­¤æ¨¡å‹ï¼Œå»ºè®®æ ¹æ®å®˜æ–¹æ–‡æ¡£ä¸­çš„ **[æ¨¡å‹åº“](https://www.mindspore.cn/mindformers/docs/zh-CN/r1.5.0/start/models.html)** é€‰æ‹©åˆé€‚çš„ç‰ˆæœ¬è¿›è¡Œä½¿ç”¨ã€‚
>
> å¦‚æœ‰ä»»ä½•é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·é€šè¿‡ **[ç¤¾åŒºIssue](https://gitee.com/mindspore/mindformers/issues/new)** æäº¤åé¦ˆã€‚æ„Ÿè°¢æ‚¨çš„ç†è§£ä¸æ”¯æŒï¼

## æ¨¡å‹æè¿°

Code Llamaæ˜¯åŸºäºLlama 2çš„ä¸€ç³»åˆ—å¤§å‹ä»£ç è¯­è¨€æ¨¡å‹ï¼Œå®ƒåœ¨å¼€æºæ¨¡å‹ä¸­æä¾›äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€å¡«å……èƒ½åŠ›ã€å¯¹å¤§å‹è¾“å…¥ä¸Šä¸‹æ–‡çš„æ”¯æŒä»¥åŠzero-shotæŒ‡ä»¤è·Ÿéšèƒ½åŠ›ï¼Œç”¨äºç¼–ç¨‹ä»»åŠ¡ã€‚ç°æœ‰å¤šç§ä¸åŒç‰ˆæœ¬æ¥è¦†ç›–å¹¿æ³›çš„åº”ç”¨é¢†åŸŸï¼šåŸºç¡€æ¨¡å‹ï¼ˆCode Llamaï¼‰ã€Pythonä¸“ä¸šåŒ–æ¨¡å‹ï¼ˆCode Llama - Pythonï¼‰å’ŒæŒ‡ä»¤è·Ÿéšæ¨¡å‹ï¼ˆCode Llama - Instructï¼‰ï¼Œæ¯ä¸ªæ¨¡å‹åˆ†åˆ«å…·æœ‰7Bã€13Bå’Œ34Bä¸ªå‚æ•°ã€‚æ‰€æœ‰æ¨¡å‹éƒ½æ˜¯åœ¨16kæ ‡è®°åºåˆ—ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶å¯¹é«˜è¾¾100kæ ‡è®°çš„è¾“å…¥æ˜¾ç¤ºå‡ºæ”¹è¿›æ•ˆæœã€‚7Bå’Œ13Bç‰ˆæœ¬çš„Code Llamaä»¥åŠCode Llama - Instructå˜ä½“æ”¯æŒåŸºäºå‘¨å›´å†…å®¹çš„å¡«å……åŠŸèƒ½ã€‚Code Llamaæ˜¯é€šè¿‡å¯¹Llama 2è¿›è¡Œæ›´é«˜æ¯”ä¾‹çš„ä»£ç å–æ ·è¿›è¡Œå¾®è°ƒè€Œå¼€å‘çš„ã€‚

[Code Llama: Open Foundation Models for Code](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/)

``` text
@article{roziere2023code,
  title={Code llama: Open foundation models for code},
  author={Roziere, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Remez, Tal and Rapin, J{\'e}r{\'e}my and others},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023}
}
```

## æ¨¡å‹æ€§èƒ½

ä»¥ä¸‹æ¨¡å‹æ€§èƒ½å‡ç”±Atlas 800T A2ç¡¬ä»¶ç¯å¢ƒä¸‹æµ‹è¯•å¾—å‡ºã€‚

| Config                                                                       |      Task       | SeqLength | Datasets |  Performance   |  Phase   |
|:-----------------------------------------------------------------------------|:---------------:|:---------:|:--------:|:--------------:|:--------:|
| [codellama_34b_32p](../../configs/codellama/finetune_codellama_34b_32p.yaml) | text_generation |   4096    |  belle   | 667 tokens/s/p | Finetune |
| [codellama_34b](../../configs/codellama/predict_codellama_34b.yaml)          | text_generation |   4096    |    /     |  139 tokens/s  | Predict  |

ä»¥ä¸‹æ¨¡å‹æ€§èƒ½å‡ç”±Atlas 900 A2 PoDcç¡¬ä»¶ç¯å¢ƒä¸‹æµ‹è¯•å¾—å‡ºã€‚

| Config                                                                       |      Task       | SeqLength |  Datasets   |  Performance   |  Phase   |
|:-----------------------------------------------------------------------------|:---------------:|:---------:|:-----------:|:--------------:|:--------:|
| [codellama_34b_16p](../../configs/codellama/finetune_codellama_34b_16p.yaml) | text_generation |   4096    | code-alpaca | 669 tokens/s/p | Finetune |
| [codellama_34b_32p](../../configs/codellama/finetune_codellama_34b_32p.yaml) | text_generation |   4096    | code-alpaca | 747 tokens/s/p | Finetune |

## æ¨¡å‹æ–‡ä»¶

`Code Llama` åŸºäº `mindformers` å®ç°ï¼Œæœ¬ä»“åº“å½“å‰æ”¯æŒ34bæ¨¡å‹é…ç½®ï¼Œä¸»è¦æ¶‰åŠçš„æ–‡ä»¶æœ‰ï¼š

1. æ¨¡å‹å…·ä½“å®ç°ï¼š

   ```text
   mindformers/models/llama
       â”œâ”€â”€ __init__.py
       â”œâ”€â”€ llama.py                  # æ¨¡å‹å®ç°
       â”œâ”€â”€ llama_config.py           # æ¨¡å‹é…ç½®é¡¹
       â”œâ”€â”€ llama_layer.py            # llamaç½‘ç»œå±‚å®šä¹‰
       â”œâ”€â”€ llama_processor.py        # llamaé¢„å¤„ç†
       â”œâ”€â”€ llama_tokenizer.py        # tokenizer
       â””â”€â”€ llama_transformer.py      # transformerå±‚å®ç°
   ```

2. æ¨¡å‹é…ç½®ï¼š

   ```text
   configs/codellama
       â”œâ”€â”€ pretrain_codellama_34b.yaml             # 34bæ¨¡å‹é¢„è®­ç»ƒå¯åŠ¨é…ç½®
       â”œâ”€â”€ finetune_codellama_34b_16p.yaml         # 34bæ¨¡å‹2æœº16på¾®è°ƒå¯åŠ¨é…ç½®
       â”œâ”€â”€ finetune_codellama_34b_32p.yaml         # 34bæ¨¡å‹4æœº32på¾®è°ƒå¯åŠ¨é…ç½®
       â””â”€â”€ predict_codellama_34b.yaml              # 34bæ¨¡å‹æ¨ç†é…ç½®
   ```

3. æ•°æ®é¢„å¤„ç†è„šæœ¬ï¼š

   ```text
   mindformers/tools/dataset_preprocess/llama/
       â”œâ”€â”€ alpaca_converter.py     # åŸºäºfschatçš„alpacaæ•°æ®é›†æ ¼å¼è½¬æ¢è„šæœ¬
       â””â”€â”€ llama_preprocess.py     # llamaæ¨¡å‹çš„mindrecordæ•°æ®å¤„ç†è„šæœ¬
   ```

## ç¯å¢ƒåŠæ•°æ®å‡†å¤‡

### å®‰è£…ç¯å¢ƒ

MindFormersè½¯ç¡¬ä»¶é…å¥—å…³ç³»ä»¥åŠå®‰è£…å‚è€ƒ[ç¯å¢ƒå®‰è£…æŒ‡å—](../../README.md#æºç ç¼–è¯‘å®‰è£…)å’Œ[ç‰ˆæœ¬åŒ¹é…å…³ç³»](../../README.md#ç‰ˆæœ¬åŒ¹é…å…³ç³»)ã€‚

> æ³¨ï¼š34bæ¨ç†ä½¿ç”¨Atlas 800T A2 è‡³å°‘ä½¿ç”¨2å¡ï¼Œå…¨é‡å¾®è°ƒè‡³å°‘éœ€è¦2æœº16å¡ï¼Œå»ºè®®4æœº32å¡ã€‚

### æ•°æ®åŠæƒé‡å‡†å¤‡

#### æ•°æ®é›†ä¸‹è½½

MindFormersæä¾›`Wikitext2`ä½œä¸º[é¢„è®­ç»ƒ](#é¢„è®­ç»ƒ)æ•°æ®é›†ï¼Œ`code-alpaca`ä½œä¸º[å¾®è°ƒ](#å¾®è°ƒ)æ•°æ®é›†ã€‚

| æ•°æ®é›†åç§°       |     é€‚ç”¨æ¨¡å‹      |   é€‚ç”¨é˜¶æ®µ   |                                          ä¸‹è½½é“¾æ¥                                           |
|:------------|:-------------:|:--------:|:---------------------------------------------------------------------------------------:|
| Wikitext2   | CodeLlama_34b | Pretrain |    [Link](https://www.mindspore.cn/mindformers/docs/zh-CN/r1.5.0/faq/func_related.html)     |
| code-alpaca | CodeLlama_34b | Finetune | [Link](https://github.com/sahil280114/codealpaca/blob/master/data/code_alpaca_20k.json) |
| HumanEval   | CodeLlama_34b | Evaluate |                      [Link](https://github.com/openai/human-eval)                       |

æ•°æ®é¢„å¤„ç†ä¸­æ‰€ç”¨çš„`tokenizer.model`å¯ä»¥ç‚¹å‡»[é“¾æ¥](https://huggingface.co/codellama/CodeLlama-34b-hf/blob/main/tokenizer.model)è¿›è¡Œä¸‹è½½ã€‚

- **Wikitext2 æ•°æ®é¢„å¤„ç†**

  ä½¿ç”¨`mindformers/tools/dataset_preprocess/llama/llama_preprocess.py`å¯¹ä¸‹è½½åçš„æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼Œå¹¶ç”ŸæˆMindrecordæ•°æ®ã€‚

  ```shell
  python llama_preprocess.py \
    --dataset_type wiki \
    --input_glob /{path}/wiki.train.tokens \
    --model_file /{path}/tokenizer.model \
    --seq_length 4096 \
    --output_file /{path}/wiki4096.mindrecord

  # å‚æ•°è¯´æ˜
  dataset_type: é¢„å¤„ç†æ•°æ®ç±»å‹
  input_glob:   è¾“å…¥ä¸‹è½½åwiki.train.tokensçš„æ–‡ä»¶è·¯å¾„
  model_file:   æ¨¡å‹tokenizer.modelæ–‡ä»¶è·¯å¾„
  seq_length:   è¾“å‡ºæ•°æ®çš„åºåˆ—é•¿åº¦
  output_file:  è¾“å‡ºæ–‡ä»¶çš„ä¿å­˜è·¯å¾„
  ```

- **code-alpaca æ•°æ®é¢„å¤„ç†**

1. æ‰§è¡Œ`mindformers/tools/dataset_preprocess/llama/alpaca_converter.py`ï¼Œä½¿ç”¨fastchatå·¥å…·æ·»åŠ promptsæ¨¡æ¿ï¼Œå°†åŸå§‹æ•°æ®é›†è½¬æ¢ä¸ºå¤šè½®å¯¹è¯æ ¼å¼ã€‚

   ```shell
   python alpaca_converter.py \
    --data_path /{path}/code_alpaca_data.json \
    --output_path /{path}/code-alpaca-data-conversation.json

   # å‚æ•°è¯´æ˜
   data_path:   ä¸‹è½½çš„alpacaæ•°æ®è·¯å¾„
   output_path: è¾“å‡ºè½¬æ¢åå¯¹è¯æ ¼å¼çš„æ•°æ®è·¯å¾„
   ```

2. æ‰§è¡Œ`mindformers/tools/dataset_preprocess/llama/llama_preprocess.py`ï¼Œè¿›è¡Œæ•°æ®é¢„å¤„ç†ã€Mindrecordæ•°æ®ç”Ÿæˆï¼Œå°†å¸¦æœ‰promptæ¨¡æ¿çš„æ•°æ®è½¬æ¢ä¸ºmindrecordæ ¼å¼ã€‚

   ```shell
   # ç”±äºæ­¤å·¥å…·ä¾èµ–fschatå·¥å…·åŒ…è§£æpromptæ¨¡æ¿ï¼Œè¯·æå‰å®‰è£…fschat >= 0.2.13 python = 3.9
   python llama_preprocess.py \
    --dataset_type qa \
    --input_glob /{path}/code-alpaca-data-conversation.json \
    --model_file /{path}/tokenizer.model \
    --seq_length 4096 \
    --output_file /{path}/code-alpaca-fastchat4096.mindrecord
   ```

#### æ¨¡å‹æƒé‡ä¸‹è½½

MindFormersæä¾›ä¸‹è½½HuggingFaceå®˜æ–¹æƒé‡çš„ä¸‹è½½é“¾æ¥ï¼Œç”¨æˆ·å¯é€šè¿‡é“¾æ¥ä¸‹è½½æƒé‡å¹¶ç»è¿‡[æ¨¡å‹æƒé‡è½¬æ¢](#æ¨¡å‹æƒé‡è½¬æ¢)åè¿›è¡Œä½¿ç”¨ã€‚

è¯è¡¨ä¸‹è½½é“¾æ¥ï¼š[tokenizer.model](https://huggingface.co/codellama/CodeLlama-34b-hf/blob/main/tokenizer.model)

| æ¨¡å‹åç§°                        |                           HuggingFaceæƒé‡                            |
|:----------------------------|:------------------------------------------------------------------:|
| CodeLlama-34b               |     [Link](https://huggingface.co/codellama/CodeLlama-34b-hf)      |
| CodeLlama-34b-Python        |  [Link](https://huggingface.co/codellama/CodeLlama-34b-Python-hf)  |
| CodeLlama_34b-Instruct      | [Link](https://huggingface.co/codellama/CodeLlama-34b-Instruct-hf) |

#### æ¨¡å‹æƒé‡è½¬æ¢

ä¸‹è½½å®Œæˆåï¼Œè¿è¡Œè½¬æ¢è„šæœ¬`mindformers/convert_weight.py`ï¼Œå°†huggingfaceçš„æƒé‡è½¬æ¢ä¸ºå®Œæ•´çš„ckptæƒé‡ã€‚

```shell
# ä½¿ç”¨transformers = 4.34.0ï¼Œtorch>=2.0è¿›è¡Œè½¬æ¢
python convert_weight.py --model llama --input_path TORCH_CKPT_DIR --output_path {path}/MS_CKPT_NAME

# å‚æ•°è¯´æ˜
model:       æ¨¡å‹åç§°
input_path:  ä¸‹è½½HuggingFaceæƒé‡çš„æ–‡ä»¶å¤¹è·¯å¾„
output_path: è½¬æ¢åçš„MindSporeæƒé‡æ–‡ä»¶ä¿å­˜è·¯å¾„
```

## é¢„è®­ç»ƒ

MindFormersæä¾›äº†`Code Llama 34b`å¤šæœºé¢„è®­ç»ƒç¤ºä¾‹ï¼Œä½¿ç”¨`Wikitext2`æ•°æ®é›†å¯¹æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œæ•°æ®é›†å¯ä»¥å‚è€ƒ[æ•°æ®é›†ä¸‹è½½](#æ•°æ®é›†ä¸‹è½½)è·å¾—ã€‚

`Code Llama 34b`ç”±äºæ¨¡å‹è§„æ¨¡è¾ƒå¤§ï¼Œä»…æ”¯æŒå¤šæœºé¢„è®­ç»ƒï¼Œè‡³å°‘ä½¿ç”¨2æœº16å¡è¿›è¡Œè®­ç»ƒã€‚

1. ä¿®æ”¹é…ç½®æ–‡ä»¶`config/codellama/pretrain_codellama_34b.yaml`

   æ ¹æ®æœåŠ¡å™¨èŠ‚ç‚¹æ•°ç­‰ä¿¡æ¯ï¼Œä¿®æ”¹ç›¸åº”çš„å¹¶è¡Œé…ç½®ã€‚

   ```yaml
   parallel_config:
     data_parallel: 1
     model_parallel: 8
     pipeline_stage: 2
     use_seq_parallel: True
     micro_batch_num: 128
     vocab_emb_dp: True
     gradient_aggregation_group: 4
   ```

2. åœ¨åˆ†å¸ƒå¼èŠ‚ç‚¹ä¸Šæ‰§è¡Œè„šæœ¬

   å¤šæœºå¤šå¡è®­ç»ƒéœ€è¦ä¸åŒèŠ‚ç‚¹ä¸Šæ‰§è¡Œå¯åŠ¨å‘½ä»¤ï¼Œå°†å‚æ•°`MASTER_ADDR`è®¾ç½®ä¸ºä¸»èŠ‚ç‚¹çš„ipåœ°å€ï¼Œ æ‰€æœ‰èŠ‚ç‚¹è®¾ç½®çš„ipåœ°å€ç›¸åŒï¼Œä¸åŒèŠ‚ç‚¹ä¹‹é—´ä»…å‚æ•°`NODE_RANK`ä¸åŒï¼Œå…·ä½“å¯å‚è€ƒ[ä½¿ç”¨æŒ‡å—](../../README.md#ä¸‰ä½¿ç”¨æŒ‡å—)ã€‚

   ```shell
   # èŠ‚ç‚¹0ï¼ŒèŠ‚ç‚¹ipä¸º{ip_addr}ï¼Œä½œä¸ºä¸»èŠ‚ç‚¹ï¼Œæ€»å…±16å¡ä¸”æ¯ä¸ªèŠ‚ç‚¹8å¡
   bash scripts/msrun_launcher.sh "run_mindformer.py \
    --config configs/codellama/pretrain_codellama_34b.yaml \
    --train_dataset_dir /path/wiki4096.mindrecord \
    --run_mode train" \
   16 8 {ip_addr} 8118 0 output/msrun_log False 300

   # èŠ‚ç‚¹1ï¼ŒèŠ‚ç‚¹0ä¸èŠ‚ç‚¹1å¯åŠ¨å‘½ä»¤ä»…å‚æ•°NODE_RANKä¸åŒ
   bash scripts/msrun_launcher.sh "run_mindformer.py \
    --config configs/codellama/pretrain_codellama_34b.yaml \
    --train_dataset_dir /path/wiki4096.mindrecord \
    --run_mode train" \
   16 8 {ip_addr} 8118 1 output/msrun_log False 300

   # å‚æ•°è¯´æ˜
   config:            é…ç½®æ–‡ä»¶è·¯å¾„
   train_dataset_dir: è®­ç»ƒæ•°æ®é›†è·¯å¾„
   run_mode:          è¿è¡Œæ¨¡å¼, é¢„è®­ç»ƒæ—¶è®¾ç½®ä¸ºtrain
   ```

## å¾®è°ƒ

MindFormersæä¾›`Code Llama 34b`çš„å¾®è°ƒç¤ºä¾‹ï¼Œä½¿ç”¨`code-alpaca`æ•°æ®é›†å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œæ•°æ®é›†å¯ä»¥å‚è€ƒ[æ•°æ®é›†ä¸‹è½½](#æ•°æ®é›†ä¸‹è½½)è·å¾—ã€‚

### å…¨å‚å¾®è°ƒ

`Code Llama 34b`ç”±äºæ¨¡å‹è§„æ¨¡è¾ƒå¤§ï¼Œä»…æ”¯æŒå¤šæœºå¾®è°ƒï¼Œè‡³å°‘ä½¿ç”¨2æœº16å¡è¿›è¡Œè®­ç»ƒã€‚

1. ç”Ÿæˆå¤šæœºåˆ†å¸ƒå¼æƒé‡

   å¦‚æœä½¿ç”¨å…±äº«å­˜å‚¨ï¼Œå¯ä»¥å°†æ¨¡å‹å®Œæ•´æƒé‡æ”¾åœ¨å…±äº«å­˜å‚¨å†…ï¼ŒåŒæ—¶è®¾ç½®é…ç½®æ–‡ä»¶æˆ–è„šæœ¬å‚æ•°`auto_trans_ckpt=True`ï¼Œä½¿ç”¨æƒé‡è‡ªåŠ¨è½¬æ¢åŠŸèƒ½ã€‚

   å¦‚æœä¸ä½¿ç”¨å…±äº«å­˜å‚¨ï¼Œå¯ä»¥å‚è€ƒ[å¤šæœºå¤šå¡æƒé‡è½¬æ¢](../../docs/feature_cards/Transform_Ckpt.md#ç‰©ç†æœºå¤šæœºå¤šå¡è®­ç»ƒ)å®Œæˆåˆ†å¸ƒå¼æƒé‡è½¬æ¢åæ‹‰èµ·é¢„è®­ç»ƒä»»åŠ¡ã€‚

2. ä¿®æ”¹é…ç½®æ–‡ä»¶`config/codellama/finetune_codellama_34b_16p.yaml`

   æ ¹æ®æœåŠ¡å™¨èŠ‚ç‚¹æ•°ç­‰ä¿¡æ¯ï¼Œä¿®æ”¹ç›¸åº”çš„å¹¶è¡Œé…ç½®ã€‚

   ```yaml
   parallel_config:
     data_parallel: 1
     model_parallel: 8
     pipeline_stage: 2
     use_seq_parallel: True
     micro_batch_num: 128
     vocab_emb_dp: True
     gradient_aggregation_group: 4
   ```

3. åœ¨åˆ†å¸ƒå¼èŠ‚ç‚¹ä¸Šæ‰§è¡Œè„šæœ¬ï¼Œè¿›è¡Œ2æœº16å¡å¾®è°ƒ

   å¤šæœºå¤šå¡è®­ç»ƒéœ€è¦ä¸åŒèŠ‚ç‚¹ä¸Šæ‰§è¡Œå¯åŠ¨å‘½ä»¤ï¼Œå°†å‚æ•°`MASTER_ADDR`è®¾ç½®ä¸ºä¸»èŠ‚ç‚¹çš„ipåœ°å€ï¼Œ æ‰€æœ‰èŠ‚ç‚¹è®¾ç½®çš„ipåœ°å€ç›¸åŒï¼Œä¸åŒèŠ‚ç‚¹ä¹‹é—´ä»…å‚æ•°`NODE_RANK`ä¸åŒï¼Œå…·ä½“å¯å‚è€ƒ[ä½¿ç”¨æŒ‡å—](../../README.md#ä¸‰ä½¿ç”¨æŒ‡å—)ã€‚

   ç¤ºä¾‹ä½¿ç”¨å…±äº«å­˜å‚¨å¹¶å¼€å¯`auto_trans_ckpt`è¿›è¡Œæƒé‡è‡ªåŠ¨è½¬æ¢ã€‚

   ```shell
   # èŠ‚ç‚¹0ï¼ŒèŠ‚ç‚¹ipä¸º{ip_addr}ï¼Œä½œä¸ºä¸»èŠ‚ç‚¹ï¼Œæ€»å…±16å¡ä¸”æ¯ä¸ªèŠ‚ç‚¹8å¡
   bash scripts/msrun_launcher.sh "run_mindformer.py \
    --config configs/codellama/finetune_codellama_34b_16p.yaml \
    --load_checkpoint /path/codellama_34b.ckpt \
    --auto_trans_ckpt True \
    --train_dataset_dir /path/code-alpaca-fastchat4096.mindrecord \
    --run_mode finetune" \
   16 8 {ip_addr} 8118 0 output/msrun_log False 300

   # èŠ‚ç‚¹1ï¼ŒèŠ‚ç‚¹0ä¸èŠ‚ç‚¹1å¯åŠ¨å‘½ä»¤ä»…å‚æ•°NODE_RANKä¸åŒ
   bash scripts/msrun_launcher.sh "run_mindformer.py \
    --config configs/codellama/finetune_codellama_34b_16p.yaml \
    --load_checkpoint /path/codellama_34b.ckpt \
    --auto_trans_ckpt True \
    --train_dataset_dir /path/code-alpaca-fastchat4096.mindrecord \
    --run_mode finetune" \
   16 8 {ip_addr} 8118 1 output/msrun_log False 300

   # å‚æ•°è¯´æ˜
   config:            é…ç½®æ–‡ä»¶è·¯å¾„
   load_checkpoint:   æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„
   auto_trans_ckpt:   æ˜¯å¦å¼€å¯è‡ªåŠ¨æƒé‡è½¬æ¢
   train_dataset_dir: è®­ç»ƒæ•°æ®é›†è·¯å¾„
   run_mode:          è¿è¡Œæ¨¡å¼, å¾®è°ƒæ—¶è®¾ç½®ä¸ºfinetune
   ```

### åˆ†å¸ƒå¼è®­ç»ƒæƒé‡åˆå¹¶

åˆ†å¸ƒå¼è®­ç»ƒï¼ˆå¾®è°ƒï¼‰åæ‰€å¾—åˆ°çš„æƒé‡æ–‡ä»¶ä¸ºæ ¹æ®ç­–ç•¥åˆ‡åˆ†åçš„æƒé‡ï¼Œå¯ä»¥æ‰‹åŠ¨å°†åˆ‡åˆ†æƒé‡åˆä¸€ï¼Œä»¥ç”¨äºè¯„ä¼°å’Œæ¨ç†ã€‚

MindFormersæä¾›è‡ªåŠ¨æƒé‡è½¬æ¢å’Œç¦»çº¿æƒé‡è½¬æ¢åŠŸèƒ½ï¼Œå¯å‚è€ƒ[è‡ªåŠ¨è½¬æ¢æ¡ˆä¾‹](../feature_cards/Transform_Ckpt.md#è‡ªåŠ¨è½¬æ¢æ¡ˆä¾‹)å’Œ[ç¦»çº¿æƒé‡è½¬æ¢](../feature_cards/Transform_Ckpt.md#ç¦»çº¿æƒé‡è½¬æ¢)è¿›è¡Œåˆ†å¸ƒå¼æ¨¡å‹æƒé‡è½¬æ¢ã€‚

## æ¨ç†

MindFormersæä¾›`CodeLlama_34b`çš„å¿«é€Ÿæ¨ç†è„šæœ¬ï¼Œè„šæœ¬ä¸»è¦é€šè¿‡generateé«˜é˜¶æ¥å£å®ç°ï¼Œæ”¯æŒå¤šå¡ä»¥åŠå¤šbatchæ¨ç†ã€‚

```shell
# è„šæœ¬ä½¿ç”¨
bash scripts/examples/codellama/run_codellama_predict.sh CONFIG_PATH CKPT_PATH DEVICE_NUM

# å‚æ•°è¯´æ˜
CONFIG_PATH: æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„
CKPT_PATH:   æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„
DEVICE_NUM:  ä½¿ç”¨å¡æ•°
```

`CodeLlama_34b`ä»…æ”¯æŒå¤šå¡æ¨ç†ï¼Œä»¥`CodeLlama_34b`4å¡æ¨ç†ä¸ºä¾‹ã€‚

```shell
bash scripts/examples/codellama/run_codellama_predict.sh \
 configs/codellama/predict_codellama_34b.yaml \
 path/to/codellama_34b.ckpt 4

# æ¨ç†ç»“æœ
# <s>def bubble_sort(arr):
#     n = len(arr)
#     for i in range(n):
# ...
# def selection_sort(arr):
#     n = len(arr)
#     for i in range(n):
# ...
```

## è¯„æµ‹

`Code Llama`å½“å‰æ”¯æŒçš„è¯„æµ‹ä»»åŠ¡å¦‚ä¸‹ï¼š

| ä»»åŠ¡ç±»å‹ |  è¯„æµ‹æŒ‡æ ‡  |    æ•°æ®é›†     |
|:----:|:------:|:----------:|
| ä»£ç ç”Ÿæˆ | Pass@1 | HumanEeval |

### ä»£ç ç”Ÿæˆ

è¯„æµ‹ä½¿ç”¨`HumanEval`æ•°æ®é›†å¯é€šè¿‡[æ•°æ®é›†ä¸‹è½½](#æ•°æ®é›†ä¸‹è½½)è·å¾—ï¼Œä½¿ç”¨`git`ä¸‹è½½ä»£ç ä»“ã€‚

1. æ„å»ºå¦‚ä¸‹`preprocess.py`è„šæœ¬æ”¾å…¥æ•°æ®é›†ä»£ç ä»“ä¸­çš„`human-eval`æ–‡ä»¶å¤¹ä¸­ï¼Œè¿›è¡Œæ•°æ®é›†é¢„å¤„ç†ã€‚

   ```python
   # preprocess.py
   import argparse

   from data import stream_jsonl


   def process_data(tasks):
       prompt_input = [task["prompt"] for task in tasks]
       user_ids = [task["task_id"] for task in tasks]
       entry_inputs = [task["entry_point"] for task in tasks]
       return prompt_input, user_ids, entry_inputs


   if __name__ == "__main__":
       parser = argparse.ArgumentParser("copy prompt")
       parser.add_argument("--data_path", default="", type=str)
       args = parser.parse_args()

       data_list = []
       for data in stream_jsonl(args.data_path):
           data_list.append(data)
       prompt_input, task_ids, entry_inputs = process_data(data_list)

       print(prompt_input)
       print(task_ids)
       print(entry_inputs)
   # ['from typing import List\n\n\ndef has_close_e...
   # ['HumanEval/0', 'HumanEval/1', 'HumanEval/2',...
   # ['has_close_elements', 'separate_paren_groups',...
   ```

   æ‰§è¡Œ`preprocess.py`è„šæœ¬æå–å‡º`data/HumanEval.jsonl.gz`ä¸­çš„`prompt`å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œå¹¶å¯¹å…¶è¿›è¡Œæ¨ç†ï¼Œå¾—åˆ°æ¨ç†ç»“æœï¼š

   ```shell
    # è¿è¡Œä»¥ä¸‹å‘½ä»¤å¯ä»¥è·å¾—æ•°æ®é›†ä¸­çš„è¾“å…¥(prompt_input)ï¼Œä»»åŠ¡id(task_ids)å’Œæ‰§è¡Œå‡½æ•°(entry_points)ã€‚
    # æ¯”å¦‚"HumanEval/0"çš„è¾“å…¥æ—¶from typing import List..., è€Œè¯¥ä»£ç çš„æ‰§è¡Œå‡½æ•°å…¥å£åç§°ä¸ºhas_close_elements.
    python preprocess.py --data_path path/to/HumanEval.jsonl.gz
    ```

2. æå–å‡ºä»£ç ç”Ÿæˆå‡½æ•°çš„ä¸»å‡½æ•°

   ç”±äºç”Ÿæˆä»£ç ä¼šç”Ÿæˆå¤šä½™å‡½æ•°ï¼Œè¯„æµ‹æ—¶åªéœ€è¦è¯„æµ‹å‡½æ•°å³å¯ï¼Œå‡½æ•°åä¸º`data/HumanEval.jsonl.gz`ä¸­çš„`entry_point`ï¼ŒæŒ‰ç…§å¦‚ä¸‹ç»“æ„ä¿å­˜ä¸º`samples.jsonl`ï¼š

   ```text
   {'task_id': "HumanEval/0","completion": "inference result"}
   ```

3. å®‰è£…`HumanEval`ä¾èµ–

   ```shell
   pip install -e human-eval
   ```

   > 1. è§£é™¤`human-eval/human_eval/execution.py`çš„ç¬¬58è¡Œæ³¨é‡Š;
   > 2. ç”±äºä»£ç ç”Ÿæˆæ—¶ä¼šè‡ªå¸¦promptï¼Œå› æ­¤å°†`human-eval/human_eval/execution.py`ç¬¬39è¡Œçš„`problem["prompt"] + completion` æ”¹ä¸º `completion`å³å¯ã€‚

4. ç”Ÿæˆæµ‹è¯•åˆ†æ•°

   ```shell
   evaluate_functional_correctness samples.jsonl
    # {'pass@1': 0.4695}
   ```

