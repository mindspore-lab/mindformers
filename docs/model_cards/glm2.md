# ChatGLM2

## æ¨¡å‹æè¿°

ChatGLM**2**-6B æ˜¯å¼€æºä¸­è‹±åŒè¯­å¯¹è¯æ¨¡å‹ [ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B) çš„ç¬¬äºŒä»£ç‰ˆæœ¬ï¼Œåœ¨ä¿ç•™äº†åˆä»£æ¨¡å‹å¯¹è¯æµç•…ã€éƒ¨ç½²é—¨æ§›è¾ƒä½ç­‰ä¼—å¤šä¼˜ç§€ç‰¹æ€§çš„åŸºç¡€ä¹‹ä¸Šï¼ŒChatGLM**2**-6Bå¼•å…¥äº†æ–°ç‰¹å¾ï¼š**æ›´å¼ºå¤§çš„æ€§èƒ½**ã€**æ›´é•¿çš„ä¸Šä¸‹æ–‡**ã€**æ›´é«˜æ•ˆçš„æ¨ç†**ã€**æ›´å¼€æ”¾çš„åè®®**ã€‚

```text
@article{zeng2022glm,
  title={Glm-130b: An open bilingual pre-trained model},
  author={Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and others},
  journal={arXiv preprint arXiv:2210.02414},
  year={2022}
}
```

## æ¨¡å‹æ€§èƒ½

- ä»¥ä¸‹æ¨¡å‹æ€§èƒ½å‡ç”±Atlas 800ç¡¬ä»¶ç¯å¢ƒä¸‹æµ‹è¯•å¾—å‡ºã€‚

GLM2_6b:

| Config                                                               | Task            | Datasets | Metric                                  | Phase               | Score                                  | Performance                                    |
|----------------------------------------------------------------------|-----------------|----------|-----------------------------------------|---------------------|----------------------------------------|------------------------------------------------|
| [glm2_6b](../../configs/glm2/run_glm2_6b_finetune_800T_A2_64G.yaml)  | text_generation | ADGEN    | -                                       | [finetune](#å¾®è°ƒ)     | -                                      | 815.2059134 tokens/s/p                         |
| [glm2_6b_lora](../../configs/glm2/run_glm2_6b_lora_800T_A2_64G.yaml) | text_generation | ADGEN    | -                                       | [finetune](#loraå¾®è°ƒ) | -                                      | 3243.697479 tokens/s/p                         |
| [glm2_6b](../../configs/glm2/run_glm2_6b_finetune_eval.yaml)         | text_generation | ADGEN    | rouge-1<br>rouge-2<br>rouge-l<br>bleu-4 | [eval](#è¯„æµ‹)         | 30.7842<br>7.0734<br>24.7739<br>7.4661 | -                                              |
| [glm2_6b_lora](../../configs/glm2/run_glm2_6b_lora_eval.yaml)        | text_generation | ADGEN    | rouge-1<br>rouge-2<br>rouge-l<br>bleu-4 | [eval](#è¯„æµ‹)         | 31.0563<br>7.1753<br>24.2296<br>7.2294 | -                                              |
| [glm2_6b](../../configs/glm2/predict_glm2_6b.yaml)                   | text_generation | -        | -                                       | [predict](#æ¨ç†)      | -                                      | 32.08 tokens/s (use_past=True, seq_length=512) |

## æ¨¡å‹æ–‡ä»¶

`chatGLM2-6B` åŸºäº `mindformers` å®ç°ï¼Œä¸»è¦æ¶‰åŠçš„æ–‡ä»¶æœ‰ï¼š

1. æ¨¡å‹å…·ä½“å®ç°ï¼š

    ```text
    mindformers/models/glm2
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ glm2.py                  # æ¨¡å‹å®ç°
        â”œâ”€â”€ glm2_config.py           # æ¨¡å‹é…ç½®é¡¹
        â”œâ”€â”€ glm2_modules.py          # æ¨¡ç»„å®ç°
        â”œâ”€â”€ glm2_tokenizer.py        # tokenizer
        â””â”€â”€ glm2_transformer.py      # transformerå±‚å®ç°
    ```

2. æ¨¡å‹é…ç½®ï¼š

    ```text
    configs/glm2
      â”œâ”€â”€ run_glm2_6b.yaml
      â”œâ”€â”€ run_glm2_6b_finetune_2k_800T_A2_64G.yaml  # Atlas 800T A2 æœ€ä½³æ€§èƒ½å…¨é‡å¾®è°ƒå¯åŠ¨é…ç½®
      â”œâ”€â”€ run_glm2_6b_finetune_2k_800_32G.yaml      # Atlas 800 æœ€ä½³æ€§èƒ½å…¨é‡å¾®è°ƒå¯åŠ¨é…ç½®
      â”œâ”€â”€ run_glm2_6b_finetune_800T_A2_64G.yaml     # Atlas 800T A2 ADGENå…¨é‡å¾®è°ƒå¯åŠ¨é…ç½®
      â”œâ”€â”€ run_glm2_6b_finetune_800_32G.yaml         # Atlas 800 ADGENå…¨é‡å¾®è°ƒå¯åŠ¨é…ç½®
      â”œâ”€â”€ run_glm2_6b_finetune_eval.yaml            # å…¨é‡å¾®è°ƒåè¯„ä¼°é…ç½®
      â”œâ”€â”€ run_glm2_6b_lora_2k_800T_A2_64G.yaml      # Atlas 800T A2æœ€ä½³æ€§èƒ½ loraå¾®è°ƒå¯åŠ¨é…ç½®
      â”œâ”€â”€ run_glm2_6b_lora_2k_800_32G.yaml          # Atlas 800 æœ€ä½³æ€§èƒ½ loraå¾®è°ƒå¯åŠ¨é…ç½®
      â”œâ”€â”€ run_glm2_6b_lora_800T_A2_64G.yaml         # Atlas 800T A2 ADGEN loraå¾®è°ƒå¯åŠ¨é…ç½®
      â”œâ”€â”€ run_glm2_6b_lora_800_32G.yaml             # Atlas 800 ADGEN loraå¾®è°ƒå¯åŠ¨é…ç½®
      â””â”€â”€ run_glm2_6b_lora_eval.yaml                # loraå¾®è°ƒè¯„ä¼°é…ç½®
    ```

## ç¯å¢ƒåŠæ•°æ®å‡†å¤‡

### å®‰è£…ç¯å¢ƒ

MindFormersè½¯ç¡¬ä»¶é…å¥—å…³ç³»ä»¥åŠå®‰è£…å‚è€ƒ[ç¯å¢ƒå®‰è£…æŒ‡å—](../../README.md#æºç ç¼–è¯‘å®‰è£…)å’Œ[ç‰ˆæœ¬åŒ¹é…å…³ç³»](../../README.md#ç‰ˆæœ¬åŒ¹é…å…³ç³»)ã€‚

### æ•°æ®åŠæƒé‡å‡†å¤‡

#### æ•°æ®é›†ä¸‹è½½

æ¨¡å‹ä½¿ç”¨[`ADGEN`](https://aclanthology.org/D19-1321.pdf)ï¼ˆå¹¿å‘Šç”Ÿæˆï¼‰æ•°æ®é›†ä½œä¸ºå¾®è°ƒæ•°æ®é›†ã€‚

| æ•°æ®é›†åç§° |    é€‚ç”¨æ¨¡å‹     |   é€‚ç”¨é˜¶æ®µ   |                                ä¸‹è½½é“¾æ¥                                |
|:------|:-----------:|:--------:|:------------------------------------------------------------------:|
| ADGEN | ChatGLM2-6b | Finetune | [Link](https://cloud.tsinghua.edu.cn/f/b3f119a008264b1cabd1/?dl=1) |

#### æ¨¡å‹æƒé‡ä¸‹è½½

MindFormersæä¾›å·²ç»è½¬æ¢å®Œæˆçš„é¢„è®­ç»ƒæƒé‡ã€è¯è¡¨æ–‡ä»¶ç”¨äºå¾®è°ƒå’Œæ¨ç†ï¼Œç”¨æˆ·ä¹Ÿå¯ä»¥ä¸‹è½½HuggingFaceå®˜æ–¹æƒé‡ç»è¿‡[æ¨¡å‹æƒé‡è½¬æ¢](#æ¨¡å‹æƒé‡è½¬æ¢)åè¿›è¡Œä½¿ç”¨ã€‚

è¯è¡¨ä¸‹è½½é“¾æ¥ï¼š[tokenizer.model](https://huggingface.co/THUDM/chatglm2-6b/blob/main/tokenizer.model)

| æ¨¡å‹åç§°            |                                                   MindSporeæƒé‡                                                   |                  HuggingFaceæƒé‡                   |
|:----------------|:---------------------------------------------------------------------------------------------------------------:|:------------------------------------------------:|
| ChatGLM2-6b     |                                                        /                                                        | [Link](https://huggingface.co/THUDM/chatglm2-6b) |

#### æ¨¡å‹æƒé‡è½¬æ¢

æ‰§è¡Œ`convert_weight.py`è½¬æ¢è„šæœ¬ï¼Œå°†HuggingFaceçš„æƒé‡è½¬æ¢ä¸ºå®Œæ•´çš„ckptæƒé‡ã€‚

```shell
python convert_weight.py --model glm-n --input_path TORCH_CKPT_DIR --output_path {path}/MS_CKPT_NAME

# å‚æ•°è¯´æ˜
model:       æ¨¡å‹åç§°
input_path:  ä¸‹è½½HuggingFaceæƒé‡çš„æ–‡ä»¶å¤¹è·¯å¾„
output_path: è½¬æ¢åçš„MindSporeæƒé‡æ–‡ä»¶ä¿å­˜è·¯å¾„
```

## å¾®è°ƒ

MindFormersæä¾›`ChatGLM2-6B`çš„å¾®è°ƒç¤ºä¾‹ï¼Œ è¿‡ç¨‹ä¸­ä½¿ç”¨`ADGEN`æ•°æ®é›†å¯¹æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œæ•°æ®é›†å¯ä»¥å‚è€ƒ[æ•°æ®é›†ä¸‹è½½](#æ•°æ®é›†ä¸‹è½½)è·å¾—ã€‚

### å…¨å‚å¾®è°ƒ

å…¨å‚å¾®è°ƒä½¿ç”¨`configs/glm2/run_glm2_6b_finetune_800T_A2_64G.yaml`é…ç½®æ–‡ä»¶ï¼Œé…ç½®æ–‡ä»¶ä¸­å®šä¹‰äº†å¾®è°ƒæ‰€éœ€çš„å„é…ç½®é¡¹ã€‚

> æ³¨ï¼šå¾®è°ƒæ—¶æ¨¡å‹çš„`seq_length`éœ€è¦ç­‰äºå¾®è°ƒæ•°æ®é›†çš„`max_source_length + max_target_length + 1`ã€‚ åœ¨é…ç½®æ–‡ä»¶ä¸­é»˜è®¤çš„`seq_length: 192`ä»¥åŠ`max_source_length: 64`å’Œ`max_target_length: 127`é€‚ç”¨äº`ADGEN`æ•°æ®é›†ï¼Œ
> å¯¹äºå…¶ä»–æ•°æ®é›†ï¼Œå¯ä»¥å°†æ•°æ®é›†è½¬æ¢ä¸º`token_id`ï¼Œä½¿`seq_length`ç­‰äº`token_id`çš„æœ€å¤§é•¿åº¦ï¼Œ`seq_length`å¤ªå¤§å½±å“è®­ç»ƒæ€§èƒ½ï¼Œå¤ªå°å½±å“è®­ç»ƒç²¾åº¦ï¼Œéœ€è¦åšå‡ºæƒè¡¡ã€‚

ä»¥`glm2_6b`å•æœº8å¡å¾®è°ƒä¸ºä¾‹ã€‚

1. ä¿®æ”¹é…ç½®æ–‡ä»¶`configs/glm2/run_glm2_6b_finetune_800T_A2_64G.yaml`

   ```yaml
   train_dataset: &train_dataset
     tokenizer:
       type: ChatGLM2Tokenizer
       vocab_file: "/path/to/tokenizer.model"
   ```

2. æ‰§è¡Œè®­ç»ƒå‘½ä»¤

   ```shell
   bash scripts/msrun_launcher.sh "run_mindformer.py \
    --config configs/glm2/run_glm2_6b_finetune_800T_A2_64G.yaml \
    --load_checkpoint {path}/glm2_6b.ckpt \
    --train_dataset_dir {path}/AdvertiseGen/train.json \
    --use_parallel True \
    --run_mode finetune"
   ```

è¡¥å……è¯´æ˜ï¼š

1. è®­ç»ƒçš„logæ—¥å¿—è·¯å¾„ï¼š`mindformers/output/log`
2. checkpoint(å«ä¼˜åŒ–å™¨å‚æ•°)å­˜å‚¨è·¯å¾„ï¼š`mindformers/output/checkpoint`
3. checkpoint(ä¸å«ä¼˜åŒ–å™¨å‚æ•°)å­˜å‚¨è·¯å¾„ï¼š`mindformers/output/checkpoint_network`
4. è‹¥æƒ³åˆå¹¶ckptç”¨äºåç»­è¯„ä¼°ï¼Œé€‰æ‹©ä¸å«ä¼˜åŒ–å™¨å‚æ•°çš„æƒé‡å³å¯

### LoRAå¾®è°ƒ

å…¨å‚å¾®è°ƒèƒ½å¤Ÿåœ¨å¾®è°ƒæ•°æ®é›†ä¸Šå–å¾—è‰¯å¥½æ•ˆæœï¼Œä½†å­˜åœ¨é—å¿˜é¢„è®­ç»ƒçŸ¥è¯†çš„ç°è±¡ã€‚ å› æ­¤æ¨èä½¿ç”¨ä½å‚å¾®è°ƒç®—æ³•ï¼Œå†»ç»“åŸæ¨¡å‹æƒé‡ï¼Œä»…åœ¨å°è§„æ¨¡å‚æ•°é‡ä¸Šè¿›è¡Œè®­ç»ƒï¼Œåœ¨å¾®è°ƒæ•°æ®é›†ä¸Šå–å¾—è‰¯å¥½æ•ˆæœçš„åŒæ—¶ï¼Œç¼“è§£æ¨¡å‹é—å¿˜ç°è±¡ã€‚

ä½¿ç”¨LoRAç®—æ³•è¿›è¡Œä½å‚å¾®è°ƒæ—¶ï¼Œä½¿ç”¨ `configs/glm2/run_glm2_6b_lora_800T_A2_64G.yaml` é…ç½®æ–‡ä»¶ï¼Œè¯¥é…ç½®æ–‡ä»¶åŒ…å«äº†loraä½å‚å¾®è°ƒç®—æ³•æ‰€éœ€çš„é…ç½®é¡¹ã€‚

1. ä¿®æ”¹é…ç½®æ–‡ä»¶`configs/glm2/run_glm2_6b_lora_800T_A2_64G.yaml`

   ```yaml
   train_dataset: &train_dataset
     tokenizer:
       type: ChatGLM2Tokenizer
       vocab_file: "/path/to/tokenizer.model"
   ```

2. æ‰§è¡Œè®­ç»ƒå‘½ä»¤

   ```shell
   bash scripts/msrun_launcher.sh "run_mindformer.py \
    --config configs/glm2/run_glm2_6b_lora_800T_A2_64G.yaml \
    --load_checkpoint {path}/glm2_6b.ckpt \
    --train_dataset_dir {path}/AdvertiseGen/train.json \
    --use_parallel True \
    --run_mode finetune"
   ```

### åˆ†å¸ƒå¼è®­ç»ƒæƒé‡åˆå¹¶

åˆ†å¸ƒå¼è®­ç»ƒï¼ˆå¾®è°ƒï¼‰åæ‰€å¾—åˆ°çš„æƒé‡æ–‡ä»¶ä¸ºæ ¹æ®ç­–ç•¥åˆ‡åˆ†åçš„æƒé‡ï¼Œå¯ä»¥æ‰‹åŠ¨å°†åˆ‡åˆ†æƒé‡åˆä¸€ï¼Œä»¥ç”¨äºè¯„ä¼°å’Œæ¨ç†ã€‚

MindFormersæä¾›è‡ªåŠ¨æƒé‡è½¬æ¢å’Œç¦»çº¿æƒé‡è½¬æ¢åŠŸèƒ½ï¼Œå¯å‚è€ƒ[è‡ªåŠ¨è½¬æ¢æ¡ˆä¾‹](../feature_cards/Transform_Ckpt.md#è‡ªåŠ¨è½¬æ¢æ¡ˆä¾‹)å’Œ[ç¦»çº¿æƒé‡è½¬æ¢](../feature_cards/Transform_Ckpt.md#ç¦»çº¿æƒé‡è½¬æ¢)è¿›è¡Œåˆ†å¸ƒå¼æ¨¡å‹æƒé‡è½¬æ¢ã€‚

## æ¨ç†

MindFormersæä¾›`GLM2-6b`çš„å¿«é€Ÿæ¨ç†è„šæœ¬ï¼Œè„šæœ¬ä¸»è¦é€šè¿‡generateé«˜é˜¶æ¥å£å®ç°ï¼Œæ”¯æŒå•å¡å¤šè½®æ¨ç†ã€‚

```shell
# è„šæœ¬ä½¿ç”¨
bash scripts/examples/glm2/run_glm2_predict.sh CONFIG_PATH CKPT_PATH

# å‚æ•°è¯´æ˜
CONFIG_PATH: æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„
CKPT_PATH:   æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„
```

è¿è¡Œå¦‚ä¸‹å‘½ä»¤è¿›è¡Œæ¨ç†ï¼š

```shell
bash scripts/examples/glm2/run_glm2_predict.sh \
 configs/glm2/predict_glm2_6b.yaml \
 path/to/glm2_6b.ckpt

# æ¨ç†ç»“æœï¼š
# ä½ å¥½:
# ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚
# è¯·ä»‹ç»ä¸€ä¸‹æ­å·:
# æ­å·æ˜¯ä¸­å›½æµ™æ±Ÿçœçœä¼šï¼Œä½äºæµ™æ±Ÿçœä¸œå—éƒ¨ï¼Œåœ°å¤„æµ™æ±ŸçœåŒ—éƒ¨ï¼Œä¸œä¸´ä¸œæµ·ï¼Œå—æ¥ç¦å»ºçœï¼ŒåŒ—ä¸æ±Ÿè‹çœæ¯—é‚»ï¼Œæ˜¯ä¸­å›½è‘—åçš„æ—…æ¸¸åŸå¸‚ä¹‹ä¸€ã€‚
# æ­å·æœ‰ç€æ‚ ä¹…çš„å†å²å’Œæ–‡åŒ–ï¼Œè¢«èª‰ä¸ºâ€œäººé—´å¤©å ‚â€ï¼Œè¢«èª‰ä¸ºâ€œå—å®‹éƒ½åŸâ€ï¼Œæ˜¯ä¸­å›½å—æ–¹è‘—åçš„å†å²æ–‡åŒ–ååŸä¹‹ä¸€ã€‚æ­å·è¿˜è¢«èª‰ä¸ºâ€œå…¨å›½æœ€å…·å¹¸ç¦æ„ŸåŸå¸‚â€ï¼Œå…·æœ‰ä¸°å¯Œçš„å†å²é—å­˜ã€ä¼˜ç¾çš„è‡ªç„¶é£å…‰å’Œæµ“éƒçš„æ–‡åŒ–æ°›å›´ã€‚
# æ­å·çš„ç»æµä»¥æœåŠ¡ä¸šä¸ºä¸»å¯¼äº§ä¸šï¼Œç‰¹åˆ«æ˜¯äº¤é€šè¿è¾“ã€ä»“å‚¨å’Œé‚®æ”¿ä¸šã€‚åŒæ—¶ï¼Œæ­å·ä¹Ÿæ˜¯ä¸­å›½é‡è¦çš„ç”µå­å•†åŠ¡å’Œäº’è”ç½‘äº§ä¸šåŸºåœ°ä¹‹ä¸€ï¼Œè¢«èª‰ä¸ºâ€œä¸­å›½ç”µå­å•†åŠ¡ä¹‹éƒ½â€ã€‚
# æ­å·çš„è‘—åæ™¯ç‚¹åŒ…æ‹¬è¥¿æ¹–ã€çµéšå¯ºã€åƒå²›æ¹–ã€é’±å¡˜æ±Ÿç­‰ã€‚è¥¿æ¹–æ˜¯ä¸­å›½è‘—åçš„é£æ™¯åèƒœåŒºä¹‹ä¸€ï¼Œè¢«èª‰ä¸ºâ€œäººé—´å¤©å ‚â€ï¼Œçµéšå¯ºæ˜¯ä¸­å›½è‘—åçš„ä½›æ•™å¯ºåº™ä¹‹ä¸€ï¼Œåƒå²›æ¹–å’Œé’±å¡˜æ±Ÿæ˜¯ä¸­å›½è‘—åçš„è‡ªç„¶é£æ™¯åŒºä¹‹ä¸€ã€‚
# æ­å·è¿˜æ‹¥æœ‰ä¸°å¯Œçš„äººæ–‡èµ„æºï¼Œè¢«èª‰ä¸ºâ€œäººé—´å¤©å ‚â€çš„æ­å·è¥¿æ¹–ã€çµéšå¯ºã€åƒå²›æ¹–ã€é’±å¡˜æ±Ÿç­‰æ™¯ç‚¹ï¼Œä»¥åŠå®‹åŸã€å—å®‹å¾¡è¡—ç­‰å†å²æ–‡åŒ–æ™¯ç‚¹ï¼Œéƒ½æ˜¯æ¸¸å®¢å‰æ¥æ­å·æ—…æ¸¸çš„çƒ­é—¨æ™¯ç‚¹ã€‚
# é‚£é‡Œæœ‰ä»€ä¹ˆå¥½åƒçš„å—:
# æ­å·æ˜¯ä¸­å›½è‘—åçš„ç¾é£ŸåŸå¸‚ä¹‹ä¸€ï¼Œæœ‰è®¸å¤šç‰¹è‰²ç¾é£Ÿå’Œä¼ ç»Ÿèœè‚´ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›æ­å·çš„è‘—åç¾é£Ÿ:
# 1. è¥¿æ¹–é†‹é±¼ï¼šè¿™æ˜¯æ­å·æœ€è‘—åçš„èœè‚´ä¹‹ä¸€ï¼Œé±¼è‚‰é²œç¾ï¼Œå…¥å£å³åŒ–ï¼Œä½ä»¥é¦™é†‹ã€ç³–ã€å§œä¸ç­‰è°ƒæ–™ï¼Œå£æ„Ÿé…¸ç”œé€‚ä¸­ã€‚
# 2. é¾™äº•è™¾ä»ï¼šä»¥å½“åœ°ç‰¹äº§çš„é¾™äº•èŒ¶ä¸ºä½æ–™ï¼Œå°†é²œå«©çš„è™¾ä»ç‚’åˆ¶è€Œæˆï¼Œå£æ„Ÿæ¸…é¦™å¯å£ã€‚
# 3. çŒæ±¤åŒ…ï¼šåˆç§°å°ç¬¼åŒ…ï¼Œæ˜¯æ­å·çš„ä¼ ç»Ÿç‚¹å¿ƒä¹‹ä¸€ã€‚åŒ…å­çš„çš®è½¯é¦…é²œï¼Œæ±¤æ±é²œç¾ï¼Œéå¸¸å—æ¬¢è¿ã€‚
# 4. å§œæ¯é¸­ï¼šè¿™æ˜¯ä¸€é“æ­å¸®èœï¼Œä»¥é¸­è‚‰ã€å§œæ¯ã€è‘±ç­‰è°ƒæ–™çƒ¹åˆ¶è€Œæˆï¼Œå£æ„Ÿé²œç¾ã€‚
# 5. è€å­—å·å°åƒï¼šæ­å·è¿˜æœ‰å¾ˆå¤šè€å­—å·å°åƒåº—ï¼Œå¦‚èƒ¡åŒå£çƒ¤è‚‰ä¸²ã€å­”åºœå®¶å®´ã€å®‹å«‚é±¼ç¾¹ç­‰ï¼Œæ˜¯å½“åœ°å±…æ°‘å’Œæ¸¸å®¢çš„ç¾é£Ÿé€‰æ‹©ã€‚
# æ­¤å¤–ï¼Œæ­å·è¿˜æœ‰è®¸å¤šç‰¹è‰²å°åƒï¼Œå¦‚ç²½å­ã€è‡­è±†è…ã€ç³¯ç±³é¸¡ã€è‚‰å¤¹é¦ã€é¸­è¡€ç²‰ä¸æ±¤ç­‰ï¼Œè®©äººå‚æ¶æ¬²æ»´ã€‚
```

## è¯„æµ‹

è¯„æµ‹ä½¿ç”¨ `configs/glm2/run_glm2_6b_finetune_eval.yaml` å’Œ`configs/glm2/run_glm2_6b_lora_eval.yaml`é…ç½®æ–‡ä»¶ï¼Œé…ç½®æ–‡ä»¶ä¸­å®šä¹‰äº†è¯„æµ‹æ‰€éœ€çš„å„é…ç½®é¡¹ã€‚

### æ–‡æœ¬ç”Ÿæˆ

è¯„æµ‹æ•°æ®é›†å¯å‚è€ƒ[æ•°æ®é›†ä¸‹è½½](#æ•°æ®é›†ä¸‹è½½)ã€‚

é…ç½®æ–‡ä»¶ä¿®æ”¹éƒ¨åˆ†å¦‚ä¸‹ï¼š

```yaml
load_checkpoint: '{path}/glm2_6b.ckpt'          # æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„
model:
  model_config:
    seq_length: 256
eval_dataset: &eval_dataset
  data_loader:
    dataset_dir: "{path}/AdvertiseGen/dev.json" # æ•°æ®é›†è·¯å¾„
    origin_columns: ["content", "summary"]
  tokenizer:
    vocab_file: "{path}/tokenizer.model"        # è¯è¡¨è·¯å¾„
  max_source_length: 256
  max_target_length: 256
```

> æ³¨ï¼šè¯„æµ‹æ—¶æ¨¡å‹`seq_length`éœ€è¦ç­‰äºè¯„æµ‹æ•°æ®é›†çš„`max_source_length`å’Œ`max_target_length`ã€‚å› æ­¤ä¿®æ”¹yamlä¸­æ¨¡å‹`seq_length`ä¸º256ã€‚

ä½¿ç”¨å…¨å‚å¾®è°ƒæƒé‡æ—¶ï¼Œå¯åŠ¨å¦‚ä¸‹shellè„šæœ¬ï¼Œæ‰§è¡Œå•å¡è¯„ä¼°

é…ç½®æ–‡ä»¶é€‰æ‹© `configs/glm2/run_glm2_6b_finetune_eval.yaml` glm2æ¨¡å‹æ¨ç†é…ç½®ï¼Œä¿®æ”¹å…¶ä¸­`model`å­—æ®µä¸‹`model_config`ä¸­`use_past: True`å¼€å¯å¢é‡æ¨ç†ä½¿è¯„ä¼°é€Ÿåº¦æ›´å¿«ã€‚

```shell
python run_mindformer.py \
 --config configs/glm2/run_glm2_6b_finetune_eval.yaml \
 --run_mode eval \
 --load_checkpoint {path}/glm2_6b_finetune.ckpt \
 --device_id 0 \
 --use_parallel False
```

ä½¿ç”¨LoRAä½å‚å¾®è°ƒæƒé‡æ—¶ï¼Œå¯åŠ¨å¦‚ä¸‹shellè„šæœ¬ï¼Œæ‰§è¡Œå•å¡è¯„ä¼°

é…ç½®æ–‡ä»¶é€‰æ‹© `configs/glm2/run_glm2_6b_lora_eval.yaml` glm2_loraæ¨¡å‹æ¨ç†é…ç½®ï¼Œæ­¤é…ç½®å¯ç”¨äºloraæ¨¡å‹ï¼Œä¿®æ”¹å…¶ä¸­`model`å­—æ®µä¸‹`model_config`ä¸­`use_past: True`å¼€å¯å¢é‡æ¨ç†ä½¿è¯„ä¼°é€Ÿåº¦æ›´å¿«ã€‚

```shell
python run_mindformer.py \
 --config configs/glm2/run_glm2_6b_lora_eval.yaml \
 --run_mode eval \
 --load_checkpoint {path}/glm2_6b_lora.ckpt \
 --device_id 0 \
 --use_parallel False
```

**æ³¨æ„**ï¼šå•å¡è¯„æµ‹æ—¶ï¼Œåº”å°†yamlæ–‡ä»¶ä¸­ model:model_config:batch_size ä¿®æ”¹ä¸ºç­‰äº runner_config:batch_size

### è¾¹è®­è¾¹è¯„ä¼°

1. ä½¿ç”¨ `Rouge-1`ã€`Rouge-2` ç­‰æŒ‡æ ‡è¯„æµ‹

   ä½¿ç”¨è¯¥æŒ‡æ ‡è¯„æµ‹æ—¶é€Ÿåº¦è¾ƒæ…¢ï¼Œæ¨èä½¿ç”¨ `PerplexityMetric` è¯„æµ‹ã€‚

   å°†è®­ç»ƒé…ç½®æ–‡ä»¶çš„ `do_eval: False` è®¾ç½®ä¸º `do_eval: True`ï¼Œå¹¶ä¸”éœ€è¦å°† `train_dataset` å’Œ `eval_dataset` çš„ `max_source_length`ã€`max_target_length` ä»¥åŠ `batch_size`é¡¹è®¾ç½®ä¸ºç›¸åŒå€¼ï¼Œå¹¶ä¸”ä¿æŒ `max_source_length + max_target_length + 1 = seq_length`ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

   ```yaml
   do_eval: True
   eval_step_interval: 1788
   eval_epoch_interval: -1

   metric:
     type: ADGENMetric

   model:
     model_config:
       seq_length: 192
   train_dataset: &train_dataset
     max_source_length: 64
     max_target_length: 127
     batch_size: 8
   eval_dataset: &eval_dataset
     max_source_length: 64
     max_target_length: 127
     batch_size: 8
   ```

2. ä½¿ç”¨ `PerplexityMetric` æŒ‡æ ‡è¯„æµ‹

   å°†è®­ç»ƒé…ç½®æ–‡ä»¶çš„ `do_eval: False` è®¾ç½®ä¸º `do_eval: True`ï¼Œå¹¶ä¸”éœ€è¦å°† `train_dataset` å’Œ `eval_dataset` çš„ `max_source_length`ã€`max_target_length` ã€`phase` ä»¥åŠ `batch_size`é¡¹è®¾ç½®ä¸ºç›¸åŒå€¼ï¼Œå¹¶ä¸”ä¿æŒ `max_source_length + max_target_length + 1 = seq_length`ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

   ```yaml
   do_eval: True
   eval_step_interval: 1788
   eval_epoch_interval: -1

   metric:
     type: PerplexityMetric

   model:
     model_config:
       seq_length: 192
   train_dataset: &train_dataset
     data_loader:
       phase: "train"
     max_source_length: 64
     max_target_length: 127
     batch_size: 8
   eval_dataset: &eval_dataset
     data_loader:
       phase: "train"
     max_source_length: 64
     max_target_length: 127
     batch_size: 8
   ```

mindformersé€šè¿‡ `eval_step_interval` å’Œ `eval_epoch_interval` ä¸¤é¡¹é…ç½®å‚æ•°æ¥æ§åˆ¶è¾¹è®­ç»ƒè¾¹è¯„ä¼°çš„æ‰§è¡Œé—´éš”ï¼Œå‚æ•°å«ä¹‰å¦‚ä¸‹ï¼š

- **eval_step_interval**: è¯„ä¼°stepé—´éš”, é»˜è®¤ä¸º100ï¼Œè¡¨ç¤ºæ¯100ä¸ªstepé—´éš”æ‰§è¡Œä¸€æ¬¡è¯„ä¼°ï¼›é…ç½®ä¸ºå¤§äº0çš„æ•°è¡¨ç¤ºæ¯éš”æ‰€é…ç½®çš„stepæ•°åæ‰§è¡Œä¸€æ¬¡è¯„ä¼°ï¼Œé…ç½®ä¸ºå°äº0çš„æ•°åˆ™è¡¨ç¤ºç¦ç”¨stepè¯„ä¼°ï¼›æ³¨æ„ï¼šåœ¨æ•°æ®ä¸‹æ²‰æ¨¡å¼ä¸‹ï¼Œstepé—´éš”å€¼å»ºè®®é…ç½®ä¸ºsink sizeçš„å€æ•°ã€‚
- **eval_epoch_interval**: è¯„ä¼°epoché—´éš”, é»˜è®¤ä¸º-1ï¼Œè¡¨ç¤ºç¦ç”¨epochç»“æŸæ—¶çš„è¯„ä¼°ï¼›é…ç½®ä¸ºå¤§äº0çš„æ•°è¡¨ç¤ºæ¯éš”æ‰€é…ç½®çš„epochæ•°åæ‰§è¡Œä¸€æ¬¡è¯„ä¼°ï¼Œé…ç½®ä¸ºå°äº0çš„æ•°åˆ™è¡¨ç¤ºç¦ç”¨epochè¯„ä¼°ï¼›æ³¨æ„ï¼šæ•°æ®ä¸‹æ²‰æ¨¡å¼ä¸‹ï¼Œepochæ‰€åŒ…å«çš„stepæ•°å°†ä»æ•°æ®é›†å¤§å°å˜ä¸ºsink sizeçš„å¤§å°ï¼Œå°†åœ¨ `sink_size * eval_epoch_interval` ä¸ªstepåæ‰§è¡Œä¸€æ¬¡è¯„ä¼°ã€‚
