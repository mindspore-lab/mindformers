# ChatGLM2-6B

## æ¨¡å‹æè¿°

ChatGLM**2**-6B æ˜¯å¼€æºä¸­è‹±åŒè¯­å¯¹è¯æ¨¡å‹ [ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B) çš„ç¬¬äºŒä»£ç‰ˆæœ¬ï¼Œåœ¨ä¿ç•™äº†åˆä»£æ¨¡å‹å¯¹è¯æµç•…ã€éƒ¨ç½²é—¨æ§›è¾ƒä½ç­‰ä¼—å¤šä¼˜ç§€ç‰¹æ€§çš„åŸºç¡€ä¹‹ä¸Šï¼ŒChatGLM**2**-6Bå¼•å…¥äº†æ–°ç‰¹å¾ï¼š**æ›´å¼ºå¤§çš„æ€§èƒ½**ã€**æ›´é•¿çš„ä¸Šä¸‹æ–‡**ã€**æ›´é«˜æ•ˆçš„æ¨ç†**ã€**æ›´å¼€æ”¾çš„åè®®**ã€‚

## ä»“åº“ä»‹ç»

`chatGLM2-6B` åŸºäº `mindformers` å®ç°ï¼Œä¸»è¦æ¶‰åŠçš„æ–‡ä»¶æœ‰ï¼š

1. æ¨¡å‹å…·ä½“å®ç°ï¼š`mindformers/models/glm2`

    ```bash
    glm2
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ glm2.py                  # æ¨¡å‹å®ç°
        â”œâ”€â”€ glm2_config.py           # æ¨¡å‹é…ç½®é¡¹
        â”œâ”€â”€ glm2_modules.py          # æ¨¡ç»„å®ç°
        â”œâ”€â”€ glm2_tokenizer.py        # tokenizer
        â””â”€â”€ glm2_transformer.py      # transformerå±‚å®ç°
    ```

2. æ¨¡å‹é…ç½®ï¼š`configs/glm2`

    ```bash
    glm2
        â”œâ”€â”€ export_glm2_6b.yaml       # å¯¼å‡ºMindIRé…ç½®
        â”œâ”€â”€ run_glm2_6b_fintune.yaml  # å…¨é‡å¾®è°ƒå¯åŠ¨é…ç½®
        â””â”€â”€ run_glm2_6b_lora.yaml     # loraä½å‚å¾®è°ƒå¯åŠ¨é…ç½®
    ```

## ç¯å¢ƒè¦æ±‚

- ç¡¬ä»¶ï¼šAscend 910A
- MindSporeï¼š2.0

æ¨ç†å¯åœ¨å•æœºå•å¡ä¸Šå®Œæˆéƒ¨ç½²

å…¨é‡å¾®è°ƒè®­ç»ƒéœ€è¦æœ€å°‘å•æœº8å¡ï¼ŒLoraå¾®è°ƒè®­ç»ƒæœ€å°‘éœ€è¦1å¡

## åŸºçº¿

æµ‹è¯•ç¯å¢ƒåŒä¸Šè¿°ç¯å¢ƒè¦æ±‚

### æ€§èƒ½

|          | data parallel | model parallel | pipeline parallel | batch size | sink size | sequence length | accumulate | per step time (ms) | tokens/s/p  | ä¼˜åŒ–å™¨å¹¶è¡Œ | é‡è®¡ç®— | Memory (GB) |
| -------- | ------------- | -------------- | ----------------- | ---------- | --------- | --------------- | ---------- | ------------------ | ----------- | ---------- | ------ | ----------- |
| å…¨é‡å¾®è°ƒ | 8             | 1              | 1                 | 8          | 4         | 193             | 1          | 1894               | 815.2059134 | True       | True   | 25.2        |
| LoRAå¾®è°ƒ | 4             | 1              | 1                 | 8          | 4         | 193             | 1          | 476                | 3243.697479 | False      | False  | 22.38       |

### è¯„ä¼°æŒ‡æ ‡

|          | rouge-1            | rouge-2           | rouge-l            | bleu-4            |
| -------- | ------------------ | ----------------- |--------------------| ----------------- |
| å…¨é‡å¾®è°ƒ | 30.784298224299064 | 7.073415046728972 | 24.773958598130843 | 7.466147757009345 |
| LoRAå¾®è°ƒ | 31.05639289719626  | 7.1753861682243   | 24.229674859813084 | 7.229435140186916 |

## ChatGLM2-6Bæ¨ç†

> éœ€å¼€å‘è€…æå‰pipå®‰è£…ã€‚å…·ä½“æ¥å£è¯´æ˜è¯·å‚[APIæ¥å£](https://gitee.com/mindspore/transformer/wikis/API/)

### AutoClassæ¨ç†

å¯ä»¥ä½¿ç”¨AutoClassæ¥å£ï¼Œé€šè¿‡æ¨¡å‹åç§°è·å–ç›¸åº”çš„æ¨¡å‹/tokenizerå®ä¾‹ï¼Œå¹¶è‡ªåŠ¨ä¸‹è½½å¹¶åŠ è½½æƒé‡

`from_pretrained()` æ¥å£ä¼šè‡ªåŠ¨ä»äº‘ä¸Šä¸‹è½½é¢„è®­ç»ƒçš„æ¨¡å‹ï¼Œå­˜å‚¨è·¯å¾„ï¼š`mindformers/checkpoint_download/glm2`

é¦–æ¬¡è¿è¡Œpipelineæ¨ç†æ—¶éœ€è¦è¿›è¡Œæ¨¡å‹ç¼–è¯‘ï¼Œéœ€ç­‰å¾…ä¸€æ®µæ—¶é—´

```python
from mindformers import AutoTokenizer, AutoModel


tokenizer = AutoTokenizer.from_pretrained("glm2_6b")
model = AutoModel.from_pretrained("glm2_6b")

query = "ä½ å¥½"

prompted_inputs = tokenizer.build_prompt(query)
input_tokens = tokenizer([prompted_inputs])

outputs = model.generate(input_tokens["input_ids"], max_length=100)
response = tokenizer.decode(outputs)[0]
print(response)
```

### pipelineæ¨ç†

ä¹Ÿå¯ä»¥ä¸å®ä¾‹åŒ–æ„é€ æ¨¡å‹ï¼Œç›´æ¥é€šè¿‡æŒ‡å®šä»»åŠ¡æ¨¡å‹ä¸æ¨¡å‹åçš„æ–¹å¼è¿›è¡Œpipelineçš„æ„é€ 

```python
>>> from mindformers import pipeline, TextGenerationPipeline
>>> task_pipeline = pipeline(task='text_generation', model='glm2_6b', max_length=2048)
>>> task_pipeline('ä½ å¥½')
[{'text_generation_text': ['ä½ å¥½ï¼Œæˆ‘æ˜¯ ChatGLM2-6Bï¼Œ ä¸€ä¸ªäººå·¥æ™ºèƒ½åŠ©æ‰‹ã€‚æˆ‘èƒŒåä½¿ç”¨çš„æ¨¡å‹æ˜¯ GLM2-6Bï¼Œ æ˜¯ä¸€ç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œ å…·æœ‰è¶…è¿‡ 2000 äº¿å‚æ•°ï¼Œæ”¯æŒå¤šç§ä»»åŠ¡ã€‚']}]
>>> pipeline = TextGenerationPipeline(model='glm2_6b', max_length=2048)
>>> pipeline("ä½ å¥½")
[{'text_generation_text': ['ä½ å¥½ï¼Œæˆ‘æ˜¯ ChatGLM2-6Bï¼Œ ä¸€ä¸ªäººå·¥æ™ºèƒ½åŠ©æ‰‹ã€‚æˆ‘èƒŒåä½¿ç”¨çš„æ¨¡å‹æ˜¯ GLM2-6Bï¼Œ æ˜¯ä¸€ç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œ å…·æœ‰è¶…è¿‡ 2000 äº¿å‚æ•°ï¼Œæ”¯æŒå¤šç§ä»»åŠ¡ã€‚']}]
```

## å¾®è°ƒ

ä¸‹é¢ä»¥ [ADGEN](https://aclanthology.org/D19-1321.pdf) (å¹¿å‘Šç”Ÿæˆ) æ•°æ®é›†ä¸ºä¾‹ä»‹ç»ä»£ç çš„ä½¿ç”¨æ–¹æ³•

### æ•°æ®å¤„ç†

ADGEN æ•°æ®é›†ä»»åŠ¡ä¸ºæ ¹æ®è¾“å…¥ï¼ˆcontentï¼‰ç”Ÿæˆä¸€æ®µå¹¿å‘Šè¯ï¼ˆsummaryï¼‰ã€‚

```json
{
    "content": "ç±»å‹#ä¸Šè¡£*ç‰ˆå‹#å®½æ¾*ç‰ˆå‹#æ˜¾ç˜¦*å›¾æ¡ˆ#çº¿æ¡*è¡£æ ·å¼#è¡¬è¡«*è¡£è¢–å‹#æ³¡æ³¡è¢–*è¡£æ¬¾å¼#æŠ½ç»³",
    "summary": "è¿™ä»¶è¡¬è¡«çš„æ¬¾å¼éå¸¸çš„å®½æ¾ï¼Œåˆ©è½çš„çº¿æ¡å¯ä»¥å¾ˆå¥½çš„éšè—èº«æä¸Šçš„å°ç¼ºç‚¹ï¼Œç©¿åœ¨èº«ä¸Šæœ‰ç€å¾ˆå¥½çš„æ˜¾ç˜¦æ•ˆæœã€‚é¢†å£è£…é¥°äº†ä¸€ä¸ªå¯çˆ±çš„æŠ½ç»³ï¼Œæ¼‚äº®çš„ç»³ç»“å±•ç°å‡ºäº†åè¶³çš„ä¸ªæ€§ï¼Œé…åˆæ—¶å°šçš„æ³¡æ³¡è¢–å‹ï¼Œå°½æ˜¾å¥³æ€§ç”œç¾å¯çˆ±çš„æ°”æ¯ã€‚"
}
```

ä» [Google Drive](https://drive.google.com/file/d/13_vf0xRTQsyneRKdD1bZIr93vBGOczrk/view?usp=sharing) æˆ–è€… [Tsinghua Cloud](https://cloud.tsinghua.edu.cn/f/b3f119a008264b1cabd1/?dl=1) ä¸‹è½½å¤„ç†å¥½çš„ ADGEN æ•°æ®é›†ï¼Œç›®å½•ç»“æ„ä¸º

```shell
AdvertiseGen
  â”œâ”€â”€ train.json
  â””â”€â”€ dev.json
```

å°†ä»»åŠ¡é…ç½®æ–‡ä»¶ `configs/glm2/run_glm2_6b_*.yaml` ä¸­çš„ `==== dataset config ====` éƒ¨åˆ†æ›¿æ¢æˆï¼š

```yaml
train_dataset: &train_dataset
  data_loader:
    type: ADGenDataLoader
    dataset_dir: "/path/to/AdvertiseGen/train.json"
    shuffle: True
    phase: "train"
    version: 2
    origin_columns: ["content", "summary"]
  tokenizer:
    type: ChatGLM2Tokenizer
    vocab_file: "/path/to/tokenizer.model"
  input_columns: ["input_ids", "labels"]
  max_source_length: 64
  max_target_length: 128
  ignore_pad_token_for_loss: True
  num_parallel_workers: 8
  python_multiprocessing: False
  drop_remainder: True
  batch_size: 1
  repeat: 1
  numa_enable: False
  prefetch_size: 1
  seed: 0

train_dataset_task:
  type: KeyWordGenDataset
  dataset_config: *train_dataset

eval_dataset: &eval_dataset
  data_loader:
    type: ADGenDataLoader
    dataset_dir: "/path/to/AdvertiseGen/dev.json"
    shuffle: False
    phase: "eval"
    version: 2
    origin_columns: ["content", "summary"]
  tokenizer:
    type: ChatGLM2Tokenizer
    vocab_file: "/path/to/tokenizer.model"
  max_source_length: 256
  max_target_length: 256
  ignore_pad_token_for_loss: True
  input_columns: ["input_ids", "labels"]
  num_parallel_workers: 8
  python_multiprocessing: False
  drop_remainder: True
  batch_size: 1
  repeat: 1
  numa_enable: False
  prefetch_size: 1
  seed: 0

eval_dataset_task:
  type: KeyWordGenDataset
  dataset_config: *eval_dataset
```

### ç”ŸæˆHCCLæ–‡ä»¶

è¿è¡Œmindformers/tools/hccl_tools.pyç”ŸæˆRANK_TABLE_FILEçš„jsonæ–‡ä»¶ï¼›

```shell
# step1ï¼šæœºå™¨ä¸Šè¿è¡Œå¦‚ä¸‹å‘½ä»¤ï¼Œç”Ÿæˆå„è‡ªçš„RANK_TABLE_FILEçš„jsonæ–‡ä»¶
python ./mindformers/tools/hccl_tools.py --device_num "[0,8)"
```

> æ³¨ï¼šè‹¥ä½¿ç”¨ModelArtsçš„notebookç¯å¢ƒï¼Œå¯ä» `/user/config/jobstart_hccl.json` è·¯å¾„ä¸‹ç›´æ¥è·å–rank tableï¼Œæ— éœ€æ‰‹åŠ¨ç”Ÿæˆ

RANK_TABLE_FILE å•æœº8å¡å‚è€ƒæ ·ä¾‹:

```json
{
    "version": "1.0",
    "server_count": "1",
    "server_list": [
        {
            "server_id": "xx.xx.xx.xx",
            "device": [
                {"device_id": "0","device_ip": "192.1.27.6","rank_id": "0"},
                {"device_id": "1","device_ip": "192.2.27.6","rank_id": "1"},
                {"device_id": "2","device_ip": "192.3.27.6","rank_id": "2"},
                {"device_id": "3","device_ip": "192.4.27.6","rank_id": "3"},
                {"device_id": "4","device_ip": "192.1.27.7","rank_id": "4"},
                {"device_id": "5","device_ip": "192.2.27.7","rank_id": "5"},
                {"device_id": "6","device_ip": "192.3.27.7","rank_id": "6"},
                {"device_id": "7","device_ip": "192.4.27.7","rank_id": "7"}],
             "host_nic_ip": "reserve"
        }
    ],
    "status": "completed"
}
```

### å…¨å‚å¾®è°ƒ

#### run_mindformersè„šæœ¬å¯åŠ¨å…¨å‚å¾®è°ƒ

å…¨å‚å¾®è°ƒä½¿ç”¨ `configs/glm2/run_glm2_6b.yaml` é…ç½®æ–‡ä»¶ï¼Œé…ç½®æ–‡ä»¶ä¸­å®šä¹‰äº†å¾®è°ƒæ‰€éœ€çš„å„é…ç½®é¡¹

ä¿®æ”¹æ•°æ®é›†/æ¨¡å‹æƒé‡é…ç½®è·¯å¾„ï¼š

- æ•°æ®é›†ï¼šä¿®æ”¹ `mindformers/configs/glm2/run_glm2_6b.yaml` è„šæœ¬ä¸­`train_dataset` çš„ `dataset_dir` ä¸ºå‰æ–‡ç”Ÿæˆçš„æ•°æ®é›†è·¯å¾„ã€‚
- åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æƒé‡ï¼šä¿®æ”¹ `mindformers/configs/glm2/run_glm2_6b.yaml` è„šæœ¬ä¸­çš„ `load_checkpoint` ä¸ºé¢„è®­ç»ƒæ¨¡å‹æƒé‡è·¯å¾„ã€‚

å¯åŠ¨å…¨å‚å¾®è°ƒè„šæœ¬ï¼š

```shell
cd scripts
# Usage Help: bash run_distribute.sh [RANK_TABLE_FILE] [CONFIG_PATH] [DEVICE_RANGE] [RUN_STATUS]
bash run_distribute.sh /path/to/hccl_8p_01234567_127.0.1.1.json ../configs/glm2/run_glm2_6b.yaml '[0,8]' finetune
# å°†æ­¤å¤„rank_table_fileæ›¿æ¢ä¸ºå®é™…è·¯å¾„
```

å‚æ•°è¯´æ˜

```text
RANK_TABLE_FILE: ç”±mindformers/tools/hccl_tools.pyç”Ÿæˆçš„åˆ†å¸ƒå¼jsonæ–‡ä»¶
CONFIG_PATH: ä¸ºconfigsæ–‡ä»¶å¤¹ä¸‹é¢çš„glm2/run_glm2_6b.yamlé…ç½®æ–‡ä»¶
DEVICE_RANGE: ä¸ºå•æœºåˆ†å¸ƒå¼å¡çš„èŒƒå›´ï¼Œå¦‚ '[0,8]' ä¸º8å¡åˆ†å¸ƒå¼ï¼Œä¸åŒ…å«8æœ¬èº«
RUN_STATUS: ä¸ºä»»åŠ¡è¿è¡ŒçŠ¶æ€ï¼Œæ”¯æŒå…³é”®å­— train\finetune\eval\predict
```

> æ³¨ï¼šç”±äºGLM2_6Bçš„æ¨¡å‹è¾ƒå¤§ï¼Œæ— æ³•åœ¨å•å¡ä¸Šè¿è¡Œï¼Œæ­¤å¤„ä»…æä¾›åˆ†å¸ƒå¼å¯åŠ¨è„šæœ¬

è®­ç»ƒçš„logæ—¥å¿—è·¯å¾„ï¼šmindformers/output/log

checkpointå­˜å‚¨è·¯å¾„ï¼šmindformers/output/checkpoint

### LoRAä½å‚å¾®è°ƒ

å…¨å‚å¾®è°ƒèƒ½å¤Ÿåœ¨å¾®è°ƒæ•°æ®é›†ä¸Šå–å¾—è‰¯å¥½æ•ˆæœï¼Œä½†å­˜åœ¨é—å¿˜é¢„è®­ç»ƒçŸ¥è¯†çš„ç°è±¡
å› æ­¤æ¨èä½¿ç”¨ä½å‚å¾®è°ƒç®—æ³•ï¼Œå†»ç»“åŸæ¨¡å‹æƒé‡ï¼Œä»…åœ¨å°è§„æ¨¡å‚æ•°é‡ä¸Šè¿›è¡Œè®­ç»ƒï¼Œåœ¨å¾®è°ƒæ•°æ®é›†ä¸Šå–å¾—è‰¯å¥½æ•ˆæœçš„åŒæ—¶ï¼Œç¼“è§£æ¨¡å‹é—å¿˜ç°è±¡

#### run_mindformersè„šæœ¬å¯åŠ¨LoRAä½å‚å¾®è°ƒ

ä½¿ç”¨LoRAç®—æ³•è¿›è¡Œä½å‚å¾®è°ƒæ—¶ï¼Œä½¿ç”¨ `configs/glm2/run_glm2_6b_lora.yaml` é…ç½®æ–‡ä»¶ï¼Œè¯¥é…ç½®æ–‡ä»¶åŒ…å«äº†loraä½å‚å¾®è°ƒç®—æ³•æ‰€éœ€çš„é…ç½®é¡¹

ä¿®æ”¹æ•°æ®é›†/æ¨¡å‹æƒé‡é…ç½®è·¯å¾„ï¼š

- æ•°æ®é›†ï¼šä¿®æ”¹ `mindformers/configs/glm2/run_glm2_6b_lora.yaml` è„šæœ¬ä¸­`train_dataset` çš„ `dataset_dir` ä¸ºå‰æ–‡ç”Ÿæˆçš„æ•°æ®é›†è·¯å¾„ã€‚
- åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æƒé‡ï¼šä¿®æ”¹ `mindformers/configs/glm2/run_glm2_6b_lora.yaml` è„šæœ¬ä¸­çš„ `load_checkpoint` ä¸ºé¢„è®­ç»ƒæ¨¡å‹æƒé‡è·¯å¾„ã€‚

#### å¯åŠ¨LoRAä½å‚å¾®è°ƒè„šæœ¬(1å¡)ï¼š

æ‰§è¡Œå‘½ä»¤ï¼š

```shell
cd scripts
# Usage Help: bash run_stanalone.sh [CONFIG_PATH] [DEVICE_ID] [RUN_STATUS]
bash run_standalone.sh ../configs/glm2/run_glm2_6b_lora.yaml 0 finetune
```

è®­ç»ƒçš„logæ—¥å¿—è·¯å¾„ï¼šmindformers/scripts/mf_standalone/

checkpointå­˜å‚¨è·¯å¾„ï¼šmindformers/scripts/mf_standalone/output/checkpoint

#### Traineré«˜é˜¶æ¥å£å¯åŠ¨LoRAä½å‚å¾®è°ƒ

ç¤ºä¾‹è„šæœ¬å¦‚ä¸‹ï¼Œéœ€è¦æŒ‡å®šè®­ç»ƒæ•°æ®é›†è·¯å¾„å’Œå¾®è°ƒæƒé‡ã€‚

```python
from mindformers import Trainer
trainer = Trainer(task="text_generation", model="glm2_6b", pet_method="lora",
                  train_dataset="/path/to/AdvertiseGen/train.json")
trainer.finetune(finetune_checkpoint="glm2_6b")
```

### å¾®è°ƒåæ¨ç†

#### æ¨ç†æ ·ä¾‹è„šæœ¬

ä¸‹é¢æä¾›ä¸€ä¸ªæ¨¡å‹æ¨ç†æ ·ä¾‹è„šæœ¬ `infer.py`

```python
from mindformers import AutoConfig, AutoModel, AutoTokenizer
import mindspore as ms

ms.set_context(mode=ms.GRAPH_MODE, device_target="Ascend", device_id=0)

config = AutoConfig.from_pretrained("glm2_6b")
config.checkpoint_name_or_path = "/path/to/glm2_6b_finetune.ckpt"
model = AutoModel.from_config(config)
tokenizer = AutoTokenizer.from_pretrained("glm2_6b")

inputs = tokenizer(tokenizer.build_prompt("ä½ å¥½"))["input_ids"]
print(inputs)
print(tokenizer.decode(inputs))
outputs = model.generate(inputs, max_length=128)
print(tokenizer.decode(outputs))
inputs = tokenizer(tokenizer.build_prompt("è¯·ä»‹ç»ä¸€ä¸‹åä¸º"))["input_ids"]
print(inputs)
outputs = model.generate(inputs, max_length=128)
print(tokenizer.decode(outputs))
inputs = tokenizer(tokenizer.build_prompt("æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠ"))["input_ids"]
print(inputs)
outputs = model.generate(inputs, max_length=128)
print(tokenizer.decode(outputs))
inputs = tokenizer(tokenizer.build_prompt("ç±»å‹#ä¸Šè¡£*æè´¨#ç‰›ä»”å¸ƒ*é¢œè‰²#ç™½è‰²*é£æ ¼#ç®€çº¦*å›¾æ¡ˆ#åˆºç»£*è¡£æ ·å¼#å¤–å¥—*è¡£æ¬¾å¼#ç ´æ´"))["input_ids"]
print(inputs)
outputs = model.generate(inputs, max_length=128)
print(tokenizer.decode(outputs))
```

## è¯„ä¼°

### æ¨¡å‹æƒé‡æ–‡ä»¶åˆä¸€

å¾®è°ƒæ‰€å¾—åˆ°çš„æƒé‡æ–‡ä»¶ä¸ºæ ¹æ®æ¨¡å‹åˆ‡åˆ†ç­–ç•¥åˆ‡åˆ†åçš„æƒé‡ï¼Œæˆ‘ä»¬éœ€è¦æ‰‹åŠ¨å°†åˆ‡åˆ†æƒé‡åˆä¸€ï¼Œä»¥ç”¨äºè¯„ä¼°å’Œæ¨ç†

1. è·å–æ¨¡å‹åˆ‡åˆ†ç­–ç•¥æ–‡ä»¶ï¼š
   åœ¨æ‰§è¡Œå…¨å‚å¾®è°ƒè„šæœ¬æ—¶ï¼Œæ¨¡å‹å®Œæˆç¼–è¯‘åï¼Œå°†ä¼šåœ¨è¿è¡Œè·¯å¾„ä¸‹ï¼Œç”Ÿæˆåä¸º `ckpt_strategy.ckpt` çš„åˆ‡åˆ†ç­–ç•¥æ–‡ä»¶ï¼Œè¯¥æ–‡ä»¶å°†ç”¨äºç¬¬äºŒæ­¥æ¨¡å‹åˆæˆ

2. MindSporeæä¾›äº†æ ¹æ®åˆ‡åˆ†ç­–ç•¥è½¬æ¢æ¨¡å‹æƒé‡åˆ‡åˆ†çš„æ¥å£ï¼Œ[mindspore.transform_checkpoints](https://www.mindspore.cn/docs/zh-CN/r2.0/api_python/mindspore/mindspore.transform_checkpoints.html)ï¼Œæ‰§è¡Œä»¥ä¸‹pythonè„šæœ¬ï¼Œå°†8ä»½æ¨¡å‹æ–‡ä»¶åˆæˆä¸€ä»½

    ```python
    from mindspore import transform_checkpoints
    transform_checkpoints(
        src_checkpoints_dir="./output/checkpoint/", # åŸåˆ‡åˆ†æƒé‡æ–‡ä»¶å¤¹
        dst_checkpoints_dir="./target_checkpoint/", # ç›®æ ‡è·¯å¾„
        ckpt_prefix="glm2-6b", # .ckptæ–‡ä»¶å‰ç¼€å
        src_strategy_file="ckpt_stragery.ckpt", # æ­¥éª¤1ä¸­çš„åˆ‡åˆ†ç­–ç•¥æ–‡ä»¶è·¯å¾„
        dst_strategy_file=None # Noneè¡¨ç¤ºä¸åˆ‡åˆ†ï¼Œæƒé‡åˆä¸€
    )
    ```

> æ³¨ï¼š`transform_checkpoints` æ¥å£å½“å‰ä»…mindspore 2.0ä»¥ä¸Šç‰ˆæœ¬æ”¯æŒï¼Œå¦‚å½“å‰ç¡¬ä»¶ç¯å¢ƒåªæ”¯æŒ2.0ä»¥ä¸‹ç‰ˆæœ¬ï¼Œå¯ä»¥æ–°å»ºcondaç¯å¢ƒå®‰è£…mindspore 2.0çš„cpuç‰ˆæœ¬ä»¥æ‰§è¡Œè¯¥è„šæœ¬

### ä½¿ç”¨å…¨å‚å¾®è°ƒæƒé‡

#### run_mindformerså¯åŠ¨eval

ä½¿ç”¨å…¨å‚å¾®è°ƒæƒé‡æ—¶ï¼Œå¯åŠ¨å¦‚ä¸‹shellè„šæœ¬ï¼Œæ‰§è¡Œå•å¡è¯„ä¼°

é…ç½®æ–‡ä»¶é€‰æ‹© `configs/glm2/run_glm2_6b.yaml` glm2æ¨¡å‹æ¨ç†é…ç½®ï¼Œä¿®æ”¹å…¶ä¸­`model`å­—æ®µä¸‹`model_config`ä¸­`use_past: True`å¼€å¯å¢é‡æ¨ç†ä½¿è¯„ä¼°é€Ÿåº¦æ›´å¿«

```bash
python run_mindformer.py --config configs/glm2/run_glm2_6b.yaml --run_mode eval --load_checkpoint /path/to/glm2_6b_finetune.ckpt --eval_dataset_dir /path/to/data/AdvertiseGen/ --device_id 0
```

> æ³¨ï¼šä½¿ç”¨ç¦»çº¿ç”Ÿæˆæ•°æ®æ–¹å¼æ—¶ï¼Œå°† `eval_dataset_dir` ä¸€é¡¹æŒ‡å‘`.mindrecord`æ–‡ä»¶ï¼Œå¦‚ `/path/to/data/AdvertiseGen/adgen_dev.mindrecord`ã€‚

å„é¡¹å‚æ•°ï¼š

- `config`: æŒ‡å®šç”¨äºè¯„ä¼°çš„é…ç½®æ–‡ä»¶åç§°ï¼Œæ­¤å¤„ä¸º`configs/glm2/run_glm2_6b.yaml`
- `run_mode`: æŒ‡å®šæ‰§è¡Œæ¨¡å¼ï¼Œæ­¤ä¸º`eval`ï¼Œè¡¨ç¤ºä¸ºè¯„ä¼°æ¨¡å¼
- `load_checkpoint`: æŒ‡å®šè¦åŠ è½½çš„checkpointè·¯å¾„ï¼Œæ­¤å¤„ä¸º`/path/to/glm2_6b_finetune.ckpt`ï¼Œæ›¿æ¢ä¸ºéœ€åŠ è½½çš„æƒé‡çš„çœŸå®è·¯å¾„
- `eval_dataset_dir`: è¯„ä¼°æ•°æ®é›†çš„è·¯å¾„
- `device_id`: æŒ‡å®šè¦ä½¿ç”¨çš„è®¾å¤‡ç¼–å·ï¼ˆä»0å¼€å§‹ï¼‰

è¯„ä¼°å®Œæˆåä¼šæ‰“å°è¯„ä¼°æŒ‡æ ‡ `bleu-4`ã€`rouge-1`ã€`rouge-2`ã€`rouge-l`

> æ³¨ï¼šç”±äºé»˜è®¤è¯„ä¼°æŒ‡æ ‡çš„è·å–æ–¹å¼ä¸ºç”Ÿæˆå®Œæ•´æ–‡æœ¬åä¸é¢„æœŸæ–‡æœ¬åšæ¯”è¾ƒï¼Œè¯„ä¼°é€Ÿåº¦å°†å—é™äºæ¨¡å‹å¤§å°ä¸æ–‡æœ¬ç”Ÿæˆé€Ÿåº¦ï¼Œè¯„ä¼°æµç¨‹å¯èƒ½è¾ƒä¸ºç¼“æ…¢

#### Traineré«˜é˜¶æ¥å£å¯åŠ¨eval

ä¸ä¸Šæ–‡ç±»ä¼¼ï¼š

```bash
from mindformers import Trainer, ChatGLM2Config, ChatGLM2ForConditionalGeneration

# å¼€å¯å¢é‡æ¨ç†ä½¿è¯„ä¼°é€Ÿåº¦æ›´å¿«
config = ChatGLM2Config(use_past=True)
model = ChatGLM2ForConditionalGeneration(config)
trainer = Trainer(task="text_generation", model=model,
                  eval_dataset="/path/to/AdvertiseGen/dev.json")
trainer.evaluate(eval_checkpoint="/path/to/glm2_6b_finetune.ckpt")
```

### ä½¿ç”¨LoRAä½å‚å¾®è°ƒæƒé‡

#### run_mindformerså¯åŠ¨lora eval

ä½¿ç”¨LoRAä½å‚å¾®è°ƒæƒé‡æ—¶ï¼Œå¯åŠ¨å¦‚ä¸‹shellè„šæœ¬ï¼Œæ‰§è¡Œå•å¡è¯„ä¼°

é…ç½®æ–‡ä»¶é€‰æ‹© `configs/glm2/run_glm2_6b_lora.yaml` glm2_loraæ¨¡å‹æ¨ç†é…ç½®ï¼Œæ­¤é…ç½®å¯ç”¨äºloraæ¨¡å‹ï¼Œä¿®æ”¹å…¶ä¸­`model`å­—æ®µä¸‹`model_config`ä¸­`use_past: True`å¼€å¯å¢é‡æ¨ç†ä½¿è¯„ä¼°é€Ÿåº¦æ›´å¿«

```bash
python run_mindformer.py --config configs/glm2/run_glm2_6b_lora.yaml --run_mode eval --load_checkpoint /path/to/glm2_6b_lora.ckpt --eval_dataset_dir /path/to/data/AdvertiseGen/ --device_id 0
```

å„é¡¹å‚æ•°åŒä¸Šï¼Œè·¯å¾„éœ€æ›¿æ¢ä¸ºå®é™…è·¯å¾„

#### Traineré«˜é˜¶æ¥å£å¯åŠ¨lora eval

ä¸ä¸Šæ–‡ç±»ä¼¼ï¼š

```bash
from mindformers import Trainer, ChatGLM2Config, ChatGLM2WithLora
from mindformers.pet.pet_config import LoraConfig

# å¼€å¯å¢é‡æ¨ç†ä½¿è¯„ä¼°é€Ÿåº¦æ›´å¿«
config = ChatGLM2Config(use_past=True)
config.pet_config = LoraConfig()
model = ChatGLM2WithLora(config)
trainer = Trainer(task="text_generation", model=model,
                  eval_dataset="/path/to/AdvertiseGen/dev.json")
trainer.evaluate(eval_checkpoint="/path/to/glm2_6b_lora.ckpt")
```

## æ¨¡å‹æƒé‡è½¬åŒ–

æœ¬ä»“åº“ä¸­çš„`glm2`æ¥è‡ªäºHuggingFaceçš„ [ChatGLM2-6B](https://huggingface.co/THUDM/chatglm2-6b)ï¼ŒåŸºäºä¸‹è¿°çš„æ­¥éª¤è·å–ï¼š

1. å…‹éš†chatglm2-6bä»£ç ä»“ï¼Œä¸‹è½½åˆ†å¸ƒå¼çš„æ¨¡å‹æ–‡ä»¶ã€‚

   ```shell
   git lfs install
   git clone https://huggingface.co/THUDM/chatglm2-6b
   ```

2. æ‰§è¡Œ python è„šæœ¬ï¼Œåˆå¹¶æ¨¡å‹æƒé‡ã€‚

   ```python
   from transformers import AutoTokenizer, AutoModel
   import torch

   tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True)
   model = AutoModel.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True)

   with open("pt_model_arch.txt", "w") as fp:
       print(model, file=fp, flush=True)
   with open("pt_ckpt.txt", "w") as fp:
       for name, param in model.named_parameters():
           fp.write(f"{name} {param.shape} {param.dtype}\n")
   torch.save(model.state_dict(), "glm2_6b.pth")
   ```

3. æ‰§è¡Œè½¬æ¢è„šæœ¬ï¼Œå¾—åˆ°è½¬æ¢åçš„è¾“å‡ºæ–‡ä»¶`glm2_6b.ckpt`ã€‚

   ```python
   import mindspore as ms
   import torch as pt
   from tqdm import tqdm

   pt_ckpt_path = "glm2_6b.pth"
   pt_param = pt.load(pt_ckpt_path)

   type_map = {"torch.float16": "ms.float16",
               "torch.float32": "ms.float32"}
   ms_param = []
   with open("check_pt_ckpt.txt", "w") as fp:
       for k, v in tqdm(pt_param.items()):
           if "word_embeddings.weight" in k:
               k = k.replace("word_embeddings.weight", "embedding_table")
           fp.write(f"{k} {v.shape} {v.dtype}\n")
           ms_param.append({"name": k, "data": ms.Tensor(v.numpy())})

   ms.save_checkpoint(ms_param, "glm2_6b.ckpt")
   ```

## Mindspore-Lite æ¨ç†åŠé‡åŒ–

### åŸºæœ¬ä»‹ç»

ã€€ã€€MindFormers å®šä½æ‰“é€ è®­ç»ƒ->å¾®è°ƒ->éƒ¨ç½²çš„ç«¯åˆ°ç«¯å¤§æ¨¡å‹å·¥å…·å¥—ä»¶ï¼Œä¸ºäº†æ›´å¥½æ€§èƒ½åœ°éƒ¨ç½²å·²ç»å¾®è°ƒè®­ç»ƒå¥½çš„å¤§æ¨¡å‹ï¼Œæˆ‘ä»¬åˆ©ç”¨MindSporeæ‰“é€ çš„æ¨ç†å¼•æ“ [MindSpore_lite](https://gitee.com/link?target=https%3A%2F%2Fwww.mindspore.cn%2Flite)ï¼Œä¸ºç”¨æˆ·æä¾›äº†å¼€ç®±å³ç”¨çš„æ¨ç†éƒ¨ç½²æ–¹æ¡ˆï¼Œä¸ºç”¨æˆ·æä¾›ç«¯åˆ°ç«¯çš„å¤§æ¨¡å‹è§£å†³æ–¹æ¡ˆï¼Œå¸®åŠ©ç”¨æˆ·ä½¿èƒ½å¤§æ¨¡å‹ä¸šåŠ¡ã€‚

ã€€ã€€Lite æ¨ç†å¤§è‡´åˆ†ä¸¤æ­¥ï¼šæƒé‡è½¬æ¢å¯¼å‡º MindIR -> Lite æ¨ç†ï¼Œæ¥ä¸‹æ¥åˆ†åˆ«æè¿°ä¸Šè¿°ä¸¤ä¸ªè¿‡ç¨‹ã€‚

### MindIR å¯¼å‡º

ã€€ã€€1. ä¿®æ”¹æ¨¡å‹ç›¸å…³çš„é…ç½®æ–‡ä»¶ configs/glm2/export_glm2_6b.yamlï¼Œå…¶ä¸­éœ€è¦å…³æ³¨è¿™å‡ é¡¹ï¼š

```yaml
# export
infer:
    prefill_model_path: "glm2_export/glm2_6b_prefill_seq512.mindir" # ä¿å­˜mindirçš„ä½ç½®
    increment_model_path: "glm2_export/glm2_6b_inc_seq512.mindir"   # ä¿å­˜mindirçš„ä½ç½®
    infer_seq_length: 512 # éœ€è¦ä¿æŒè·Ÿ model-model_config-seq_length ä¸€è‡´

# ==== model config ====
model:
  model_config:
    seq_length: 512
    checkpoint_name_or_path: "/path/to/your/checkpoint"
```

2. æ‰§è¡Œexport.pyï¼Œå®Œæˆæ¨¡å‹è½¬æ¢

```bash
python mindformers/tools/export.py --config_path configs/glm2/export_glm2_6b.yaml
```

1. åˆ†åˆ«å¯¹ `prefill_model`â€‹ å’Œ `increment_model`â€‹ æ‰§è¡Œè½¬æ¢

### æ‰§è¡Œæ¨ç†

1. æ–°å»ºæ¨ç†é…ç½®æ–‡ä»¶ï¼šlite.ini

    ```ini
    [ascend_context]
    provider=ge

    [ge_session_options]
    ge.exec.formatMode=1
    ge.exec.precision_mode=must_keep_origin_dtype
    ```

2. æ‰§è¡Œå‘½ä»¤ï¼š

```bash
python run_infer_main.py --device_id 0 --model_name glm2 --prefill_model_path glm2_export/glm2_6b_prefill_seq512_graph.mindir --increment_model_path glm2_export/glm2_6b_inc_seq512_graph.mindir --config_path lite.ini --is_sample_acceleration False --seq_length 512 --add_special_tokens True
```

ã€€ã€€ç­‰å¾…æ¨¡å‹è½½å…¥ã€ç¼–è¯‘åï¼Œå‡ºç°ï¼š

```bash
Please enter your predict data:
```

ã€€ã€€è¾“å…¥ï¼š

```bash
[Round 1]

é—®ï¼šä½ å¥½ã€‚

ç­”ï¼š
```

ã€€ã€€è¾“å‡ºï¼š

```bash
['[Round 1]\n\né—®ï¼šä½ å¥½ã€‚\n\nç­”ï¼š ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚']
```
