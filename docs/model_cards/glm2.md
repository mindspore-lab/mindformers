# ChatGLM2

## æ¨¡å‹æè¿°

ChatGLM**2**-6B æ˜¯å¼€æºä¸­è‹±åŒè¯­å¯¹è¯æ¨¡å‹ [ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B) çš„ç¬¬äºŒä»£ç‰ˆæœ¬ï¼Œåœ¨ä¿ç•™äº†åˆä»£æ¨¡å‹å¯¹è¯æµç•…ã€éƒ¨ç½²é—¨æ§›è¾ƒä½ç­‰ä¼—å¤šä¼˜ç§€ç‰¹æ€§çš„åŸºç¡€ä¹‹ä¸Šï¼ŒChatGLM**2**-6Bå¼•å…¥äº†æ–°ç‰¹å¾ï¼š**æ›´å¼ºå¤§çš„æ€§èƒ½**ã€**æ›´é•¿çš„ä¸Šä¸‹æ–‡**ã€**æ›´é«˜æ•ˆçš„æ¨ç†**ã€**æ›´å¼€æ”¾çš„åè®®**ã€‚

```text
@article{zeng2022glm,
  title={Glm-130b: An open bilingual pre-trained model},
  author={Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and others},
  journal={arXiv preprint arXiv:2210.02414},
  year={2022}
}
```

## æ¨¡å‹æ€§èƒ½

- åŸºäºAtlas 800

**GLM2_6b**:

| config                                                      | task            | Datasets | metric                                  | phase                   | score                                   | performance                                    |
|-------------------------------------------------------------|-----------------|----------|-----------------------------------------|-------------------------|-----------------------------------------|------------------------------------------------|
| [glm2_6b](../../configs/glm2/run_glm2_6b.yaml)              | text_generation | ADGEN    | -                                       | [finetune](#å…¨é‡å¾®è°ƒ)       | -                                       | 815.2059134 tokens/s/p                         |
| [glm2_6b_lora](../../configs/glm2/run_glm2_6b_lora.yaml)    | text_generation | ADGEN    | -                                       | [finetune](#loraå¾®è°ƒ)     | -                                       | 3243.697479 tokens/s/p                         |
| [glm2_6b_ptuning2](../../configs/glm2/run_glm2_6b_ptuning2.yaml) | text_generation | ADGEN    | -                                       | [finetune](#ptuning2å¾®è°ƒ) | -                                       | 4150.537634 tokens/s/p                         |
| [glm2_6b](../../configs/glm2/run_glm2_6b.yaml)              | text_generation | ADGEN    | rouge-1<br>rouge-2<br>rouge-l<br>bleu-4 | [eval](#è¯„æµ‹)             | 30.7842<br>7.0734<br>24.7739<br>7.4661  | -                                              |
| [glm2_6b_lora](../../configs/glm2/run_glm2_6b_lora.yaml)    | text_generation | ADGEN    | rouge-1<br>rouge-2<br>rouge-l<br>bleu-4 | [eval](#è¯„æµ‹)             | 31.0563<br>7.1753<br>24.2296<br>7.2294  | -                                              |
| [glm2_6b_ptuning2](../../configs/glm2/run_glm2_6b_ptuning2.yaml)      | text_generation | ADGEN    | rouge-1<br>rouge-2<br>rouge-l<br>bleu-4 | [eval](#è¯„æµ‹)             | 31.5933<br>7.4504<br>24.7071<br>7.3042  | -                                              |
| [glm2_6b](../../configs/glm2/run_glm2_6b.yaml)              | text_generation | -        | -                                       | [predict](#æ¨ç†)          | -                                       | 32.08 tokens/s (use_past=True, seq_length=512) |

## ä»“åº“ä»‹ç»

`chatGLM2-6B` åŸºäº `mindformers` å®ç°ï¼Œä¸»è¦æ¶‰åŠçš„æ–‡ä»¶æœ‰ï¼š

1. æ¨¡å‹å…·ä½“å®ç°ï¼š`mindformers/models/glm2`

    ```text
    glm2
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ glm2.py                  # æ¨¡å‹å®ç°
        â”œâ”€â”€ glm2_config.py           # æ¨¡å‹é…ç½®é¡¹
        â”œâ”€â”€ glm2_modules.py          # æ¨¡ç»„å®ç°
        â”œâ”€â”€ glm2_tokenizer.py        # tokenizer
        â””â”€â”€ glm2_transformer.py      # transformerå±‚å®ç°
    ```

2. æ¨¡å‹é…ç½®ï¼š`configs/glm2`

    ```bash
    configs/glm2
      â”œâ”€â”€ export_glm2_6b.yaml
      â”œâ”€â”€ run_glm2_6b.yaml
      â”œâ”€â”€ run_glm2_6b_finetune_2k_800T_A2_64G.yaml  # Atlas 800T A2 æœ€ä½³æ€§èƒ½å…¨é‡å¾®è°ƒå¯åŠ¨é…ç½®
      â”œâ”€â”€ run_glm2_6b_finetune_2k_800_32G.yaml      # Atlas 800 æœ€ä½³æ€§èƒ½å…¨é‡å¾®è°ƒå¯åŠ¨é…ç½®
      â”œâ”€â”€ run_glm2_6b_finetune_800T_A2_64G.yaml     # Atlas 800T A2 ADGENå…¨é‡å¾®è°ƒå¯åŠ¨é…ç½®
      â”œâ”€â”€ run_glm2_6b_finetune_800_32G.yaml         # Atlas 800 ADGENå…¨é‡å¾®è°ƒå¯åŠ¨é…ç½®
      â”œâ”€â”€ run_glm2_6b_finetune_eval.yaml            # å…¨é‡å¾®è°ƒåè¯„ä¼°é…ç½®
      â”œâ”€â”€ run_glm2_6b_lora_2k_800T_A2_64G.yaml      # Atlas 800T A2æœ€ä½³æ€§èƒ½ loraå¾®è°ƒå¯åŠ¨é…ç½®
      â”œâ”€â”€ run_glm2_6b_lora_2k_800_32G.yaml          # Atlas 800 æœ€ä½³æ€§èƒ½ loraå¾®è°ƒå¯åŠ¨é…ç½®
      â”œâ”€â”€ run_glm2_6b_lora_800T_A2_64G.yaml         # Atlas 800T A2 ADGEN loraå¾®è°ƒå¯åŠ¨é…ç½®
      â”œâ”€â”€ run_glm2_6b_lora_800_32G.yaml             # Atlas 800 ADGEN loraå¾®è°ƒå¯åŠ¨é…ç½®
      â”œâ”€â”€ run_glm2_6b_lora_eval.yaml                # loraå¾®è°ƒè¯„ä¼°é…ç½®
      â””â”€â”€ run_glm2_6b_ptuning2.yaml                 # Atlas 800 ADGEN ptuningå¾®è°ƒå¯åŠ¨é…ç½®
    ```

## å‰æœŸå‡†å¤‡

### ç”ŸæˆRANK_TABLE_FILE

è¿è¡Œmindformers/tools/hccl_tools.pyç”ŸæˆRANK_TABLE_FILEçš„jsonæ–‡ä»¶

```bash
# è¿è¡Œå¦‚ä¸‹å‘½ä»¤ï¼Œç”Ÿæˆå½“å‰æœºå™¨çš„RANK_TABLE_FILEçš„jsonæ–‡ä»¶
python ./mindformers/tools/hccl_tools.py --device_num "[0,8)"
```

**æ³¨ï¼šè‹¥ä½¿ç”¨ModelArtsçš„notebookç¯å¢ƒï¼Œå¯ä» `/user/config/jobstart_hccl.json` è·¯å¾„ä¸‹ç›´æ¥è·å–rank tableï¼Œæ— éœ€æ‰‹åŠ¨ç”Ÿæˆ**

RANK_TABLE_FILE å•æœº8å¡å‚è€ƒæ ·ä¾‹:

```json
{
    "version": "1.0",
    "server_count": "1",
    "server_list": [
        {
            "server_id": "xx.xx.xx.xx",
            "device": [
                {"device_id": "0","device_ip": "192.1.27.6","rank_id": "0"},
                {"device_id": "1","device_ip": "192.2.27.6","rank_id": "1"},
                {"device_id": "2","device_ip": "192.3.27.6","rank_id": "2"},
                {"device_id": "3","device_ip": "192.4.27.6","rank_id": "3"},
                {"device_id": "4","device_ip": "192.1.27.7","rank_id": "4"},
                {"device_id": "5","device_ip": "192.2.27.7","rank_id": "5"},
                {"device_id": "6","device_ip": "192.3.27.7","rank_id": "6"},
                {"device_id": "7","device_ip": "192.4.27.7","rank_id": "7"}],
             "host_nic_ip": "reserve"
        }
    ],
    "status": "completed"
}
```

### å¤šæœºRANK_TABLE_FILEåˆå¹¶

- step 1. é¦–å…ˆæ ¹æ®ä¸Šç« èŠ‚å†…å®¹ï¼Œåœ¨æ¯ä¸ªæœºå™¨ä¸Šç”Ÿæˆå„è‡ªçš„`RANK_TABLE_FILE`æ–‡ä»¶ï¼Œç„¶åå°†ä¸åŒæœºå™¨ä¸Šç”Ÿæˆçš„`RANK_TABLE_FILE`æ–‡ä»¶å…¨éƒ¨æ‹·è´åˆ°åŒä¸€å°æœºå™¨ä¸Šã€‚

```bash
# è¿è¡Œå¦‚ä¸‹å‘½ä»¤ï¼Œç”Ÿæˆå½“å‰æœºå™¨çš„RANK_TABLE_FILEçš„jsonæ–‡ä»¶
python ./mindformers/tools/hccl_tools.py --device_num "[0,8)" --server_ip xx.xx.xx.xx
```

**æ³¨ï¼šéœ€è¦æ ¹æ®æœºå™¨çš„ipåœ°å€æŒ‡å®š --server_ipï¼Œé¿å…ç”±äºä¸åŒæœºå™¨server_ipä¸åŒï¼Œå¯¼è‡´å¤šèŠ‚ç‚¹é—´é€šä¿¡å¤±è´¥ã€‚**

- step 2. è¿è¡Œmindformers/tools/merge_hccl.pyå°†ä¸åŒæœºå™¨ä¸Šç”Ÿæˆçš„`RANK_TABLE_FILE`æ–‡ä»¶åˆå¹¶

```bash
# è¿è¡Œå¦‚ä¸‹å‘½ä»¤ï¼Œåˆå¹¶æ¯ä¸ªæœºå™¨ä¸Šçš„RANK_TABLE_FILEçš„jsonæ–‡ä»¶ã€‚
python ./mindformers/tools/merge_hccl.py hccl*.json
```

- step 3. å°†åˆå¹¶åçš„`RANK_TABLE_FILE`æ–‡ä»¶æ‹·è´åˆ°æ‰€æœ‰æœºå™¨ä¸­ï¼Œä¿è¯ä¸åŒæœºå™¨ä¸Šçš„`RANK_TABLE_FILE`ç›¸åŒã€‚

RANK_TABLE_FILE åŒæœº16å¡å‚è€ƒæ ·ä¾‹:

```json
{
    "version": "1.0",
    "server_count": "2",
    "server_list": [
        {
            "server_id": "xx.xx.xx.xx",
            "device": [
                {
                    "device_id": "0", "device_ip": "192.168.0.0", "rank_id": "0"
                },
                {
                    "device_id": "1", "device_ip": "192.168.1.0", "rank_id": "1"
                },
                {
                    "device_id": "2", "device_ip": "192.168.2.0", "rank_id": "2"
                },
                {
                    "device_id": "3", "device_ip": "192.168.3.0", "rank_id": "3"
                },
                {
                    "device_id": "4", "device_ip": "192.168.0.1", "rank_id": "4"
                },
                {
                    "device_id": "5", "device_ip": "192.168.1.1", "rank_id": "5"
                },
                {
                    "device_id": "6", "device_ip": "192.168.2.1", "rank_id": "6"
                },
                {
                    "device_id": "7", "device_ip": "192.168.3.1", "rank_id": "7"
                }
            ],
            "host_nic_ip": "reserve"
        },
        {
            "server_id": "xx.xx.xx.xx",
            "device": [
                {
                    "device_id": "0", "device_ip": "192.168.0.1", "rank_id": "8"
                },
                {
                    "device_id": "1", "device_ip": "192.168.1.1", "rank_id": "9"
                },
                {
                    "device_id": "2", "device_ip": "192.168.2.1", "rank_id": "10"
                },
                {
                    "device_id": "3", "device_ip": "192.168.3.1", "rank_id": "11"
                },
                {
                    "device_id": "4", "device_ip": "192.168.0.2", "rank_id": "12"
                },
                {
                    "device_id": "5", "device_ip": "192.168.1.2", "rank_id": "13"
                },
                {
                    "device_id": "6", "device_ip": "192.168.2.2", "rank_id": "14"
                },
                {
                    "device_id": "7", "device_ip": "192.168.3.2", "rank_id": "15"
                }
            ],
            "host_nic_ip": "reserve"
        }
    ],
    "status": "completed"
}
```

### æ¨¡å‹æƒé‡ä¸‹è½½ä¸è½¬æ¢

å¼€å‘è€…å¯ä»¥ä¸‹è½½è·å–å®˜æ–¹æƒé‡åï¼Œé€šè¿‡ä¸‹é¢æä¾›çš„**æƒé‡è½¬æ¢è„šæœ¬**ï¼Œå°†å®˜æ–¹æƒé‡è½¬æ¢ä¸ºMindSporeæƒé‡ï¼›æˆ–ç›´æ¥ä½¿ç”¨MindFormersæä¾›çš„**å·²è½¬æ¢æƒé‡**

1. ä½¿ç”¨å®˜æ–¹æƒé‡è¿›è¡Œè½¬æ¢

   å…‹éš†glm2-6bä»£ç ä»“ï¼Œä¸‹è½½åˆ†å¸ƒå¼çš„æ¨¡å‹æ–‡ä»¶ã€‚

   ```shell
   git lfs install
   git clone https://huggingface.co/THUDM/chatglm2-6b
   ```

   æ‰§è¡Œ python è„šæœ¬ï¼Œåˆå¹¶æ¨¡å‹æƒé‡ã€‚

   ```python
   from transformers import AutoTokenizer, AutoModel
   import torch

   tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True)
   model = AutoModel.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True)

   with open("pt_model_arch.txt", "w") as fp:
       print(model, file=fp, flush=True)
   with open("pt_ckpt.txt", "w") as fp:
       for name, param in model.named_parameters():
           fp.write(f"{name} {param.shape} {param.dtype}\n")
   torch.save(model.state_dict(), "glm2_6b.pth")
   ```

   æ‰§è¡Œè½¬æ¢è„šæœ¬ï¼Œå¾—åˆ°è½¬æ¢åçš„è¾“å‡ºæ–‡ä»¶`glm2_6b.ckpt`ã€‚

   ```python
   import mindspore as ms
   import torch as pt
   from tqdm import tqdm

   pt_ckpt_path = "glm2_6b.pth"
   pt_param = pt.load(pt_ckpt_path)

   type_map = {"torch.float16": "ms.float16",
               "torch.float32": "ms.float32"}
   ms_param = []
   with open("check_pt_ckpt.txt", "w") as fp:
       for k, v in tqdm(pt_param.items()):
           if "word_embeddings.weight" in k:
               k = k.replace("word_embeddings.weight", "embedding_table")
           fp.write(f"{k} {v.shape} {v.dtype}\n")
           ms_param.append({"name": k, "data": ms.Tensor(v.numpy())})

   ms.save_checkpoint(ms_param, "glm2_6b.ckpt")
   ```

2. è·å–MindFormersæä¾›çš„å·²è½¬æ¢æƒé‡

   å¯é€šè¿‡from_pretrainedæ¥å£ä¸‹è½½ï¼Œä¹Ÿå¯ç›´æ¥ä»ä¸‹é¢çš„é“¾æ¥è·å–

   [glm2_6bæƒé‡](https://ascend-repo-modelzoo.obs.cn-east-2.myhuaweicloud.com/XFormer_for_mindspore/glm2/glm2_6b.ckpt)

   [tokenizeræ–‡ä»¶](https://ascend-repo-modelzoo.obs.cn-east-2.myhuaweicloud.com/XFormer_for_mindspore/glm2/tokenizer.model)

### [åˆ†å¸ƒå¼è®­ç»ƒ/å¾®è°ƒæƒé‡åˆå¹¶](../feature_cards/Transform_Ckpt.md)

åˆ†å¸ƒå¼è®­ç»ƒ/å¾®è°ƒåæ‰€å¾—åˆ°çš„æƒé‡æ–‡ä»¶ä¸ºæ ¹æ®ç­–ç•¥åˆ‡åˆ†åçš„æƒé‡ï¼Œéœ€è¦æ‰‹åŠ¨å°†åˆ‡åˆ†æƒé‡åˆä¸€ï¼Œä»¥ç”¨äºè¯„ä¼°å’Œæ¨ç†ã€‚

æ¶‰åŠåˆ°ckptçš„å•å¡ï¼Œå¤šå¡è½¬æ¢ï¼Œè¯¦ç»†æ•™ç¨‹è¯·å‚è€ƒç‰¹æ€§æ–‡æ¡£æ¨¡å‹[æƒé‡åˆ‡åˆ†ä¸åˆå¹¶](../feature_cards/Transform_Ckpt.md)

- step 1. è·å–æ¨¡å‹åˆ‡åˆ†ç­–ç•¥æ–‡ä»¶ï¼š

åœ¨æ‰§è¡Œå¾®è°ƒè„šæœ¬æ—¶ï¼Œæ¨¡å‹å®Œæˆç¼–è¯‘åï¼Œå°†ä¼šåœ¨`output/strategy`è·¯å¾„ä¸‹ç”Ÿæˆå„å¡çš„åˆ‡åˆ†ç­–ç•¥æ–‡ä»¶ï¼Œç”¨äºæƒé‡åˆå¹¶ã€‚

> æ³¨ï¼šloraå¾®è°ƒæ—¶éœ€è¦ç¡®è®¤é…ç½®æ–‡ä»¶`parallel context config`ä¸­`only_trainable_params`è®¾ä¸º`False`ï¼Œä»¥è·å–æ‰€æœ‰å‚æ•°å®Œæ•´ç­–ç•¥ã€‚

- step 2. è¿è¡Œ`mindformers/tools/transform_ckpt.py`è„šæœ¬è¿›è¡Œå¤šå¡æƒé‡åˆå¹¶ï¼š

```shell
python transform_ckpt.py \
--src_ckpt_strategy {path}/output/strategy/ \
--src_ckpt_dir {path}/output/checkpoint/ \
--dst_ckpt_dir {path}/target_checkpoint/ \
--prefix glm2_6b
```

```text
# å‚æ•°è¯´æ˜
src_ckpt_strategy: æ­¥éª¤1ä¸­çš„åˆ‡åˆ†ç­–ç•¥æ–‡ä»¶è·¯å¾„
src_ckpt_dir: åŸåˆ‡åˆ†æƒé‡æ–‡ä»¶å¤¹
dst_ckpt_dir: ç›®æ ‡è·¯å¾„
prefix: ckptæ–‡ä»¶å‰ç¼€å
```

> æ³¨ï¼š`transform_checkpoints` æ¥å£å½“å‰ä»…mindspore 2.0ä»¥ä¸Šç‰ˆæœ¬æ”¯æŒï¼Œå¦‚å½“å‰ç¡¬ä»¶ç¯å¢ƒåªæ”¯æŒ2.0ä»¥ä¸‹ç‰ˆæœ¬ï¼Œå¯ä»¥æ–°å»ºcondaç¯å¢ƒå®‰è£…mindspore 2.0çš„cpuç‰ˆæœ¬ä»¥æ‰§è¡Œè¯¥è„šæœ¬

## åŸºäºAPIçš„å¿«é€Ÿä½¿ç”¨

### åŸºäºAutoClassçš„å¿«é€Ÿä½¿ç”¨

å¯ä»¥ä½¿ç”¨AutoClassæ¥å£ï¼Œé€šè¿‡æ¨¡å‹åç§°è·å–ç›¸åº”çš„model/preprocess/tokenizerç­‰å®ä¾‹ï¼Œå¹¶è‡ªåŠ¨ä¸‹è½½å¹¶åŠ è½½æƒé‡

`from_pretrained()` æ¥å£ä¼šè‡ªåŠ¨ä»äº‘ä¸Šä¸‹è½½é¢„è®­ç»ƒçš„æ¨¡å‹ï¼Œå­˜å‚¨è·¯å¾„ï¼š`./checkpoint_download/glm2`

```python
import mindspore
from mindformers import AutoConfig, AutoModel, AutoTokenizer

# æŒ‡å®šå›¾æ¨¡å¼ï¼ŒæŒ‡å®šä½¿ç”¨è®­ç»ƒå¡id
mindspore.set_context(mode=0, device_id=0)

tokenizer = AutoTokenizer.from_pretrained('glm2_6b')

# modelçš„å®ä¾‹åŒ–æœ‰ä»¥ä¸‹ä¸¤ç§æ–¹å¼ï¼Œé€‰æ‹©å…¶ä¸­ä¸€ç§è¿›è¡Œå®ä¾‹åŒ–å³å¯
# 1. ç›´æ¥æ ¹æ®é»˜è®¤é…ç½®å®ä¾‹åŒ–
model = AutoModel.from_pretrained('glm2_6b')
# 2. è‡ªå®šä¹‰ä¿®æ”¹é…ç½®åå®ä¾‹åŒ–
config = AutoConfig.from_pretrained('glm2_6b')
config.use_past = True                  # æ­¤å¤„ä¿®æ”¹é»˜è®¤é…ç½®ï¼Œå¼€å¯å¢é‡æ¨ç†èƒ½å¤ŸåŠ é€Ÿæ¨ç†æ€§èƒ½
# config.xxx = xxx                      # æ ¹æ®éœ€æ±‚è‡ªå®šä¹‰ä¿®æ”¹å…¶ä½™æ¨¡å‹é…ç½®
model = AutoModel.from_config(config)   # ä»è‡ªå®šä¹‰é…ç½®é¡¹ä¸­å®ä¾‹åŒ–æ¨¡å‹

inputs = tokenizer("ä½ å¥½")["input_ids"]
# é¦–æ¬¡è°ƒç”¨model.generate()è¿›è¡Œæ¨ç†å°†åŒ…å«å›¾ç¼–è¯‘æ—¶é—´ï¼Œæ¨ç†æ€§èƒ½æ˜¾ç¤ºä¸å‡†ç¡®ï¼Œå¤šæ¬¡é‡å¤è°ƒç”¨ä»¥è·å–å‡†ç¡®çš„æ¨ç†æ€§èƒ½
outputs = model.generate(inputs, max_new_tokens=20, do_sample=True, top_k=3)
response = tokenizer.decode(outputs)
print(response)
# ['ä½ å¥½ï¼Œä½œä¸ºä¸€åäººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œæˆ‘æ¬¢è¿æ‚¨éšæ—¶å‘æˆ‘æé—®ã€‚']
```

### åŸºäºTrainerçš„å¿«é€Ÿè®­ç»ƒï¼Œå¾®è°ƒï¼Œè¯„æµ‹ï¼Œæ¨ç†

> æ³¨ï¼šä¸‹é¢ä»…æ˜¾ç¤ºæ¥å£ä½¿ç”¨æ–¹å¼ï¼Œæ¨¡å‹å¯åŠ¨è®­ç»ƒéœ€æ±‚å¤šå¡åˆ†å¸ƒå¼è®­ç»ƒï¼Œè®­ç»ƒè„šæœ¬éœ€é…åˆåˆ†å¸ƒå¼è„šæœ¬å¯åŠ¨

```python
import mindspore
from mindformers.trainer import Trainer

# æŒ‡å®šå›¾æ¨¡å¼ï¼ŒæŒ‡å®šä½¿ç”¨è®­ç»ƒå¡id
mindspore.set_context(mode=0, device_id=0)

# åˆå§‹åŒ–é¢„è®­ç»ƒä»»åŠ¡
trainer = Trainer(task='text_generation',
                  model='glm2_6b',
                  train_dataset='path/to/train_dataset',
                  eval_dataset='path/to/eval_dataset')

# å¼€å¯é¢„è®­ç»ƒ
# è¯·å‚ç…§å¤šå¡è®­ç»ƒï¼Œglm2_6bä¸æ”¯æŒå•å¡å¯åŠ¨è®­ç»ƒ
# trainer.train()

# å¼€å¯å…¨é‡å¾®è°ƒ
# è¯·å‚ç…§å¤šå¡å¾®è°ƒï¼Œglm2_6bä¸æ”¯æŒå•å¡å¯åŠ¨å…¨é‡å¾®è°ƒ
# trainer.finetune()

# å¼€å¯è¯„æµ‹
# éœ€è¦åœ¨configs/glm2/run_glm2_6b.yamlä¸­å°†seq_lengthä¿®æ”¹ä¸º256
trainer.evaluate()

# å¼€å¯æ¨ç†
predict_result = trainer.predict(input_data="ä½ å¥½")
print(predict_result)
# [{'text_generation_text': ['ä½ å¥½ï¼Œæˆ‘æ˜¯ ChatGLM2-6Bï¼Œ ä¸€ä¸ªäººå·¥æ™ºèƒ½åŠ©æ‰‹ã€‚æˆ‘èƒŒåä½¿ç”¨çš„æ¨¡å‹æ˜¯ GLM2-6Bï¼Œ æ˜¯ä¸€ç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œ å…·æœ‰è¶…è¿‡ 2000 äº¿å‚æ•°ï¼Œæ”¯æŒå¤šç§ä»»åŠ¡ã€‚']}]
```

### åŸºäºPipelineçš„å¿«é€Ÿæ¨ç†

```python
import mindspore
mindspore.set_context(mode=0, device_id=0)

from mindformers import pipeline
task_pipeline = pipeline(task='text_generation', model='glm2_6b', max_length=2048)
task_pipeline('ä½ å¥½')
# [{'text_generation_text': ['ä½ å¥½ï¼Œæˆ‘æ˜¯ ChatGLM2-6Bï¼Œ ä¸€ä¸ªäººå·¥æ™ºèƒ½åŠ©æ‰‹ã€‚æˆ‘èƒŒåä½¿ç”¨çš„æ¨¡å‹æ˜¯ GLM2-6Bï¼Œ æ˜¯ä¸€ç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œ å…·æœ‰è¶…è¿‡ 2000 äº¿å‚æ•°ï¼Œæ”¯æŒå¤šç§ä»»åŠ¡ã€‚']}]

from mindformers import AutoModel, AutoTokenizer, TextGenerationPipeline
model = AutoModel.from_pretrained('glm2_6b')
tokenizer = AutoTokenizer.from_pretrained('glm2_6b')
pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)
predict_result = pipeline("ä½ å¥½")
print(predict_result)
# [{'text_generation_text': ['ä½ å¥½ï¼Œæˆ‘æ˜¯ ChatGLM2-6Bï¼Œ ä¸€ä¸ªäººå·¥æ™ºèƒ½åŠ©æ‰‹ã€‚æˆ‘èƒŒåä½¿ç”¨çš„æ¨¡å‹æ˜¯ GLM2-6Bï¼Œ æ˜¯ä¸€ç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œ å…·æœ‰è¶…è¿‡ 2000 äº¿å‚æ•°ï¼Œæ”¯æŒå¤šç§ä»»åŠ¡ã€‚']}]
```

## å¾®è°ƒ

ä¸‹é¢ä»¥ [ADGEN](https://aclanthology.org/D19-1321.pdf) (å¹¿å‘Šç”Ÿæˆ) æ•°æ®é›†ä¸ºä¾‹ä»‹ç»ä»£ç çš„ä½¿ç”¨æ–¹æ³•

### æ•°æ®é›†å‡†å¤‡

ADGEN æ•°æ®é›†ä»»åŠ¡ä¸ºæ ¹æ®è¾“å…¥ï¼ˆcontentï¼‰ç”Ÿæˆä¸€æ®µå¹¿å‘Šè¯ï¼ˆsummaryï¼‰ã€‚

```json
{"content": "ç±»å‹#ä¸Šè¡£*ç‰ˆå‹#å®½æ¾*ç‰ˆå‹#æ˜¾ç˜¦*å›¾æ¡ˆ#çº¿æ¡*è¡£æ ·å¼#è¡¬è¡«*è¡£è¢–å‹#æ³¡æ³¡è¢–*è¡£æ¬¾å¼#æŠ½ç»³", "summary": "è¿™ä»¶è¡¬è¡«çš„æ¬¾å¼éå¸¸çš„å®½æ¾ï¼Œåˆ©è½çš„çº¿æ¡å¯ä»¥å¾ˆå¥½çš„éšè—èº«æä¸Šçš„å°ç¼ºç‚¹ï¼Œç©¿åœ¨èº«ä¸Šæœ‰ç€å¾ˆå¥½çš„æ˜¾ç˜¦æ•ˆæœã€‚é¢†å£è£…é¥°äº†ä¸€ä¸ªå¯çˆ±çš„æŠ½ç»³ï¼Œæ¼‚äº®çš„ç»³ç»“å±•ç°å‡ºäº†åè¶³çš„ä¸ªæ€§ï¼Œé…åˆæ—¶å°šçš„æ³¡æ³¡è¢–å‹ï¼Œå°½æ˜¾å¥³æ€§ç”œç¾å¯çˆ±çš„æ°”æ¯ã€‚"}
```

ä» [Google Drive](https://drive.google.com/file/d/13_vf0xRTQsyneRKdD1bZIr93vBGOczrk/view?usp=sharing) æˆ–è€… [Tsinghua Cloud](https://cloud.tsinghua.edu.cn/f/b3f119a008264b1cabd1/?dl=1) ä¸‹è½½å¤„ç†å¥½çš„ ADGEN æ•°æ®é›†ï¼Œç›®å½•ç»“æ„ä¸º

```text
AdvertiseGen
  â”œâ”€â”€ train.json
  â””â”€â”€ dev.json
```

ä¿®æ”¹é…ç½®æ–‡ä»¶ `configs/glm2/run_glm2_6b_*.yaml` ä¸­çš„ä»¥ä¸‹é¡¹ï¼š

```yaml
train_dataset: &train_dataset
    dataset_dir: "/path/to/AdvertiseGen/train.json"
    origin_columns: ["content", "summary"]
  tokenizer:
    vocab_file: "/path/to/tokenizer.model"
  input_columns: ["input_ids", "labels"]
  max_source_length: 64
  max_target_length: 127

eval_dataset: &eval_dataset
  data_loader:
    dataset_dir: "/path/to/AdvertiseGen/dev.json"
    origin_columns: ["content", "summary"]
  tokenizer:
    vocab_file: "/path/to/tokenizer.model"
  max_source_length: 256
  max_target_length: 256
```

**æ³¨æ„**ï¼šå¾®è°ƒæ—¶çš„æ¨¡å‹`seq_length`éœ€è¦ç­‰äºå¾®è°ƒæ•°æ®é›†çš„`max_source_length + max_target_length + 1`ã€‚
yamlæ–‡ä»¶ä¸­é»˜è®¤çš„`seq_length: 192`ä»¥åŠ`max_source_length: 64`å’Œ`max_target_length: 127`é€‚ç”¨äºADGENæ•°æ®é›†ï¼Œ
å…¶ä»–æ•°æ®é›†çš„`seq_length`è®¾ç½®ï¼Œå¯ä»¥éå†å¹¶å°†æ•°æ®é›†è½¬æ¢ä¸ºtoken_idï¼Œå–token_idæœ€å¤§é•¿åº¦ï¼Œ`seq_length`å¤ªå¤§å½±å“è®­ç»ƒæ€§èƒ½ï¼Œ
å¤ªå°å½±å“è®­ç»ƒç²¾åº¦ï¼Œéœ€è¦åšå‡ºæƒè¡¡ã€‚

### å…¨å‚å¾®è°ƒ

å…¨å‚å¾®è°ƒä½¿ç”¨ `configs/glm2/run_glm2_6b_finetune*.yaml` é…ç½®æ–‡ä»¶ï¼Œé…ç½®æ–‡ä»¶ä¸­å®šä¹‰äº†å¾®è°ƒæ‰€éœ€çš„å„é…ç½®é¡¹

ä¿®æ”¹æ•°æ®é›†/æ¨¡å‹æƒé‡é…ç½®è·¯å¾„ï¼š

- æ•°æ®é›†ï¼šä¿®æ”¹ `configs/glm2/run_glm2_6b_finetune*.yaml` è„šæœ¬ä¸­`train_dataset` çš„ `dataset_dir` ä¸ºå‰æ–‡ç”Ÿæˆçš„æ•°æ®é›†è·¯å¾„ã€‚
- åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æƒé‡ï¼šä¿®æ”¹ `configs/glm2/run_glm2_6b_finetune*.yaml` è„šæœ¬ä¸­çš„ `load_checkpoint` ä¸ºé¢„è®­ç»ƒæ¨¡å‹æƒé‡è·¯å¾„ã€‚

å½“å‰æ¨¡å‹å·²æ”¯æŒä½¿ç”¨**Flash Attentionç®—æ³•**è¿›è¡Œå…¨å‚å¾®è°ƒï¼Œè¯·å‚è€ƒ [Flash Attentionä½¿ç”¨æ–‡æ¡£](../feature_cards/Training_Algorithms.md#flash-attention)

#### å•å¡å¾®è°ƒ

ç”±äºglm2_6bæ¨¡å‹è¾ƒå¤§ï¼Œå…¨é‡å¾®è°ƒä¸æ”¯æŒå•å¡è¿è¡Œ

#### å¤šå¡å¾®è°ƒ

- å•æœºå¤šå¡

å¤šå¡è¿è¡Œéœ€è¦RANK_FILE_TABLEï¼Œè¯·å‚è€ƒå‰æœŸå‡†å¤‡-[ç”ŸæˆRANK_TABLE_FILE](#ç”Ÿæˆranktablefile)

```shell
cd scripts
# Usage Help: bash run_distribute.sh [RANK_TABLE_FILE] [CONFIG_PATH] [DEVICE_RANGE] [RUN_STATUS]
bash run_distribute.sh /path/to/hccl_8p_01234567_127.0.1.1.json ../configs/glm2/run_glm2_6b_finetune*.yaml '[0,8]' finetune
# å°†æ­¤å¤„rank_table_fileæ›¿æ¢ä¸ºå®é™…è·¯å¾„
```

> å¤šå¡å¾®è°ƒçš„æ¨¡å‹éœ€è¦åˆå¹¶æƒé‡åæ‰èƒ½è¿›è¡Œå•å¡è¯„ä¼°ã€‚

å‚æ•°è¯´æ˜

```text
RANK_TABLE_FILE: ç”±mindformers/tools/hccl_tools.pyç”Ÿæˆçš„åˆ†å¸ƒå¼jsonæ–‡ä»¶
CONFIG_PATH: ä¸ºconfigsæ–‡ä»¶å¤¹ä¸‹é¢çš„glm2/run_glm2_6b_finetune*.yamlé…ç½®æ–‡ä»¶
DEVICE_RANGE: ä¸ºå•æœºåˆ†å¸ƒå¼å¡çš„èŒƒå›´ï¼Œå¦‚ '[0,8]' ä¸º8å¡åˆ†å¸ƒå¼ï¼Œä¸åŒ…å«8æœ¬èº«
RUN_STATUS: ä¸ºä»»åŠ¡è¿è¡ŒçŠ¶æ€ï¼Œæ”¯æŒå…³é”®å­— train\finetune\eval\predict
```

> è®­ç»ƒçš„logæ—¥å¿—è·¯å¾„ï¼šmindformers/output/log
>
> checkpoint(å«ä¼˜åŒ–å™¨å‚æ•°)å­˜å‚¨è·¯å¾„ï¼šmindformers/output/checkpoint
>
> checkpoint(ä¸å«ä¼˜åŒ–å™¨å‚æ•°)å­˜å‚¨è·¯å¾„ï¼šmindformers/output/checkpoint_network
>
> è‹¥æƒ³åˆå¹¶ckptç”¨äºåç»­è¯„ä¼°ï¼Œé€‰æ‹©ä¸å«ä¼˜åŒ–å™¨å‚æ•°çš„æƒé‡å³å¯ã€‚

- å¤šæœºå¤šå¡

å¤šæœºå¤šå¡è¿è¡Œéœ€è¦åˆå¹¶ä¸åŒæœºå™¨çš„RANK_FILE_TABLEï¼Œå‚è€ƒå‰æœŸå‡†å¤‡-[å¤šæœºRANK_TABLE_FILEåˆå¹¶](#å¤šæœºranktablefileåˆå¹¶)

åœ¨æ¯å°æœºå™¨ä¸Šå¯åŠ¨`bash run_distribute.sh`ã€‚

```bash
server_count=12
device_num=8*$server_count
# launch ranks in the 0th server
cd scripts
bash run_distribute.sh $RANK_TABLE_FILE path/to/config.yaml [0,8] finetune $device_num

# launch ranks in the 1-11 server via ssh
for idx in {1..11}
do
    let rank_start=8*$idx
    let rank_end=$rank_start+8
    ssh ${IP_LIST[$idx]} "cd scripts; bash run_distribute.sh $RANK_TABLE_FILE path/to/config.yaml [$rank_start,$rank_end] finetune $device_num"
done
```

å…¶ä¸­

- `RANK_TABLE_FILE`ä¸ºä¸Šä¸€æ­¥æ±‡æ€»å¹¶åˆ†å‘çš„æ€»rank tableæ–‡ä»¶ï¼›
- `IP_LIST`ä¸º12å°æœåŠ¡å™¨çš„IPåœ°å€ã€‚å¦‚192.168.0.[0-11]

```bash
IP_LIST=("192.168.0.0", "192.168.0.1", ..., "192.168.0.11")
```

> å¤šå¡å¾®è°ƒçš„æ¨¡å‹éœ€è¦åˆå¹¶æƒé‡åæ‰èƒ½è¿›è¡Œå•å¡è¯„ä¼°ã€‚

### LoRAå¾®è°ƒ

å…¨å‚å¾®è°ƒèƒ½å¤Ÿåœ¨å¾®è°ƒæ•°æ®é›†ä¸Šå–å¾—è‰¯å¥½æ•ˆæœï¼Œä½†å­˜åœ¨é—å¿˜é¢„è®­ç»ƒçŸ¥è¯†çš„ç°è±¡ã€‚
å› æ­¤æ¨èä½¿ç”¨ä½å‚å¾®è°ƒç®—æ³•ï¼Œå†»ç»“åŸæ¨¡å‹æƒé‡ï¼Œä»…åœ¨å°è§„æ¨¡å‚æ•°é‡ä¸Šè¿›è¡Œè®­ç»ƒï¼Œåœ¨å¾®è°ƒæ•°æ®é›†ä¸Šå–å¾—è‰¯å¥½æ•ˆæœçš„åŒæ—¶ï¼Œç¼“è§£æ¨¡å‹é—å¿˜ç°è±¡

ä½¿ç”¨LoRAç®—æ³•è¿›è¡Œä½å‚å¾®è°ƒæ—¶ï¼Œä½¿ç”¨ `configs/glm2/run_glm2_6b_lora*.yaml` é…ç½®æ–‡ä»¶ï¼Œè¯¥é…ç½®æ–‡ä»¶åŒ…å«äº†loraä½å‚å¾®è°ƒç®—æ³•æ‰€éœ€çš„é…ç½®é¡¹

ä¿®æ”¹æ•°æ®é›†/æ¨¡å‹æƒé‡é…ç½®è·¯å¾„ï¼š

- æ•°æ®é›†ï¼šä¿®æ”¹ `mindformers/configs/glm2/run_glm2_6b_lora*.yaml` è„šæœ¬ä¸­`train_dataset` çš„ `dataset_dir` ä¸ºå‰æ–‡ç”Ÿæˆçš„æ•°æ®é›†è·¯å¾„ã€‚
- åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æƒé‡ï¼šä¿®æ”¹ `mindformers/configs/glm2/run_glm2_6b_lora*.yaml` è„šæœ¬ä¸­çš„ `load_checkpoint` ä¸ºé¢„è®­ç»ƒæ¨¡å‹æƒé‡è·¯å¾„ã€‚

#### å•å¡å¾®è°ƒ

```shell
cd scripts
# Usage Help: bash run_stanalone.sh [CONFIG_PATH] [DEVICE_ID] [RUN_STATUS]
bash run_standalone.sh ../configs/glm2/run_glm2_6b_lora*.yaml 0 finetune
```

> è®­ç»ƒçš„logæ—¥å¿—è·¯å¾„ï¼šmindformers/output/log
>
> checkpoint(å«ä¼˜åŒ–å™¨å‚æ•°)å­˜å‚¨è·¯å¾„ï¼šmindformers/output/checkpoint
>
> checkpoint(ä¸å«ä¼˜åŒ–å™¨å‚æ•°)å­˜å‚¨è·¯å¾„ï¼šmindformers/output/checkpoint_network
>
> è‹¥æƒ³åˆå¹¶ckptç”¨äºåç»­è¯„ä¼°ï¼Œé€‰æ‹©ä¸å«ä¼˜åŒ–å™¨å‚æ•°çš„æƒé‡å³å¯ã€‚

#### å¤šå¡å¾®è°ƒ

- å•æœºå¤šå¡

å¤šå¡è¿è¡Œéœ€è¦RANK_FILE_TABLEï¼Œè¯·å‚è€ƒå‰æœŸå‡†å¤‡-[ç”ŸæˆRANK_TABLE_FILE](#ç”Ÿæˆranktablefile)

```shell
cd scripts
# Usage Help: bash run_distribute.sh [RANK_TABLE_FILE] [CONFIG_PATH] [DEVICE_RANGE] [RUN_STATUS]
bash run_distribute.sh /path/to/hccl_8p_01234567_127.0.1.1.json ../configs/glm2/run_glm2_6b_lora*.yaml '[0,8]' finetune
# å°†æ­¤å¤„rank_table_fileæ›¿æ¢ä¸ºå®é™…è·¯å¾„
```

> å¤šå¡å¾®è°ƒçš„æ¨¡å‹éœ€è¦åˆå¹¶æƒé‡åæ‰èƒ½è¿›è¡Œå•å¡è¯„ä¼°ã€‚
>
> è®­ç»ƒçš„logæ—¥å¿—è·¯å¾„ï¼šmindformers/output/log
>
> checkpoint(å«ä¼˜åŒ–å™¨å‚æ•°)å­˜å‚¨è·¯å¾„ï¼šmindformers/output/checkpoint
>
> checkpoint(ä¸å«ä¼˜åŒ–å™¨å‚æ•°)å­˜å‚¨è·¯å¾„ï¼šmindformers/output/checkpoint_network
>
> è‹¥æƒ³åˆå¹¶ckptç”¨äºåç»­è¯„ä¼°ï¼Œé€‰æ‹©ä¸å«ä¼˜åŒ–å™¨å‚æ•°çš„æƒé‡å³å¯ã€‚

- å¤šæœºå¤šå¡

åœ¨æ¯å°æœºå™¨ä¸Šå¯åŠ¨`bash run_distribute.sh`ã€‚

```bash
server_count=12
device_num=8*$server_count
# launch ranks in the 0th server
cd scripts
bash run_distribute.sh $RANK_TABLE_FILE path/to/config_lora.yaml [0,8] finetune $device_num

# launch ranks in the 1-11 server via ssh
for idx in {1..11}
do
    let rank_start=8*$idx
    let rank_end=$rank_start+8
    ssh ${IP_LIST[$idx]} "cd scripts; bash run_distribute.sh $RANK_TABLE_FILE path/to/config_lora.yaml [$rank_start,$rank_end] finetune $device_num"
done
```

å…¶ä¸­

- `RANK_TABLE_FILE`ä¸ºä¸Šä¸€æ­¥æ±‡æ€»å¹¶åˆ†å‘çš„æ€»rank tableæ–‡ä»¶ï¼›
- `IP_LIST`ä¸º12å°æœåŠ¡å™¨çš„IPåœ°å€ã€‚å¦‚192.168.0.[0-11]

```bash
IP_LIST=("192.168.0.0", "192.168.0.1", ..., "192.168.0.11")
```

### P-Tuning å¾®è°ƒ

å¯¹äºæ¯ä¸ªä¸‹æ¸¸ä»»åŠ¡ï¼Œåœ¨ç½‘ç»œçš„æ¯ä¸€å±‚æ·»åŠ ä¸€ä»½è¿ç»­æç¤ºå‘é‡ï¼Œå†»ç»“é¢„è®­ç»ƒæ¨¡å‹çš„å…¶ä»–å‚æ•°ï¼Œåªè®­ç»ƒè¿™äº›å‘é‡ã€‚

#### å•å¡å¾®è°ƒ

ä½¿ç”¨P-Tuningç®—æ³•è¿›è¡Œä½å‚å¾®è°ƒæ—¶ï¼Œä½¿ç”¨ `configs/glm2/run_glm2_6b_ptuning2.yaml` é…ç½®æ–‡ä»¶ï¼Œè¯¥é…ç½®æ–‡ä»¶åŒ…å«äº†P-Tuningä½å‚å¾®è°ƒç®—æ³•æ‰€éœ€çš„é…ç½®é¡¹

ä¿®æ”¹æ•°æ®é›†/æ¨¡å‹æƒé‡é…ç½®è·¯å¾„ï¼š

- æ•°æ®é›†ï¼šä¿®æ”¹ `mindformers/configs/glm2/run_glm2_6b_ptuning2.yaml` è„šæœ¬ä¸­`train_dataset` çš„ `dataset_dir` ä¸ºå‰æ–‡ç”Ÿæˆçš„æ•°æ®é›†è·¯å¾„ã€‚
- åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æƒé‡ï¼šä¿®æ”¹ `mindformers/configs/glm2/run_glm2_6b_ptuning2.yaml` è„šæœ¬ä¸­çš„ `load_checkpoint` ä¸ºé¢„è®­ç»ƒæ¨¡å‹æƒé‡è·¯å¾„ã€‚

æ‰§è¡Œå‘½ä»¤ï¼š

```shell
cd scripts
# Usage Help: bash run_stanalone.sh [CONFIG_PATH] [DEVICE_ID] [RUN_STATUS]
bash run_standalone.sh ../configs/glm2/run_glm2_6b_ptuning2.yaml 0 finetune
```

> è®­ç»ƒçš„logæ—¥å¿—è·¯å¾„ï¼šmindformers/output/log
>
> checkpoint(å«ä¼˜åŒ–å™¨å‚æ•°)å­˜å‚¨è·¯å¾„ï¼šmindformers/output/checkpoint
>
> checkpoint(ä¸å«ä¼˜åŒ–å™¨å‚æ•°)å­˜å‚¨è·¯å¾„ï¼šmindformers/output/checkpoint_network
>
> è‹¥æƒ³åˆå¹¶ckptç”¨äºåç»­è¯„ä¼°ï¼Œé€‰æ‹©ä¸å«ä¼˜åŒ–å™¨å‚æ•°çš„æƒé‡å³å¯ã€‚

### è¾¹è®­è¾¹è¯„ä¼°

#### 1. ä½¿ç”¨ `Rouge-1`ã€`Rouge-2` ç­‰æŒ‡æ ‡è¯„æµ‹

ä½¿ç”¨è¯¥æŒ‡æ ‡è¯„æµ‹æ—¶é€Ÿåº¦è¾ƒæ…¢ï¼Œæ¨èä½¿ç”¨ `PerplexityMetric` è¯„æµ‹ã€‚

å°†è®­ç»ƒé…ç½®æ–‡ä»¶çš„ `do_eval: False` è®¾ç½®ä¸º `do_eval: True`ï¼Œå¹¶ä¸”éœ€è¦å°† `train_dataset` å’Œ `eval_dataset` çš„ `max_source_length`ã€`max_target_length` ä»¥åŠ `batch_size`é¡¹è®¾ç½®ä¸ºç›¸åŒå€¼ï¼Œå¹¶ä¸”ä¿æŒ `max_source_length + max_target_length + 1 = seq_length`ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```yaml
do_eval: True
eval_step_interval: 1788
eval_epoch_interval: -1

metric:
  type: ADGENMetric

model:
  model_config:
    seq_length: 192
train_dataset: &train_dataset
  max_source_length: 64
  max_target_length: 127
  batch_size: 8
eval_dataset: &eval_dataset
  max_source_length: 64
  max_target_length: 127
  batch_size: 8
```

#### 2. ä½¿ç”¨ `PerplexityMetric` æŒ‡æ ‡è¯„æµ‹

å°†è®­ç»ƒé…ç½®æ–‡ä»¶çš„ `do_eval: False` è®¾ç½®ä¸º `do_eval: True`ï¼Œå¹¶ä¸”éœ€è¦å°† `train_dataset` å’Œ `eval_dataset` çš„ `max_source_length`ã€`max_target_length` ã€`phase` ä»¥åŠ `batch_size`é¡¹è®¾ç½®ä¸ºç›¸åŒå€¼ï¼Œå¹¶ä¸”ä¿æŒ `max_source_length + max_target_length + 1 = seq_length`ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```yaml
do_eval: True
eval_step_interval: 1788
eval_epoch_interval: -1

metric:
  type: PerplexityMetric

model:
  model_config:
    seq_length: 192
train_dataset: &train_dataset
  data_loader:
    phase: "train"
  max_source_length: 64
  max_target_length: 127
  batch_size: 8
eval_dataset: &eval_dataset
  data_loader:
    phase: "train"
  max_source_length: 64
  max_target_length: 127
  batch_size: 8
```

mindformersé€šè¿‡ `eval_step_interval` å’Œ `eval_epoch_interval` ä¸¤é¡¹é…ç½®å‚æ•°æ¥æ§åˆ¶è¾¹è®­ç»ƒè¾¹è¯„ä¼°çš„æ‰§è¡Œé—´éš”ï¼Œå‚æ•°å«ä¹‰å¦‚ä¸‹ï¼š

- **eval_step_interval**: è¯„ä¼°stepé—´éš”, é»˜è®¤ä¸º100ï¼Œè¡¨ç¤ºæ¯100ä¸ªstepé—´éš”æ‰§è¡Œä¸€æ¬¡è¯„ä¼°ï¼›é…ç½®ä¸ºå¤§äº0çš„æ•°è¡¨ç¤ºæ¯éš”æ‰€é…ç½®çš„stepæ•°åæ‰§è¡Œä¸€æ¬¡è¯„ä¼°ï¼Œé…ç½®ä¸ºå°äº0çš„æ•°åˆ™è¡¨ç¤ºç¦ç”¨stepè¯„ä¼°ï¼›æ³¨æ„ï¼šåœ¨æ•°æ®ä¸‹æ²‰æ¨¡å¼ä¸‹ï¼Œstepé—´éš”å€¼å»ºè®®é…ç½®ä¸ºsink sizeçš„å€æ•°
- **eval_epoch_interval**: è¯„ä¼°epoché—´éš”, é»˜è®¤ä¸º-1ï¼Œè¡¨ç¤ºç¦ç”¨epochç»“æŸæ—¶çš„è¯„ä¼°ï¼›é…ç½®ä¸ºå¤§äº0çš„æ•°è¡¨ç¤ºæ¯éš”æ‰€é…ç½®çš„epochæ•°åæ‰§è¡Œä¸€æ¬¡è¯„ä¼°ï¼Œé…ç½®ä¸ºå°äº0çš„æ•°åˆ™è¡¨ç¤ºç¦ç”¨epochè¯„ä¼°ï¼›æ³¨æ„ï¼šæ•°æ®ä¸‹æ²‰æ¨¡å¼ä¸‹ï¼Œepochæ‰€åŒ…å«çš„stepæ•°å°†ä»æ•°æ®é›†å¤§å°å˜ä¸ºsink sizeçš„å¤§å°ï¼Œå°†åœ¨ `sink_size * eval_epoch_interval` ä¸ªstepåæ‰§è¡Œä¸€æ¬¡è¯„ä¼°

## è¯„æµ‹

### æ–‡æœ¬ç”Ÿæˆ

### æ•°æ®é›†å‡†å¤‡-æ–‡æœ¬ç”Ÿæˆ

è§å¾®è°ƒç« èŠ‚çš„[æ•°æ®é›†å‡†å¤‡](#æ•°æ®é›†å‡†å¤‡)

è¯„æµ‹æ—¶æ¨¡å‹`seq_length`éœ€è¦ç­‰äºè¯„æµ‹æ•°æ®é›†çš„`max_source_length`å’Œ`max_target_length`ã€‚å› æ­¤ä¿®æ”¹yamlä¸­æ¨¡å‹`seq_length`ä¸º256ï¼š

```yaml
model:
  model_config:
    seq_length: 256
```

### å•å¡è¯„æµ‹

ä½¿ç”¨å…¨å‚å¾®è°ƒæƒé‡æ—¶ï¼Œå¯åŠ¨å¦‚ä¸‹shellè„šæœ¬ï¼Œæ‰§è¡Œå•å¡è¯„ä¼°

é…ç½®æ–‡ä»¶é€‰æ‹© `configs/glm2/run_glm2_6b_finetune_eval.yaml` glm2æ¨¡å‹æ¨ç†é…ç½®ï¼Œä¿®æ”¹å…¶ä¸­`model`å­—æ®µä¸‹`model_config`ä¸­`use_past: True`å¼€å¯å¢é‡æ¨ç†ä½¿è¯„ä¼°é€Ÿåº¦æ›´å¿«

```bash
python run_mindformer.py --config configs/glm2/run_glm2_6b_finetune_eval.yaml--run_mode eval --load_checkpoint /path/to/glm2_6b_finetune.ckpt --device_id 0 --use_parallel False
```

ä½¿ç”¨LoRAä½å‚å¾®è°ƒæƒé‡æ—¶ï¼Œå¯åŠ¨å¦‚ä¸‹shellè„šæœ¬ï¼Œæ‰§è¡Œå•å¡è¯„ä¼°

é…ç½®æ–‡ä»¶é€‰æ‹© `configs/glm2/run_glm2_6b_lora_eval.yaml` glm2_loraæ¨¡å‹æ¨ç†é…ç½®ï¼Œæ­¤é…ç½®å¯ç”¨äºloraæ¨¡å‹ï¼Œä¿®æ”¹å…¶ä¸­`model`å­—æ®µä¸‹`model_config`ä¸­`use_past: True`å¼€å¯å¢é‡æ¨ç†ä½¿è¯„ä¼°é€Ÿåº¦æ›´å¿«

```bash
python run_mindformer.py --config configs/glm2/run_glm2_6b_lora_eval.yaml --run_mode eval --load_checkpoint /path/to/glm2_6b_lora.ckpt --device_id 0 --use_parallel False
```

> å•å¡è¯„æµ‹æ—¶ï¼Œåº”å°†yamlä¸­ model:model_config:batch_size ä¿®æ”¹ä¸ºç­‰äº runner_config:batch_size

### å¤šå¡è¯„æµ‹

æ‰§è¡Œè„šæœ¬ï¼š

```bash
cd scripts
bash run_distribute.sh /path/to/hccl_8p_01234567_127.0.1.1.json ../configs/glm2/run_glm2_6b_*_eval.yaml '[0,8]' eval
```

> å…¨å‚å¾®è°ƒè¯·é€‰æ‹© `configs/glm2/run_glm2_6b_finetune_eval.yaml`
> loraå¾®è°ƒè¯·é€‰æ‹© `configs/glm2/run_glm2_6b_lora_eval.yaml`
> å¤šå¡è¯„æµ‹æ—¶ï¼Œåº”å°†yamlä¸­ model:model_config:batch_size ä¿®æ”¹ä¸ºç­‰äº global_batch_sizeã€‚ä¾‹å¦‚ bs8/dp4/mp2çš„é…ç½®, batch_size = 8 * 4 = 32

## æ¨ç†

### åŸºäºgenerateçš„æ¨ç†

ä¸‹é¢æä¾›ä¸€ä¸ªæ¨¡å‹æ¨ç†æ ·ä¾‹è„šæœ¬ `infer.py`

```python
from mindformers import AutoConfig, AutoModel, AutoTokenizer, ChatGLM2Tokenizer
import mindspore as ms

ms.set_context(mode=ms.GRAPH_MODE, device_target="Ascend", device_id=0)

# **æ³¨æ„** LoRAå¾®è°ƒæ¨¡å‹æ›¿æ¢æˆ â€œglm2_6b_loraâ€,
# **æ³¨æ„** P-Tuning å¾®è°ƒæ¨¡å‹æ›¿æ¢æˆ â€œglm2_6b_ptuning2â€
config = AutoConfig.from_pretrained("glm2_6b")
# å¯ä»¥åœ¨æ­¤ä½¿ç”¨ä¸‹è¡Œä»£ç æŒ‡å®šè‡ªå®šä¹‰æƒé‡è¿›è¡Œæ¨ç†ï¼Œé»˜è®¤ä½¿ç”¨è‡ªåŠ¨ä»obsä¸Šä¸‹è½½çš„é¢„è®­ç»ƒæƒé‡
# config.checkpoint_name_or_path = "/path/to/your/chatglm2_6b.ckpt"
config.use_past = True
config.seq_length = 1024
model = AutoModel.from_config(config)

# ä»¥ä¸‹ä¸¤ç§tokenizerå®ä¾‹åŒ–æ–¹å¼é€‰å…¶ä¸€å³å¯
# 1. åœ¨çº¿åŠ è½½æ–¹å¼
tokenizer = AutoTokenizer.from_pretrained("glm2_6b")
# 2. æœ¬åœ°åŠ è½½æ–¹å¼
# tokenizer = ChatGLM2Tokenizer("/path/to/your/tokenizer.model")

kwargs={}
gen_kwargs = {"max_length": config.seq_length, "num_beams": 1, "do_sample": False, "top_p": 3,"top_k": 0.7,
              "temperature": 1, **kwargs}

queries = ["ä½ å¥½", "è¯·ä»‹ç»ä¸€ä¸‹æ­å·", "é‚£é‡Œæœ‰ä»€ä¹ˆå¥½åƒçš„å—"]
history = []
for query in queries:
    # å¦‚æœæƒ³å…³é—­historyï¼Œæ­¤å¤„ä¼ å…¥ `history=[]` å³å¯
    prompt = tokenizer.build_prompt(query, history=history)
    input_id = tokenizer(prompt)["input_ids"]

    output = model.generate([input_id], **gen_kwargs)

    # output åŒ…æ‹¬äº†[input_id, output]ä¸¤ä¸ªéƒ¨åˆ†
    output = output[0][len(input_id):]
    response = tokenizer.decode(output)
    print(response)
    history += [(query, response)]

    '''
    response1:
    ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚

    response2:
    æ­å·æ˜¯ä¸­å›½æµ™æ±Ÿçœçœä¼šï¼Œä½äºæµ™æ±Ÿçœä¸œå—éƒ¨ï¼Œåœ°å¤„æµ™æ±ŸçœåŒ—éƒ¨ï¼Œä¸œä¸´ä¸œæµ·ï¼Œå—æ¥ç¦å»ºçœï¼ŒåŒ—ä¸æ±Ÿè‹çœæ¯—é‚»ï¼Œæ˜¯ä¸­å›½è‘—åçš„æ—…æ¸¸åŸå¸‚ä¹‹ä¸€ã€‚

    æ­å·æœ‰ç€æ‚ ä¹…çš„å†å²å’Œæ–‡åŒ–ï¼Œè¢«èª‰ä¸ºâ€œäººé—´å¤©å ‚â€ï¼Œè¢«èª‰ä¸ºâ€œå—å®‹éƒ½åŸâ€ï¼Œæ˜¯ä¸­å›½å—æ–¹è‘—åçš„å†å²æ–‡åŒ–ååŸä¹‹ä¸€ã€‚æ­å·è¿˜è¢«èª‰ä¸ºâ€œå…¨å›½æœ€å…·å¹¸ç¦æ„ŸåŸå¸‚â€ï¼Œå…·æœ‰ä¸°å¯Œçš„å†å²é—å­˜ã€ä¼˜ç¾çš„è‡ªç„¶é£å…‰å’Œæµ“éƒçš„æ–‡åŒ–æ°›å›´ã€‚

    æ­å·çš„ç»æµä»¥æœåŠ¡ä¸šä¸ºä¸»å¯¼äº§ä¸šï¼Œç‰¹åˆ«æ˜¯äº¤é€šè¿è¾“ã€ä»“å‚¨å’Œé‚®æ”¿ä¸šã€‚åŒæ—¶ï¼Œæ­å·ä¹Ÿæ˜¯ä¸­å›½é‡è¦çš„ç”µå­å•†åŠ¡å’Œäº’è”ç½‘äº§ä¸šåŸºåœ°ä¹‹ä¸€ï¼Œè¢«èª‰ä¸ºâ€œä¸­å›½ç”µå­å•†åŠ¡ä¹‹éƒ½â€ã€‚

    æ­å·çš„è‘—åæ™¯ç‚¹åŒ…æ‹¬è¥¿æ¹–ã€çµéšå¯ºã€åƒå²›æ¹–ã€é’±å¡˜æ±Ÿç­‰ã€‚è¥¿æ¹–æ˜¯ä¸­å›½è‘—åçš„é£æ™¯åèƒœåŒºä¹‹ä¸€ï¼Œè¢«èª‰ä¸ºâ€œäººé—´å¤©å ‚â€ï¼Œçµéšå¯ºæ˜¯ä¸­å›½è‘—åçš„ä½›æ•™å¯ºåº™ä¹‹ä¸€ï¼Œåƒå²›æ¹–å’Œé’±å¡˜æ±Ÿæ˜¯ä¸­å›½è‘—åçš„è‡ªç„¶é£æ™¯åŒºä¹‹ä¸€ã€‚

    æ­å·è¿˜æ‹¥æœ‰ä¸°å¯Œçš„äººæ–‡èµ„æºï¼Œè¢«èª‰ä¸ºâ€œäººé—´å¤©å ‚â€çš„æ­å·è¥¿æ¹–ã€çµéšå¯ºã€åƒå²›æ¹–ã€é’±å¡˜æ±Ÿç­‰æ™¯ç‚¹ï¼Œä»¥åŠå®‹åŸã€å—å®‹å¾¡è¡—ç­‰å†å²æ–‡åŒ–æ™¯ç‚¹ï¼Œéƒ½æ˜¯æ¸¸å®¢å‰æ¥æ­å·æ—…æ¸¸çš„çƒ­é—¨æ™¯ç‚¹ã€‚

    response3:
    æ­å·æ˜¯ä¸­å›½è‘—åçš„ç¾é£ŸåŸå¸‚ä¹‹ä¸€ï¼Œæœ‰è®¸å¤šç‰¹è‰²ç¾é£Ÿå’Œä¼ ç»Ÿèœè‚´ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›æ­å·çš„è‘—åç¾é£Ÿ:

    1. è¥¿æ¹–é†‹é±¼ï¼šè¿™æ˜¯æ­å·æœ€è‘—åçš„èœè‚´ä¹‹ä¸€ï¼Œé±¼è‚‰é²œç¾ï¼Œå…¥å£å³åŒ–ï¼Œä½ä»¥é¦™é†‹ã€ç³–ã€å§œä¸ç­‰è°ƒæ–™ï¼Œå£æ„Ÿé…¸ç”œé€‚ä¸­ã€‚

    2. é¾™äº•è™¾ä»ï¼šä»¥å½“åœ°ç‰¹äº§çš„é¾™äº•èŒ¶ä¸ºä½æ–™ï¼Œå°†é²œå«©çš„è™¾ä»ç‚’åˆ¶è€Œæˆï¼Œé¦™æ°”æ‰‘é¼»ï¼Œé²œå«©å¯å£ã€‚

    3. çŒæ±¤åŒ…ï¼šåˆç§°å°ç¬¼åŒ…ï¼Œæ˜¯æ­å·çš„ä¼ ç»Ÿç‚¹å¿ƒä¹‹ä¸€ã€‚åŒ…å­çš„çš®è–„é¦…å¤šï¼Œæ±¤æ±é²œç¾ï¼Œéå¸¸å—æ¬¢è¿ã€‚

    4. å§œæ¯é¸­ï¼šè¿™æ˜¯ä¸€é“æ­å¸®èœï¼Œä»¥é¸­è‚‰ã€å§œæ¯ã€è‘±ç­‰è°ƒæ–™çƒ¹åˆ¶è€Œæˆï¼Œå£æ„Ÿé²œç¾ã€‚

    5. è€å­—å·å°åƒï¼šæ­å·è¿˜æœ‰å¾ˆå¤šè€å­—å·å°åƒåº—ï¼Œå¦‚èƒ¡åŒå£çƒ¤è‚‰ä¸²ã€å­”åºœå®¶å®´ã€å®‹å«‚é±¼ç¾¹ç­‰ï¼Œæ˜¯å½“åœ°å±…æ°‘å’Œæ¸¸å®¢çš„ç¾é£Ÿé€‰æ‹©ã€‚

    æ­¤å¤–ï¼Œæ­å·è¿˜æœ‰è®¸å¤šç‰¹è‰²å°åƒï¼Œå¦‚ç²½å­ã€è‡­è±†è…ã€ç³¯ç±³é¸¡ã€è‚‰å¤¹é¦ã€é¸­è¡€ç²‰ä¸æ±¤ç­‰ï¼Œè®©äººå‚æ¶æ¬²æ»´ã€‚
    '''
```

### è„šæœ¬å¯åŠ¨

> GLM2ä½¿ç”¨è„šæœ¬è¿›è¡Œæ¨ç†æ—¶éœ€è¦æ‰‹åŠ¨å¯¹è¾“å…¥é—®é¢˜æ·»åŠ promptï¼Œpromptæ¨¡æ¿çš„å½¢å¼ä¸º`[Round 1]\n\né—®ï¼š{æ­¤å¤„å¡«å†™é—®é¢˜}\n\nç­”ï¼š`ã€‚
>
> å¦‚æœé—®é¢˜æ˜¯`ä¸ºä»€ä¹ˆè¯´åœ°çƒæ˜¯ç‹¬ä¸€æ— äºŒçš„`ï¼Œæ·»åŠ promptåä¸º`[Round 1]\n\né—®ï¼šä¸ºä»€ä¹ˆè¯´åœ°çƒæ˜¯ç‹¬ä¸€æ— äºŒçš„\n\nç­”ï¼š`ã€‚

#### å•å¡æ¨ç†

```bash
python run_mindformer.py --config path/to/config.yaml --run_mode predict --predict_data "[Round 1]\n\né—®ï¼šä½ å¥½\n\nç­”ï¼š"
#  [{'text_generation_text': ['[Round 1]\n\né—®ï¼šä½ å¥½\n\nç­”ï¼š ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚']}]
```

#### å¤šå¡æ¨ç†

æš‚æœªæ”¯æŒ

## Mindspore-Lite æ¨ç†åŠé‡åŒ–

### åŸºæœ¬ä»‹ç»

ã€€ã€€MindFormers å®šä½æ‰“é€ è®­ç»ƒ->å¾®è°ƒ->éƒ¨ç½²çš„ç«¯åˆ°ç«¯å¤§æ¨¡å‹å·¥å…·å¥—ä»¶ï¼Œä¸ºäº†æ›´å¥½æ€§èƒ½åœ°éƒ¨ç½²å·²ç»å¾®è°ƒè®­ç»ƒå¥½çš„å¤§æ¨¡å‹ï¼Œæˆ‘ä»¬åˆ©ç”¨MindSporeæ‰“é€ çš„æ¨ç†å¼•æ“ [MindSpore_lite](https://gitee.com/link?target=https%3A%2F%2Fwww.mindspore.cn%2Flite)ï¼Œä¸ºç”¨æˆ·æä¾›äº†å¼€ç®±å³ç”¨çš„æ¨ç†éƒ¨ç½²æ–¹æ¡ˆï¼Œä¸ºç”¨æˆ·æä¾›ç«¯åˆ°ç«¯çš„å¤§æ¨¡å‹è§£å†³æ–¹æ¡ˆï¼Œå¸®åŠ©ç”¨æˆ·ä½¿èƒ½å¤§æ¨¡å‹ä¸šåŠ¡ã€‚

ã€€ã€€Lite æ¨ç†å¤§è‡´åˆ†ä¸¤æ­¥ï¼šæƒé‡è½¬æ¢å¯¼å‡º MindIR -> Lite æ¨ç†ï¼Œæ¥ä¸‹æ¥åˆ†åˆ«æè¿°ä¸Šè¿°ä¸¤ä¸ªè¿‡ç¨‹ã€‚

### MindIR å¯¼å‡º

ã€€ã€€1. ä¿®æ”¹æ¨¡å‹ç›¸å…³çš„é…ç½®æ–‡ä»¶ configs/glm2/export_glm2_6b.yamlï¼Œå…¶ä¸­éœ€è¦å…³æ³¨è¿™å‡ é¡¹ï¼š

```yaml
# export
infer:
    prefill_model_path: "glm2_export/glm2_6b_prefill_seq512.mindir" # ä¿å­˜mindirçš„ä½ç½®
    increment_model_path: "glm2_export/glm2_6b_inc_seq512.mindir"   # ä¿å­˜mindirçš„ä½ç½®
    infer_seq_length: 512 # éœ€è¦ä¿æŒè·Ÿ model-model_config-seq_length ä¸€è‡´

# ==== model config ====
model:
  model_config:
    seq_length: 512
    checkpoint_name_or_path: "/path/to/your/*.ckpt"
```

2. æ‰§è¡Œexport.pyï¼Œå®Œæˆæ¨¡å‹è½¬æ¢

```bash
python mindformers/tools/export.py --config_path configs/glm2/export_glm2_6b.yaml
```

### int8 é‡åŒ–ï¼ˆå¯é€‰ï¼‰

ã€€ã€€int8 é‡åŒ–å…·æœ‰æ¨ç†æé€Ÿä½œç”¨ï¼Œæ˜¯ä¸€ä¸ªéå¿…é€‰é¡¹ï¼Œå¯ä»¥å¸¦æ¥ 7~10% çš„æ€§èƒ½å¢ç›Šã€‚ä»…æ”¯æŒ Ascend åç«¯ï¼Œç›®å‰ä»…åœ¨ Mindspore2.2/Atlas 800T A2 æµ‹è¯•é€šè¿‡ã€‚è¯¦è§ [Ascend ON_THE_FLYé‡åŒ–](https://www.mindspore.cn/lite/docs/zh-CN/master/use/post_training_quantization.html#ascend-on-the-fly%E9%87%8F%E5%8C%96)ã€‚

1. ä¸‹è½½è½¬æ¢å·¥å…·ï¼Œ[å–åŒ…åœ°å€](https://repo.mindspore.cn/mindspore/mindspore/)ï¼Œè·¯å¾„ lite/linux\_aarch64/cloud\_fusion/python**/*.tar.gz
2. è§£å‹åˆ°ä»»æ„è·¯å¾„ tar -xvzf *.tar.gz
3. ä¿®æ”¹è„šæœ¬ convert.sh:

    ```bash
    PACKAGE_ROOT_PATH=${PWD}/mindspore-lite-2.2.0.20230926-linux-aarch64 # ä¿®æ”¹ä¸ºä½ çš„è·¯å¾„

    export LD_LIBRARY_PATH=${PACKAGE_ROOT_PATH}/runtime/lib:${PACKAGE_ROOT_PATH}/tools/converter/lib:${PACKAGE_ROOT_PATH}/runtime/third_party/dnnl/:${LD_LIBRARY_PATH}

    # MindSporeæ˜‡è…¾åç«¯æŒ‡å®šgeæ¨¡å¼
    export ASCEND_BACK_POLICY="ge"
    export ASCEND_DEVICE_ID=0

    in_model="/path/to/your/mindir"
    out_model="${in_model}.int8"

    ${PACKAGE_ROOT_PATH}/tools/converter/converter/converter_lite --fmk=MINDIR --modelFile=${in_model}  --outputFile=${out_model} --optimize=ascend_oriented:910b --configFile=ascend_on_the_fly_quant.cfg # for Atlas 800T A2
    ```

4. åˆ†åˆ«å¯¹ `prefill_model`â€‹ å’Œ `increment_model`â€‹ æ‰§è¡Œè½¬æ¢

### æ‰§è¡Œæ¨ç†

1. æ–°å»ºæ¨ç†é…ç½®æ–‡ä»¶ï¼šlite.ini

    ```ini
    [ascend_context]
    provider=ge

    [ge_session_options]
    ge.exec.formatMode=1
    ge.exec.precision_mode=must_keep_origin_dtype
    ```

2. æ‰§è¡Œå‘½ä»¤ï¼š

```bash
python run_infer_main.py --device_id 0 --model_name glm2_6b --prefill_model_path glm2_export/glm2_6b_prefill_seq512_graph.mindir --increment_model_path glm2_export/glm2_6b_inc_seq512_graph.mindir --config_path lite.ini --is_sample_acceleration False --seq_length 512 --add_special_tokens True
```

> æ³¨ï¼šå¦‚æœæ˜¯int8é‡åŒ–åæ¨ç†ï¼Œå°† `prefill_model_path`â€‹ å’Œ `increment_model_path`â€‹ ä¿®æ”¹ä¸º int8 é‡åŒ–åçš„ MindIR å³å¯ã€‚

ã€€ã€€ç­‰å¾…æ¨¡å‹è½½å…¥ã€ç¼–è¯‘åï¼Œå‡ºç°ï¼š

```bash
Please enter your predict data:
```

ã€€ã€€è¾“å…¥ï¼š

```bash
[Round 1]

é—®ï¼šä½ å¥½ã€‚

ç­”ï¼š
```

ã€€ã€€è¾“å‡ºï¼š

```bash
['[Round 1]\n\né—®ï¼šä½ å¥½ã€‚\n\nç­”ï¼š ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚']
```

## Q & A

### Q1: ç½‘ç»œè®­ç»ƒ loss ä¸ä¸‹é™ã€ç½‘ç»œè®­ç»ƒæº¢å‡ºã€`overflow_cond=True` æ€ä¹ˆåŠï¼Ÿ

A1: æ‰§è¡Œè®­ç»ƒå‰è®¾ç½®ç¯å¢ƒå˜é‡ï¼š

```bash
export MS_ASCEND_CHECK_OVERFLOW_MODE="INFNAN_MODE"
```

é‡æ–°å¯åŠ¨è®­ç»ƒã€‚

### Q2: æ¨ç†é€Ÿåº¦éå¸¸æ…¢ã€Mindsporeåªèƒ½è·‘åœ¨CPUä¸Šã€æŠ¥é”™ä¸­å«æœ‰ `te`ã€`tbe`ã€`tvm`ç­‰å­—æ ·ï¼Ÿ

A2: ä¸€èˆ¬æ˜¯ Mindspore + Ascend ç¯å¢ƒå®‰è£…é—®é¢˜ï¼Œç¡®è®¤ç¯å¢ƒå®‰è£…è¿‡ç¨‹å‚ç…§
[å®‰è£…æŒ‡å—](https://www.mindspore.cn/install/#%E6%89%8B%E5%8A%A8%E5%AE%89%E8%A3%85)å¹¶ä¸”æˆåŠŸè®¾ç½®äº†ç¯å¢ƒå˜é‡ã€‚æ‰§è¡Œï¼š

```python
python -c "import mindspore;mindspore.set_context(device_target='Ascend');mindspore.run_check()"
```

å‡å¦‚æ‰§è¡Œè¾“å‡ºï¼š

```bash
MindSpore version: ç‰ˆæœ¬å·
The result of multiplication calculation is correct, MindSpore has been installed on platform [Ascend] successfully!
```

å¹¶ä¸”æ²¡æœ‰æŠ¥é”™ï¼Œåˆ™è¯´æ˜æˆåŠŸå®‰è£…äº†ç¯å¢ƒã€‚

æˆ–è®¸ä½ æƒ³é—®ï¼Œæœ‰æ²¡æœ‰æ›´æ–¹ä¾¿çš„ç¯å¢ƒå®‰è£…æ–¹å¼ï¼Ÿæ­å–œä½ ï¼Œæœ‰çš„ï¼Œæˆ‘ä»¬è¿˜æä¾›ç°æˆçš„
[dockeré•œåƒ](http://mirrors.cn-central-221.ovaijisuan.com/mirrors.html)ï¼Œå¯ä»¥ä¾æ®éœ€æ±‚è‡ªè¡Œå–ç”¨ã€‚

### Q3: Sync stream Failedã€exec graph xxx failedï¼Ÿ

A3:è¿™ç±»æŠ¥é”™è¾ƒä¸ºå®½æ³›ï¼Œå¯ä»¥æ‰“å¼€æ˜‡è…¾hostæ—¥å¿—è¿›ä¸€æ­¥å®šä½ã€‚

```bash
export ASCEND_GLOBAL_EVENT_ENABLE=0
export ASCEND_GLOBAL_LOG_LEVEL=2
export ASCEND_SLOG_PRINT_TO_STDOUT=1
```

æ‰“å¼€æ˜‡è…¾hostæ—¥å¿—åæ¨¡å‹æ€§èƒ½å°†æ˜æ˜¾ä¸‹é™ï¼Œå®šä½é—®é¢˜ç»“æŸåéœ€è¦å–æ¶ˆæ˜‡è…¾æ—¥å¿—ï¼š

```bash
unset ASCEND_GLOBAL_EVENT_ENABLE ASCEND_GLOBAL_LOG_LEVEL ASCEND_SLOG_PRINT_TO_STDOUT
```

### Q4: the strategy is xxxxxx, shape xxxx cannot be divisible by value x

A4: æ£€æŸ¥æ¨¡å‹å¥é•¿æ˜¯å¦æ»¡è¶³ `max_source_length + max_target_length + 1 = seq_length` çš„è¦æ±‚ã€‚

### ä»ç„¶æœ‰ç–‘é—®ï¼Ÿæ¬¢è¿å‘æˆ‘ä»¬æå‡ºissueï¼Œæˆ‘ä»¬å°†å°½å¿«ä¸ºæ‚¨è§£å†³

æé—®æ—¶éº»çƒ¦æä¾›ä»¥ä¸‹ä¿¡æ¯ï¼š

1. æ‰§è¡Œå‘½ä»¤
2. è¿è¡Œç¯å¢ƒï¼ŒåŒ…æ‹¬ç¡¬ä»¶ç‰ˆæœ¬ã€CANNç‰ˆæœ¬ã€Mindsporeç‰ˆæœ¬ã€Mindformersç‰ˆæœ¬
3. æŠ¥é”™å®Œæ•´æ—¥å¿—
