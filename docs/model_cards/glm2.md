# ChatGLM2

## æ¨¡å‹æè¿°

ChatGLM**2**-6B æ˜¯å¼€æºä¸­è‹±åŒè¯­å¯¹è¯æ¨¡å‹ [ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B) çš„ç¬¬äºŒä»£ç‰ˆæœ¬ï¼Œåœ¨ä¿ç•™äº†åˆä»£æ¨¡å‹å¯¹è¯æµç•…ã€éƒ¨ç½²é—¨æ§›è¾ƒä½ç­‰ä¼—å¤šä¼˜ç§€ç‰¹æ€§çš„åŸºç¡€ä¹‹ä¸Šï¼ŒChatGLM**2**-6Bå¼•å…¥äº†æ–°ç‰¹å¾ï¼š**æ›´å¼ºå¤§çš„æ€§èƒ½**ã€**æ›´é•¿çš„ä¸Šä¸‹æ–‡**ã€**æ›´é«˜æ•ˆçš„æ¨ç†**ã€**æ›´å¼€æ”¾çš„åè®®**ã€‚

```text
@article{zeng2022glm,
  title={Glm-130b: An open bilingual pre-trained model},
  author={Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and others},
  journal={arXiv preprint arXiv:2210.02414},
  year={2022}
}
```

## æ¨¡å‹æ€§èƒ½

- åŸºäºAtlas 800

**GLM2_6b**:

| config                                                           | task            | Datasets | metric                                  | phase                   | score                                  | performance                                    |
|------------------------------------------------------------------|-----------------|----------|-----------------------------------------|-------------------------|----------------------------------------|------------------------------------------------|
| [glm2_6b](../../configs/glm2/finetune_glm2_6b_fp16.yaml)         | text_generation | ADGEN    | -                                       | [finetune](#å¾®è°ƒ)         | -                                      | 815.2059134 tokens/s/p                         |
| [glm2_6b_lora](../../configs/glm2/lora_glm2_6b_fp16.yaml)        | text_generation | ADGEN    | -                                       | [finetune](#loraå¾®è°ƒ)     | -                                      | 3243.697479 tokens/s/p                         |
| [glm2_6b_ptuning2](../../configs/glm2/run_glm2_6b_ptuning2.yaml) | text_generation | ADGEN    | -                                       | [finetune](#P-Tuningå¾®è°ƒ) | -                                      | 4150.537634 tokens/s/p                         |
| [glm2_6b](../../configs/glm2/run_glm2_6b.yaml)                   | text_generation | ADGEN    | rouge-1<br>rouge-2<br>rouge-l<br>bleu-4 | [eval](#è¯„æµ‹)             | 30.7842<br>7.0734<br>24.7739<br>7.4661 | -                                              |
| [glm2_6b_lora](../../configs/glm2/run_glm2_6b_lora_eval.yaml)    | text_generation | ADGEN    | rouge-1<br>rouge-2<br>rouge-l<br>bleu-4 | [eval](#è¯„æµ‹)             | 31.0563<br>7.1753<br>24.2296<br>7.2294 | -                                              |
| [glm2_6b_ptuning2](../../configs/glm2/run_glm2_6b_ptuning2.yaml) | text_generation | ADGEN    | rouge-1<br>rouge-2<br>rouge-l<br>bleu-4 | [eval](#è¯„æµ‹)             | 31.5933<br>7.4504<br>24.7071<br>7.3042 | -                                              |
| [glm2_6b](../../configs/glm2/predict_glm2_6b.yaml)               | text_generation | -        | -                                       | [predict](#æ¨ç†)          | -                                      | 32.08 tokens/s (use_past=True, seq_length=512) |

## ä»“åº“ä»‹ç»

`chatGLM2-6B` åŸºäº `mindformers` å®ç°ï¼Œä¸»è¦æ¶‰åŠçš„æ–‡ä»¶æœ‰ï¼š

1. æ¨¡å‹å…·ä½“å®ç°ï¼š`mindformers/models/glm2`

    ```text
    glm2
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ glm2.py                  # æ¨¡å‹å®ç°
        â”œâ”€â”€ glm2_config.py           # æ¨¡å‹é…ç½®é¡¹
        â”œâ”€â”€ glm2_modules.py          # æ¨¡ç»„å®ç°
        â”œâ”€â”€ glm2_tokenizer.py        # tokenizer
        â””â”€â”€ glm2_transformer.py      # transformerå±‚å®ç°
    ```

2. æ¨¡å‹é…ç½®ï¼š`configs/glm2`

    ```bash
    configs/glm2
      â”œâ”€â”€ export_glm2_6b.yaml
      â”œâ”€â”€ run_glm2_6b.yaml
      â”œâ”€â”€ run_glm2_6b_finetune_2k_800T_A2_64G.yaml  # Atlas 800T A2 æœ€ä½³æ€§èƒ½å…¨é‡å¾®è°ƒå¯åŠ¨é…ç½®
      â”œâ”€â”€ run_glm2_6b_finetune_2k_800_32G.yaml      # Atlas 800 æœ€ä½³æ€§èƒ½å…¨é‡å¾®è°ƒå¯åŠ¨é…ç½®
      â”œâ”€â”€ run_glm2_6b_finetune_800T_A2_64G.yaml     # Atlas 800T A2 ADGENå…¨é‡å¾®è°ƒå¯åŠ¨é…ç½®
      â”œâ”€â”€ run_glm2_6b_finetune_800_32G.yaml         # Atlas 800 ADGENå…¨é‡å¾®è°ƒå¯åŠ¨é…ç½®
      â”œâ”€â”€ run_glm2_6b_finetune_eval.yaml            # å…¨é‡å¾®è°ƒåè¯„ä¼°é…ç½®
      â”œâ”€â”€ run_glm2_6b_lora_2k_800T_A2_64G.yaml      # Atlas 800T A2æœ€ä½³æ€§èƒ½ loraå¾®è°ƒå¯åŠ¨é…ç½®
      â”œâ”€â”€ run_glm2_6b_lora_2k_800_32G.yaml          # Atlas 800 æœ€ä½³æ€§èƒ½ loraå¾®è°ƒå¯åŠ¨é…ç½®
      â”œâ”€â”€ run_glm2_6b_lora_800T_A2_64G.yaml         # Atlas 800T A2 ADGEN loraå¾®è°ƒå¯åŠ¨é…ç½®
      â”œâ”€â”€ run_glm2_6b_lora_800_32G.yaml             # Atlas 800 ADGEN loraå¾®è°ƒå¯åŠ¨é…ç½®
      â”œâ”€â”€ run_glm2_6b_lora_eval.yaml                # loraå¾®è°ƒè¯„ä¼°é…ç½®
      â””â”€â”€ run_glm2_6b_ptuning2.yaml                 # Atlas 800 ADGEN ptuningå¾®è°ƒå¯åŠ¨é…ç½®
    ```

## å‰æœŸå‡†å¤‡

### ç¯å¢ƒè¦æ±‚

**MindFormerså®‰è£…**ä»¥åŠ**è½¯ç¡¬ä»¶é…å¥—å…³ç³»**å‚è€ƒ[MindFormerså®‰è£…](../../README.md#äºŒMindFormerså®‰è£…)å’Œ[ç‰ˆæœ¬åŒ¹é…å…³ç³»](../../README.md#ä¸‰ç‰ˆæœ¬åŒ¹é…å…³ç³»)ã€‚

### æ¨¡å‹æƒé‡ä¸‹è½½ä¸è½¬æ¢

å¼€å‘è€…å¯ä»¥ä¸‹è½½è·å–å®˜æ–¹æƒé‡åï¼Œé€šè¿‡ä¸‹é¢æä¾›çš„**æƒé‡è½¬æ¢è„šæœ¬**ï¼Œå°†å®˜æ–¹æƒé‡è½¬æ¢ä¸ºMindSporeæƒé‡ï¼›æˆ–ç›´æ¥ä½¿ç”¨MindFormersæä¾›çš„**å·²è½¬æ¢æƒé‡**

1. ä½¿ç”¨å®˜æ–¹æƒé‡è¿›è¡Œè½¬æ¢

   å…‹éš†glm2-6bä»£ç ä»“ï¼Œä¸‹è½½åˆ†å¸ƒå¼çš„æ¨¡å‹æ–‡ä»¶ã€‚

   ```shell
   git lfs install
   git clone https://huggingface.co/THUDM/chatglm2-6b
   ```

   æ‰§è¡Œ python è„šæœ¬ï¼Œåˆå¹¶æ¨¡å‹æƒé‡ã€‚

   ```python
   from transformers import AutoTokenizer, AutoModel
   import torch

   tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True)
   model = AutoModel.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True)

   with open("pt_model_arch.txt", "w") as fp:
       print(model, file=fp, flush=True)
   with open("pt_ckpt.txt", "w") as fp:
       for name, param in model.named_parameters():
           fp.write(f"{name} {param.shape} {param.dtype}\n")
   torch.save(model.state_dict(), "glm2_6b.pth")
   ```

   æ‰§è¡Œè½¬æ¢è„šæœ¬ï¼Œå¾—åˆ°è½¬æ¢åçš„è¾“å‡ºæ–‡ä»¶`glm2_6b.ckpt`ã€‚

   ```python
   import mindspore as ms
   import torch as pt
   from tqdm import tqdm

   pt_ckpt_path = "glm2_6b.pth"
   pt_param = pt.load(pt_ckpt_path)

   type_map = {"torch.float16": "ms.float16",
               "torch.float32": "ms.float32"}
   ms_param = []
   with open("check_pt_ckpt.txt", "w") as fp:
       for k, v in tqdm(pt_param.items()):
           if "word_embeddings.weight" in k:
               k = k.replace("word_embeddings.weight", "embedding_table")
           fp.write(f"{k} {v.shape} {v.dtype}\n")
           ms_param.append({"name": k, "data": ms.Tensor(v.numpy())})

   ms.save_checkpoint(ms_param, "glm2_6b.ckpt")
   ```

2. è·å–MindFormersæä¾›çš„å·²è½¬æ¢æƒé‡

   å¯é€šè¿‡from_pretrainedæ¥å£ä¸‹è½½ï¼Œä¹Ÿå¯ç›´æ¥ä»ä¸‹é¢çš„é“¾æ¥è·å–

   [glm2_6bæƒé‡](https://ascend-repo-modelzoo.obs.cn-east-2.myhuaweicloud.com/XFormer_for_mindspore/glm2/glm2_6b.ckpt)

   [tokenizeræ–‡ä»¶](https://ascend-repo-modelzoo.obs.cn-east-2.myhuaweicloud.com/XFormer_for_mindspore/glm2/tokenizer.model)

### [åˆ†å¸ƒå¼è®­ç»ƒ/å¾®è°ƒæƒé‡åˆå¹¶](../feature_cards/Transform_Ckpt.md)

åˆ†å¸ƒå¼è®­ç»ƒ/å¾®è°ƒåæ‰€å¾—åˆ°çš„æƒé‡æ–‡ä»¶ä¸ºæ ¹æ®ç­–ç•¥åˆ‡åˆ†åçš„æƒé‡ï¼Œéœ€è¦æ‰‹åŠ¨å°†åˆ‡åˆ†æƒé‡åˆä¸€ï¼Œä»¥ç”¨äºè¯„ä¼°å’Œæ¨ç†ã€‚

æ¶‰åŠåˆ°ckptçš„å•å¡ï¼Œå¤šå¡è½¬æ¢ï¼Œè¯¦ç»†æ•™ç¨‹è¯·å‚è€ƒç‰¹æ€§æ–‡æ¡£æ¨¡å‹[æƒé‡åˆ‡åˆ†ä¸åˆå¹¶](../feature_cards/Transform_Ckpt.md)

- step 1. è·å–æ¨¡å‹åˆ‡åˆ†ç­–ç•¥æ–‡ä»¶ï¼š

åœ¨æ‰§è¡Œå¾®è°ƒè„šæœ¬æ—¶ï¼Œæ¨¡å‹å®Œæˆç¼–è¯‘åï¼Œå°†ä¼šåœ¨`output/strategy`è·¯å¾„ä¸‹ç”Ÿæˆå„å¡çš„åˆ‡åˆ†ç­–ç•¥æ–‡ä»¶ï¼Œç”¨äºæƒé‡åˆå¹¶ã€‚

> æ³¨ï¼šloraå¾®è°ƒæ—¶éœ€è¦ç¡®è®¤é…ç½®æ–‡ä»¶`parallel context config`ä¸­`only_trainable_params`è®¾ä¸º`False`ï¼Œä»¥è·å–æ‰€æœ‰å‚æ•°å®Œæ•´ç­–ç•¥ã€‚

- step 2. è¿è¡Œ`mindformers/tools/transform_ckpt.py`è„šæœ¬è¿›è¡Œå¤šå¡æƒé‡åˆå¹¶ï¼š

```shell
python transform_ckpt.py \
--src_ckpt_strategy {path}/output/strategy/ \
--src_ckpt_dir {path}/output/checkpoint/ \
--dst_ckpt_dir {path}/target_checkpoint/ \
--prefix glm2_6b
```

```text
# å‚æ•°è¯´æ˜
src_ckpt_strategy: æ­¥éª¤1ä¸­çš„åˆ‡åˆ†ç­–ç•¥æ–‡ä»¶è·¯å¾„
src_ckpt_dir: åŸåˆ‡åˆ†æƒé‡æ–‡ä»¶å¤¹
dst_ckpt_dir: ç›®æ ‡è·¯å¾„
prefix: ckptæ–‡ä»¶å‰ç¼€å
```

> æ³¨ï¼š`transform_checkpoints` æ¥å£å½“å‰ä»…mindspore 2.0ä»¥ä¸Šç‰ˆæœ¬æ”¯æŒï¼Œå¦‚å½“å‰ç¡¬ä»¶ç¯å¢ƒåªæ”¯æŒ2.0ä»¥ä¸‹ç‰ˆæœ¬ï¼Œå¯ä»¥æ–°å»ºcondaç¯å¢ƒå®‰è£…mindspore 2.0çš„cpuç‰ˆæœ¬ä»¥æ‰§è¡Œè¯¥è„šæœ¬

## åŸºäºAPIçš„å¿«é€Ÿä½¿ç”¨

### åŸºäºAutoClassçš„å¿«é€Ÿä½¿ç”¨

```python
import mindspore
from mindformers import AutoConfig, AutoModel, AutoTokenizer

# æŒ‡å®šå›¾æ¨¡å¼ï¼ŒæŒ‡å®šä½¿ç”¨è®­ç»ƒå¡id
mindspore.set_context(mode=0, device_id=0)

tokenizer = AutoTokenizer.from_pretrained('glm2_6b')

# è‡ªå®šä¹‰ä¿®æ”¹é…ç½®åå®ä¾‹åŒ–,é…ç½®ä¸ºyamlæ–‡ä»¶è·¯å¾„ï¼Œç¤ºä¾‹yamlæ–‡ä»¶ä¸ºconfigs/glm2/predict_glm2_6b.yaml
# éœ€è¦ä¿®æ”¹yamlä¸­çš„checkpoint_name_or_pathä¸ºæƒé‡ä¸‹è½½ç« èŠ‚ä¸‹è½½çš„æƒé‡æ–‡ä»¶
config = AutoConfig.from_pretrained('/path/to/predict_glm2_6b.yaml')
model = AutoModel.from_config(config)   # ä»è‡ªå®šä¹‰é…ç½®é¡¹ä¸­å®ä¾‹åŒ–æ¨¡å‹

inputs = tokenizer("ä½ å¥½")["input_ids"]
# é¦–æ¬¡è°ƒç”¨model.generate()è¿›è¡Œæ¨ç†å°†åŒ…å«å›¾ç¼–è¯‘æ—¶é—´ï¼Œæ¨ç†æ€§èƒ½æ˜¾ç¤ºä¸å‡†ç¡®ï¼Œå¤šæ¬¡é‡å¤è°ƒç”¨ä»¥è·å–å‡†ç¡®çš„æ¨ç†æ€§èƒ½
outputs = model.generate(inputs, max_new_tokens=20, do_sample=True, top_k=3)
response = tokenizer.decode(outputs)
print(response)
# ['ä½ å¥½ï¼Œä½œä¸ºä¸€åäººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œæˆ‘æ¬¢è¿æ‚¨éšæ—¶å‘æˆ‘æé—®ã€‚']
```

### åŸºäºTrainerçš„å¿«é€Ÿè®­ç»ƒï¼Œå¾®è°ƒï¼Œè¯„æµ‹ï¼Œæ¨ç†

glm2_6bæš‚ä¸æ”¯æŒä½¿ç”¨Trainerè¿›è¡Œå•å¡è®­ç»ƒå’Œå¾®è°ƒï¼Œè¯·å‚è€ƒå¤šå¡è®­ç»ƒå’Œå¾®è°ƒã€‚

> æ³¨ï¼šä¸‹é¢ä»…æ˜¾ç¤ºæ¥å£ä½¿ç”¨æ–¹å¼ï¼Œæ¨¡å‹å¯åŠ¨è®­ç»ƒéœ€æ±‚å¤šå¡åˆ†å¸ƒå¼è®­ç»ƒï¼Œè®­ç»ƒè„šæœ¬éœ€é…åˆåˆ†å¸ƒå¼è„šæœ¬å¯åŠ¨

```python
import mindspore
from mindformers.trainer import Trainer

# æŒ‡å®šå›¾æ¨¡å¼ï¼ŒæŒ‡å®šä½¿ç”¨è®­ç»ƒå¡id
mindspore.set_context(mode=0, device_id=0)

# åˆå§‹åŒ–é¢„è®­ç»ƒä»»åŠ¡
trainer = Trainer(task='text_generation',
                  model='glm2_6b',
                  train_dataset='path/to/train_dataset',
                  eval_dataset='path/to/eval_dataset')

# å¼€å¯è¯„æµ‹
# éœ€è¦åœ¨configs/glm2/run_glm2_6b.yamlä¸­å°†seq_lengthä¿®æ”¹ä¸º256
trainer.evaluate()

# å¼€å¯æ¨ç†
# éœ€è¦åœ¨configs/glm2/run_glm2_6b.yamlä¸­å°†param_init_typeã€compute_dtypeä¿®æ”¹ä¸º"float16"
predict_result = trainer.predict(input_data="ä½ å¥½")
print(predict_result)
```

### åŸºäºPipelineçš„å¿«é€Ÿæ¨ç†

```python
import mindspore
mindspore.set_context(mode=0, device_id=0)

from mindformers import AutoModel, AutoTokenizer, TextGenerationPipeline
# è‡ªå®šä¹‰ä¿®æ”¹é…ç½®åå®ä¾‹åŒ–,é…ç½®ä¸ºyamlæ–‡ä»¶è·¯å¾„ï¼Œç¤ºä¾‹yamlæ–‡ä»¶ä¸ºconfigs/glm2/predict_glm2_6b.yaml
# éœ€è¦ä¿®æ”¹yamlä¸­çš„checkpoint_name_or_pathä¸ºæƒé‡ä¸‹è½½ç« èŠ‚ä¸‹è½½çš„æƒé‡æ–‡ä»¶
config = AutoConfig.from_pretrained('/path/to/predict_glm2_6b.yaml')
model = AutoModel.from_config(config)   # ä»è‡ªå®šä¹‰é…ç½®é¡¹ä¸­å®ä¾‹åŒ–æ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained('glm2_6b')
pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)
predict_result = pipeline("ä½ å¥½")
print(predict_result)
# [{'text_generation_text': ['ä½ å¥½ï¼Œæˆ‘æ˜¯ ChatGLM2-6Bï¼Œ ä¸€ä¸ªäººå·¥æ™ºèƒ½åŠ©æ‰‹ã€‚æˆ‘èƒŒåä½¿ç”¨çš„æ¨¡å‹æ˜¯ GLM2-6Bï¼Œ æ˜¯ä¸€ç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œ å…·æœ‰è¶…è¿‡ 2000 äº¿å‚æ•°ï¼Œæ”¯æŒå¤šç§ä»»åŠ¡ã€‚']}]
```

## å¾®è°ƒ

ä¸‹é¢ä»¥ [ADGEN](https://aclanthology.org/D19-1321.pdf) (å¹¿å‘Šç”Ÿæˆ) æ•°æ®é›†ä¸ºä¾‹ä»‹ç»ä»£ç çš„ä½¿ç”¨æ–¹æ³•

### æ•°æ®é›†å‡†å¤‡

ADGEN æ•°æ®é›†ä»»åŠ¡ä¸ºæ ¹æ®è¾“å…¥ï¼ˆcontentï¼‰ç”Ÿæˆä¸€æ®µå¹¿å‘Šè¯ï¼ˆsummaryï¼‰ã€‚

```json
{"content": "ç±»å‹#ä¸Šè¡£*ç‰ˆå‹#å®½æ¾*ç‰ˆå‹#æ˜¾ç˜¦*å›¾æ¡ˆ#çº¿æ¡*è¡£æ ·å¼#è¡¬è¡«*è¡£è¢–å‹#æ³¡æ³¡è¢–*è¡£æ¬¾å¼#æŠ½ç»³", "summary": "è¿™ä»¶è¡¬è¡«çš„æ¬¾å¼éå¸¸çš„å®½æ¾ï¼Œåˆ©è½çš„çº¿æ¡å¯ä»¥å¾ˆå¥½çš„éšè—èº«æä¸Šçš„å°ç¼ºç‚¹ï¼Œç©¿åœ¨èº«ä¸Šæœ‰ç€å¾ˆå¥½çš„æ˜¾ç˜¦æ•ˆæœã€‚é¢†å£è£…é¥°äº†ä¸€ä¸ªå¯çˆ±çš„æŠ½ç»³ï¼Œæ¼‚äº®çš„ç»³ç»“å±•ç°å‡ºäº†åè¶³çš„ä¸ªæ€§ï¼Œé…åˆæ—¶å°šçš„æ³¡æ³¡è¢–å‹ï¼Œå°½æ˜¾å¥³æ€§ç”œç¾å¯çˆ±çš„æ°”æ¯ã€‚"}
```

å‚ç…§ [ChatGLMä»“åº“](https://github.com/THUDM/ChatGLM-6B/tree/main/ptuning) çš„æŒ‡å¼•ä¸‹è½½å¤„ç†å¥½çš„ ADGEN æ•°æ®é›†ï¼Œç›®å½•ç»“æ„ä¸º

```text
AdvertiseGen
  â”œâ”€â”€ train.json
  â””â”€â”€ dev.json
```

ä¿®æ”¹é…ç½®æ–‡ä»¶ `configs/glm2/run_glm2_6b_*.yaml` ä¸­çš„ä»¥ä¸‹é¡¹ï¼š

```yaml
train_dataset: &train_dataset
    dataset_dir: "/path/to/AdvertiseGen/train.json"
    origin_columns: ["content", "summary"]
  tokenizer:
    vocab_file: "/path/to/tokenizer.model"
  input_columns: ["input_ids", "labels"]
  max_source_length: 64
  max_target_length: 127

eval_dataset: &eval_dataset
  data_loader:
    dataset_dir: "/path/to/AdvertiseGen/dev.json"
    origin_columns: ["content", "summary"]
  tokenizer:
    vocab_file: "/path/to/tokenizer.model"
  max_source_length: 256
  max_target_length: 256
```

**æ³¨æ„**ï¼šå¾®è°ƒæ—¶çš„æ¨¡å‹`seq_length`éœ€è¦ç­‰äºå¾®è°ƒæ•°æ®é›†çš„`max_source_length + max_target_length + 1`ã€‚
yamlæ–‡ä»¶ä¸­é»˜è®¤çš„`seq_length: 192`ä»¥åŠ`max_source_length: 64`å’Œ`max_target_length: 127`é€‚ç”¨äºADGENæ•°æ®é›†ï¼Œ
å…¶ä»–æ•°æ®é›†çš„`seq_length`è®¾ç½®ï¼Œå¯ä»¥éå†å¹¶å°†æ•°æ®é›†è½¬æ¢ä¸ºtoken_idï¼Œå–token_idæœ€å¤§é•¿åº¦ï¼Œ`seq_length`å¤ªå¤§å½±å“è®­ç»ƒæ€§èƒ½ï¼Œ
å¤ªå°å½±å“è®­ç»ƒç²¾åº¦ï¼Œéœ€è¦åšå‡ºæƒè¡¡ã€‚

### å…¨å‚å¾®è°ƒ

å…¨å‚å¾®è°ƒä½¿ç”¨ `configs/glm2/run_glm2_6b_finetune*.yaml` é…ç½®æ–‡ä»¶ï¼Œé…ç½®æ–‡ä»¶ä¸­å®šä¹‰äº†å¾®è°ƒæ‰€éœ€çš„å„é…ç½®é¡¹

ä¿®æ”¹æ•°æ®é›†/æ¨¡å‹æƒé‡é…ç½®è·¯å¾„ï¼š

- æ•°æ®é›†ï¼šä¿®æ”¹ `configs/glm2/run_glm2_6b_finetune*.yaml` è„šæœ¬ä¸­`train_dataset` çš„ `dataset_dir` ä¸ºå‰æ–‡ç”Ÿæˆçš„æ•°æ®é›†è·¯å¾„ã€‚
- åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æƒé‡ï¼šä¿®æ”¹ `configs/glm2/run_glm2_6b_finetune*.yaml` è„šæœ¬ä¸­çš„ `load_checkpoint` ä¸ºé¢„è®­ç»ƒæ¨¡å‹æƒé‡è·¯å¾„ã€‚

å½“å‰æ¨¡å‹å·²æ”¯æŒä½¿ç”¨**Flash Attentionç®—æ³•**è¿›è¡Œå…¨å‚å¾®è°ƒï¼Œè¯·å‚è€ƒ [Flash Attentionä½¿ç”¨æ–‡æ¡£](../feature_cards/Training_Algorithms.md#flash-attention)

#### å•å¡å¾®è°ƒ

ç”±äºglm2_6bæ¨¡å‹è¾ƒå¤§ï¼Œå…¨é‡å¾®è°ƒä¸æ”¯æŒå•å¡è¿è¡Œ

#### å¤šå¡å¾®è°ƒ

```shell
# ä»¥glm2-6bæ¨¡å‹ä¸ºä¾‹ï¼Œé»˜è®¤é…ç½®å•æœº8å¡ï¼Œå¦‚æœèŠ‚ç‚¹æ•°æœ‰å˜ï¼Œéœ€è¦ä¿®æ”¹ç›¸åº”çš„é…ç½®ã€‚
# é…ç½®æ–‡ä»¶è·¯å¾„ï¼šconfigs/glm2/run_glm2_6b_finetune*.yaml
parallel_config:
  data_parallel: 8
  model_parallel: 1
  pipeline_stage: 1
  expert_parallel: 1
  micro_batch_num: 1
  vocab_emb_dp: True
  gradient_aggregation_group: 4
```

```shell
cd {mindformersæ ¹ç›®å½•}
bash scripts/msrun_launcher.sh "run_mindformer.py --config configs/glm2/run_glm2_6b_finetune*.yaml --run_mode finetune"
```

```text
# å‚æ•°è¯´æ˜
config: é…ç½®æ–‡ä»¶è·¯å¾„
run_mode: è¿è¡Œæ¨¡å¼ï¼Œå¾®è°ƒæ—¶è®¾ç½®ä¸ºfinetune
```

> è®­ç»ƒçš„logæ—¥å¿—è·¯å¾„ï¼šmindformers/output/log
>
> checkpoint(å«ä¼˜åŒ–å™¨å‚æ•°)å­˜å‚¨è·¯å¾„ï¼šmindformers/output/checkpoint
>
> checkpoint(ä¸å«ä¼˜åŒ–å™¨å‚æ•°)å­˜å‚¨è·¯å¾„ï¼šmindformers/output/checkpoint_network
>
> è‹¥æƒ³åˆå¹¶ckptç”¨äºåç»­è¯„ä¼°ï¼Œé€‰æ‹©ä¸å«ä¼˜åŒ–å™¨å‚æ•°çš„æƒé‡å³å¯ã€‚

### LoRAå¾®è°ƒ

å…¨å‚å¾®è°ƒèƒ½å¤Ÿåœ¨å¾®è°ƒæ•°æ®é›†ä¸Šå–å¾—è‰¯å¥½æ•ˆæœï¼Œä½†å­˜åœ¨é—å¿˜é¢„è®­ç»ƒçŸ¥è¯†çš„ç°è±¡ã€‚
å› æ­¤æ¨èä½¿ç”¨ä½å‚å¾®è°ƒç®—æ³•ï¼Œå†»ç»“åŸæ¨¡å‹æƒé‡ï¼Œä»…åœ¨å°è§„æ¨¡å‚æ•°é‡ä¸Šè¿›è¡Œè®­ç»ƒï¼Œåœ¨å¾®è°ƒæ•°æ®é›†ä¸Šå–å¾—è‰¯å¥½æ•ˆæœçš„åŒæ—¶ï¼Œç¼“è§£æ¨¡å‹é—å¿˜ç°è±¡

ä½¿ç”¨LoRAç®—æ³•è¿›è¡Œä½å‚å¾®è°ƒæ—¶ï¼Œä½¿ç”¨ `configs/glm2/run_glm2_6b_lora*.yaml` é…ç½®æ–‡ä»¶ï¼Œè¯¥é…ç½®æ–‡ä»¶åŒ…å«äº†loraä½å‚å¾®è°ƒç®—æ³•æ‰€éœ€çš„é…ç½®é¡¹

ä¿®æ”¹æ•°æ®é›†/æ¨¡å‹æƒé‡é…ç½®è·¯å¾„ï¼š

- æ•°æ®é›†ï¼šä¿®æ”¹ `mindformers/configs/glm2/run_glm2_6b_lora*.yaml` è„šæœ¬ä¸­`train_dataset` çš„ `dataset_dir` ä¸ºå‰æ–‡ç”Ÿæˆçš„æ•°æ®é›†è·¯å¾„ã€‚
- åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æƒé‡ï¼šä¿®æ”¹ `mindformers/configs/glm2/run_glm2_6b_lora*.yaml` è„šæœ¬ä¸­çš„ `load_checkpoint` ä¸ºé¢„è®­ç»ƒæ¨¡å‹æƒé‡è·¯å¾„ã€‚

#### å•å¡å¾®è°ƒ

```shell
cd {mindformersæ ¹ç›®å½•}
python run_mindformer.py --config configs/glm2/run_glm2_6b_lora*.yaml --run_mode finetune
```

```text
# å‚æ•°è¯´æ˜
config: é…ç½®æ–‡ä»¶è·¯å¾„
run_mode: è¿è¡Œæ¨¡å¼ï¼Œå¾®è°ƒæ—¶è®¾ç½®ä¸ºfinetune
```

#### å¤šå¡å¾®è°ƒ

```shell
# ä»¥glm2-6bæ¨¡å‹ä¸ºä¾‹ï¼Œé»˜è®¤é…ç½®å•æœº8å¡ï¼Œå¦‚æœèŠ‚ç‚¹æ•°æœ‰å˜ï¼Œéœ€è¦ä¿®æ”¹ç›¸åº”çš„é…ç½®ã€‚
# é…ç½®æ–‡ä»¶è·¯å¾„ï¼šconfigs/glm2/run_glm2_6b_lora*.yaml
parallel_config:
  data_parallel: 8
  model_parallel: 1
  pipeline_stage: 1
  expert_parallel: 1
  micro_batch_num: 1
  vocab_emb_dp: True
  gradient_aggregation_group: 4
```

```shell
cd {mindformersæ ¹ç›®å½•}
bash scripts/msrun_launcher.sh "run_mindformer.py --config configs/glm2/run_glm2_6b_lora*.yaml --run_mode finetune"
```

```text
# å‚æ•°è¯´æ˜
config: é…ç½®æ–‡ä»¶è·¯å¾„
run_mode: è¿è¡Œæ¨¡å¼ï¼Œå¾®è°ƒæ—¶è®¾ç½®ä¸ºfinetune
```

> è®­ç»ƒçš„logæ—¥å¿—è·¯å¾„ï¼šmindformers/output/log
>
> checkpoint(å«ä¼˜åŒ–å™¨å‚æ•°)å­˜å‚¨è·¯å¾„ï¼šmindformers/output/checkpoint
>
> checkpoint(ä¸å«ä¼˜åŒ–å™¨å‚æ•°)å­˜å‚¨è·¯å¾„ï¼šmindformers/output/checkpoint_network
>
> è‹¥æƒ³åˆå¹¶ckptç”¨äºåç»­è¯„ä¼°ï¼Œé€‰æ‹©ä¸å«ä¼˜åŒ–å™¨å‚æ•°çš„æƒé‡å³å¯ã€‚

### P-Tuningå¾®è°ƒ

å¯¹äºæ¯ä¸ªä¸‹æ¸¸ä»»åŠ¡ï¼Œåœ¨ç½‘ç»œçš„æ¯ä¸€å±‚æ·»åŠ ä¸€ä»½è¿ç»­æç¤ºå‘é‡ï¼Œå†»ç»“é¢„è®­ç»ƒæ¨¡å‹çš„å…¶ä»–å‚æ•°ï¼Œåªè®­ç»ƒè¿™äº›å‘é‡ã€‚

#### å•å¡å¾®è°ƒ

ä½¿ç”¨P-Tuningç®—æ³•è¿›è¡Œä½å‚å¾®è°ƒæ—¶ï¼Œä½¿ç”¨ `configs/glm2/run_glm2_6b_ptuning2.yaml` é…ç½®æ–‡ä»¶ï¼Œè¯¥é…ç½®æ–‡ä»¶åŒ…å«äº†P-Tuningä½å‚å¾®è°ƒç®—æ³•æ‰€éœ€çš„é…ç½®é¡¹

ä¿®æ”¹æ•°æ®é›†/æ¨¡å‹æƒé‡é…ç½®è·¯å¾„ï¼š

- æ•°æ®é›†ï¼šä¿®æ”¹ `mindformers/configs/glm2/run_glm2_6b_ptuning2.yaml` è„šæœ¬ä¸­`train_dataset` çš„ `dataset_dir` ä¸ºå‰æ–‡ç”Ÿæˆçš„æ•°æ®é›†è·¯å¾„ã€‚
- åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æƒé‡ï¼šä¿®æ”¹ `mindformers/configs/glm2/run_glm2_6b_ptuning2.yaml` è„šæœ¬ä¸­çš„ `load_checkpoint` ä¸ºé¢„è®­ç»ƒæ¨¡å‹æƒé‡è·¯å¾„ã€‚

æ‰§è¡Œå‘½ä»¤ï¼š

```shell
cd {mindformersæ ¹ç›®å½•}
python run_mindformer.py --config configs/glm2/run_glm2_6b_ptuning2.yaml --run_mode finetune
```

```text
# å‚æ•°è¯´æ˜
config: é…ç½®æ–‡ä»¶è·¯å¾„
run_mode: è¿è¡Œæ¨¡å¼ï¼Œå¾®è°ƒæ—¶è®¾ç½®ä¸ºfinetune
```

> è®­ç»ƒçš„logæ—¥å¿—è·¯å¾„ï¼šmindformers/output/log
>
> checkpoint(å«ä¼˜åŒ–å™¨å‚æ•°)å­˜å‚¨è·¯å¾„ï¼šmindformers/output/checkpoint
>
> checkpoint(ä¸å«ä¼˜åŒ–å™¨å‚æ•°)å­˜å‚¨è·¯å¾„ï¼šmindformers/output/checkpoint_network
>
> è‹¥æƒ³åˆå¹¶ckptç”¨äºåç»­è¯„ä¼°ï¼Œé€‰æ‹©ä¸å«ä¼˜åŒ–å™¨å‚æ•°çš„æƒé‡å³å¯ã€‚

### è¾¹è®­è¾¹è¯„ä¼°

#### 1. ä½¿ç”¨ `Rouge-1`ã€`Rouge-2` ç­‰æŒ‡æ ‡è¯„æµ‹

ä½¿ç”¨è¯¥æŒ‡æ ‡è¯„æµ‹æ—¶é€Ÿåº¦è¾ƒæ…¢ï¼Œæ¨èä½¿ç”¨ `PerplexityMetric` è¯„æµ‹ã€‚

å°†è®­ç»ƒé…ç½®æ–‡ä»¶çš„ `do_eval: False` è®¾ç½®ä¸º `do_eval: True`ï¼Œå¹¶ä¸”éœ€è¦å°† `train_dataset` å’Œ `eval_dataset` çš„ `max_source_length`ã€`max_target_length` ä»¥åŠ `batch_size`é¡¹è®¾ç½®ä¸ºç›¸åŒå€¼ï¼Œå¹¶ä¸”ä¿æŒ `max_source_length + max_target_length + 1 = seq_length`ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```yaml
do_eval: True
eval_step_interval: 1788
eval_epoch_interval: -1

metric:
  type: ADGENMetric

model:
  model_config:
    seq_length: 192
train_dataset: &train_dataset
  max_source_length: 64
  max_target_length: 127
  batch_size: 8
eval_dataset: &eval_dataset
  max_source_length: 64
  max_target_length: 127
  batch_size: 8
```

#### 2. ä½¿ç”¨ `PerplexityMetric` æŒ‡æ ‡è¯„æµ‹

å°†è®­ç»ƒé…ç½®æ–‡ä»¶çš„ `do_eval: False` è®¾ç½®ä¸º `do_eval: True`ï¼Œå¹¶ä¸”éœ€è¦å°† `train_dataset` å’Œ `eval_dataset` çš„ `max_source_length`ã€`max_target_length` ã€`phase` ä»¥åŠ `batch_size`é¡¹è®¾ç½®ä¸ºç›¸åŒå€¼ï¼Œå¹¶ä¸”ä¿æŒ `max_source_length + max_target_length + 1 = seq_length`ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```yaml
do_eval: True
eval_step_interval: 1788
eval_epoch_interval: -1

metric:
  type: PerplexityMetric

model:
  model_config:
    seq_length: 192
train_dataset: &train_dataset
  data_loader:
    phase: "train"
  max_source_length: 64
  max_target_length: 127
  batch_size: 8
eval_dataset: &eval_dataset
  data_loader:
    phase: "train"
  max_source_length: 64
  max_target_length: 127
  batch_size: 8
```

mindformersé€šè¿‡ `eval_step_interval` å’Œ `eval_epoch_interval` ä¸¤é¡¹é…ç½®å‚æ•°æ¥æ§åˆ¶è¾¹è®­ç»ƒè¾¹è¯„ä¼°çš„æ‰§è¡Œé—´éš”ï¼Œå‚æ•°å«ä¹‰å¦‚ä¸‹ï¼š

- **eval_step_interval**: è¯„ä¼°stepé—´éš”, é»˜è®¤ä¸º100ï¼Œè¡¨ç¤ºæ¯100ä¸ªstepé—´éš”æ‰§è¡Œä¸€æ¬¡è¯„ä¼°ï¼›é…ç½®ä¸ºå¤§äº0çš„æ•°è¡¨ç¤ºæ¯éš”æ‰€é…ç½®çš„stepæ•°åæ‰§è¡Œä¸€æ¬¡è¯„ä¼°ï¼Œé…ç½®ä¸ºå°äº0çš„æ•°åˆ™è¡¨ç¤ºç¦ç”¨stepè¯„ä¼°ï¼›æ³¨æ„ï¼šåœ¨æ•°æ®ä¸‹æ²‰æ¨¡å¼ä¸‹ï¼Œstepé—´éš”å€¼å»ºè®®é…ç½®ä¸ºsink sizeçš„å€æ•°
- **eval_epoch_interval**: è¯„ä¼°epoché—´éš”, é»˜è®¤ä¸º-1ï¼Œè¡¨ç¤ºç¦ç”¨epochç»“æŸæ—¶çš„è¯„ä¼°ï¼›é…ç½®ä¸ºå¤§äº0çš„æ•°è¡¨ç¤ºæ¯éš”æ‰€é…ç½®çš„epochæ•°åæ‰§è¡Œä¸€æ¬¡è¯„ä¼°ï¼Œé…ç½®ä¸ºå°äº0çš„æ•°åˆ™è¡¨ç¤ºç¦ç”¨epochè¯„ä¼°ï¼›æ³¨æ„ï¼šæ•°æ®ä¸‹æ²‰æ¨¡å¼ä¸‹ï¼Œepochæ‰€åŒ…å«çš„stepæ•°å°†ä»æ•°æ®é›†å¤§å°å˜ä¸ºsink sizeçš„å¤§å°ï¼Œå°†åœ¨ `sink_size * eval_epoch_interval` ä¸ªstepåæ‰§è¡Œä¸€æ¬¡è¯„ä¼°

## è¯„æµ‹

### æ–‡æœ¬ç”Ÿæˆ

### æ•°æ®é›†å‡†å¤‡-æ–‡æœ¬ç”Ÿæˆ

è§å¾®è°ƒç« èŠ‚çš„[æ•°æ®é›†å‡†å¤‡](#æ•°æ®é›†å‡†å¤‡)

è¯„æµ‹æ—¶æ¨¡å‹`seq_length`éœ€è¦ç­‰äºè¯„æµ‹æ•°æ®é›†çš„`max_source_length`å’Œ`max_target_length`ã€‚å› æ­¤ä¿®æ”¹yamlä¸­æ¨¡å‹`seq_length`ä¸º256ï¼š

```yaml
model:
  model_config:
    seq_length: 256
```

### å•å¡è¯„æµ‹

ä½¿ç”¨å…¨å‚å¾®è°ƒæƒé‡æ—¶ï¼Œå¯åŠ¨å¦‚ä¸‹shellè„šæœ¬ï¼Œæ‰§è¡Œå•å¡è¯„ä¼°

é…ç½®æ–‡ä»¶é€‰æ‹© `configs/glm2/run_glm2_6b_finetune_eval.yaml` glm2æ¨¡å‹æ¨ç†é…ç½®ï¼Œä¿®æ”¹å…¶ä¸­`model`å­—æ®µä¸‹`model_config`ä¸­`use_past: True`å¼€å¯å¢é‡æ¨ç†ä½¿è¯„ä¼°é€Ÿåº¦æ›´å¿«

```bash
python run_mindformer.py --config configs/glm2/run_glm2_6b_finetune_eval.yaml--run_mode eval --load_checkpoint /path/to/glm2_6b_finetune.ckpt --device_id 0 --use_parallel False
```

ä½¿ç”¨LoRAä½å‚å¾®è°ƒæƒé‡æ—¶ï¼Œå¯åŠ¨å¦‚ä¸‹shellè„šæœ¬ï¼Œæ‰§è¡Œå•å¡è¯„ä¼°

é…ç½®æ–‡ä»¶é€‰æ‹© `configs/glm2/run_glm2_6b_lora_eval.yaml` glm2_loraæ¨¡å‹æ¨ç†é…ç½®ï¼Œæ­¤é…ç½®å¯ç”¨äºloraæ¨¡å‹ï¼Œä¿®æ”¹å…¶ä¸­`model`å­—æ®µä¸‹`model_config`ä¸­`use_past: True`å¼€å¯å¢é‡æ¨ç†ä½¿è¯„ä¼°é€Ÿåº¦æ›´å¿«

```bash
python run_mindformer.py --config configs/glm2/run_glm2_6b_lora_eval.yaml --run_mode eval --load_checkpoint /path/to/glm2_6b_lora.ckpt --device_id 0 --use_parallel False
```

> å•å¡è¯„æµ‹æ—¶ï¼Œåº”å°†yamlä¸­ model:model_config:batch_size ä¿®æ”¹ä¸ºç­‰äº runner_config:batch_size

## æ¨ç†

### åŸºäºgenerateçš„æ¨ç†

ä¸‹é¢æä¾›ä¸€ä¸ªæ¨¡å‹æ¨ç†æ ·ä¾‹è„šæœ¬ `infer.py`

```python
from mindformers import AutoConfig, AutoModel, AutoTokenizer, ChatGLM2Tokenizer
import mindspore as ms

ms.set_context(mode=ms.GRAPH_MODE, device_target="Ascend", device_id=0)

# è‡ªå®šä¹‰ä¿®æ”¹é…ç½®åå®ä¾‹åŒ–,é…ç½®ä¸ºyamlæ–‡ä»¶è·¯å¾„ï¼Œç¤ºä¾‹yamlæ–‡ä»¶ä¸ºconfigs/glm2/predict_glm2_6b.yaml
# éœ€è¦ä¿®æ”¹yamlä¸­çš„checkpoint_name_or_pathä¸ºæƒé‡ä¸‹è½½ç« èŠ‚ä¸‹è½½çš„æƒé‡æ–‡ä»¶
config = AutoConfig.from_pretrained("/path/to/predict_glm2_6b.yaml")
config.seq_length = 1024
model = AutoModel.from_config(config)

# æœ¬åœ°åŠ è½½æ–¹å¼
tokenizer = ChatGLM2Tokenizer("/path/to/your/tokenizer.model")

kwargs={}
gen_kwargs = {"max_length": config.seq_length, "num_beams": 1, "do_sample": False, "top_p": 3,"top_k": 0.7,
              "temperature": 1, **kwargs}

queries = ["ä½ å¥½", "è¯·ä»‹ç»ä¸€ä¸‹æ­å·", "é‚£é‡Œæœ‰ä»€ä¹ˆå¥½åƒçš„å—"]
history = []
for query in queries:
    # å¦‚æœæƒ³å…³é—­historyï¼Œæ­¤å¤„ä¼ å…¥ `history=[]` å³å¯
    prompt = tokenizer.build_prompt(query, history=history)
    input_id = tokenizer(prompt)["input_ids"]

    output = model.generate([input_id], **gen_kwargs)

    # output åŒ…æ‹¬äº†[input_id, output]ä¸¤ä¸ªéƒ¨åˆ†
    output = output[0][len(input_id):]
    response = tokenizer.decode(output)
    print(response)
    history += [(query, response)]

    '''
    response1:
    ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚

    response2:
    æ­å·æ˜¯ä¸­å›½æµ™æ±Ÿçœçœä¼šï¼Œä½äºæµ™æ±Ÿçœä¸œå—éƒ¨ï¼Œåœ°å¤„æµ™æ±ŸçœåŒ—éƒ¨ï¼Œä¸œä¸´ä¸œæµ·ï¼Œå—æ¥ç¦å»ºçœï¼ŒåŒ—ä¸æ±Ÿè‹çœæ¯—é‚»ï¼Œæ˜¯ä¸­å›½è‘—åçš„æ—…æ¸¸åŸå¸‚ä¹‹ä¸€ã€‚

    æ­å·æœ‰ç€æ‚ ä¹…çš„å†å²å’Œæ–‡åŒ–ï¼Œè¢«èª‰ä¸ºâ€œäººé—´å¤©å ‚â€ï¼Œè¢«èª‰ä¸ºâ€œå—å®‹éƒ½åŸâ€ï¼Œæ˜¯ä¸­å›½å—æ–¹è‘—åçš„å†å²æ–‡åŒ–ååŸä¹‹ä¸€ã€‚æ­å·è¿˜è¢«èª‰ä¸ºâ€œå…¨å›½æœ€å…·å¹¸ç¦æ„ŸåŸå¸‚â€ï¼Œå…·æœ‰ä¸°å¯Œçš„å†å²é—å­˜ã€ä¼˜ç¾çš„è‡ªç„¶é£å…‰å’Œæµ“éƒçš„æ–‡åŒ–æ°›å›´ã€‚

    æ­å·çš„ç»æµä»¥æœåŠ¡ä¸šä¸ºä¸»å¯¼äº§ä¸šï¼Œç‰¹åˆ«æ˜¯äº¤é€šè¿è¾“ã€ä»“å‚¨å’Œé‚®æ”¿ä¸šã€‚åŒæ—¶ï¼Œæ­å·ä¹Ÿæ˜¯ä¸­å›½é‡è¦çš„ç”µå­å•†åŠ¡å’Œäº’è”ç½‘äº§ä¸šåŸºåœ°ä¹‹ä¸€ï¼Œè¢«èª‰ä¸ºâ€œä¸­å›½ç”µå­å•†åŠ¡ä¹‹éƒ½â€ã€‚

    æ­å·çš„è‘—åæ™¯ç‚¹åŒ…æ‹¬è¥¿æ¹–ã€çµéšå¯ºã€åƒå²›æ¹–ã€é’±å¡˜æ±Ÿç­‰ã€‚è¥¿æ¹–æ˜¯ä¸­å›½è‘—åçš„é£æ™¯åèƒœåŒºä¹‹ä¸€ï¼Œè¢«èª‰ä¸ºâ€œäººé—´å¤©å ‚â€ï¼Œçµéšå¯ºæ˜¯ä¸­å›½è‘—åçš„ä½›æ•™å¯ºåº™ä¹‹ä¸€ï¼Œåƒå²›æ¹–å’Œé’±å¡˜æ±Ÿæ˜¯ä¸­å›½è‘—åçš„è‡ªç„¶é£æ™¯åŒºä¹‹ä¸€ã€‚

    æ­å·è¿˜æ‹¥æœ‰ä¸°å¯Œçš„äººæ–‡èµ„æºï¼Œè¢«èª‰ä¸ºâ€œäººé—´å¤©å ‚â€çš„æ­å·è¥¿æ¹–ã€çµéšå¯ºã€åƒå²›æ¹–ã€é’±å¡˜æ±Ÿç­‰æ™¯ç‚¹ï¼Œä»¥åŠå®‹åŸã€å—å®‹å¾¡è¡—ç­‰å†å²æ–‡åŒ–æ™¯ç‚¹ï¼Œéƒ½æ˜¯æ¸¸å®¢å‰æ¥æ­å·æ—…æ¸¸çš„çƒ­é—¨æ™¯ç‚¹ã€‚

    response3:
    æ­å·æ˜¯ä¸­å›½è‘—åçš„ç¾é£ŸåŸå¸‚ä¹‹ä¸€ï¼Œæœ‰è®¸å¤šç‰¹è‰²ç¾é£Ÿå’Œä¼ ç»Ÿèœè‚´ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›æ­å·çš„è‘—åç¾é£Ÿ:

    1. è¥¿æ¹–é†‹é±¼ï¼šè¿™æ˜¯æ­å·æœ€è‘—åçš„èœè‚´ä¹‹ä¸€ï¼Œé±¼è‚‰é²œç¾ï¼Œå…¥å£å³åŒ–ï¼Œä½ä»¥é¦™é†‹ã€ç³–ã€å§œä¸ç­‰è°ƒæ–™ï¼Œå£æ„Ÿé…¸ç”œé€‚ä¸­ã€‚

    2. é¾™äº•è™¾ä»ï¼šä»¥å½“åœ°ç‰¹äº§çš„é¾™äº•èŒ¶ä¸ºä½æ–™ï¼Œå°†é²œå«©çš„è™¾ä»ç‚’åˆ¶è€Œæˆï¼Œå£æ„Ÿæ¸…é¦™å¯å£ã€‚

    3. çŒæ±¤åŒ…ï¼šåˆç§°å°ç¬¼åŒ…ï¼Œæ˜¯æ­å·çš„ä¼ ç»Ÿç‚¹å¿ƒä¹‹ä¸€ã€‚åŒ…å­çš„çš®è½¯é¦…é²œï¼Œæ±¤æ±é²œç¾ï¼Œéå¸¸å—æ¬¢è¿ã€‚

    4. å§œæ¯é¸­ï¼šè¿™æ˜¯ä¸€é“æ­å¸®èœï¼Œä»¥é¸­è‚‰ã€å§œæ¯ã€è‘±ç­‰è°ƒæ–™çƒ¹åˆ¶è€Œæˆï¼Œå£æ„Ÿé²œç¾ã€‚

    5. è€å­—å·å°åƒï¼šæ­å·è¿˜æœ‰å¾ˆå¤šè€å­—å·å°åƒåº—ï¼Œå¦‚èƒ¡åŒå£çƒ¤è‚‰ä¸²ã€å­”åºœå®¶å®´ã€å®‹å«‚é±¼ç¾¹ç­‰ï¼Œæ˜¯å½“åœ°å±…æ°‘å’Œæ¸¸å®¢çš„ç¾é£Ÿé€‰æ‹©ã€‚

    æ­¤å¤–ï¼Œæ­å·è¿˜æœ‰è®¸å¤šç‰¹è‰²å°åƒï¼Œå¦‚ç²½å­ã€è‡­è±†è…ã€ç³¯ç±³é¸¡ã€è‚‰å¤¹é¦ã€é¸­è¡€ç²‰ä¸æ±¤ç­‰ï¼Œè®©äººå‚æ¶æ¬²æ»´ã€‚
    '''
```