# PromptFlashAttention(PFA)å’ŒIncreFlashAttention(IFA)æ¥å…¥æŒ‡å—

> ## ğŸš¨ å¼ƒç”¨è¯´æ˜
>
> æœ¬æ–‡æ¡£å·²è¿‡æ—¶ï¼Œä¸å†è¿›è¡Œç»´æŠ¤ï¼Œå¹¶å°†åœ¨ *1.6.0* ç‰ˆæœ¬ä¸‹æ¶ï¼Œå…¶ä¸­å¯èƒ½åŒ…å«è¿‡æ—¶çš„ä¿¡æ¯æˆ–å·²è¢«æ›´æ–°çš„åŠŸèƒ½æ›¿ä»£ã€‚å»ºè®®å‚è€ƒæœ€æ–°çš„ **[å®˜æ–¹æ–‡æ¡£](https://www.mindspore.cn/mindformers/docs/zh-CN/r1.5.0/index.html)** ï¼Œä»¥è·å–å‡†ç¡®çš„ä¿¡æ¯ã€‚
>
> å¦‚æœæ‚¨ä»éœ€ä½¿ç”¨æœ¬æ–‡æ¡£ä¸­çš„å†…å®¹ï¼Œè¯·ä»”ç»†æ ¸å¯¹å…¶é€‚ç”¨æ€§ï¼Œå¹¶ç»“åˆæœ€æ–°ç‰ˆæœ¬çš„ç›¸å…³èµ„æºè¿›è¡ŒéªŒè¯ã€‚
>
> å¦‚æœ‰ä»»ä½•é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·é€šè¿‡ **[ç¤¾åŒºIssue](https://gitee.com/mindspore/mindformers/issues/new)** æäº¤åé¦ˆã€‚æ„Ÿè°¢æ‚¨çš„ç†è§£ä¸æ”¯æŒï¼

## æ¦‚è¿°

PromptFlashAttention(PFA)
åœ¨ç®—æ³•ä¸­å¯ä»¥å–ä»£Self-Attentionçš„è®¡ç®—ï¼Œç›®å‰åœ¨ç®—æ³•ä¸­å¯ä»¥è·å¾—æ€§èƒ½ä»¥åŠæ˜¾å­˜æ”¶ç›Šã€‚PromptFlashAttentionä»…å¯ç”¨äºå…¨é‡æ¨ç†ï¼Œç›®å‰ä¸æ”¯æŒå¢é‡æ¨ç†åœºæ™¯(
seq_length=1)ä¸”ä¸å¯ç”¨äºè®­ç»ƒã€‚PFAæ”¯æŒå¤šå¡åœºæ™¯ã€‚

IncreFlashAttention(IFA)ä»…æ”¯æŒå¢é‡æ¨ç†åœºæ™¯ä¸‹çš„éé¦–æ¬¡æ¨ç†(seq_length=1)
ï¼Œä¸”ä¸å¯ç”¨äºè®­ç»ƒã€‚IFAç›®å‰ä¸æ”¯æŒå¤šå¡åœºæ™¯ã€‚å› æ­¤ç›®å‰åœ¨GPT2ä¸­çš„åˆ†å¸ƒå¼å¢é‡æ¨ç†çš„åœºæ™¯ä¸ºPFA + SAï¼Œè€Œå•å¡æ¨ç†çš„åœºæ™¯æ‰ä¸ºPFA + IFAã€‚

## APIä»‹ç»

### PromptFlashAttention

```python
class PromptFlashAttention(Primitive):
    r"""
    The interface for fully inference.
    B -- Batch size
    S -- Sequence length
    H -- Hidden size

    Note:
    experiment ops

    .. warning::
        This is an experimental API that is subject to change or deletion.

    Args:
        num_heads (int): The number of heads.
        scale_value (float): The scale value indicating the scale coefficient, which is used as the scalar of
          Muls in the calculation. Default: 1.0.
        pre_tokens (int): Previous tokens. Default: 2147483547.
        next_tokens (int): next tokens.  Default: 0.
          indicate the upper triangle, Indicate the number of data blocks involved in the calculation. The value 0
          indicates that the data blocks in the upper triangle are not involved in the calculation
        input_layout (str): the data layout of the input qkv, support `(BSH)` and `(BNSD)`, Default `BSH`.
        num_key_value_heads (int): head numbers of key/value which are used in GQA algorithm.
          The value o indicates if the key and value have the same head nums, use numHeads.  Default: 0.
        sparse_mode (int): Default: 0
        inner_precise (int): 0, float16 high precision. 1, high performance. default 1

    Inputs:
        - **query** (Tensor) - The query tensor with data type of float16 or float32.
          Input tensor of shape :math:`(B, S, H)` / `(B, N, S, D)`.
        - **key** (Tensor) - The key tensor with data type of float16 or float32.
          Input tensor of shape :math:`(B, S, H)` / `(B, N, S, D)`.
        - **value** (Tensor) - The value tensor with data type of float16 or float32.
          Input tensor of shape :math:`(B, S, H)` / `(B, N, S, D)`.
        - **attn_mask** (Tensor) - The attention mask tensor with data type of float16 or float32.
          For each element, 0 indicates retention and 1 indicates discard. Input tensor of shape :math:`(B, 1, S, S)`.
        - **padding_mask** (Tensor) - The padding mask tensor with data type of float16 or float32
        - **actual_seq_lengths** (Tensor): Describe actual sequence length of each input with data type of int64.
        - **actual_seq_lengths_kv** (Tensor): Describe actual sequence length of each input with data type of int64.
        - **dep_scale1** (Tensor)
        - **quant_scale1** (Tensor)
        - **deq_scale2** (Tensor)
        - **quant_scale2** (Tensor)
        - **quant_offset2** (Tensor)

    Outputs:
        - **attention_out** (Tensor) - Input tensor of shape :math:`(B, S, H)` / `(B, N, S, D)`.
    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore.ops.operations.nn_ops as P
        >>> from mindspore import Tensor
        >>> import numpy as np
        >>> B = 1
        >>> N = 16
        >>> S = 256
        >>> D = 16
        >>> query = Tensor(np.ones((B, N, S, D), dtype=np.float16))
        >>> key = Tensor(np.ones((B, N, S, D), dtype=np.float16))
        >>> value = Tensor(np.ones((B, N, S, D), dtype=np.float16))
        >>> attn_mask = Tensor(np.ones((B, 1, S, S), dtype=np.float16))
        >>> pfa = P.PromptFlashAttention(N, input_layout='BNSD')
        >>> out = pfa(query, key, value, attn_mask, None, None, None, None, None, None, None, None)
        >>> print(out[0].shape)
        (1, 16, 256, 16)
    """
```

å…¶ä¸­pre_tokenå’Œnext_tokençš„æ„ä¹‰ä¸ºå°†ä¸€ä¸ª`attention_mask`çš„å·¦ä¸Šè§’å‘å³åç§»`next_tokens`
ä¸ªä½ç½®ï¼Œä»è¿™ä¸ªä½ç½®å‘å³ä¸‹45Â°ç”»ä¸€æ¡çº¿ï¼›å³ä¸‹è§’å‘å·¦åç§»`pre_tokens`
ä¸ªä½ç½®ï¼Œå‘å·¦ä¸Š45Â°ç”»ä¸€æ¡çº¿ã€‚è¿™ä¸¤æ¡çº¿ç›¸äº¤çš„ä½ç½®ä¸ºæœ‰æ•ˆçš„`attention_mask`ã€‚å…¶ä»–çš„å…¥å‚æ„ä¹‰æ¯”è¾ƒå¥½ç†è§£ï¼Œè§ä¸Šè¿°APIæ–‡æ¡£ã€‚

### IncreFlashAttention

```python
class IncreFlashAttention(Primitive):
    r"""
    The interface for fully inference.

    B -- Batch size

    S -- Sequence length

    H -- Hidden size

    .. warning::
        This is an experimental API that is subject to change or deletion.

    Inputs:
        - **query** (Tensor) - The query tensor with data type of float16 or bfloat16.
          Input tensor of shape :math:`(B, 1, H)` / :math:`(B, N, 1, D)`.
        - **key** (TensorList) - The key tensor with data type of float16 or bfloat16.
          Input tensor of shape :math:`(B, S, H)` / :math:`(B, N, S, D)`.
        - **value** (TensorList) - The value tensor with data type of float16 or bfloat16.
          Input tensor of shape :math:`(B, S, H)` / :math:`(B, N, S, D)`.
        - **attn_mask** (Tensor) - The attention mask tensor with data type of float16 or bool.
          Input tensor of shape :math:`(B, S)` / :math:`(B, 1, S)` / :math:`(B, 1, 1, S)`.
        - **actual_seq_lengths** (Tensor) - Describe actual sequence length of each input with data type of int.
        - **padding_mask** (Tensor) - The padding mask tensor with data type of float16.
        - **dequant_scale1** (Tensor) - Quantitative parametor, the tensor with data type of uint64.
        - **quant_scale1** (Tensor) - Quantitative parametor, the tensor with data type of float.
        - **dequant_scale2** (Tensor) - Quantitative parametor, the tensor with data type of uint64.
        - **quant_scale2** (Tensor) - Quantitative parametor, the tensor with data type of float.
        - **quant_offset2** (Tensor) - Quantitative parametor, the tensor with data type of float.
        - **antiquant_scale** (Tensor) - Quantitative parametor, the tensor with data type of float.
        - **antiquant_offset** (Tensor) - Quantitative parametor, the tensor with data type of float.
        - **block_table** (Tensor) - The tensor with data type of float.
        - **num_heads**  (int) - The number of heads.
        - **input_layout** (str) - the data layout of the input qkv, support `(BSH)` and `(BNSD)`. Default `BSH`.
        - **scale_value** (double) - The scale value indicating the scale coefficient, which is used as the scalar of
          Muls in the calculation. Default: 1.0.
        - **num_key_value_heads** (int) - head numbers of key/value which are used in GQA algorithm.
          The value o indicates if the key and value have the same head nums, use numHeads.  Default: 0.
        - **block_size** (int) - Default: 0.
        - **inner_precise** (int) - Default: 1.

    Outputs:
        - **attention_out** (Tensor) - Input tensor of shape :math:`(B, 1, H)` / :math:`(B, N, 1, D)`.

    Supported Platforms:
        ``Ascend``
    """
```

IFAçš„å…¥å‚å’ŒPFAçš„å…¥å‚åŸºæœ¬ä¸€è‡´ï¼Œå‚è€ƒä¸Šè¿°APIæ–‡æ¡£ï¼Œä½†æ³¨æ„IFAä¸ºä»…æ”¯æŒéé¦–æ¬¡æ¨ç†çš„å¢é‡å›¾é‡Œåœºæ™¯ï¼Œæ‰€ä»¥`key`çš„`seq_length`ä¸º1ã€‚

## ä½¿ç”¨æ–¹æ³•

åœ¨GPT2ä¸­PFAçš„å®šä¹‰å’Œä½¿ç”¨å¦‚ä¸‹ï¼š

```python
self.prompt_flash_attention = PromptFlashAttention(num_heads=num_heads,
                                                   scale_value=1.0,
                                                   pre_tokens=self.src_seq_length,
                                                   next_tokens=0,
                                                   input_layout='BNSD',
                                                   num_key_value_heads=0)
attention = self.prompt_flash_attention(query, key, value, attention_mask,
                                        None, None, None, None, None, None, None, None)[0]
```

åœ¨GPT2ä¸­IFAçš„å®šä¹‰ä»¥åŠä½¿ç”¨å¦‚ä¸‹ï¼š

```python
self.incre_flash_attention = IncreFlashAttention(num_heads=num_heads,
                                                 scale_value=1.0,
                                                 input_layout='BNSD',
                                                 num_key_value_heads=0)
attention = self.incre_flash_attention(query, key, value, attention_mask,
                                       None, None, None, None, None, None, None, None)
```

IFAçš„è¾“å…¥ç›®å‰å¯ä»¥å‚è€ƒPFAï¼Œä½†æ˜¯`query`å’Œ`value`åœ¨æ–‡æ¡£ä¸­æ”¯æŒçš„æ˜¯TensorListï¼Œå¯èƒ½ä¸å½“å‰ç”¨ä¾‹ä¸å¤ªä¸€è‡´ï¼Œæ¥å£å¯èƒ½ä¼šæœ‰è°ƒæ•´ã€‚è¯·ä»¥æœ€æ–°çš„æ–‡æ¡£ä¸ºå‡†ã€‚

## ä»£ç ä¿®æ”¹

PFAå’ŒIFAçš„è°ƒç”¨å¯ä»¥ç›´æ¥æ›¿æ¢åŸæœ‰çš„`_attn`çš„é€»è¾‘å³å¯ã€‚æœ‰ä»¥ä¸‹å‡ ç‚¹éœ€è¦ä¿®æ”¹ï¼š

1. attnä¸­çš„merge_headåœ¨PFAå’ŒIFAä¸­æ²¡æœ‰ï¼Œéœ€è¦å•ç‹¬è°ƒç”¨ã€‚
2. å¦‚æœè®¾ç½®scale_valueä¸º1.0æ—¶ï¼Œé‚£ä¹ˆæ‰‹åŠ¨å°†è¾“å…¥è¿›è¡Œnormalize(é™¤ä»¥sqrt(d))ï¼Œå¦åˆ™è®¡ç®—å°†ä¼šä¸ç­‰ä»·ã€‚
3. attention_maskçš„ç¿»è½¬é€»è¾‘åŸæ¥åœ¨attnä¸­è®¡ç®—ï¼Œç°åœ¨éœ€è¦å°†ç¿»è½¬å®Œçš„attention_maskä½œä¸ºå…¥å‚ä¼ å…¥PFAå’ŒIFAä¸­ã€‚

```python
if not self.training and self.use_prompt_flash_attention:
    if self.use_past and not self.is_first_iteration:
        if self.use_incre_flash_attention:
            query, key, attention_mask = self._pfa_ifa_data_preprocess(query, key, attention_mask,
                                                                       batch_valid_length)
            attention = self.incre_flash_attention(query, key, value, attention_mask,
                                                   None, None, None, None, None, None, None, None)
            attention = self._merge_heads(attention)
        else:
            key = self.transpose(key, (0, 1, 3, 2))
            attention = self._attn(query, key, value, attention_mask, batch_valid_length)
    else:
        query, key, attention_mask = self._pfa_ifa_data_preprocess(query, key, attention_mask,
                                                                   batch_valid_length)
        attention = self.prompt_flash_attention(query, key, value, attention_mask,
                                                None, None, None, None, None, None, None, None)[0]
        attention = self._merge_heads(attention)
elif self.use_flash_attention:
    attention = self._flash_attn(query, key, value, attention_mask)
else:
    attention = self._attn(query, key, value, attention_mask, batch_valid_length)
```
