# Chat Web

## åŸºæœ¬ä»‹ç»

Chat Webæä¾›äº†ä¸€å¥—å¯¹è¯æ¨ç†æœåŠ¡(chat server)å’Œç½‘é¡µåº”ç”¨ï¼ˆweb demoï¼‰ï¼Œè®©ç”¨æˆ·å¯ä»¥é€šè¿‡ç±»ä¼¼çº¿ä¸ŠèŠå¤©çš„æ–¹å¼ä½¿ç”¨MindFormerså¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›ã€‚

æ¨ç†æœåŠ¡æ”¯æŒæ‹‰èµ·å•å¡æˆ–å¤šå¡çš„æ¨ç†ä»»åŠ¡ï¼Œå¹¶æä¾›APIæ¥å£ï¼Œå¯ä»¥ä½¿ç”¨æä¾›çš„web demoæˆ–è€…ä½¿ç”¨postmanè®¿é—®æ¨ç†æœåŠ¡ã€‚

> æ³¨ï¼šæ¨ç†æœåŠ¡ä»…ä¾›demoä½¿ç”¨ï¼Œä¸æ”¯æŒå¹¶å‘æœåŠ¡å’ŒåŠ¨æ€èµ„æºåˆ†é…ã€‚

## æ”¯æŒæ¨¡å‹

Chat Webæ”¯æŒMindFormersä¸­æ‰€æœ‰[æ”¯æŒText Generationä»»åŠ¡çš„å¤§è¯­è¨€æ¨¡å‹](../model_support_list.md#textgeneration)ã€‚
å…¶ä¸­ï¼Œåœ¨`mindformers/models`ç›®å½•ä¸‹çš„æ¨¡å‹å¯ä»¥ç›´æ¥ä½¿ç”¨ã€‚è€Œåœ¨`research`ç›®å½•ä¸‹çš„æ¨¡å‹çš„ä½¿ç”¨åˆ™éœ€è¦ä¸€äº›é¢å¤–çš„æ“ä½œï¼Œè¯¦è§[æ‰©å±•åŠŸèƒ½-ä½¿ç”¨researchç›®å½•ä¸‹çš„æ¨¡å‹](#ä½¿ç”¨researchç›®å½•ä¸‹çš„æ¨¡å‹)

## ä½¿ç”¨æ–¹å¼

### å®‰è£…ä¾èµ–

è¿è¡Œå¦‚ä¸‹å‘½ä»¤å®‰è£…Chat Webæ‰€éœ€ä¾èµ–ï¼š

```bash
cd chat_web
pip install -r requirements.txt
```

### å¯åŠ¨æ¨ç†æœåŠ¡

Chat Webæ”¯æŒæ‹‰èµ·å•å¡å’Œå¤šå¡ä»»åŠ¡ã€‚é¦–å…ˆï¼Œéœ€è¦è¿›å…¥Chat Webç›®å½•ä¸‹

```bash
cd chat_web
```

#### å•å¡å¯åŠ¨

1. å‡†å¤‡æ¨¡å‹æƒé‡

    åœ¨æ¨¡å‹yamlæ–‡ä»¶ä¸­é…ç½®å•å¡æƒé‡

    ```yaml
    model:
      model_config:
        checkpoint_name_or_path: ""  # æ­¤å¤„é…ç½®æ¨¡å‹åæˆ–æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„
    ```

    - å¦‚æœé…ç½®æ¨¡å‹åï¼ˆä¾‹å¦‚`glm2_6b`ï¼‰ï¼Œåˆ™ä¼šè‡ªåŠ¨ä»obsä¸Šä¸‹è½½æ¨¡å‹çš„é¢„è®­ç»ƒæƒé‡ã€‚
    - å¦‚æœé…ç½®æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„ï¼Œåˆ™ä¼šåŠ è½½æŒ‡å®šæƒé‡æ–‡ä»¶ã€‚

2. é…ç½®`config/config.yaml`

    `config/config.yaml`åŒ…å«Chat Webæ‰€éœ€çš„æ‰€æœ‰é…ç½®é¡¹ï¼Œé…ç½®é¡¹ä»‹ç»è§[é™„å½•-config.yamlé…ç½®é¡¹ç®€ä»‹](#configyamlé…ç½®é¡¹ç®€ä»‹)

    - ä¿®æ”¹é…ç½®æ–‡ä»¶ä¸­`model`éƒ¨åˆ†
        - `config`è®¾ç½®ä¸ºæ¨¡å‹yamlæ–‡ä»¶è·¯å¾„ï¼Œæ¨¡å‹yamlæ–‡ä»¶ä¿®æ”¹é€»è¾‘å’Œrun_mindformers.pyå¯åŠ¨æ¨ç†ç›¸åŒï¼Œå¯ä»¥å‚è€ƒ[ä½¿ç”¨æ ·ä¾‹](#ä½¿ç”¨æ ·ä¾‹)è¿›è¡Œä¿®æ”¹ã€‚
        - `device_num`è®¾ç½®ä¸º1ã€‚
        - `device_id`è®¾ç½®ä¸ºç©ºé—²çš„NPUå¡å·ã€‚

    - é…ç½®æ–‡ä»¶ä¸­å…¶ä»–é…ç½®ä¸€èˆ¬æƒ…å†µå¯ä»¥ä¸ä¿®æ”¹ï¼Œå¦‚ç«¯å£è¢«å ç”¨å¯ä»¥ä¿®æ”¹åˆ°ç©ºé—²çš„ç«¯å£å·ã€‚

3. å¯åŠ¨æœåŠ¡

    è¿è¡Œå¦‚ä¸‹å‘½ä»¤å¯åŠ¨æ¨ç†æœåŠ¡ï¼š

    ```bash
    python run_chat_server.py &> server.log &
    ```

    æœåŠ¡æ—¥å¿—è¢«é‡å®šå‘åˆ°server.logä¸­ã€‚

#### å¤šå¡å¯åŠ¨

1. å‡†å¤‡æ¨¡å‹åˆ†å¸ƒå¼æƒé‡

    > æ³¨ï¼šæ¨ç†æœåŠ¡ä¸æ”¯æŒæƒé‡è‡ªåŠ¨åˆ‡åˆ†ï¼Œéœ€è¦å…ˆå°†æƒé‡åˆ‡å¥½ã€‚æ¨èä½¿ç”¨`scripts/run_distribute.sh`æ‹‰èµ·å¤šå¡æ¨ç†å¹¶è¿›è¡Œæƒé‡è‡ªåŠ¨åˆ‡åˆ†ã€‚å…·ä½“æƒé‡åˆ‡åˆ†æ–¹æ³•è¯·å‚è€ƒ[æƒé‡è½¬æ¢æ–‡æ¡£](Transform_Ckpt.md)

    åˆ†å¸ƒå¼æƒé‡ç›®å½•ç»“æ„å¦‚ä¸‹ï¼š

    ```text
    model_ckpt_dir
      â”œâ”€rank_0
      â”‚   â””â”€ckpt_0.ckpt
      â”œâ”€rank_1
      â”‚   â””â”€ckpt_1.ckpt
      â””â”€...
          â””â”€...
    ```

    åœ¨æ¨¡å‹yamlæ–‡ä»¶ä¸­é…ç½®å¤šå¡æƒé‡

    ```yaml
    load_checkpoint: "model_ckpt_dir"  # æ­¤å¤„é…ç½®æ¨¡å‹æƒé‡æ–‡ä»¶å¤¹è·¯å¾„ï¼Œéœ€æŒ‡å®šrank_*ä¸Šçº§ç›®å½•
    ```

2. é…ç½®`config/config.yaml`

    `config/config.yaml`åŒ…å«Chat Webæ‰€éœ€çš„æ‰€æœ‰é…ç½®é¡¹ï¼Œé…ç½®é¡¹ä»‹ç»è§[é™„å½•-config.yamlé…ç½®é¡¹ç®€ä»‹](#configyamlé…ç½®é¡¹ç®€ä»‹)

    - ä¿®æ”¹é…ç½®æ–‡ä»¶ä¸­`model`éƒ¨åˆ†
        - `config`è®¾ç½®ä¸ºæ¨¡å‹yamlæ–‡ä»¶è·¯å¾„ï¼Œæ¨¡å‹yamlæ–‡ä»¶ä¿®æ”¹é€»è¾‘å’Œrun_mindformers.pyå¯åŠ¨æ¨ç†ç›¸åŒï¼Œå¯ä»¥å‚è€ƒ[ä½¿ç”¨æ ·ä¾‹](#ä½¿ç”¨æ ·ä¾‹)è¿›è¡Œä¿®æ”¹ã€‚
        - `device_num`è®¾ç½®ä¸ºä½¿ç”¨çš„å¡æ•°ã€‚
        - `device_range`è®¾ç½®ä¸ºä½¿ç”¨çš„å¡çš„èŒƒå›´ã€‚
        - `rank_table_file`è®¾ç½®ä¸ºrank_table_fileè·¯å¾„ã€‚

    - é…ç½®æ–‡ä»¶ä¸­å…¶ä»–é…ç½®ä¸€èˆ¬æƒ…å†µå¯ä»¥ä¸ä¿®æ”¹ï¼Œå¦‚ç«¯å£è¢«å ç”¨å¯ä»¥ä¿®æ”¹åˆ°ç©ºé—²çš„ç«¯å£å·ã€‚

    > æ³¨æ„ï¼šä¸MindFormerså…¶ä»–åˆ†å¸ƒå¼ä»»åŠ¡ç›¸ä¼¼åœ°ï¼Œè®¾ç½®çš„å¡æ•°ã€rank_table_fileã€æ¨¡å‹yamlä¸­çš„åˆ‡åˆ†ç­–ç•¥ã€æƒé‡æ–‡ä»¶çš„åˆ‡åˆ†ç­–ç•¥éœ€è¦ç›¸åŒ¹é…ã€‚å»ºè®®ä½¿ç”¨åŒä¸€ä¸ªæ¨¡å‹yamlæ–‡ä»¶è¿›è¡Œæƒé‡åˆ‡åˆ†å’Œæ­¤æ¨ç†æœåŠ¡ã€‚

3. å¯åŠ¨æœåŠ¡

    è¿è¡Œå¦‚ä¸‹å‘½ä»¤å¯åŠ¨æ¨ç†æœåŠ¡ï¼š

    ```bash
    python run_chat_server.py &> server.log &
    ```

    æœåŠ¡æ—¥å¿—è¢«é‡å®šå‘åˆ°server.logä¸­ã€‚

### ä½¿ç”¨ç½‘é¡µåº”ç”¨è®¿é—®æ¨ç†æœåŠ¡

ç½‘é¡µåº”ç”¨åœ¨`config/config.yaml`ä¸­çš„é»˜è®¤é…ç½®å¦‚ä¸‹ï¼š

```yaml
web_demo:
  host: 0.0.0.0
  port: 7860
```

è¿è¡Œå¦‚ä¸‹å‘½ä»¤å¯åŠ¨ç½‘é¡µåº”ç”¨ï¼š

```bash
python run_chat_web_demo.py &> web.log &
```

æ—¥å¿—è¢«é‡å®šå‘åˆ°web.logä¸­ã€‚

ç½‘é¡µåº”ç”¨é»˜è®¤è¿è¡Œåœ¨`7860`ç«¯å£ï¼Œå¦‚æœé»˜è®¤ç«¯å£è¢«å ç”¨å¯ä»¥åœ¨`config/config.yaml`ä¸­ä¿®æ”¹`web_demo - port`é…ç½®é¡¹ã€‚

é€šè¿‡æµè§ˆå™¨è®¿é—®Chat Webç½‘é¡µåœ°å€

> å¦‚æœåº”ç”¨å¯åŠ¨åœ¨æœ¬åœ°ï¼Œå³é…ç½®äº†`web_demo - host`ä¸º`127.0.0.1`ï¼Œåˆ™è®¿é—®`http://127.0.0.1:7860`æˆ–`http://localhost:7860`
>
> å¦‚æœåº”ç”¨å¯åŠ¨åœ¨è¿œç¨‹ï¼Œå³é…ç½®äº†`web_demo - host`ä¸º`0.0.0.0`ï¼Œå‡è®¾æœåŠ¡å™¨IPåœ°å€ä¸º`12.23.34.45`ï¼Œåˆ™è®¿é—®`http://12.23.34.45:7860`

åœ¨è¾“å…¥æ¡†ä¸­è¾“å…¥æ–‡å­—ï¼Œç‚¹å‡»***æäº¤***æŒ‰é’®ï¼Œç¨ç­‰ç‰‡åˆ»åå±å¹•ä¸Šå°†ä¼šæ˜¾ç¤ºLLMçš„å›ç­”ã€‚ç‚¹å‡»***æ¸…é™¤***æŒ‰é’®å¯ä»¥æ¸…ç©ºèŠå¤©è®°å½•ã€‚

#### é…ç½®é¡¹

èŠå¤©ç•Œé¢å³ä¾§æä¾›äº†è‹¥å¹²é…ç½®é¡¹ï¼Œå¯ä»¥åœ¨ç‚¹å‡»æäº¤æŒ‰é’®å‰è‡ªè¡Œè¿›è¡Œé…ç½®ï¼Œæ¯æ¬¡è¾“å…¥å°†ä¼šå®æ—¶ç”Ÿæ•ˆã€‚ç›®å‰æä¾›çš„é…ç½®é¡¹å¦‚ä¸‹ï¼š

- **sampling** (å¼€å…³) - æ‰“å¼€è¡¨ç¤ºä½¿ç”¨é‡‡æ ·ï¼›å…³é—­åˆ™è¡¨ç¤ºä½¿ç”¨è´ªå¿ƒè§£ç ã€‚æ‰“å¼€åå¯ä»¥è°ƒæ•´ä¸‹è¿°**top k**å’Œ**top p**ã€‚

    - **top k** (æ»‘å—) - ä»å‰kä¸ªå¯èƒ½æ€§æœ€å¤§çš„å€™é€‰è¯ä¸­é‡‡æ ·ã€‚å–å€¼èŒƒå›´ï¼š`[0,10]`ã€‚
    - **top p** (æ»‘å—) - ä»å¯èƒ½æ€§åŠ èµ·æ¥ä¸ºpçš„å€™é€‰è¯ä¸­é‡‡æ ·ã€‚å–å€¼èŒƒå›´ï¼š`[0,1]`ã€‚
    - **temperature** (è¾“å…¥æ¡†) - ç”¨æ¥è°ƒèŠ‚å€™é€‰è¯çš„å¯èƒ½æ€§å¾—åˆ†ã€‚å–å€¼èŒƒå›´ï¼š`(0,âˆ)`ã€‚

- **repetition penalty** (è¾“å…¥æ¡†) - é‡å¤æƒ©ç½šå› å­ã€‚`1.0`è¡¨ç¤ºæ— æƒ©ç½šã€‚å–å€¼èŒƒå›´ï¼š`(0,âˆ)`ã€‚

- **max length** (è¾“å…¥æ¡†) - è¾“å…¥ä¸å›ç­”çš„æœ€å¤§é•¿åº¦ï¼Œä¸èƒ½è¶…è¿‡æ¨¡å‹çš„`seq_length`ï¼ˆæ³¨æ„ï¼šå¤šè½®å¯¹è¯æ—¶ï¼Œè¾“å…¥å°†åŒ…æ‹¬å‰å‡ è½®å¯¹è¯ï¼‰ã€‚å–å€¼èŒƒå›´ï¼š`(è¾“å…¥é•¿åº¦,seq_length)`ã€‚

- **prompt** (è¾“å…¥æ¡†) - æç¤ºè¯æ¨¡æ¿ï¼Œä¸è¾“å…¥æ‹¼æ¥åä¼ è¿›æ¨¡å‹ã€‚è¾“å…¥æ¡†ä¸‹æ–¹æä¾›äº†ä¸€äº›æ ·ä¾‹æ¨¡æ¿ï¼Œç”¨æˆ·ä¹Ÿå¯ä»¥è¾“å…¥è‡ªå®šä¹‰çš„æ¨¡æ¿ï¼Œéœ€è¦åŒ…å«å ä½ç¬¦`{}`ï¼Œä»£è¡¨æ›¿æ¢è¾“å…¥çš„ä½ç½®ã€‚

### ä½¿ç”¨postmanè®¿é—®æ¨ç†æœåŠ¡

postmanæ˜¯ä¸€æ¬¾APIåœ¨çº¿è°ƒè¯•çš„å·¥å…·ï¼Œå¯ä»¥ç”¨æ¥å‘æ¨ç†æœåŠ¡å‘é€è¯·æ±‚å¹¶è·å–åŒ…å«æ¨ç†ç»“æœçš„å“åº”ã€‚[ä¸‹è½½é“¾æ¥](https://www.postman.com/downloads/)

å‘æ¨ç†æœåŠ¡çš„APIæ¥å£`http://xx.xx.xx.xx:11111/generate` å‘é€POSTè¯·æ±‚ï¼Œå…¶ä¸­`xx.xx.xx.xx`ä¸ºè¿è¡Œæ¨ç†æœåŠ¡çš„æœåŠ¡å™¨IPåœ°å€ï¼Œç«¯å£å·ä»¥å®é™…è®¾ç½®ä¸ºå‡†ã€‚

è¯·æ±‚ä½“å¦‚ä¸‹ï¼Œå…¶ä¸­`"content"`ä¸­ä¸ºè¾“å…¥çš„é—®é¢˜ï¼Œ`"stream"`æ§åˆ¶æ˜¯å¦ä¸ºæµå¼å“åº”ï¼Œå…¶ä½™å‚æ•°å«ä¹‰å‚è€ƒä¸Šä¸€èŠ‚ï¼š

```json
{
    "messages": [
        {
            "role": "user",
            "content": ""
        }
    ],
    "max_length": 128,
    "do_sample": true,
    "top_k": 3,
    "top_p": 0.8,
    "temperature": 1.0,
    "repetition_penalty": 1.05,
    "stream": false
}
```

ç¤ºæ„å›¾å¦‚ä¸‹ï¼š

![postman](assets/Chat_Web/postman.png)

## æ‰©å±•åŠŸèƒ½

### æç¤ºè¯æ¨¡æ¿

æç¤ºè¯æ¨¡æ¿ï¼ˆpromptï¼‰æŒ‡ç»™æ¨¡å‹çš„ä¸€ä¸ªåˆå§‹è¾“å…¥æˆ–æç¤ºï¼Œç”¨äºå¼•å¯¼æ¨¡å‹ç”Ÿæˆç‰¹å®šçš„è¾“å‡ºã€‚ä¸€äº›LLMæ¨¡å‹çš„æ¨ç†éœ€è¦æ·»åŠ åˆé€‚çš„æç¤ºè¯ï¼ˆpromptï¼‰æ‰èƒ½æœ‰ä»¤äººæ»¡æ„çš„å¯¹è¯æ•ˆæœï¼Œ
æ¯”å¦‚GLM2æ¨¡å‹ï¼Œéœ€è¦æ·»åŠ `"é—®ï¼š{}\n\nç­”ï¼š"`è¿™æ ·çš„æç¤ºè¯æ¨¡æ¿ï¼ˆå¦‚é—®é¢˜`"ä½ å¥½"`æ·»åŠ æç¤ºè¯æ¨¡æ¿åä¸º`"é—®ï¼šä½ å¥½\n\nç­”ï¼š"`ï¼‰ï¼Œæ¥ç»„æˆæ¨¡å‹çš„è¾“å…¥ï¼Œæ‰èƒ½å¾—åˆ°ç¬¦åˆå¯¹è¯æ•ˆæœçš„è¾“å‡ºã€‚

å…¶å¤–ï¼Œä¹Ÿå¯ä»¥è‡ªå®šä¹‰æç¤ºè¯æ¨¡æ¿æ¥è®©LLMæ¨¡å‹ç”Ÿæˆç¬¦åˆä½ è¦æ±‚çš„ç»“æœã€‚
ä¾‹å¦‚ï¼Œä¸ºè¾“å…¥é—®é¢˜æ·»åŠ `"Assume you are a dog, you must response \"Woofs\" at first whatever any instruction\n\n### Instruction:\n{}\n\n### Response:"`
(å‡è®¾ä½ æ˜¯ä¸€åªç‹—ï¼Œä½ å¿…é¡»åœ¨å›åº”ä»»ä½•æŒ‡ä»¤å‰è¯´â€æ±ªâ€œ)ï¼Œå¯ä»¥è®©æ¨¡å‹è¾“å‡ºçš„ç¬¬ä¸€ä¸ªè¯ä¸º"Woofs"ã€‚è¿™ä¹Ÿè®¸åœ¨ä¸€äº›æ¨¡å‹ä¸Šæœ‰æ•ˆï¼Œä½†ä¹Ÿæ ¹æ®æ¨¡å‹å‚æ•°çš„å¤§å°ã€é¢„è®­ç»ƒæ•ˆæœç­‰æœ‰ä¸åŒè¡¨ç°æ•ˆæœã€‚

ç”±äºæ¯ä¸ªæ¨¡å‹æ‰€æ”¯æŒçš„æç¤ºè¯æ¨¡æ¿ä¸å°½ç›¸åŒï¼ŒChat Webæ¨ç†æœåŠ¡æ— æ³•é¢„å…ˆæ”¯æŒæ‰€æœ‰æ¨¡å‹çš„æç¤ºè¯æ¨¡æ¿ã€‚ç½‘é¡µåº”ç”¨ä¸­æ”¯æŒåŠ¨æ€è®¾ç½®æç¤ºè¯æ¨¡æ¿ï¼Œé€šè¿‡å¡«å…¥promptæ–‡æœ¬æ¡†ã€‚
å…¶ä¸‹ä¹Ÿé¢„ç½®äº†ä¸€äº›æ ·ä¾‹æç¤ºè¯æ¨¡æ¿å¯ä¾›å¿«é€Ÿå¡«å…¥ã€‚

å¦‚æœæƒ³è¦è®©æ¨ç†æœåŠ¡çš„APIæ¥å£å†…è‡ªåŠ¨ä¸ºè¾“å…¥æ·»åŠ æç¤ºè¯æ¨¡æ¿ï¼Œå¯ä»¥é€šè¿‡é‡å†™`predict_process.py`æ–‡ä»¶ä¸­çš„`build_prompt`æ–¹æ³•æ¥å®ç°æƒ³è¦çš„æç¤ºè¯æ•ˆæœã€‚
å‡è®¾ä½¿ç”¨GLM2æ¨¡å‹è¿›è¡Œæ¨ç†ï¼Œå¦‚æœæƒ³ä¸ºæ‰€æœ‰è¾“å…¥æ·»åŠ æ¨¡å‹é»˜è®¤çš„æç¤ºè¯æ¨¡æ¿ï¼Œå¯ä»¥å°†`build_prompt`æ–¹æ³•ä¿®æ”¹æˆå¦‚ä¸‹ï¼š

```python
def build_prompt(inputs: str):
    """Build prompt"""
    prompt = "é—®ï¼š{}\n\nç­”ï¼š"  # You can modify this to build prompt for your model input
    return prompt.format(inputs)
```

æ³¨æ„åœ¨æ­¤ä¾‹ä¸­`prompt`å¿…é¡»åŒ…å«`{}`å ä½ç¬¦ï¼Œæ­¤å ä½ç¬¦ä¼šæ›¿æ¢ä¸ºè¾“å…¥çš„é—®é¢˜ã€‚

### å¤šè½®å¯¹è¯

æœ‰çš„LLMæ¨¡å‹æœ‰å¤šè½®å¯¹è¯çš„èƒ½åŠ›ï¼Œå¦‚GLM2ã€BaiChuan2ç­‰ã€‚å®ƒæ˜¯æŒ‡æ¨¡å‹ä¼šè®°ä½å‰å‡ è½®çš„å¯¹è¯å†…å®¹ï¼Œå¹¶å¯¹äºç”¨æˆ·æå‡ºçš„æ–°çš„æŒ‡ä»¤ä¼šç»™å‡ºç¬¦åˆä¸Šä¸‹æ–‡è¯­å¢ƒçš„å›å¤ã€‚

ä¸æç¤ºè¯æ¨¡æ¿ç±»ä¼¼ï¼Œå¤šè½®å¯¹è¯æ˜¯é€šè¿‡å°†å¯¹è¯çš„ä¸Šä¸‹æ–‡è¿›è¡Œæ‹¼æ¥ï¼Œå¹¶åœ¨æ¯è½®å¯¹è¯é—´æ·»åŠ ç‰¹å®šæ¨¡æ¿æ¥ä½œä¸ºæ¨¡å‹çš„è¾“å…¥ã€‚ä¾‹å¦‚GLM2æ¨¡å‹çš„å¤šè½®å¯¹è¯ä¼šæ‹¼æ¥æ¯è½®é—®é¢˜å’Œå›ç­”ï¼Œå¹¶åœ¨æ¯è½®å¯¹è¯å‰æ·»åŠ æ¨¡æ¿`"[Round n]\n\n"`ã€‚
å‡è®¾ä¸€ä¸ªä¸¤è½®çš„å¯¹è¯ï¼Œç»è¿‡æ·»åŠ æç¤ºè¯æ¨¡æ¿å’Œå¤šè½®å¯¹è¯æ‹¼æ¥åè¾“å…¥æ¨¡å‹çš„æœ€ç»ˆè¾“å…¥æ˜¯

```text
[Round 1]

é—®ï¼šä½ å¥½

ç­”ï¼šä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚
[Round 2]

é—®ï¼šè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±

ç­”ï¼š
```

è‹¥è¦ä½¿Chat Webæ¨ç†æœåŠ¡æ”¯æŒå¤šè½®å¯¹è¯ï¼Œå¯ä»¥é€šè¿‡é‡å†™`predict_process.py`æ–‡ä»¶ä¸­çš„`build_multi_round`æ–¹æ³•æ¥å®ç°æƒ³è¦çš„å¤šè½®å¯¹è¯æ•ˆæœã€‚
å‡è®¾ä½¿ç”¨GLM2æ¨¡å‹è¿›è¡Œæ¨ç†ï¼Œå¦‚æœæƒ³ä¸ºæ‰€æœ‰è¾“å…¥æ·»åŠ æ¨¡å‹é»˜è®¤çš„æç¤ºè¯æ¨¡æ¿ï¼Œå¯ä»¥å°†`build_multi_round`æ–¹æ³•ä¿®æ”¹æˆå¦‚ä¸‹ï¼š

```python
def build_multi_round(inputs, history):
    """Build multi round"""
    multi_round_prompt = "[Round {}]\n\n{}{}"  # You can modify this to build multi-round input for your model input
    prev_rounds = ""
    for i, (query, response) in enumerate(history):
        prev_rounds += multi_round_prompt.format(i, query, response)
    return prev_rounds + inputs
```

æ³¨æ„åœ¨æ­¤ä¾‹ä¸­`multi_round_prompt`ä¸­çš„ä¸‰ä¸ª`{}`å ä½ç¬¦ä¼šä¾æ¬¡æ›¿æ¢ä¸º"å¯¹è¯è½®æ¬¡"ã€"é—®é¢˜"ã€"å›ç­”"ã€‚

### ä½¿ç”¨`/research`ç›®å½•ä¸‹çš„æ¨¡å‹

ç”±äº`/research`ç›®å½•ä¸åœ¨MindFormersåŒ…å†…ï¼Œæ‰€ä»¥æ— æ³•ç›´æ¥åœ¨Chat Webæ¨ç†æœåŠ¡ä¸­ä½¿ç”¨ã€‚å¯ä»¥é€šè¿‡å¦‚ä¸‹æ­¥éª¤è°ƒç”¨`/research`ç›®å½•ä¸‹çš„æ¨¡å‹ï¼Œå¦‚BaiChuan2ï¼ŒInternLMç­‰ã€‚

1. æ‹·è´æ•´ä¸ªæ¨¡å‹æ–‡ä»¶å¤¹ï¼ˆä¾‹å¦‚`/baichuan2`ã€`/internlm`ï¼‰è‡³`Chat_web`ç›®å½•ä¸‹
2. é‡å†™`predict_process.py`æ–‡ä»¶ä¸­çš„`get_model`å’Œ`get_tokenizer`æ–¹æ³•ã€‚å…·ä½“å®ä¾‹åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨çš„ä»£ç è¯·å‚è€ƒæ¨¡å‹é…å¥—è¯´æ˜æ–‡æ¡£ã€‚

    å‡è®¾ä½¿ç”¨BaiChuan2 13Bæ¨¡å‹ï¼Œåˆ™å°†ä¸¤ä¸ªæ–¹æ³•é‡å†™å¦‚ä¸‹ï¼š

```python
def get_model(config):
    from mindformers import LlamaConfig
    from baichuan2.baichuan2_13b import Baichuan13BV2ForCausalLM

    model_config = LlamaConfig(**config.model.model_config)
    model = Baichuan13BV2ForCausalLM(model_config)
    return model


def get_tokenizer(config):
    from baichuan2.baichuan2_tokenizer import Baichuan2Tokenizer

    tokenizer = Baichuan2Tokenizer(config.processor.tokenizer.vocab_file)
    return tokenizer
```

å…¶ä»–æ­¥éª¤ä¸[ä½¿ç”¨æ–¹å¼](#ä½¿ç”¨æ–¹å¼)ç›¸åŒã€‚

## ä½¿ç”¨æ ·ä¾‹

### 1. æ¨ç†æœåŠ¡ä½¿ç”¨GLM2 6Bè¿›è¡Œå•å¡æ¨ç†

#### å‡†å¤‡æ¨¡å‹yamlæ–‡ä»¶

ä½¿ç”¨å¦‚ä¸‹`predict_glm2_6b.yaml`æ–‡ä»¶ï¼Œå°†å…¶æ”¾ç½®åœ¨`config`ç›®å½•ä¸‹:

```yaml
# ==== context config ====
context:
  mode: 0 #0--Graph Mode; 1--Pynative Mode
  device_target: "Ascend"
  enable_graph_kernel: False
  graph_kernel_flags: "--disable_expand_ops=Softmax,Dropout --enable_parallel_fusion=true --reduce_fuse_depth=8 --enable_auto_tensor_inplace=true"
  max_call_depth: 10000
  max_device_memory: "30GB" # 59GB for Atlas 800T A2
  save_graphs: False
  device_id: 0

# ==== model config ====
model:
  model_config:
    type: ChatGLM2Config
    batch_size: 1   # only for incremental infer
    num_layers: 28
    padded_vocab_size: 65024
    hidden_size: 4096
    ffn_hidden_size: 13696
    kv_channels: 128
    num_attention_heads: 32
    seq_length: 193
    hidden_dropout: 0.0
    attention_dropout: 0.0
    layernorm_epsilon: 1e-5
    rmsnorm: True
    apply_residual_connection_post_layernorm: False
    post_layer_norm: True
    add_bias_linear: False
    add_qkv_bias: True
    bias_dropout_fusion: True
    multi_query_attention: True
    multi_query_group_num: 2
    apply_query_key_layer_scaling: True
    attention_softmax_in_fp32: True
    fp32_residual_connection: False
    quantization_bit: 0
    pre_seq_len: None
    prefix_projection: False
    param_init_type: "float16"
    compute_dtype: "float16"
    layernorm_compute_type: "float32"
    use_past: True
    eos_token_id: 2
    pad_token_id: 0
    repetition_penalty: 1.0
    max_decode_length: 256
    checkpoint_name_or_path: "glm2_6b"
    top_k: 1
    top_p: 1
    do_sample: True
  arch:
    type: ChatGLM2ForConditionalGeneration

trainer:
  type: CausalLanguageModelingTrainer
  model_name: 'glm2_6b'
# if True do, evaluate during the training process. if false, do nothing.
# note that the task trainer should support _evaluate_in_training function.
do_eval: False
eval_step_interval: 500
eval_epoch_interval: -1

processor:
  return_tensors: ms
  tokenizer:
    type: ChatGLM2Tokenizer
    bos_token: '<sop>'
    eos_token: '<eop>'
    end_token: '</s>'
    mask_token: '[MASK]'
    gmask_token: '[gMASK]'
    pad_token: '<pad>'
    unk_token: '<unk>'
  type: GLMProcessor

# parallel config
use_parallel: False
parallel:
  parallel_mode: 1 # 0-dataset, 1-semi, 2-auto, 3-hybrid
  gradients_mean: False
  loss_repeated_mean: True
  enable_alltoall: False
  full_batch: True
  search_mode: "sharding_propagation"
  enable_parallel_optimizer: True  # optimizer shard
  strategy_ckpt_config:
    save_file: "./ckpt_strategy.ckpt"
parallel_config:
  data_parallel: 1
  model_parallel: 1
  pipeline_stage: 1
  expert_parallel: 1
  micro_batch_num: 1
  vocab_emb_dp: True
  gradient_aggregation_group: 4
micro_batch_interleave_num: 1
```

#### é…ç½®`config/config.yaml`

ä½¿ç”¨å¦‚ä¸‹`config.yaml`æ–‡ä»¶ï¼š

```yaml
server:
  host: "0.0.0.0"
  port: 11111  # the port that server is listening
  uvicorn_level: 'info'
  access_log: True  # whether open server logging

web_demo:
  host: "0.0.0.0"
  port: 7860

model:
  config: "config/predict_glm2_6b.yaml"
  device_num: 1
  device_id: 0
  rank_table_file: ""
  hccl_connect_time: "3600"

default_generation_args:
  do_sample: False
  top_p: 1
  top_k: 1
  temperature: 1
  repetition_penalty: 1
  max_length: 512
```

#### å¯åŠ¨æ¨ç†æœåŠ¡

è¿è¡Œå¦‚ä¸‹å‘½ä»¤å¯åŠ¨æ¨ç†æœåŠ¡ï¼š

```bash
python run_chat_server.py &> server.log &
```

è¿è¡Œå¦‚ä¸‹å‘½ä»¤æŸ¥çœ‹æœåŠ¡æ—¥å¿—ï¼š

```bash
tail -f server.log
```

æœ‰å¦‚ä¸‹æ—¥å¿—ä»£è¡¨è¿è¡ŒæˆåŠŸï¼š

```text
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:11111 (Press CTRL+C to quit)
```

#### å¯åŠ¨ç½‘é¡µåº”ç”¨

è¿è¡Œå¦‚ä¸‹å‘½ä»¤å¯åŠ¨ç½‘é¡µåº”ç”¨ï¼š

```bash
python run_chat_web_demo.py &> web.log &
```

è¿è¡Œå¦‚ä¸‹å‘½ä»¤æŸ¥çœ‹ç½‘é¡µæ—¥å¿—ï¼š

```bash
tail -f web.log
```

å‡è®¾æœåŠ¡å™¨IPä¸º`192.168.10.10`ï¼Œæµè§ˆå™¨è®¿é—®`http://192.168.10.10:7860`ã€‚

å³ä¸‹è§’é€‰æ‹©GLM2é»˜è®¤æç¤ºè¯æ¨¡æ¿`"é—®ï¼š{}\n\nç­”ï¼š"`ï¼Œåœ¨è¾“å…¥æ¡†ä¸­è¾“å…¥é—®é¢˜ï¼Œå¹¶ç‚¹å‡»æäº¤ã€‚ç¤ºæ„å›¾å¦‚ä¸‹ï¼š

![glm2_demo](assets/Chat_Web/glm2_demo.png)

### 2. æ¨ç†æœåŠ¡ä½¿ç”¨LLaMA2 13Bè¿›è¡Œ2å¡æ¨ç†

#### å‡†å¤‡rank_table_file

è¿è¡Œå¦‚ä¸‹å‘½ä»¤ç”Ÿæˆrank_table_fileï¼š

```bash
python ../mindformers/tools/hccl_tools.py --device_num [0,2]
```

#### å‡†å¤‡2å¡åˆ†å¸ƒå¼æƒé‡

ä¸‹è½½[LLaMA2 13Bæƒé‡](https://ascend-repo-modelzoo.obs.cn-east-2.myhuaweicloud.com/MindFormers/llama2/llama2-13b-fp16.ckpt)

å‚è€ƒ[æƒé‡è½¬æ¢æ–‡æ¡£](Transform_Ckpt.md)ï¼Œå°†åˆ†å¸ƒå¼æƒé‡ç›®å½•å‘½åä¸º`"llama2_13b_ckpt"`ï¼Œç›®å½•æ ¼å¼å¦‚ä¸‹ï¼š

```text
llama2_13b_ckpt
  â”œâ”€rank_0
  â”‚   â””â”€ckpt_0.ckpt
  â””â”€rank_1
      â””â”€ckpt_1.ckpt
```

#### å‡†å¤‡æ¨¡å‹yamlæ–‡ä»¶

ä½¿ç”¨å¦‚ä¸‹`predict_llama2_13b.yaml`æ–‡ä»¶ï¼Œå°†å…¶æ”¾ç½®åœ¨`config`ç›®å½•ä¸‹:

```yaml
seed: 0
output_dir: './output' # path to save checkpoint/strategy
load_checkpoint: 'llama2_13b_ckpt'
src_strategy_path_or_dir: ''
auto_trans_ckpt: False  # If true, auto transform load_checkpoint to load in distributed model
only_save_strategy: False
resume_training: False
run_mode: 'predict'

# trainer config
trainer:
  type: CausalLanguageModelingTrainer
  model_name: 'llama2_13b'
# if True, do evaluate during the training process. if false, do nothing.
# note that the task trainer should support _evaluate_in_training function.
do_eval: False
eval_step_interval: -1        # num of step intervals between each eval, -1 means no step end eval.
eval_epoch_interval: 50        # num of epoch intervals between each eval, 1 means eval on every epoch end.

# runner config
runner_config:
  epochs: 2
  batch_size: 1
  sink_mode: True
  sink_size: 2

# optimizer
optimizer:
  type: FP32StateAdamWeightDecay
  beta1: 0.9
  beta2: 0.95
  eps: 1.e-8 # 1e-8
  learning_rate: 3.e-4

use_parallel: True
# parallel context config
parallel:
  parallel_mode: 1 # 0-data parallel, 1-semi-auto parallel, 2-auto parallel, 3-hybrid parallel
  gradients_mean: False
  enable_alltoall: False
  full_batch: True
  search_mode: "sharding_propagation"
  enable_parallel_optimizer: True
  strategy_ckpt_save_file: "./ckpt_strategy.ckpt"
  parallel_optimizer_config:
    gradient_accumulation_shard: False
    parallel_optimizer_threshold: 64
# default parallel of device num = 16 for Atlas 800
parallel_config:
  data_parallel: 1
  model_parallel: 2
  pipeline_stage: 1
  use_seq_parallel: False
  micro_batch_num: 16
  vocab_emb_dp: True
  gradient_aggregation_group: 4
# when model parallel is greater than 1, we can set micro_batch_interleave_num=2, that may accelerate the train process.
micro_batch_interleave_num: 1

# recompute config
recompute_config:
  recompute: True
  select_recompute: False
  parallel_optimizer_comm_recompute: False
  mp_comm_recompute: True
  recompute_slice_activation: True

# callbacks
callbacks:
  - type: MFLossMonitor
  - type: CheckpointMointor
    prefix: "llama_13b"
    save_checkpoint_steps: 100
    integrated_save: False
    async_save: False
  - type: ObsMonitor

# mindspore context init config
context:
  mode: 0 #0--Graph Mode; 1--Pynative Mode
  device_target: "Ascend"
  enable_graph_kernel: False
  graph_kernel_flags: "--disable_expand_ops=Softmax,Dropout --enable_parallel_fusion=true --reduce_fuse_depth=8 --enable_auto_tensor_inplace=true"
  max_call_depth: 10000
  max_device_memory: "31GB"
  save_graphs: False
  save_graphs_path: "./graph"
  device_id: 0

# model config
model:
  model_config:
    type: LlamaConfig
    batch_size: 1 # add for increase predict
    seq_length: 4096
    hidden_size: 5120
    num_layers: 40
    num_heads: 40
    vocab_size: 32000
    multiple_of: 256
    rms_norm_eps: 1.0e-5
    bos_token_id: 1
    eos_token_id: 2
    pad_token_id: 0
    ignore_token_id: -100
    compute_dtype: "float16"
    layernorm_compute_type: "float32"
    softmax_compute_type: "float16"
    rotary_dtype: "float16"
    param_init_type: "float16"
    use_past: True
    pretrain_seqlen: 4096 # seqlen of the pretrain checkpoint: 2048 for llama and 4096 for llama2
    extend_method: "None" # support "None", "PI", "NTK"
    compute_in_2d: False
    use_flash_attention: False
    offset: 0
    use_past_shard: False
    checkpoint_name_or_path: ""
    repetition_penalty: 1
    max_decode_length: 512
    top_k: 3
    top_p: 1
    do_sample: False
  arch:
    type: LlamaForCausalLM

processor:
  return_tensors: ms
  tokenizer:
    unk_token: '<unk>'
    bos_token: '<s>'
    eos_token: '</s>'
    pad_token: '<unk>'
    type: LlamaTokenizer
  type: LlamaProcessor

# metric
metric:
  type: PerplexityMetric

# wrapper cell config
runner_wrapper:
  type: MFTrainOneStepCell
  scale_sense:
    type: DynamicLossScaleUpdateCell
    loss_scale_value: 4294967296
    scale_factor: 2
    scale_window: 1000
  use_clip_grad: True

eval_callbacks:
  - type: ObsMonitor

auto_tune: False
filepath_prefix: './autotune'
autotune_per_step: 10

profile: False
profile_start_step: 1
profile_stop_step: 10
init_start_profile: False
profile_communication: False
profile_memory: True
layer_scale: False
layer_decay: 0.65
lr_scale_factor: 256

# aicc
remote_save_url: "Please input obs url on AICC platform."
```

#### é…ç½®`config/config.yaml`

ä½¿ç”¨å¦‚ä¸‹`config.yaml`æ–‡ä»¶ï¼š

```yaml
server:
  host: "0.0.0.0"
  port: 11111  # the port that server is listening
  uvicorn_level: 'info'
  access_log: True  # whether open server logging

web_demo:
  host: "0.0.0.0"
  port: 7860

model:
  config: "config/predict_llama2_13b.yaml"
  device_num: 2
  device_id: 0
  device_range: [0,2]
  rank_table_file: "hccl_2p_01_127.0.1.1.json"  # ä»¥å®é™…rank_table_fileæ–‡ä»¶å‘½åä¸ºå‡†
  hccl_connect_time: "3600"

default_generation_args:
  do_sample: False
  top_p: 1
  top_k: 1
  temperature: 1
  repetition_penalty: 1
  max_length: 512
```

#### å¯åŠ¨æ¨ç†æœåŠ¡

è¿è¡Œå¦‚ä¸‹å‘½ä»¤å¯åŠ¨æ¨ç†æœåŠ¡ï¼š

```bash
python run_chat_server.py &> server.log &
```

è¿è¡Œå¦‚ä¸‹å‘½ä»¤æŸ¥çœ‹æœåŠ¡æ—¥å¿—ï¼š

```bash
tail -f server.log
```

æœ‰å¦‚ä¸‹æ—¥å¿—ä»£è¡¨è¿è¡ŒæˆåŠŸï¼š

```text
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:11111 (Press CTRL+C to quit)
```

#### å¯åŠ¨ç½‘é¡µåº”ç”¨

è¿è¡Œå¦‚ä¸‹å‘½ä»¤å¯åŠ¨ç½‘é¡µåº”ç”¨ï¼š

```bash
python run_chat_web_demo.py &> web.log &
```

è¿è¡Œå¦‚ä¸‹å‘½ä»¤æŸ¥çœ‹ç½‘é¡µæ—¥å¿—ï¼š

```bash
tail -f web.log
```

å‡è®¾æœåŠ¡å™¨IPä¸º`192.168.10.10`ï¼Œæµè§ˆå™¨è®¿é—®`http://192.168.10.10:7860`ã€‚

llama2_13bé¢„è®­ç»ƒæƒé‡æ²¡æœ‰å¯¹è¯æ•ˆæœï¼Œåªæœ‰ç»­å†™çš„æ•ˆæœï¼Œåœ¨è¾“å…¥æ¡†ä¸­è¾“å…¥é—®é¢˜ï¼Œå¹¶ç‚¹å‡»æäº¤ã€‚ç¤ºæ„å›¾å¦‚ä¸‹ï¼š

![llama2_13b_demo](assets/Chat_Web/llama2_demo.png)

### 3. æ¨ç†æœåŠ¡ä½¿ç”¨BaiChuan2 13Bè¿›è¡Œ2å¡æ¨ç†

#### æ‹·è´æ¨¡å‹ç›®å½•

æ‹·è´`research/baichuan2`è‡³`Chat_web`ç›®å½•ä¸‹

#### é‡å†™æ–¹æ³•

é‡å†™`predict_process.py`æ–‡ä»¶ä¸­çš„`get_model`å’Œ`get_tokenizer`æ–¹æ³•ï¼Œå¦‚ä¸‹ï¼š

```python
def get_model(config):
    from mindformers import LlamaConfig
    from baichuan2.baichuan2_13b import Baichuan13BV2ForCausalLM

    model_config = LlamaConfig(**config.model.model_config)
    model = Baichuan13BV2ForCausalLM(model_config)
    return model


def get_tokenizer(config):
    from baichuan2.baichuan2_tokenizer import Baichuan2Tokenizer

    tokenizer = Baichuan2Tokenizer(config.processor.tokenizer.vocab_file)
    return tokenizer
```

#### å‡†å¤‡rank_table_file

è¿è¡Œå¦‚ä¸‹å‘½ä»¤ç”Ÿæˆrank_table_fileï¼š

```bash
python ../mindformers/tools/hccl_tools.py --device_num [0,2]
```

#### å‡†å¤‡2å¡åˆ†å¸ƒå¼æƒé‡

ä¸‹è½½[Baichuan2-13B-Chatæƒé‡](https://ascend-repo-modelzoo.obs.cn-east-2.myhuaweicloud.com/MindFormers/baichuan2/Baichuan2-13B-Chat.ckpt)

å‚è€ƒ[æƒé‡è½¬æ¢æ–‡æ¡£](Transform_Ckpt.md)ï¼Œå°†åˆ†å¸ƒå¼æƒé‡ç›®å½•å‘½åä¸º`"baichuan2_13b_ckpt"`ï¼Œç›®å½•æ ¼å¼å¦‚ä¸‹ï¼š

```text
baichuan2_13b_ckpt
  â”œâ”€rank_0
  â”‚   â””â”€ckpt_0.ckpt
  â””â”€rank_1
      â””â”€ckpt_1.ckpt
```

#### å‡†å¤‡æ¨¡å‹yamlæ–‡ä»¶

ä½¿ç”¨å¦‚ä¸‹`predict_llama2_13b.yaml`æ–‡ä»¶ï¼Œå°†å…¶æ”¾ç½®åœ¨`config`ç›®å½•ä¸‹:

```yaml
seed: 0
output_dir: './output' # path to save checkpoint/strategy
load_checkpoint: 'baichuan2_13b_ckpt'
src_strategy_path_or_dir: ''
auto_trans_ckpt: False  # If true, auto transform load_checkpoint to load in distributed model
only_save_strategy: False
resume_training: False
run_mode: 'predict'

# runner config
runner_config:
  epochs: 1
  batch_size: 1
  sink_mode: True
  sink_size: 2

use_parallel: True
# parallel context config
parallel:
  parallel_mode: 1 # 0-data parallel, 1-semi-auto parallel, 2-auto parallel, 3-hybrid parallel
  gradients_mean: False
  enable_alltoall: False
  full_batch: True
  search_mode: "sharding_propagation"
  enable_parallel_optimizer: True
  strategy_ckpt_save_file: "./ckpt_strategy.ckpt"
  parallel_optimizer_config:
    gradient_accumulation_shard: False
    parallel_optimizer_threshold: 64
# default parallel of device num = 16 for Atlas 800
parallel_config:
  data_parallel: 1
  model_parallel: 2
  pipeline_stage: 1
  use_seq_parallel: False
  micro_batch_num: 1
  vocab_emb_dp: True
  gradient_aggregation_group: 4
# when model parallel is greater than 1, we can set micro_batch_interleave_num=2, that may accelerate the train process.
micro_batch_interleave_num: 1

# recompute config
recompute_config:
  recompute: True
  select_recompute: False
  parallel_optimizer_comm_recompute: False
  mp_comm_recompute: True
  recompute_slice_activation: True

# mindspore context init config
context:
  mode: 0 #0--Graph Mode; 1--Pynative Mode
  device_target: "Ascend"
  enable_graph_kernel: False
  graph_kernel_flags: "--disable_expand_ops=Softmax,Dropout --enable_parallel_fusion=true --reduce_fuse_depth=8 --enable_auto_tensor_inplace=true"
  max_call_depth: 10000
  max_device_memory: "30GB"
  save_graphs: False
  save_graphs_path: "./graph"
  device_id: 0

# model config
model:
  model_config:
    type: LlamaConfig
    batch_size: 1 # add for increase predict
    seq_length: 512
    hidden_size: 5120
    num_layers: 40
    num_heads: 40
    vocab_size: 125696
    multiple_of: 128
    rms_norm_eps: 1.0e-6
    bos_token_id: 1
    eos_token_id: 2
    pad_token_id: 0
    ignore_token_id: -100
    compute_dtype: "float16"
    layernorm_compute_type: "float32"
    softmax_compute_type: "float16"
    param_init_type: "float16"
    use_past: True
    pretrain_seqlen: 2048 # seqlen of the pretrain checkpoint: 2048 for llama and 4096 for llama2
    extend_method: "None" # support "None", "PI", "NTK"
    compute_in_2d: False
    use_flash_attention: False
    offset: 0
    use_past_shard: False
    checkpoint_name_or_path: ""
    repetition_penalty: 1
    max_decode_length: 512
    top_k: 3
    top_p: 1
    do_sample: False
  arch:
    type: Baichuan13BV2ForCausalLM

processor:
  return_tensors: ms
  tokenizer:
    vocab_file: "../checkpoint_download/baichuan2_13b/tokenizer.model"
    unk_token: '<unk>'
    bos_token: '<s>'
    eos_token: '</s>'
    pad_token: '<pad>'
    type: Baichuan2Tokenizer
  type: LlamaProcessor

# trainer config
trainer:
  type: CausalLanguageModelingTrainer
  model_name: 'baichuan2_13b'
# if True, do evaluate during the training process. if false, do nothing.
# note that the task trainer should support _evaluate_in_training function.
do_eval: False
```

#### é…ç½®`config/config.yaml`

ä½¿ç”¨å¦‚ä¸‹`config.yaml`æ–‡ä»¶ï¼š

```yaml
server:
  host: "0.0.0.0"
  port: 11111  # the port that server is listening
  uvicorn_level: 'info'
  access_log: True  # whether open server logging

web_demo:
  host: "0.0.0.0"
  port: 7860

model:
  config: "config/predict_baichuan2_13b.yaml"
  device_num: 2
  device_id: 0
  device_range: [0,2]
  rank_table_file: "hccl_2p_01_127.0.1.1.json"  # ä»¥å®é™…rank_table_fileæ–‡ä»¶å‘½åä¸ºå‡†
  hccl_connect_time: "3600"

default_generation_args:
  do_sample: False
  top_p: 1
  top_k: 1
  temperature: 1
  repetition_penalty: 1
  max_length: 512
```

#### å¯åŠ¨æ¨ç†æœåŠ¡

è¿è¡Œå¦‚ä¸‹å‘½ä»¤å¯åŠ¨æ¨ç†æœåŠ¡ï¼š

```bash
python run_chat_server.py &> server.log &
```

è¿è¡Œå¦‚ä¸‹å‘½ä»¤æŸ¥çœ‹æœåŠ¡æ—¥å¿—ï¼š

```bash
tail -f server.log
```

æœ‰å¦‚ä¸‹æ—¥å¿—ä»£è¡¨è¿è¡ŒæˆåŠŸï¼š

```text
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:11111 (Press CTRL+C to quit)
```

#### å¯åŠ¨ç½‘é¡µåº”ç”¨

è¿è¡Œå¦‚ä¸‹å‘½ä»¤å¯åŠ¨ç½‘é¡µåº”ç”¨ï¼š

```bash
python run_chat_web_demo.py &> web.log &
```

è¿è¡Œå¦‚ä¸‹å‘½ä»¤æŸ¥çœ‹ç½‘é¡µæ—¥å¿—ï¼š

```bash
tail -f web.log
```

å‡è®¾æœåŠ¡å™¨IPä¸º`192.168.10.10`ï¼Œæµè§ˆå™¨è®¿é—®`http://192.168.10.10:7860`ã€‚

å³ä¸‹è§’é€‰æ‹©BaiChuan2é»˜è®¤æç¤ºè¯æ¨¡æ¿`"<reserved_106>{} <reserved_107>"`ï¼Œåœ¨è¾“å…¥æ¡†ä¸­è¾“å…¥é—®é¢˜ï¼Œå¹¶ç‚¹å‡»æäº¤ã€‚ç¤ºæ„å›¾å¦‚ä¸‹ï¼š

![baichuan2_13b_demo](assets/Chat_Web/baichuan2_demo.png)

## é™„å½•

### config.yamlé…ç½®é¡¹ç®€ä»‹

- **server**: æ¨ç†æœåŠ¡ç›¸å…³é…ç½®
    - **host**: æ¨ç†æœåŠ¡ä¸»æœºIPã€‚
    - **port**: æ¨ç†æœåŠ¡ç«¯å£å·ã€‚
    - **uvicorn_level**: æ¨ç†æœåŠ¡æ—¥å¿—ç­‰çº§ã€‚å¯é€‰['info', 'debug', 'error']ã€‚
    - **access_log**: æ˜¯å¦æ‰“å¼€æ¨ç†æœåŠ¡æ—¥å¿—ã€‚å¯é€‰[True, False]ã€‚

- **web_demo**: ç½‘é¡µç•Œé¢ç›¸å…³é…ç½®
    - **host**: ç½‘é¡µåº”ç”¨ä¸»æœºIPã€‚
    - **port**: ç½‘é¡µåº”ç”¨ç«¯å£å·ã€‚

- **model**: æ¨ç†æ¨¡å‹ç›¸å…³é…ç½®
    - **config**: MindFormersæ¨¡å‹é…ç½®æ–‡ä»¶ã€‚
    - **device_num**: æ¨ç†ä½¿ç”¨å¡æ•°ã€‚å¯é€‰[1, 2, 4, 8]ã€‚
    - **device_id**: ï¼ˆdevice_num=1æ—¶ç”Ÿæ•ˆï¼‰æ¨ç†ä½¿ç”¨å¡å·ã€‚
    - **device_range**: ï¼ˆdevice_num>1æ—¶ç”Ÿæ•ˆï¼‰æ¨ç†ä½¿ç”¨çš„å¡çš„èŒƒå›´ã€‚
    - **rank_table_file**: ï¼ˆdevice_num>1æ—¶ç”Ÿæ•ˆï¼‰rank table fileè·¯å¾„ã€‚
    - **hccl_connect_time**: ï¼ˆdevice_num>1æ—¶ç”Ÿæ•ˆï¼‰å¡é—´é€šä¿¡è¶…æ—¶æ—¶é—´ã€‚

- **default_generation_args**: é»˜è®¤ç”Ÿæˆé…ç½®ï¼Œé…ç½®é¡¹ä»‹ç»å¯è§[generation_configæ¥å£æ–‡æ¡£](https://mindformers.readthedocs.io/zh-cn/latest/docs/api_python/generation/mindformers.generation.generation_config.GenerationConfig.html#mindformers.generation.generation_config.GenerationConfig)
