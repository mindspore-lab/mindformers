# æ–‡æœ¬ç”Ÿæˆæ¨ç†

> ## ğŸš¨ å¼ƒç”¨è¯´æ˜
>
> æœ¬æ–‡æ¡£å·²è¿‡æ—¶ï¼Œä¸å†è¿›è¡Œç»´æŠ¤ï¼Œå¹¶å°†åœ¨ *1.6.0* ç‰ˆæœ¬ä¸‹æ¶ï¼Œå…¶ä¸­å¯èƒ½åŒ…å«è¿‡æ—¶çš„ä¿¡æ¯æˆ–å·²è¢«æ›´æ–°çš„åŠŸèƒ½æ›¿ä»£ã€‚å»ºè®®å‚è€ƒæœ€æ–°çš„ **[å®˜æ–¹æ–‡æ¡£](https://www.mindspore.cn/mindformers/docs/zh-CN/r1.5.0/index.html)** ï¼Œä»¥è·å–å‡†ç¡®çš„ä¿¡æ¯ã€‚
>
> å¦‚æœæ‚¨ä»éœ€ä½¿ç”¨æœ¬æ–‡æ¡£ä¸­çš„å†…å®¹ï¼Œè¯·ä»”ç»†æ ¸å¯¹å…¶é€‚ç”¨æ€§ï¼Œå¹¶ç»“åˆæœ€æ–°ç‰ˆæœ¬çš„ç›¸å…³èµ„æºè¿›è¡ŒéªŒè¯ã€‚
>
> å¦‚æœ‰ä»»ä½•é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·é€šè¿‡ **[ç¤¾åŒºIssue](https://gitee.com/mindspore/mindformers/issues/new)** æäº¤åé¦ˆã€‚æ„Ÿè°¢æ‚¨çš„ç†è§£ä¸æ”¯æŒï¼

Mindformerså¤§æ¨¡å‹å¥—ä»¶æä¾›äº†text generatoræ–¹æ³•ï¼Œæ—¨åœ¨è®©ç”¨æˆ·èƒ½å¤Ÿä¾¿æ·åœ°ä½¿ç”¨ç”Ÿæˆç±»è¯­è¨€æ¨¡å‹è¿›è¡Œæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºè§£ç­”é—®é¢˜ã€å¡«å……ä¸å®Œæ•´æ–‡æœ¬æˆ–ç¿»è¯‘æºè¯­è¨€åˆ°ç›®æ ‡è¯­è¨€ç­‰ã€‚

å½“å‰è¯¥æ–¹æ³•æ”¯æŒMinformerså¤§æ¨¡å‹å¥—ä»¶ä¸­6ä¸ªç”Ÿæˆç±»è¯­è¨€æ¨¡å‹

## [Text Generatoræ”¯æŒåº¦è¡¨](../model_support_list.md#text-generatoræ”¯æŒåº¦è¡¨)

## å¢é‡æ¨ç†

Mindformerså¤§æ¨¡å‹å¥—ä»¶çš„`text generator`æ–¹æ³•æ”¯æŒå¢é‡æ¨ç†é€»è¾‘ï¼Œè¯¥é€»è¾‘æ—¨åœ¨åŠ å¿«ç”¨æˆ·åœ¨è°ƒç”¨`text generator`æ–¹æ³•è¿›è¡Œæ–‡æœ¬ç”Ÿæˆæ—¶çš„æ–‡æœ¬ç”Ÿæˆé€Ÿåº¦ã€‚

åœ¨æ­¤æä¾›ä½¿ç”¨é«˜é˜¶æ¥å£è¿›è¡Œå„æ¨¡å‹å¢é‡æ¨ç†çš„**æµ‹è¯•æ ·ä¾‹è„šæœ¬**ï¼š

```python
# mindsporeè®¾ç½®å›¾æ¨¡å¼å’Œç¯å¢ƒ
import mindspore; mindspore.set_context(mode=0, device_id=0, jit_config={"infer_boost": "on"})
from mindformers import AutoConfig, AutoModel, AutoTokenizer

# æŒ‰éœ€è®¾ç½®æ¨¡å‹ç±»å‹åï¼Œé«˜é˜¶æ¥å£å°†æ ¹æ®ç±»å‹åå®ä¾‹åŒ–ç›¸åº”æ¨¡å‹
model_type = "glm_6b"
# æŒ‰éœ€è®¾ç½®æµ‹è¯•çš„è¾“å…¥æ–‡æœ¬
input_text = "ä¸­å›½çš„é¦–éƒ½æ˜¯å“ªä¸ªåŸå¸‚ï¼Ÿ"

# è·å–æ¨¡å‹é»˜è®¤é…ç½®é¡¹å¹¶æŒ‰éœ€ä¿®æ”¹
config = AutoConfig.from_pretrained(model_type)
# use_pastè®¾ç½®ä¸ºTrueæ—¶ä¸ºå¢é‡æ¨ç†ï¼Œåä¹‹ä¸ºè‡ªå›å½’æ¨ç†
config.use_past = True
# ä¿®æ”¹batch_sizeå’Œæ¨¡å‹seq_length
config.batch_size = 1; config.seq_length=512

# æ ¹æ®é…ç½®é¡¹å®ä¾‹åŒ–æ¨¡å‹
model = AutoModel.from_config(config)
# å®ä¾‹åŒ–tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_type)
# å¯¹è¾“å…¥è¿›è¡Œtokenizerç¼–ç 
input_ids = tokenizer(input_text)["input_ids"]
# è°ƒç”¨model.generateæ¥å£æ‰§è¡Œå¢é‡æ¨ç†
output = model.generate(input_ids, max_length=128, do_sample=False)
# è§£ç å¹¶æ‰“å°è¾“å‡º
print(tokenizer.decode(output))
```

> æ³¨ï¼š
>
> 1. é¦–æ¬¡è°ƒç”¨generateæ—¶éœ€è¦è¿›è¡Œmindsporeå›¾ç¼–è¯‘ï¼Œè€—æ—¶è¾ƒé•¿ï¼›åœ¨ç»Ÿè®¡åœ¨çº¿æ¨ç†çš„æ–‡æœ¬ç”Ÿæˆé€Ÿåº¦æ—¶ï¼Œå¯ä»¥å¤šæ¬¡é‡å¤è°ƒç”¨å¹¶æ’é™¤é¦–æ¬¡è°ƒç”¨çš„æ‰§è¡Œæ—¶é—´
> 2. ä½¿ç”¨å¢é‡æ¨ç†(use_past=True)æ—¶çš„ç”Ÿæˆé€Ÿåº¦é¢„æœŸå¿«äºè‡ªå›å½’æ¨ç†(use_past=False)

## Batchæ¨ç†

`text generator`æ–¹æ³•ä¹Ÿæ”¯æŒåŒæ—¶å¯¹å¤šä¸ªè¾“å…¥æ ·æœ¬è¿›è¡Œbatchæ¨ç†ï¼›åœ¨å•batchæ¨ç†ç®—åŠ›ä¸è¶³çš„æƒ…å†µä¸‹ï¼Œå¤šbatchæ¨ç†èƒ½å¤Ÿæå‡æ¨ç†æ—¶çš„ååç‡

ä»¥ä¸‹ç»™å‡ºæµ‹è¯•batchæ¨ç†èƒ½åŠ›çš„**æ ‡å‡†æµ‹è¯•è„šæœ¬**ï¼Œä»…ä¸Šè¿°å¢é‡æ¨ç†æµ‹è¯•è„šæœ¬ä»…æœ‰å°‘æ•°åŒºåˆ«

```python
import mindspore;mindspore.set_context(mode=0, device_id=0, jit_config={"infer_boost": "on"})
from mindformers import AutoConfig, AutoModel, AutoTokenizer

model_type = "glm_6b"
# å¤šbatchè¾“å…¥æ–‡æœ¬
input_text = [
    "ä¸­å›½çš„é¦–éƒ½æ˜¯å“ªä¸ªåŸå¸‚ï¼Ÿ",
    "ä½ å¥½",
    "è¯·ä»‹ç»ä¸€ä¸‹åä¸º",
    "I love Beijing, because"
]
# æ˜¯å¦ä½¿ç”¨å¢é‡æ¨ç†
use_past = True
# é¢„è®¾æ¨¡å‹seq_length
seq_len = 512

config = AutoConfig.from_pretrained(model_type)
# å°†batch sizeä¿®æ”¹ä¸ºè¾“å…¥çš„æ ·æœ¬æ•°
config.batch_size = len(input_text)
config.use_past = use_past
config.seq_length = seq_len

model = AutoModel.from_config(config)
tokenizer = AutoTokenizer.from_pretrained(model_type)

# å¯¹è¾“å…¥batchè¿›è¡Œpad
input_ids = tokenizer(input_text, max_length=config.seq_length, padding="max_length")["input_ids"]
output = model.generate(input_ids, max_length=128, do_sample=False)
print(tokenizer.decode(output))
```

> æ³¨ï¼š
> batchæ¨ç†çš„æ¨ç†ååç‡æå‡è¡¨ç°ä¸è®¾å¤‡è®¡ç®—è´Ÿè·ç›¸å…³ï¼›åœ¨seq_lenè¾ƒçŸ­å¹¶å¼€å¯å¢é‡æ¨ç†çš„æƒ…å†µä¸‹ï¼Œè®¡ç®—è´Ÿè·è¾ƒå°ï¼Œä½¿ç”¨batchæ¨ç†é€šå¸¸ä¼šè·å¾—è¾ƒå¥½çš„æå‡

## æµå¼æ¨ç†

Mindformerså¤§æ¨¡å‹å¥—ä»¶æä¾›Streamerç±»ï¼Œæ—¨åœ¨ç”¨æˆ·åœ¨è°ƒç”¨text generatoræ–¹æ³•è¿›è¡Œæ–‡æœ¬ç”Ÿæˆæ—¶èƒ½å¤Ÿå®æ—¶çœ‹åˆ°ç”Ÿæˆçš„æ¯ä¸€ä¸ªè¯ï¼Œè€Œä¸å¿…ç­‰å¾…æ‰€æœ‰ç»“æœå‡ç”Ÿæˆç»“æŸã€‚

å®ä¾‹åŒ–streamerå¹¶å‘text generatoræ–¹æ³•ä¼ å…¥è¯¥å®ä¾‹ï¼š

```python
from mindformers import AutoModel, AutoTokenizer, TextStreamer

tok = AutoTokenizer.from_pretrained("gpt2")
model = AutoModel.from_pretrained("gpt2")
inputs = tok(["An increasing sequence: one,"], return_tensors=None, add_special_tokens=False)

streamer = TextStreamer(tok)

_ = model.generate(inputs["input_ids"], streamer=streamer, max_length=20, top_k=1)
# 'An increasing sequence: one, two, three, four, five, six, seven, eight, nine, ten, eleven,'
```

ä¸Šè¿°è„šæœ¬ä¸å†å¯¹è¾“å‡ºæ–‡æœ¬è¿›è¡Œç»Ÿä¸€è§£ç æ‰“å°ï¼Œè€Œæ˜¯æ¯ç”Ÿæˆä¸€ä¸ªä¸­é—´ç»“æœå°±ç”±streamerå®æ—¶æ‰“å°

## åˆ†å¸ƒå¼æ¨ç†

å¯¹äºæ— æ³•åœ¨å•å¡ä¸Šå®Œæˆéƒ¨ç½²çš„å¤§æ¨¡å‹ï¼Œéœ€è¦é€šè¿‡å¤šå¡åˆ†å¸ƒå¼ï¼Œå¯¹æ¨¡å‹è¿›è¡Œåˆ‡åˆ†åå†è¿›è¡Œæ¨ç†

å½“å‰åˆ†å¸ƒå¼æ¨ç†ç›¸è¾ƒäºå•å¡æ¨ç†ï¼Œæµç¨‹æ˜æ˜¾æ›´ä¸ºå¤æ‚ï¼Œä¸æ˜“ä½¿ç”¨

è¿™é‡Œä»‹ç»**æ–‡æœ¬ç”Ÿæˆä»»åŠ¡åˆ†å¸ƒå¼æ¨ç†**çš„æŒ‡å¯¼æµç¨‹ï¼Œä»¥æœŸå¯¹å„æ¨¡å‹çš„åˆ†å¸ƒå¼æ¨ç†ä½¿ç”¨èµ·åˆ°æŒ‡å¯¼ä¸å‚è€ƒä½œç”¨

### åˆ†å¸ƒå¼æ¨ç†æ¦‚è¿°

åˆ†å¸ƒå¼æ¨ç†ä¸å•å¡æ¨ç†æœ‰ä»¥ä¸‹å‡ ä¸ªåŒºåˆ«ç‚¹ï¼š

1. åˆ†å¸ƒå¼æ¨ç†æ—¶ï¼Œéœ€è¦å¤šå¡å¤šè¿›ç¨‹æ‹‰èµ·åŒä¸€æ¨ç†ä»»åŠ¡
2. åˆ†å¸ƒå¼æ¨ç†æ—¶ï¼Œæ¨¡å‹åœ¨è¿›è¡Œæ¨ç†å‰éœ€è¦æŒ‰è®¾å®šçš„åˆ†å¸ƒå¼ç­–ç•¥è¿›è¡Œåˆ‡åˆ†
3. åˆ†å¸ƒå¼æ¨ç†æ—¶ï¼Œåˆ‡åˆ†çš„æ¨¡å‹åŠ è½½çš„æƒé‡ä¹Ÿéœ€è¦ä¸ºåˆ‡åˆ†æƒé‡

ç”±äºä¸Šè¿°å‡ ç‚¹åŒºåˆ«ç‚¹çš„å­˜åœ¨ï¼Œåˆ†å¸ƒå¼æ¨ç†éœ€è¦åœ¨å•å¡æ¨ç†çš„åŸºç¡€ä¸Šè¿›è¡Œæ›´å¤šå‡†å¤‡å·¥ä½œï¼Œå¦‚å‡†å¤‡åˆ†å¸ƒå¼å¯åŠ¨è„šæœ¬ï¼Œæ¨ç†å‰å¯¹æ¨¡å‹æƒé‡è¿›è¡Œåˆ‡åˆ†ï¼Œæ¨ç†ä»£ç éœ€ä¿®æ”¹ä»¥é€‚é…æ¨¡å‹åˆ‡åˆ†ä¸åŠ è½½åˆ†å¸ƒå¼æƒé‡ç­‰ï¼›æµç¨‹ç›¸å¯¹ç¹æ‚ï¼Œä¸‹æ–‡å°†é€ä¸€ä»‹ç»ã€‚

### å‰æœŸå‡†å¤‡

#### ç”ŸæˆRANK_TABLE_FILE

åˆ†å¸ƒå¼æ¨ç†éœ€è¦åˆ†å¸ƒå¼å¤šå¡å¯åŠ¨è¿›ç¨‹ï¼Œä¸ºæ­¤éœ€æå‰å‡†å¤‡RANK_TABLE_FILEæ–‡ä»¶

è¿è¡Œå¦‚ä¸‹å‘½ä»¤ï¼Œç”Ÿæˆå½“å‰æœºå™¨çš„RANK_TABLE_FILEçš„jsonæ–‡ä»¶ï¼Œå…¶ä¸­ `[0,8)` å¯ä»¥æ›¿æ¢ä¸ºå®é™…ä½¿ç”¨çš„å¡åŒºé—´

```bash
python ./mindformers/tools/hccl_tools.py --device_num "[0,8)"
```

æ³¨ï¼šè‹¥ä½¿ç”¨ModelArtsçš„notebookç¯å¢ƒï¼Œå¯ä» /user/config/jobstart_hccl.json è·¯å¾„ä¸‹ç›´æ¥è·å–rank tableï¼Œæ— éœ€æ‰‹åŠ¨ç”Ÿæˆ

RANK_TABLE_FILE å•æœº8å¡å‚è€ƒæ ·ä¾‹:

```json
{
    "version": "1.0",
    "server_count": "1",
    "server_list": [
        {
            "server_id": "xx.xx.xx.xx",
            "device": [
                {"device_id": "0","device_ip": "192.1.27.6","rank_id": "0"},
                {"device_id": "1","device_ip": "192.2.27.6","rank_id": "1"},
                {"device_id": "2","device_ip": "192.3.27.6","rank_id": "2"},
                {"device_id": "3","device_ip": "192.4.27.6","rank_id": "3"},
                {"device_id": "4","device_ip": "192.1.27.7","rank_id": "4"},
                {"device_id": "5","device_ip": "192.2.27.7","rank_id": "5"},
                {"device_id": "6","device_ip": "192.3.27.7","rank_id": "6"},
                {"device_id": "7","device_ip": "192.4.27.7","rank_id": "7"}],
             "host_nic_ip": "reserve"
        }
    ],
    "status": "completed"
}
```

#### ç¡®è®¤åˆ†å¸ƒå¼åˆ‡åˆ†ç­–ç•¥

åˆ†å¸ƒå¼æ¨ç†é€šå¸¸ç”¨äºåœ¨å•å¡ä¸Šæ— æ³•å®Œæˆéƒ¨ç½²çš„å¤§æ¨¡å‹ï¼Œä½¿ç”¨å¤šå¡åˆ‡åˆ†è¿›è¡Œæ¨ç†éƒ¨ç½²

ä½¿ç”¨åˆ†å¸ƒå¼æ¨ç†æ—¶ï¼Œéœ€æ˜ç¡®æ‰€ä½¿ç”¨çš„åˆ‡åˆ†ç­–ç•¥ï¼Œé€šå¸¸æ˜¯dp=1ï¼Œmp=å¡æ•°ï¼Œ(ppåˆ†å¸ƒå¼æ¨ç†å°šä¸æ”¯æŒ)

**å¯¹äºæ•°æ®å¹¶è¡Œdp**ï¼Œè¡¨ç¤ºå¤šå¡æ¨ç†æ—¶ï¼Œä¸åŒdpé—´ä½¿ç”¨äº†ä¸åŒçš„æ•°æ®æ‰§è¡Œæ¨ç†ï¼Œå…¶æ•ˆæœç­‰ä»·äºåœ¨ä¸åŒå¡ä¸Šå„è‡ªè¿è¡Œæ¨ç†ä»»åŠ¡ï¼Œè€Œä»…è¾“å…¥ä¸åŒï¼›
å› æ­¤dpåˆ‡åˆ†ä»…é€‚ç”¨äºåŒæ—¶æœ‰å¤§é‡æ•°æ®éœ€è¦è·å–æ¨ç†ç»“æœçš„åœºæ™¯ï¼›å¯¹äºå•å¡èƒ½å¤Ÿå®Œæˆæ¨ç†çš„æ¨¡å‹ï¼Œå»ºè®®ä½¿ç”¨å¢å¤§batchçš„æ–¹å¼æ¥å¢åŠ ååé‡ï¼Œä»¥ä»£æ›¿dpåˆ†å¸ƒå¼æ¨ç†ï¼›
æ³¨æ„ï¼Œåœ¨dpåœºæ™¯ä¸‹ï¼Œgenerateè¾“å…¥çš„æ–‡æœ¬bsåº”å½“è¢«dpæ•´é™¤ï¼Œå¦‚dp=2ï¼Œåˆ™è¾“å…¥çš„æ–‡æœ¬æ€»æ¡æ•°åº”è‡³å°‘ä¸º2æ¡ï¼Œä¸”æ¨¡å‹bsè®¾ç½®ä¸æ–‡æœ¬bsä¸€è‡´

**å¯¹äºæ¨¡å‹å¹¶è¡Œmp**ï¼Œè¡¨ç¤ºå¤šå¡æ¨ç†æ—¶ï¼Œå°†æ¨¡å‹æƒé‡åˆ‡åˆ†ä¸ºmpä»½æ”¾åœ¨ä¸åŒçš„å¡ä¸Šè¿›è¡Œè®¡ç®—ï¼Œç›¸å½“äºå¤šå¡åŒæ—¶å¤„ç†ä¸€é¡¹æ¨ç†ä»»åŠ¡ï¼›å› æ­¤mpåˆ‡åˆ†æ›´é€‚ç”¨äºå•å¡æ— æ³•éƒ¨ç½²çš„å¤§æ¨¡å‹æ¨ç†

æ˜ç¡®åˆ†å¸ƒå¼æ¨ç†æ‰€éœ€çš„å¡æ•°ä¸åˆ†å¸ƒå¼ç­–ç•¥ï¼Œå…³ç³»ä¸º `dp * mp = å¡æ•°`ï¼›è¯¥åˆ‡åˆ†ç­–ç•¥å°†åœ¨åç»­æµç¨‹ä¸­ä½¿ç”¨åˆ°ã€‚

#### æ¨¡å‹æƒé‡åˆ‡åˆ†

åœ¨åˆ†å¸ƒå¼æ¨ç†åœºæ™¯ä¸‹ï¼Œå¸¸éœ€è¦å°†æ¨¡å‹æƒé‡é‡æ–°åˆ‡åˆ†ä»¥é€‚åº”ç›®æ ‡åˆ‡åˆ†ç­–ç•¥ï¼Œå¸¸è§åœºæ™¯ä¸ºï¼š

**åœºæ™¯ä¸€**ï¼šä»å®Œæ•´æ¨¡å‹æƒé‡åˆ‡åˆ†è‡³åˆ†å¸ƒå¼æƒé‡

é€šå¸¸æ˜¯å·²æœ‰å®Œæ•´æƒé‡ï¼Œä½†ç›®æ ‡åˆ‡åˆ†ç­–ç•¥å­˜åœ¨mpåˆ‡åˆ†ï¼Œéœ€è¦å°†æƒé‡åˆ‡åˆ†ä¸ºå¯¹åº”mpç­–ç•¥ä»½ï¼Œæ­¤æ—¶å¯å‚è€ƒ[æƒé‡è½¬æ¢æ–‡æ¡£](./Transform_Ckpt.md)ï¼Œç”Ÿæˆç›®æ ‡strategyï¼Œå°†å®Œæ•´æƒé‡è½¬æ¢ä¸ºç›®æ ‡åˆ‡åˆ†æƒé‡

**åœºæ™¯äºŒ**ï¼šä»åˆ†å¸ƒå¼è®­ç»ƒè·å¾—çš„å·²åˆ‡åˆ†æƒé‡è½¬åŒ–ä¸ºå¦ä¸€ç­–ç•¥çš„åˆ†å¸ƒå¼æƒé‡

é€šå¸¸æ˜¯åœ¨åˆ†å¸ƒå¼è®­ç»ƒå®Œæˆåè·å–äº†æŒ‰è®­ç»ƒåˆ‡åˆ†ç­–ç•¥è¿›è¡Œåˆ‡åˆ†çš„æƒé‡ï¼Œåœ¨æ¨ç†é˜¶æ®µæ¨¡å‹éœ€è¦è½¬æ¢ä¸ºå¦ä¸€åˆ‡åˆ†ç­–ç•¥ï¼›
åŒæ ·å¯å‚è€ƒ[æƒé‡è½¬æ¢æ–‡æ¡£](./Transform_Ckpt.md)ï¼Œç”Ÿæˆç›®æ ‡strategyï¼Œä¸åŸæœ‰åˆ‡åˆ†startegyä¸€åŒï¼Œè½¬æ¢æ¨¡å‹åˆ‡åˆ†ç­–ç•¥

### åˆ†å¸ƒå¼æ¨ç†è„šæœ¬

#### åŸºäºgenerateæ¥å£çš„è‡ªå®šä¹‰æ¨ç†è„šæœ¬

æˆ‘ä»¬åœ¨ `scripts/examples/distribute_generate` æ–‡ä»¶å¤¹ä¸‹æä¾›äº†åŸºäºgenerateæ¥å£çš„è‡ªå®šä¹‰æ¨ç†è„šæœ¬ `generate_custom.py`ï¼Œæ”¯æŒåˆ†å¸ƒå¼æ¨ç†ã€‚

åœ¨æ­¤å¯¹è„šæœ¬ä¸­é€‚é…åˆ†å¸ƒå¼çš„å‡ ä¸ªè¦ç‚¹è¿›è¡Œè®²è§£ï¼Œæ–¹ä¾¿ç”¨æˆ·ç†è§£åˆ†å¸ƒå¼æ¨ç†æµç¨‹ï¼Œå¹¶èƒ½å¤Ÿæ ¹æ®å®é™…éœ€æ±‚æ”¹å†™è„šæœ¬ï¼Œå®ç°è‡ªå®šä¹‰åˆ†å¸ƒå¼æ¨ç†æµç¨‹ã€‚

1. åˆ†å¸ƒå¼contextç¯å¢ƒåˆå§‹åŒ–

    ```python
    def context_init(use_parallel=False, device_id=0):
        """init context for mindspore."""
        context_config = ContextConfig(mode=0, device_target="Ascend", device_id=device_id)
        parallel_config = None
        if use_parallel:
            parallel_config = ParallelContextConfig(
                parallel_mode='SEMI_AUTO_PARALLEL',     # é»˜è®¤ä½¿ç”¨åŠè‡ªåŠ¨å¹¶è¡Œæ¨¡å¼
                gradients_mean=False,                   # æ¨ç†ä¸æ¶‰åŠæ¢¯åº¦å¹³å‡
                full_batch=True                         # åŠè‡ªåŠ¨å¹¶è¡Œé»˜è®¤å¼€å¯full batch
            )
        # åˆå§‹åŒ–contextç¯å¢ƒ
        rank_id, device_num = init_context(
            use_parallel=use_parallel,
            context_config=context_config,
            parallel_config=parallel_config
        )
        print(f"Context inited for rank {rank_id}; total device num is {device_num}.")
    ```

    è°ƒç”¨init_contextæ¥å£ï¼Œåœ¨åˆ†å¸ƒå¼åœºæ™¯ä¸‹æ­£ç¡®è®¾ç½®åŠè‡ªåŠ¨å¹¶è¡Œåœºæ™¯

2. æ¨¡å‹é…ç½®åˆ†å¸ƒå¼ç­–ç•¥

    ```python
    # 2.3 é…ç½®æ¨¡å‹åˆ‡åˆ†ç­–ç•¥ï¼Œå½“å‰æš‚ä¸æ”¯æŒpipelineå¹¶è¡Œç­–ç•¥
    parallel_config = TransformerOpParallelConfig(
        data_parallel=args.data_parallel,
        model_parallel=args.model_parallel
    )
    model_config.parallel_config = parallel_config
    ```

    æ ¹æ®ç¡®å®šå¥½çš„æ¨ç†é˜¶æ®µçš„åˆ†å¸ƒå¼ç­–ç•¥ï¼Œå¯¹æ¨¡å‹é…ç½®ç›¸åº”çš„åˆ†å¸ƒå¼ç­–ç•¥

3. åˆ†å¸ƒå¼åŠ è½½åˆ‡åˆ†æƒé‡

    ```python
    # 2.4 åˆ†å¸ƒå¼æ¨ç†æ—¶éœ€é€šè¿‡åˆ†å¸ƒå¼æ¥å£åŠ è½½æƒé‡ï¼Œç§»é™¤åŸæƒé‡è·¯å¾„ä»¥é¿å…åœ¨æ¨¡å‹å®ä¾‹åŒ–æ—¶åŠ è½½
    if args.use_parallel:
        model_config.checkpoint_name_or_path = None
    ...
    # 4. åˆ†å¸ƒå¼ä¸‹ï¼Œæ¨¡å‹ç¼–è¯‘åˆ‡åˆ†å¹¶åŠ è½½æƒé‡
    # if use parallel, load distributed checkpoints
    if args.use_parallel:
        print("---------------Load Sharding Checkpoints---------------", flush=True)
        load_sharding_checkpoint(args.checkpoint_path, network, model_config)
    ...
    def load_sharding_checkpoint(checkpoint_path, network, model_config):
        if not os.path.isdir(checkpoint_path):
            raise ValueError(f"checkpoint_path {checkpoint_path} is not a directory, which is required for distribute "
                            "generate, please check your input checkpoint path.")
        # find the sharded ckpt path for this rank
        ckpt_path = os.path.join(checkpoint_path, "rank_{}".format(os.getenv("RANK_ID", "0")))
        ckpt_path = get_last_checkpoint(ckpt_path)
        print(f"ckpt path: {str(ckpt_path)}", flush=True)

        # shard model and load sharded ckpt
        model = Model(network)
        model.infer_predict_layout(ms.Tensor(np.ones(shape=(model_config.batch_size, model_config.seq_length)), ms.int32))
        checkpoint_dict = load_checkpoint(ckpt_path)
        not_load_network_params = load_param_into_net(network, checkpoint_dict)
        print(f"Network parameters are not loaded: {str(not_load_network_params)}", flush=True)
    ```

    åˆ†å¸ƒå¼æ¨ç†åœºæ™¯ä¸‹ï¼Œæ¨¡å‹åœ¨åŠ è½½æƒé‡å‰ï¼Œéœ€è°ƒç”¨ `infer_predict_layout` æ¥å£ï¼Œè¿›è¡Œæ¨¡å‹ç¼–è¯‘ä¸åˆ†å¸ƒå¼åˆ‡åˆ†ï¼›æ¨¡å‹åˆ‡åˆ†åï¼Œå†æŒ‰rank_idåŠ è½½å„è‡ªçš„åˆ‡åˆ†æƒé‡ï¼Œè¿™æ ·å°±å®Œæˆäº†åˆ†å¸ƒå¼çš„æƒé‡åŠ è½½ï¼Œè§£å†³äº†å¤§æ¨¡å‹æ— æ³•å•å¡å®ŒæˆåŠ è½½çš„é—®é¢˜

ä»¥ä¸Š3ç‚¹æ˜¯åˆ†å¸ƒå¼æ¨ç†è„šæœ¬æµç¨‹ä¸­ä¸å•å¡æ¨ç†çš„ä¸»è¦åŒºåˆ«ç‚¹ï¼Œå…¶ä½™æµç¨‹åˆ™ä¸å•å¡æ¨ç†åŸºæœ¬ä¸€è‡´ï¼Œå®ä¾‹åŒ–æ¨¡å‹å’Œç›¸åº”tokenizeråï¼Œè°ƒç”¨ `.generate()` æ¥å£å®Œæˆæ¨ç†æµç¨‹

#### ä½å‚å¾®è°ƒæ¨¡å‹ä¿®æ”¹ç‚¹

å¦‚ä¸ºä½å‚å¾®è°ƒæ¨¡å‹ï¼Œä»…éœ€åœ¨å®ä¾‹åŒ–æ¨¡å‹å‰ï¼Œé…ç½®æ¨¡å‹å¯¹åº”çš„ä½å‚å¾®è°ƒå‚æ•°ï¼Œå®ä¾‹åŒ–å‡ºå¯¹åº”çš„ä½å‚æ¨¡å‹å³å¯ï¼›

æ ·ä¾‹è„šæœ¬å¦‚ä¸‹ï¼š

```python
# ä»¥llama_loraä¸ºä¾‹ï¼Œå…¶pet configé…ç½®å¦‚ä¸‹
pet_config = {
    "pet_type": "lora"
    # configuration of lora
    "lora_rank": 16
    "lora_alpha": 16
    "lora_dropout": 0.05
    "target_modules": '.*wq|.*wk|.*wv|.*wo'
}
model_config.pet_config = pet_config
# è®¾ç½®æ¨¡å‹é…ç½®çš„pet_configé¡¹åï¼Œfrom_configæ¥å£å°†ä¼šå®ä¾‹åŒ–å‡ºå¸¦loraç»“æ„çš„æ¨¡å‹
network = AutoModel.from_config(model_config)
```

#### åˆ†å¸ƒå¼å¯åŠ¨shellè„šæœ¬

æˆ‘ä»¬æä¾›äº†å¯åŠ¨åˆ†å¸ƒå¼æ¨ç†çš„å‚è€ƒshellè„šæœ¬ `scripts/examples/distribute_generate/run_dist_gen.sh`

è„šæœ¬å¯åŠ¨å‘½ä»¤ä¸ºï¼š

```bash
bash run_dist_gen.sh [EXECUTE_ORDER] [RANK_TABLE_PATH] [DEVICE_RANGE] [RANK_SIZE]
```

å„é¡¹å…¥å‚å«ä¹‰ä¸ºï¼š

1. `EXECUTE_ORDER`ï¼šéœ€æ‰§è¡Œçš„å‘½ä»¤ï¼Œå¯ä»¥å­—ç¬¦ä¸²å½¢å¼ä¼ å…¥å®Œæ•´çš„pythonå‘½ä»¤
2. `RANK_TABLE_PATH`ï¼šrank tableæ–‡ä»¶çš„è·¯å¾„
3. `DEVICE_RANGE`ï¼šæœŸæœ›ä½¿ç”¨çš„å¡å·èŒƒå›´ï¼Œå¦‚ `[0,8]` è¡¨ç¤ºä½¿ç”¨ç¼–å·ä¸º0åˆ°7çš„å…±8å¼ å¡
4. `RANK_SIZE`ï¼šä½¿ç”¨çš„æ€»å¡æ•°

#### æ‰§è¡Œåˆ†å¸ƒå¼æ¨ç†

**æ ·ä¾‹1**ï¼š

ä»¥gpt2æ¨¡å‹ä¸ºä¾‹ï¼Œæ‹‰èµ·ä¸¤å¡æ¨¡å‹å¹¶è¡Œåˆ†å¸ƒå¼æ¨ç†ï¼š

```bash
export INPUT_DATA="An increasing sequence: one,"
bash run_dist_gen.sh "python generate_custom.py --model_type gpt2 --checkpoint_path ./gpt2_ckpt --use_parallel True --data_parallel 1 --model_parallel 2" /path/to/hccl_2p_xxx.json '[0,2]' 2
```

å‚æ•°å«ä¹‰:

- `export INPUT_DATA="An increasing sequence: one,"`ï¼šä½¿ç”¨ç¯å¢ƒå˜é‡çš„æ–¹å¼è¾“å…¥æ–‡æœ¬ï¼Œshellè„šæœ¬å°†è¯¥é¡¹ä½œä¸ºpythonè„šæœ¬çš„input_dataè¾“å…¥ï¼›ä¸»è¦ç›®çš„æ˜¯è§„é¿å­—ç¬¦ä¸²è¾“å…¥ä¸shellè§£æå­˜åœ¨çš„é—®é¢˜ï¼Œç›®å‰éœ€è¦ç”¨æˆ·æ‰‹åŠ¨æ§åˆ¶

- `python generate_custom.py --model_type gpt2 --checkpoint_path ./gpt2_ckpt --use_parallel True --data_parallel 1 --model_parallel 2"`: éœ€æ‰§è¡Œçš„å‘½ä»¤ï¼Œæ­¤å¤„å®Œæ•´è¾“å…¥generate_custom.pyçš„å¯åŠ¨å‘½ä»¤

python generate_custom.py å„é¡¹å‚æ•°å«ä¹‰ï¼š

- `model_type`: ä½¿ç”¨çš„æ¨¡å‹ç±»å‹ï¼Œæ­¤å¤„é€‰æ‹© `gpt2` æ¨¡å‹
- `checkpoint_path`: æƒé‡è·¯å¾„ï¼Œæ­¤å¤„æ›¿æ¢ä¸ºå®é™…éœ€åŠ è½½çš„æƒé‡è·¯å¾„ï¼Œæ³¨æ„ï¼Œéœ€æŒ‰ç…§[æƒé‡åˆ‡åˆ†](#æ¨¡å‹æƒé‡åˆ‡åˆ†)è¿›è¡Œåˆ‡åˆ†ï¼Œå¹¶è¾“å…¥æƒé‡æ–‡ä»¶å¤¹è·¯å¾„ï¼Œè·¯å¾„ä¸‹ç›®å½•ç»“æ„ç»„ç»‡ç±»ä¼¼ `./gpt2_ckpt/rank_xx/xxx.ckpt`
- `use_parallel`: æ˜¯å¦ä½¿ç”¨å¤šå¡å¹¶è¡Œæ¨ç†ï¼Œæ­¤å¤„ä¸º `True`
- `data_parallel`: æ•°æ®å¹¶è¡Œæ•°ï¼Œæ­¤å¤„ä¸º1è¡¨ç¤ºä¸å¼€å¯
- `model_parallel`: æ¨¡å‹å¹¶è¡Œæ•°ï¼Œæ­¤å¤„ä¸º2è¡¨ç¤º2å¡å¹¶è¡Œ
- `input_data`: æ¨ç†è¾“å…¥çš„æ–‡æœ¬

bash è„šæœ¬å…¶ä½™å‚æ•°ï¼š

- `/path/to/hccl_2p_xxx.json`: rank table fileè·¯å¾„ï¼Œæ›¿æ¢ä¸ºä¹‹å‰å‡†å¤‡çš„rank table fileçš„å®é™…è·¯å¾„
- `'[0,2]'`: å ç”¨çš„å¡èŒƒå›´ï¼Œ0åŒ…å«ï¼Œ2ä¸åŒ…å«ï¼Œè¡¨ç¤ºä½¿ç”¨ `0~1` 2å¼ å¡å¹¶è¡Œæ¨ç†
- `2`: rank sizeï¼Œä¸€å…±ä½¿ç”¨äº†å¤šå°‘å¼ å¡ï¼Œæ­¤å¤„ä¸º2

è¾“å‡ºæ—¥å¿—ï¼š

```text
['An increasing sequence: one, two, three, four, five. And so on.\n\nThe first is the first sequence of the second sequence, which is called the first and second sequence.\n\nThe second sequence is called the third and fourth sequence, and so on.\n\nThe third and fourth sequence is called the first and second sequence, and so on. The fourth sequence is called the first and second sequence, and so on.\n\nThe fifth sequence is called the second and third sequence, and so on.\n\nThe sixth sequence is called the third and fourth sequence, and so on.\n\nThe seventh sequence is called the second']
```

**æ ·ä¾‹2**ï¼š

gpt2æ¨¡å‹æµ‹è¯•dpæ¨ç†

`input.txt` æ–‡æœ¬å†…å®¹ï¼š

```text
An increasing sequence: one,
I love Beijing,
```

æ‹‰èµ·è„šæœ¬ï¼š

```bash
export INPUT_DATA=input.txt
bash run_dist_gen.sh "python generate_custom.py --model_type gpt2 --batch_size 2 --checkpoint_path ./gpt2_ckpt --use_parallel True --data_parallel 2 --model_parallel 1" /path/to/hccl_2p_xxx.json '[0,2]' 2
```

è¾“å‡ºæ—¥å¿—ï¼š

```text
['An increasing sequence: one, two, three, four, five. And so on.\n\nThe first is the first sequence of the second sequence, which is called the first and second sequence.\n\nThe second sequence is called the third and fourth sequence, and so on.\n\nThe third and fourth sequence is called the first and second sequence, and so on. The fourth sequence is called the first and second sequence, and so on.\n\nThe fifth sequence is called the second and third sequence, and so on.\n\nThe sixth sequence is called the third and fourth sequence, and so on.\n\nThe seventh sequence is called the second']
```

ä¸mpåˆ‡åˆ†åŒºåˆ«ç‚¹ï¼š

1. è¾“å…¥æ–‡æœ¬æ¡æ•°éœ€ä¸ºdpå€æ•°ï¼Œæ­¤å¤„dpä¸º2ï¼Œå› æ­¤å‡†å¤‡ä¸¤æ¡è¾“å…¥
2. `batch_size`è®¾ç½®ä¸º2ï¼Œä¸è¾“å…¥æ–‡æœ¬è¾“å…¥æ¡æ•°åŒ¹é…
3. çº¯dpæƒé‡ä¸mpåˆ‡åˆ†ä¸ä¸€è‡´ï¼Œå¯ä»¥å°†åŸæƒé‡æŒ‰é¢„æœŸæ–‡ä»¶ç»“æ„ç»„ç»‡ `./gpt2_ckpt/rank_xx/xxx.ckpt`
4. ä¿®æ”¹æ¨¡å‹å¹¶è¡Œç­–ç•¥ï¼Œdp=2ï¼Œmp=1

## SLoRAæ¨ç†

LoRAï¼ˆLow-Rank Adaptationï¼‰æ˜¯ä¸€ç§è½»é‡åŒ–çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯ï¼Œå®ƒé€šè¿‡å°†æƒé‡çŸ©é˜µåˆ†è§£ä¸ºä¸¤ä¸ªè¾ƒä½ç§©çš„çŸ©é˜µï¼Œæ˜¾è‘—å‡å°‘æ¨¡å‹å¾®è°ƒæ—¶æ‰€éœ€çš„å‚æ•°é‡ã€‚è¿™ç§æ–¹æ³•ç‰¹åˆ«é€‚ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå› ä¸ºLLMé€šå¸¸æ‹¥æœ‰æ•°ä»¥äº¿è®¡ç”šè‡³æ•°ä»¥åäº¿è®¡çš„å‚æ•°ï¼Œä¼ ç»Ÿå¾®è°ƒæ‰€æœ‰å‚æ•°çš„æ–¹å¼ä¸ä½†è€—æ—¶ï¼Œä¸”éœ€è¦å¤§é‡å­˜å‚¨å’Œè®¡ç®—èµ„æºã€‚è€ŒLoRAçš„å‡ºç°ï¼Œè§£å†³äº†è¿™ä¸€é—®é¢˜ï¼Œä½¿å¾—åœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œé™ä½äº†è®¡ç®—å¼€é”€ã€‚

åœ¨ç°å®åœºæ™¯ä¸­ï¼ŒLLMå¾€å¾€éœ€è¦åŒæ—¶åº”å¯¹ä¸åŒçš„ä»»åŠ¡ã€‚ä¾‹å¦‚ï¼Œåœ¨å¯¹è¯ç”Ÿæˆã€æœºå™¨ç¿»è¯‘ã€ä»£ç ç”Ÿæˆç­‰ä»»åŠ¡ä¸­ï¼Œè™½ç„¶å¯ä»¥ä½¿ç”¨ç›¸åŒçš„åŸºå‡†LLMæ¨¡å‹ï¼Œä½†æ¯ä¸ªä»»åŠ¡å¯èƒ½éœ€è¦ä¸åŒçš„å¾®è°ƒç­–ç•¥ã€‚é€šè¿‡ä½¿ç”¨å¤šLoRAï¼Œå¯ä»¥ä¸ºæ¯ä¸ªä»»åŠ¡åŠ è½½ä¸åŒçš„ä½ç§©æƒé‡è°ƒæ•´ï¼Œä»è€Œåœ¨ä¿æŒåŸºå‡†æ¨¡å‹ä¸å˜çš„åŸºç¡€ä¸Šï¼Œé«˜æ•ˆå¤„ç†å¤šä¸ªä»»åŠ¡ã€‚è¿™ä¸ä»…èŠ‚çœäº†å­˜å‚¨ç©ºé—´ï¼Œè¿˜å‡å°‘äº†è®­ç»ƒå’Œæ¨ç†æ—¶çš„è®¡ç®—è´Ÿæ‹…ã€‚

è¿™é‡Œä»‹ç»**æ–‡æœ¬ç”Ÿæˆä»»åŠ¡SLoRAæ¨ç†**çš„æŒ‡å¯¼æµç¨‹ï¼Œä»¥æœŸå¯¹æ¨¡å‹çš„SLoRAæ¨ç†ä½¿ç”¨èµ·åˆ°æŒ‡å¯¼ä¸å‚è€ƒä½œç”¨

### SLoRAåœºæ™¯åˆ†æ

1. æ¨ç†è¿‡ç¨‹ä¸­ï¼Œç”¨æˆ·éœ€è¦ä½¿ç”¨åŒä¸€ä¸ªé¢„è®­ç»ƒçš„åŸºå‡†æ¨¡å‹ä¸å¤šä¸ªä¸åŒçš„Adapteræƒé‡
2. ç”¨æˆ·å…·å¤‡å¤šç§Adapterå¾®è°ƒæƒé‡ï¼Œä¸”å¾®è°ƒä»¥LoRAçš„æ–¹å¼è¿›è¡Œ
3. é’ˆå¯¹ä¸åŒçš„è¯·æ±‚ï¼Œéœ€è¦ä½¿ç”¨å…±äº«å‚æ•°çš„ç›¸åŒçš„åŸºç¡€æ¨¡å‹ï¼Œä½†ä¸åŒçš„å¾®è°ƒæƒé‡è¿›è¡Œæ¨ç†

### å‰æœŸå‡†å¤‡

#### Adapteræ–‡ä»¶é…ç½®

éœ€è¦ä¸ºæ¯ä¸ªAdapterå¾®è°ƒæƒé‡é…ç½®ä¸åŒçš„config.jsonæ–‡ä»¶:

```json
{
  "lora_alpha": 16,
  "lora_dropout": 0.05,
  "r": 8,
  "target_modules": [
    "wq",
    "wv",
    "wk",
    "wo",
    "w1",
    "w2",
    "w3"
  ]
}
```

|    å‚æ•°    |    å‚æ•°è¯´æ˜        |
| :--------: | :-------------------: |
| lora_alpha | æŒ‡å®šLoRAçš„ç¼©æ”¾å› å­ |
| lora_dropout | æŒ‡å®šLoRAçš„dropoutæ¯”ä¾‹ |
| r | æŒ‡å®šä½ç§©çŸ©é˜µçš„ç§© |
| target_modules | è¡¨ç¤ºLorAå°†åº”ç”¨çš„ç‰¹å®šçš„æ¨¡å‹æ¨¡å— |

æ³¨æ„ï¼Œå¾®è°ƒæƒé‡éœ€è¦ä¸ºæ»¡è¶³MindFormersç½‘ç»œç»“æ„çš„ckpt

ä¸åŒAdapterå¾®è°ƒæƒé‡éœ€è¦åˆ†åˆ«å­˜æ”¾åœ¨ä¸åŒç›®å½•ä¸‹ï¼Œå¹¶ä¸”éœ€è¦æ­é…ä»¥å¯¹åº”çš„config.jsonæ–‡ä»¶ï¼Œä¸”ç›®å½•ä¸‹Adapteræ–‡ä»¶åä¸€è‡´ä¸ºadapter_model.ckpt

```text
ç›®å½•å±‚æ¬¡ï¼š
â””â”€Adapter1
    â”œâ”€adapter_model.ckpt
    â””â”€adapter_config.json
 ```

#### lora_adapter.jsonæ–‡ä»¶é…ç½®

æŒ‰ç…§ä»¥ä¸‹æ ¼å¼ä¾æ®å„Adapterç›®å½•è·¯å¾„é…ç½®lora_adapter.jsonã€‚å…¶ä¸­ï¼Œkeyä¸ºAdapterçš„åç§°ï¼Œvalueä¸ºLoRAæƒé‡çš„è·¯å¾„ï¼Œç”¨äºæ¨ç†æ—¶é€‰æ‹©å¯¹åº”çš„LoRAæƒé‡ã€‚

```json
{"adapter1": "/path/to/adapter1_directory",
"adapter2": "/path/to/adapter2_directory"}
```

#### æ¨ç†yamlæ–‡ä»¶é…ç½®

æˆ‘ä»¬åœ¨Petå¾®è°ƒæ¨¡å‹æ¡†æ¶ä¸­å¢åŠ äº†SLoRAæ¨ç†æ¨¡å‹ï¼Œå¯ä»¥é€šè¿‡åœ¨é…ç½®æ–‡ä»¶çš„æ¨¡å‹å‚æ•°ä¸­å¢åŠ ä»¥ä¸‹å­—æ®µå®ç°SLoRAæ¨ç†ï¼š

```yaml
model:
    xxx:
    model_config:
        pet_config:
            pet_type: slora
            adapter_path: "/path/to/your/lora_adapter.json"
    xxx:
```

### åŸºäºå•æœºå•å¡çš„æ¨ç†è„šæœ¬

```bash
    python run_mindformer.py \
    --use_parallel False \
    --run_mode predict \
    --adapter_id 'Adapter1' \
    --config configs/llama2/predict_llama2_7b.yaml \
    --predict_data 'I love Beijing because'
```

å…¶ä¸­ï¼Œ`adapter_id`æŒ‡å®šç”¨äºæ¨ç†çš„LoRA Adapteråç§°ï¼Œè‹¥ä¸ºNoneæˆ–lora_adapter.jsonä¸­ä¸å­˜åœ¨çš„åç§°åˆ™ä½¿ç”¨åŸºç¡€æ¨¡å‹

### åŸºäºå•æœºå¤šå¡çš„æ¨ç†è„šæœ¬

```bash
    bash scripts/msrun_launcher.sh "run_mindformer.py \
    --use_parallel True \
    --run_mode predict \
    --adapter_id 'Adapter1' \
    --config configs/llama2/predict_llama2_7b.yaml \
    --predict_data 'I love Beijing because' " 8
```

å½“ä½¿ç”¨å¤šå¡å¹¶è¡Œæ¨ç†æ—¶ï¼Œéœ€è¦å°†use_parallelè®¾ç½®ä¸ºTrueï¼Œæ‰“å¼€åˆ†å¸ƒå¼åŠ è½½åˆ‡åˆ†loraæƒé‡ï¼Œå…·ä½“åœºæ™¯ä¸ºï¼š

1ã€auto_trans_ckptå­—æ®µä¸ºTrueæ—¶ï¼Œå°†è‡ªåŠ¨åˆ‡åˆ†loraæƒé‡ï¼Œåˆ‡åˆ†åçš„åˆ†å¸ƒå¼æƒé‡ä¿å­˜åœ¨output/transform_ckpt/slora

2ã€auto_trans_ckptå­—æ®µä¸ºFalseæ—¶ï¼Œéœ€è¦å°†åˆ†å¸ƒå¼loraæƒé‡ä¿å­˜åœ¨lora_adapter.jsonä¸­çš„ç¬¬ä¸€ä¸ªadapterè·¯å¾„ä¸‹ã€‚
