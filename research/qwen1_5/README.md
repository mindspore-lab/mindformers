# é€šä¹‰åƒé—®

> ## ğŸš¨ å¼ƒç”¨è¯´æ˜
>
> æœ¬æ¨¡å‹å·²è¿‡æ—¶ï¼Œä¸å†è¿›è¡Œç»´æŠ¤ï¼Œå¹¶å°†åœ¨ *1.6.0* ä¹‹åçš„ç‰ˆæœ¬ä¸‹æ¶ã€‚å¦‚éœ€ä½¿ç”¨æ­¤æ¨¡å‹ï¼Œå»ºè®®æ ¹æ®å®˜æ–¹æ–‡æ¡£ä¸­çš„ **[æ¨¡å‹åº“](https://www.mindspore.cn/mindformers/docs/zh-CN/r1.5.0/start/models.html)** é€‰æ‹©åˆé€‚çš„ç‰ˆæœ¬è¿›è¡Œä½¿ç”¨ã€‚
>
> å¦‚æœ‰ä»»ä½•é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·é€šè¿‡ **[ç¤¾åŒºIssue](https://gitee.com/mindspore/mindformers/issues/new)** æäº¤åé¦ˆã€‚æ„Ÿè°¢æ‚¨çš„ç†è§£ä¸æ”¯æŒï¼

## æ¨¡å‹æè¿°

é€šä¹‰åƒé—®æ˜¯é˜¿é‡Œäº‘ç ”å‘çš„é€šä¹‰åƒé—®å¤§æ¨¡å‹ç³»åˆ—ã€‚Qwen1.5æ˜¯Qwen2çš„betaç‰ˆæœ¬, åŸºäºTransformerçš„å¤§è¯­è¨€æ¨¡å‹, åœ¨è¶…å¤§è§„æ¨¡çš„é¢„è®­ç»ƒæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒå¾—åˆ°ã€‚é¢„è®­ç»ƒæ•°æ®ç±»å‹å¤šæ ·ï¼Œè¦†ç›–å¹¿æ³›ï¼ŒåŒ…æ‹¬å¤§é‡ç½‘ç»œæ–‡æœ¬ã€ä¸“ä¸šä¹¦ç±ã€ä»£ç ç­‰ã€‚

```text
@article{qwen,
  title={Qwen Technical Report},
  author={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}
```

## æ¨¡å‹æ€§èƒ½

ä»¥ä¸‹æ¨¡å‹æ€§èƒ½å‡ç”±Atlas 800T A2ç¡¬ä»¶ç¯å¢ƒä¸‹æµ‹è¯•å¾—å‡ºã€‚

| Config                                                      |      Task       |   Datasets   | SeqLength |  Phase   |   Performance   |
|:------------------------------------------------------------|:---------------:|:------------:|:---------:|:--------:|:---------------:|
| [qwen1.5-7b](qwen1_5_7b/finetune_qwen1_5_7b.yaml)           | text_generation |    alpaca    |   4096    | Finetune | 2684 tokens/s/p |
| [qwen1.5-7b](qwen1_5_7b/pretrain_qwen1_5_7b.yaml)           | text_generation | Wikitext-103 |   32768   | Pretrain | 1417 tokens/s/p |
| [qwen1.5-14b](qwen1_5_14b/finetune_qwen1_5_14b.yaml)        | text_generation |    alpaca    |   4096    | Finetune | 1452 tokens/s/p |
| [qwen1.5-0.5b](qwen1_5_0_5b/predict_qwen1_5_0_5b_chat.yaml) | text_generation |      -       |   8192    | Predict  |  1491 tokens/s  |
| [qwen1.5-1.8b](qwen1_5_1_8b/predict_qwen1_5_1_8b_chat.yaml) | text_generation |      -       |   4096    | Predict  |  1179 tokens/s  |
| [qwen1.5-4b](qwen1_5_4b/predict_qwen1_5_4b_chat.yaml)       | text_generation |      -       |   4096    | Predict  |  625 tokens/s   |
| [qwen1.5-7b](qwen1_5_7b/predict_qwen1_5_7b_chat.yaml)       | text_generation |      -       |   8192    | Predict  |  164 tokens/s   |
| [qwen1.5-14b](qwen1_5_14b/predict_qwen1_5_14b_chat.yaml)    | text_generation |      -       |   8192    | Predict  |  104 tokens/s   |
| [qwen1.5-32b](qwen1_5_32b/predict_qwen1_5_32b_chat.yaml)    | text_generation |      -       |   4096    | Predict  |  245 tokens/s   |
| [qwen1.5-72b](qwen1_5_72b/predict_qwen1_5_72b_chat.yaml)    | text_generation |      -       |   8192    | Predict  |   74 tokens/s   |

ä»¥ä¸‹æ¨¡å‹æ€§èƒ½å‡ç”±Atlas 900 A2 PoDcç¡¬ä»¶ç¯å¢ƒä¸‹æµ‹è¯•å¾—å‡ºã€‚

| Config                                               |      Task       |   Datasets   | SeqLength |  Phase   |   Performance    |
|:-----------------------------------------------------|:---------------:|:------------:|:---------:|:--------:|:----------------:|
| [qwen1.5-0.5b](qwen1_5_0_5b/finetune_qwen1_5_0_5b.yaml)         | text_generation |    alpaca    |   8192    | Finetune | 21171 tokens/s/p |
| [qwen1.5-1.8b](qwen1_5_1_8b/finetune_qwen1_5_1_8b.yaml)         | text_generation |    alpaca    |   8192    | Finetune | 11241 tokens/s/p |
| [qwen1.5-4b](qwen1_5_4b/finetune_qwen1_5_4b.yaml)             | text_generation |    alpaca    |   8192    | Finetune | 4844 tokens/s/p  |
| [qwen1.5-32b](qwen1_5_32b/finetune_qwen1_5_32b.yaml) | text_generation |    alpaca    |   8192    | Finetune |  671 tokens/s/p  |
| [qwen1.5-14b](qwen1_5_14b/pretrain_qwen1_5_14b.yaml) | text_generation | Wikitext-103 |   32768   | Pretrain |  787 tokens/s/p  |
| [qwen1.5-72b](qwen1_5_72b/pretrain_qwen1_5_72b.yaml) | text_generation | Wikitext-103 |   32768   | Pretrain |  183 tokens/s/p  |

## æ¨¡å‹æ–‡ä»¶

`Qwen1.5` åŸºäº `MindFormers` å®ç°ï¼Œä¸»è¦æ¶‰åŠçš„æ–‡ä»¶æœ‰ï¼š

1. æ¨¡å‹å®ç°ï¼š

   ```text
   research/qwen1_5
     â””â”€â”€ qwen1_5_tokenizer.py          # æ¨¡å‹tokenizer
   ```

2. æ¨¡å‹é…ç½®ï¼š

   ```text
   research/qwen1_5
     â”œâ”€â”€ qwen1_5_0_5b                                        # qwen1.5 0.5B é…ç½®æ–‡ä»¶
     â”‚   â”œâ”€â”€ finetune_qwen1_5_0_5b.yaml                      # 0.5B å…¨å‚å¾®è°ƒå¯åŠ¨é…ç½®
     â”‚   â””â”€â”€ predict_qwen1_5_0_5b_chat.yaml                  # 0.5B åœ¨çº¿æ¨ç†å¯åŠ¨é…ç½®
     â”œâ”€â”€ qwen1_5_1_8b                                        # qwen1.5 1.8B é…ç½®æ–‡ä»¶
     â”‚   â”œâ”€â”€ finetune_qwen1_5_1_8b.yaml                      # 1.8B å…¨å‚å¾®è°ƒå¯åŠ¨é…ç½®
     â”‚   â””â”€â”€ predict_qwen1_5_1_8b_chat.yaml                  # 1.8B åœ¨çº¿æ¨ç†å¯åŠ¨é…ç½®
     â”œâ”€â”€ qwen1_5_4b                                          # qwen1.5 4B é…ç½®æ–‡ä»¶
     â”‚   â”œâ”€â”€ finetune_qwen1_5_4b.yaml                        # 4B å…¨å‚å¾®è°ƒå¯åŠ¨é…ç½®
     â”‚   â””â”€â”€ predict_qwen1_5_4b_chat.yaml                    # 4B åœ¨çº¿æ¨ç†å¯åŠ¨é…ç½®
     â”œâ”€â”€ qwen1_5_7b                                          # qwen1.5 7B é…ç½®æ–‡ä»¶
     â”‚   â”œâ”€â”€ finetune_qwen1_5_7b.yaml                        # 7B å…¨å‚å¾®è°ƒå¯åŠ¨é…ç½®
     â”‚   â”œâ”€â”€ pretrain_qwen1_5_7b.yaml                        # 7B è®­ç»ƒå¯åŠ¨é…ç½®
     â”‚   â”œâ”€â”€ predict_qwen1_5_7b.yaml                         # 7B åœ¨çº¿æ¨ç†å¯åŠ¨é…ç½®
     â”‚   â””â”€â”€ predict_qwen1_5_7b_chat.yaml                    # 7B å¤šè½®å¯¹è¯æ¨ç†å¯åŠ¨é…ç½®
     â”œâ”€â”€ qwen1_5_14b                                         # qwen1.5 14B é…ç½®æ–‡ä»¶
     â”‚   â”œâ”€â”€ finetune_qwen1_5_14b.yaml                       # 14B å…¨å‚å¾®è°ƒå¯åŠ¨é…ç½®
     â”‚   â”œâ”€â”€ pretrain_qwen1_5_14b.yaml                       # 14B è®­ç»ƒå¯åŠ¨é…ç½®
     â”‚   â”œâ”€â”€ predict_qwen1_5_14b.yaml                        # 14B åœ¨çº¿æ¨ç†å¯åŠ¨é…ç½®
     â”‚   â””â”€â”€ predict_qwen1_5_14b_chat.yaml                   # 14B å¤šè½®å¯¹è¯æ¨ç†å¯åŠ¨é…ç½®
     â”œâ”€â”€ qwen1_5_32b                                         # qwen1.5 32B é…ç½®æ–‡ä»¶
     â”‚   â”œâ”€â”€ finetune_qwen1_5_32b.yaml                       # 32B å…¨å‚å¾®è°ƒå¯åŠ¨é…ç½®
     â”‚   â””â”€â”€ predict_qwen1_5_32b_chat.yaml                   # 32B åœ¨çº¿æ¨ç†å¯åŠ¨é…ç½®
     â”œâ”€â”€ qwen1_5_72b                                         # qwen1.5 72B é…ç½®æ–‡ä»¶
     â”‚   â”œâ”€â”€ finetune_qwen1_5_72b.yaml                       # 72B å…¨å‚å¾®è°ƒå¯åŠ¨é…ç½®
     â”‚   â”œâ”€â”€ pretrain_qwen1_5_72b.yaml                       # 72B è®­ç»ƒå¯åŠ¨é…ç½®
     â”‚   â”œâ”€â”€ predict_qwen1_5_72b.yaml                        # 72B åœ¨çº¿æ¨ç†å¯åŠ¨é…ç½®
     â””â”€â”€ â””â”€â”€ predict_qwen1_5_72b_chat.yaml                   # 72B å¤šè½®å¯¹è¯æ¨ç†å¯åŠ¨é…ç½®
   ```

3. æ¨¡å‹ç›¸å…³è„šæœ¬ï¼š

   ```text
   research/qwen1_5
     â”œâ”€â”€ alpaca_converter.py           # alpacaæ•°æ®é›†æ ¼å¼è½¬æ¢è„šæœ¬
     â”œâ”€â”€ qwen1_5_preprocess.py         # æ•°æ®é›†é¢„å¤„ç†è„šæœ¬
     â”œâ”€â”€ convert_weight.py             # hf->msæƒé‡è½¬æ¢è„šæœ¬
     â”œâ”€â”€ convert_reversed.py           # ms->hfæƒé‡è½¬æ¢è„šæœ¬
     â”œâ”€â”€ qwen1_5_chat.py               # Qwen1_5 ChatåŠŸèƒ½å‡½æ•°
     â””â”€â”€ run_qwen1_5_chat.py           # Qwen1_5 ChatåŠŸèƒ½å¯åŠ¨è„šæœ¬
   ```

## ç¯å¢ƒåŠæ•°æ®å‡†å¤‡

### å®‰è£…ç¯å¢ƒ

MindFormersè½¯ç¡¬ä»¶é…å¥—å…³ç³»ä»¥åŠå®‰è£…å‚è€ƒ[ç¯å¢ƒå®‰è£…æŒ‡å—](../../README_CN.md#æºç ç¼–è¯‘å®‰è£…)å’Œ[ç‰ˆæœ¬åŒ¹é…å…³ç³»](../../README_CN.md#ç‰ˆæœ¬åŒ¹é…å…³ç³»)ã€‚

### æ•°æ®åŠæƒé‡å‡†å¤‡

#### æ•°æ®é›†ä¸‹è½½

MindFormersæä¾›`Wikitext-103`ä½œä¸º[é¢„è®­ç»ƒ](#é¢„è®­ç»ƒ)æ•°æ®é›†ï¼Œ`alpaca`ä½œä¸º[å¾®è°ƒ](#å¾®è°ƒ)æ•°æ®é›†ã€‚

| æ•°æ®é›†åç§°        |                      é€‚ç”¨æ¨¡å‹                      |   é€‚ç”¨é˜¶æ®µ   |                                            ä¸‹è½½é“¾æ¥                                            |
|:-------------|:----------------------------------------------:|:--------:|:------------------------------------------------------------------------------------------:|
| Wikitext-103 | qwen1_5-7b <br/> qwen1_5-14b <br/> qwen1_5-72b | Pretrain | [Link](https://dagshub.com/DagsHub/WIkiText-103/src/main/dataset/tokens/wiki.train.tokens) |
| alpaca       | qwen1_5-7b <br/> qwen1_5-14b <br/> qwen1_5-72b | Finetune |      [Link](https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json)       |

æ•°æ®é¢„å¤„ç†ä¸­æ‰€ç”¨çš„`vocab.json`å’Œ`merges.txt`å¯ä»¥å‚è€ƒ[æ¨¡å‹æƒé‡ä¸‹è½½](#æ¨¡å‹æƒé‡ä¸‹è½½)è¿›è¡Œä¸‹è½½ã€‚

- **Wikitext-103 æ•°æ®é¢„å¤„ç†**

  ä½¿ç”¨`research/qwen1_5/qwen1_5_preprocess.py`å¯¹ä¸‹è½½åçš„æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼Œå¹¶ç”ŸæˆMindrecordæ•°æ®ã€‚

  ```shell
  python qwen1_5_preprocess.py \
   --dataset_type 'wiki' \
   --input_glob /path/wiki.train.tokens \
   --vocab_file /path/vocab.json \
   --merges_file /path/merges.txt \
   --seq_length 32768 \
   --output_file /path/wiki.mindrecord

  # å‚æ•°è¯´æ˜
  dataset_type: é¢„å¤„ç†æ•°æ®ç±»å‹
  input_glob:   è¾“å…¥ä¸‹è½½åwiki.train.tokensçš„æ–‡ä»¶è·¯å¾„
  vocab_file:   vocab.jsonæ–‡ä»¶è·¯å¾„
  merges_file:  merges.txtæ–‡ä»¶è·¯å¾„
  seq_length:   è¾“å‡ºæ•°æ®çš„åºåˆ—é•¿åº¦
  output_file:  è¾“å‡ºæ–‡ä»¶çš„ä¿å­˜è·¯å¾„
  ```

- **alpaca æ•°æ®é¢„å¤„ç†**

  æ‰§è¡Œ`research/qwen1_5/alpaca_converter.py`ï¼Œå°†åŸå§‹æ•°æ®é›†è½¬æ¢ä¸ºæŒ‡å®šæ ¼å¼ã€‚

  ```shell
  python alpaca_converter.py \
   --data_path path/alpaca_data.json \
   --output_path /path/alpaca-data-messages.json

  # å‚æ•°è¯´æ˜
  data_path:   è¾“å…¥ä¸‹è½½çš„æ–‡ä»¶è·¯å¾„
  output_path: è¾“å‡ºæ–‡ä»¶çš„ä¿å­˜è·¯å¾„
  ```

  æ‰§è¡Œ`research/qwen1_5/qwen1_5_preprocess.py`æ–‡ä»¶ï¼Œè¿›è¡Œæ•°æ®é¢„å¤„ç†å’ŒMindrecordæ•°æ®ç”Ÿæˆã€‚

  ```shell
  python qwen1_5_preprocess.py \
   --dataset_type 'qa' \
   --input_glob /path/alpaca-data-messages.json \
   --vocab_file /path/vocab.json \
   --merges_file /path/merges.txt \
   --seq_length 4096 \
   --output_file /path/alpaca-messages.mindrecord

  # å‚æ•°è¯´æ˜
  dataset_type: é¢„å¤„ç†æ•°æ®ç±»å‹
  input_glob:   è½¬æ¢åçš„alpacaçš„æ–‡ä»¶è·¯å¾„
  vocab_file:   vocab.jsonæ–‡ä»¶è·¯å¾„
  merges_file:  merges.txtæ–‡ä»¶è·¯å¾„
  seq_length:   è¾“å‡ºæ•°æ®çš„åºåˆ—é•¿åº¦
  output_file:  è¾“å‡ºæ–‡ä»¶çš„ä¿å­˜è·¯å¾„
  ```

#### æ¨¡å‹æƒé‡ä¸‹è½½

ç”¨æˆ·å¯ä»¥ä»HuggingFaceå®˜æ–¹ä¸‹è½½é¢„è®­ç»ƒæƒé‡ï¼Œç»è¿‡[æ¨¡å‹æƒé‡è½¬æ¢](#æ¨¡å‹æƒé‡è½¬æ¢)åè¿›è¡Œä½¿ç”¨ï¼Œ`vocab.json`å’Œ`merges.txt`æ–‡ä»¶ä¹Ÿåœ¨é“¾æ¥ä¸­ä¸‹è½½ã€‚

è¯è¡¨ä¸‹è½½é“¾æ¥ï¼š[vocab.json](https://huggingface.co/Qwen/Qwen1.5-7B-Chat/blob/main/vocab.json)å’Œ[merges.txt](https://huggingface.co/Qwen/Qwen1.5-7B-Chat/blob/main/merges.txt)

| æ¨¡å‹åç§°        |                     Baseæƒé‡ï¼ˆå»ºè®®è®­ç»ƒå’Œå¾®è°ƒä½¿ç”¨ï¼‰                     |                         Chatæƒé‡ï¼ˆå»ºè®®æ¨ç†ä½¿ç”¨ï¼‰                         |
|:------------|:---------------------------------------------------------:|:--------------------------------------------------------------:|
| qwen1_5-7b  | [Link](https://huggingface.co/Qwen/Qwen1.5-7B/tree/main)  | [Link](https://huggingface.co/Qwen/Qwen1.5-7B-Chat/tree/main)  |
| qwen1_5-14b | [Link](https://huggingface.co/Qwen/Qwen1.5-14B/tree/main) | [Link](https://huggingface.co/Qwen/Qwen1.5-14B-Chat/tree/main) |
| qwen1_5-72b | [Link](https://huggingface.co/Qwen/Qwen1.5-72B/tree/main) | [Link](https://huggingface.co/Qwen/Qwen1.5-72B-Chat/tree/main) |

#### æ¨¡å‹æƒé‡è½¬æ¢

- **torchæƒé‡è½¬mindsporeæƒé‡**

  **æ³¨**: è¯·å®‰è£…`convert_weight.py`ä¾èµ–åŒ…ã€‚

  ```shell
  pip install torch transformers==4.37.2
  ```

ç„¶åè¿è¡Œ [Mindformers çš„æƒé‡è½¬æ¢å·¥å…·](https://www.mindspore.cn/mindformers/docs/zh-CN/r1.5.0/function/weight_conversion.html), å°†huggingfaceçš„æƒé‡è½¬æ¢ä¸º Mindspore çš„ckptæ ¼å¼ã€‚

> æ³¨æ„: æƒé‡è½¬æ¢å®Œæˆä¹‹åï¼Œæ³¨æ„é‡æ–°æ ¹æ®æœ¬é¡¹ç›®[requirements.txt](../../requirements.txt )æ¢å¤`tokenizers`åŒ…çš„ç‰ˆæœ¬:
> `pip install -r requirements.txt`

## é¢„è®­ç»ƒ

MindFormersæä¾›`qwen1_5-7b`å•æœºå¤šå¡ä»¥åŠ`qwen1_5-14b`ä¸`qwen1_5-72b`å¤šæœºå¤šå¡çš„é¢„è®­ç»ƒç¤ºä¾‹ï¼Œè¿‡ç¨‹ä¸­ä½¿ç”¨`Wikitext-103`æ•°æ®é›†å¯¹æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œæ•°æ®é›†å¯ä»¥å‚è€ƒ[æ•°æ®é›†ä¸‹è½½](#æ•°æ®é›†ä¸‹è½½)è·å¾—ã€‚

### å•æœºè®­ç»ƒ

ä»¥`qwen1_5-7b`å•æœº8å¡é¢„è®­ç»ƒä»»åŠ¡ä¸ºä¾‹ï¼Œæ‰§è¡Œåˆ†å¸ƒå¼å¯åŠ¨è„šæœ¬ã€‚

```shell
bash scripts/msrun_launcher.sh "run_mindformer.py \
 --register_path research/qwen1_5 \
 --config research/qwen1_5/qwen1_5_7b/pretrain_qwen1_5_7b.yaml \
 --load_checkpoint /path/qwen1.5_7b.ckpt \
 --train_dataset_dir /path/wiki.mindrecord \
 --run_mode train" 8
```

### å¤šæœºè®­ç»ƒ

1. å¯åŠ¨qwen1_5-14bé¢„è®­ç»ƒï¼Œæ‰§è¡Œ2æœº16å¡ä»»åŠ¡ã€‚

   åœ¨å¤šæœºä¸ŠåŒæ—¶æ‹‰èµ·ä»»åŠ¡ï¼Œå°†å‚æ•°`MASTER_ADDR`è®¾ç½®ä¸ºä¸»èŠ‚ç‚¹çš„ipåœ°å€ï¼Œ æ‰€æœ‰èŠ‚ç‚¹è®¾ç½®çš„ipåœ°å€ç›¸åŒï¼Œä¸åŒèŠ‚ç‚¹ä¹‹é—´ä»…å‚æ•°`NODE_RANK`ä¸åŒï¼Œå…·ä½“å¯å‚è€ƒ[ä½¿ç”¨æŒ‡å—](../../README_CN.md#ä¸‰ä½¿ç”¨æŒ‡å—)

   åœ¨mindformerså·¥ä½œç›®å½•ä¸‹ï¼Œæ‰§è¡Œï¼š

   ```shell
   # èŠ‚ç‚¹0ï¼ŒèŠ‚ç‚¹ipä¸º192.168.1.1ï¼ŒèŠ‚ç‚¹å¯åŠ¨å‘½ä»¤ä»…å‚æ•°NODE_RANKä¸åŒ
   bash scripts/msrun_launcher.sh "run_mindformer.py \
    --register_path research/qwen1_5 \
    --config research/qwen1_5/qwen1_5_14b/pretrain_qwen1_5_14b.yaml \
    --use_parallel True \
    --run_mode train \
    --train_data /path/wiki.mindrecord" \
   16 8 192.168.1.1 8118 0 output/msrun_log False 3000

   # èŠ‚ç‚¹1ï¼ŒèŠ‚ç‚¹ipä¸º192.168.1.2ï¼ŒèŠ‚ç‚¹å¯åŠ¨å‘½ä»¤ä»…å‚æ•°NODE_RANKä¸åŒ
   bash scripts/msrun_launcher.sh "run_mindformer.py \
    --register_path research/qwen1_5 \
    --config research/qwen1_5/qwen1_5_14b/pretrain_qwen1_5_14b.yaml \
    --use_parallel True \
    --run_mode train \
    --train_data /path/wiki.mindrecord" \
   16 8 192.168.1.1 8118 1 output/msrun_log False 3000

   # å‚æ•°è¯´æ˜
   config:      é…ç½®æ–‡ä»¶è·¯å¾„
   run_mode:    è¿è¡Œæ¨¡å¼, é¢„è®­ç»ƒæ—¶è®¾ç½®ä¸ºtrain
   train_data:  è®­ç»ƒæ•°æ®é›†æ–‡ä»¶å¤¹è·¯å¾„
   merges_file: è¯è¡¨æ–‡ä»¶merges.txtè·¯å¾„
   vocab_file:  è¯è¡¨æ–‡ä»¶vocab.jsonè·¯å¾„
   ```

2. å¯åŠ¨qwen1_5-72bé¢„è®­ç»ƒï¼Œæ‰§è¡Œ8æœº64å¡ä»»åŠ¡ã€‚

  åœ¨å¤šæœºä¸ŠåŒæ—¶æ‹‰èµ·ä»»åŠ¡ï¼Œå°†å‚æ•°`MASTER_ADDR`è®¾ç½®ä¸ºä¸»èŠ‚ç‚¹çš„ipåœ°å€ï¼Œ æ‰€æœ‰èŠ‚ç‚¹è®¾ç½®çš„ipåœ°å€ç›¸åŒï¼Œä¸åŒèŠ‚ç‚¹ä¹‹é—´ä»…å‚æ•°`NODE_RANK`ä¸åŒï¼Œå…·ä½“å¯å‚è€ƒ[ä½¿ç”¨æŒ‡å—](../../README_CN.md#ä¸‰ä½¿ç”¨æŒ‡å—)

  åœ¨mindformerså·¥ä½œç›®å½•ä¸‹ï¼Œæ‰§è¡Œï¼š

   ```shell
   # èŠ‚ç‚¹0ï¼ŒèŠ‚ç‚¹ipä¸º192.168.1.1ï¼Œä½œä¸ºä¸»èŠ‚ç‚¹ï¼Œæ€»å…±64å¡ä¸”æ¯ä¸ªèŠ‚ç‚¹8å¡
   bash scripts/msrun_launcher.sh "run_mindformer.py \
    --register_path research/qwen1_5 \
    --config research/qwen1_5/qwen1_5_72b/pretrain_qwen1_5_72b.yaml \
    --use_parallel True \
    --run_mode train \
    --train_data /path/wiki.mindrecord" \
   64 8 192.168.1.1 8118 0 output/msrun_log False 1200

   # èŠ‚ç‚¹1ï¼ŒèŠ‚ç‚¹ipä¸º192.168.1.2ï¼ŒèŠ‚ç‚¹0ä¸èŠ‚ç‚¹1å¯åŠ¨å‘½ä»¤ä»…å‚æ•°NODE_RANKä¸åŒ
   bash scripts/msrun_launcher.sh "run_mindformer.py \
    --register_path research/qwen1_5 \
    --config research/qwen1_5/qwen1_5_72b/pretrain_qwen1_5_72b.yaml \
    --use_parallel True \
    --run_mode train \
    --train_data /path/wiki.mindrecord" \
   64 8 192.168.1.1 8118 1 output/msrun_log False 1200

   # ...
   # çœç•¥ä¸­é—´èŠ‚ç‚¹2-6çš„æ‰§è¡Œå‘½ä»¤ä¸åŒèŠ‚ç‚¹ä¹‹é—´ä»…å‚æ•°NODE_RANKä¸åŒ

   # èŠ‚ç‚¹7ï¼ŒèŠ‚ç‚¹ipä¸º192.168.1.8ï¼ŒèŠ‚ç‚¹0ä¸èŠ‚ç‚¹7å¯åŠ¨å‘½ä»¤ä»…å‚æ•°NODE_RANKä¸åŒ
   bash scripts/msrun_launcher.sh "run_mindformer.py \
    --register_path research/qwen1_5 \
    --config research/qwen1_5/qwen1_5_72b/pretrain_qwen1_5_72b.yaml \
    --use_parallel True \
    --run_mode train \
    --train_data /path/wiki.mindrecord" \
   64 8 192.168.1.1 8118 7 output/msrun_log False 1200

   # å‚æ•°è¯´æ˜
   config:      é…ç½®æ–‡ä»¶è·¯å¾„
   run_mode:    è¿è¡Œæ¨¡å¼, é¢„è®­ç»ƒæ—¶è®¾ç½®ä¸ºtrain
   train_data:  è®­ç»ƒæ•°æ®é›†æ–‡ä»¶å¤¹è·¯å¾„
   merges_file: è¯è¡¨æ–‡ä»¶merges.txtè·¯å¾„
   vocab_file:  è¯è¡¨æ–‡ä»¶vocab.jsonè·¯å¾„
   ```

## å…¨å‚å¾®è°ƒ

MindFormersæä¾›`qwen1_5-7b`ä¸`qwen1_5-14b`å•æœºå¤šå¡ä»¥åŠ`qwen1_5-72b`å¤šæœºå¤šå¡çš„å¾®è°ƒç¤ºä¾‹ï¼Œè¿‡ç¨‹ä¸­ä½¿ç”¨`alpaca`æ•°æ®é›†å¯¹æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œæ•°æ®é›†å¯ä»¥å‚è€ƒ[æ•°æ®é›†ä¸‹è½½](#æ•°æ®é›†ä¸‹è½½)è·å¾—ã€‚

è®¾ç½®å¦‚ä¸‹ç¯å¢ƒå˜é‡ï¼š

```bash
export MS_ASCEND_CHECK_OVERFLOW_MODE=INFNAN_MODE
# å¦‚å‡ºç°OOMéœ€è¦é…ç½®:
export ENABLE_CELL_RESUSE=1          # æ‰“å¼€å†…å­˜å¤ç”¨
export MS_GE_ATOMIC_CLEAN_POLICY=1   # æ‰“å¼€å†…å­˜ä¼˜åŒ–
   ```

### å•æœºè®­ç»ƒ

ä»¥`qwen1_5-7b`å•æœº8å¡å¾®è°ƒä¸ºä¾‹ï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶`research/qwen1_5/qwen1_5_7b/finetune_qwen1_5_7b.yaml`ã€‚

æ‰§è¡Œå¦‚ä¸‹å‘½ä»¤å¯åŠ¨å¾®è°ƒä»»åŠ¡ã€‚

```shell
bash scripts/msrun_launcher.sh "run_mindformer.py \
 --register_path research/qwen1_5 \
 --config research/qwen1_5/qwen1_5_7b/finetune_qwen1_5_7b.yaml \
 --load_checkpoint /path/qwen1.5_7b.ckpt \
 --auto_trans_ckpt True \
 --train_dataset /path/alpaca.mindrecord \
 --run_mode finetune" 8
```

`qwen1_5-7b`å•æœº8å¡å¾®è°ƒä»»åŠ¡æ›¿æ¢å‘½ä»¤ä¸­çš„`--load_checkpoint /path/qwen1.5_14b.ckpt`ä»¥åŠé…ç½®æ–‡ä»¶`research/qwen1_5/qwen1_5_14b/finetune_qwen1_5_14b.yaml`å³å¯ã€‚

### å¤šæœºè®­ç»ƒ

ä»¥`qwen1_5-72b`4æœº32å¡ä¸ºä¾‹ï¼Œå¯åŠ¨å¤šæœºå¾®è°ƒä»»åŠ¡ã€‚

1. ä¿®æ”¹`research/qwen1_5/qwen1_5_72b/finetune_qwen1_5_72b.yaml`

   ```yaml
   parallel_config:
     data_parallel: 1
     model_parallel: 8
     pipeline_stage: 4
     micro_batch_num: 48
     vocab_emb_dp: True
     gradient_aggregation_group: 4
   ```

2. æ‰§è¡Œåˆ†å¸ƒå¼å¯åŠ¨å‘½ä»¤

   åœ¨å¤šæœºä¸ŠåŒæ—¶æ‹‰èµ·ä»»åŠ¡ï¼Œå°†å‚æ•°`MASTER_ADDR`è®¾ç½®ä¸ºä¸»èŠ‚ç‚¹çš„ipåœ°å€ï¼Œ æ‰€æœ‰èŠ‚ç‚¹è®¾ç½®çš„ipåœ°å€ç›¸åŒï¼Œä¸åŒèŠ‚ç‚¹ä¹‹é—´ä»…å‚æ•°`NODE_RANK`ä¸åŒï¼Œå…·ä½“å¯å‚è€ƒ[ä½¿ç”¨æŒ‡å—](../../README_CN.md#ä¸‰ä½¿ç”¨æŒ‡å—)

   åœ¨mindformerså·¥ä½œç›®å½•ä¸‹ï¼Œæ‰§è¡Œï¼š

   ```shell
   # èŠ‚ç‚¹0ï¼ŒèŠ‚ç‚¹ipä¸º192.168.1.1ï¼Œä½œä¸ºä¸»èŠ‚ç‚¹ï¼Œæ€»å…±32å¡ä¸”æ¯ä¸ªèŠ‚ç‚¹8å¡
   bash scripts/msrun_launcher.sh "run_mindformer.py \
    --register_path research/qwen1_5 \
    --config finetune_qwen1_5_72b.yaml \
    --load_checkpoint /path/model_dir \
    --use_parallel True \
    --run_mode finetune \
    --auto_trans_ckpt True \
    --train_data /path/alpaca.mindrecord" \
   32 8 192.168.1.1 8118 0 output/msrun_log False 300

   # èŠ‚ç‚¹1ï¼ŒèŠ‚ç‚¹ipä¸º192.168.1.2ï¼ŒèŠ‚ç‚¹0ä¸èŠ‚ç‚¹1å¯åŠ¨å‘½ä»¤ä»…å‚æ•°NODE_RANKä¸åŒ
   bash scripts/msrun_launcher.sh "run_mindformer.py \
    --register_path research/qwen1_5 \
    --config finetune_qwen1_5_72b.yaml \
    --load_checkpoint /path/model_dir \
    --use_parallel True \
    --run_mode finetune \
    --auto_trans_ckpt True \
    --train_data /path/alpaca.mindrecord" \
   32 8 192.168.1.1 8118 1 output/msrun_log False 300

   # èŠ‚ç‚¹2ï¼ŒèŠ‚ç‚¹ipä¸º192.168.1.3ï¼ŒèŠ‚ç‚¹0ä¸èŠ‚ç‚¹2å¯åŠ¨å‘½ä»¤ä»…å‚æ•°NODE_RANKä¸åŒ
   bash scripts/msrun_launcher.sh "run_mindformer.py \
    --register_path research/qwen1_5 \
    --config finetune_qwen1_5_72b.yaml \
    --load_checkpoint /path/model_dir \
    --use_parallel True \
    --run_mode finetune \
    --auto_trans_ckpt True \
    --train_data /path/alpaca.mindrecord" \
   32 8 192.168.1.1 8118 2 output/msrun_log False 300

   # èŠ‚ç‚¹3ï¼ŒèŠ‚ç‚¹ipä¸º192.168.1.4ï¼ŒèŠ‚ç‚¹0ä¸èŠ‚ç‚¹3å¯åŠ¨å‘½ä»¤ä»…å‚æ•°NODE_RANKä¸åŒ
   bash scripts/msrun_launcher.sh "run_mindformer.py \
    --register_path research/qwen1_5 \
    --config finetune_qwen1_5_72b.yaml \
    --load_checkpoint /path/model_dir \
    --use_parallel True \
    --run_mode finetune \
    --auto_trans_ckpt True \
    --train_data /path/alpaca.mindrecord" \
   32 8 192.168.1.1 8118 3 output/msrun_log False 300

   # å‚æ•°è¯´æ˜
   config:          é…ç½®æ–‡ä»¶è·¯å¾„
   load_checkpoint: æƒé‡æ–‡ä»¶å¤¹è·¯å¾„, æƒé‡æŒ‰ç…§'model_dir/rank_0/xxx.ckpt'æ ¼å¼å­˜æ”¾
   auto_trans_ckpt: è‡ªåŠ¨æƒé‡è½¬æ¢å¼€å…³
   run_mode:        è¿è¡Œæ¨¡å¼, å¾®è°ƒæ—¶è®¾ç½®ä¸ºfinetune
   train_data:      è®­ç»ƒæ•°æ®é›†è·¯å¾„
   ```

## æ¨ç†

å¤§æ¨¡å‹æ¨ç†å‡çº§è®­æ¨ä¸€ä½“æ¶æ„ï¼Œå®ç°è„šæœ¬ã€åˆ†å¸ƒå¼ç­–ç•¥å’Œè¿è¡Œæ—¶çš„ç»Ÿä¸€ï¼Œé€šè¿‡èåˆå¤§ç®—å­é™ä½æ¨ç†æ—¶å»¶ï¼Œæœ‰æ•ˆæå‡ç½‘ç»œååé‡ã€‚

æ³¨æ„äº‹é¡¹ï¼š

å½“å‰æ”¯æŒæ¨¡å‹å·²æä¾›æ¨ç†ç›¸å…³é…ç½®æ–‡ä»¶ï¼Œè¯·æ ¹æ®å®é™…ä½¿ç”¨æ¨¡å‹æ›´æ”¹é…ç½®æ–‡ä»¶ã€‚

### åŸºäºé«˜é˜¶æ¥å£çš„æ¨ç†

#### å•å¡æ¨ç†

ä»¥`qwen1_5_7b`å•å¡æ¨ç†ä¸ºä¾‹ï¼Œå³ä½¿ç”¨ `research/qwen1_5/qwen1_5_7b/predict_qwen1_5_7b.yaml` é…ç½®æ–‡ä»¶ã€‚

ä¿®æ”¹é…ç½®æ–‡ä»¶ `research/qwen1_5/qwen1_5_7b/predict_qwen1_5_7b.yaml` ï¼š

```yaml
model:
  model_config:
    qkv_concat: False
processor:
  tokenizer:
    vocab_file: "/path/vocab.json"
    merges_file: "/path/merges.txt"
```

æ‰§è¡Œå¦‚ä¸‹å‘½ä»¤è¿›è¡Œæ¨ç†ã€‚

```shell
# æ¨ç†å‘½ä»¤ä¸­å‚æ•°ä¼šè¦†ç›–yamlæ–‡ä»¶ä¸­çš„ç›¸åŒå‚æ•°
python run_mindformer.py \
 --register_path research/qwen1_5 \
 --config research/qwen1_5/qwen1_5_7b/predict_qwen1_5_7b.yaml \
 --load_checkpoint /path/model_dir \
 --run_mode predict \
 --use_parallel False \
 --auto_trans_ckpt False \
 --predict_data 'å¸®åŠ©æˆ‘åˆ¶å®šä¸€ä»½å»ä¸Šæµ·çš„æ—…æ¸¸æ”»ç•¥'
# å¸®åŠ©æˆ‘åˆ¶å®šä¸€ä»½å»ä¸Šæµ·çš„æ—…æ¸¸æ”»ç•¥ï¼ŒåŒ…æ‹¬æ™¯ç‚¹ã€ç¾é£Ÿã€ä½å®¿ç­‰ä¿¡æ¯...
```

#### å¤šå¡æ¨ç†

ä»¥`qwen1_5_72b`4å¡æ¨ç†ä¸ºä¾‹ï¼Œå³ä½¿ç”¨ `research/qwen1_5/qwen1_5_72b/predict_qwen1_5_72b.yaml` é…ç½®æ–‡ä»¶ã€‚

ä¿®æ”¹é…ç½®æ–‡ä»¶ `research/qwen1_5/qwen1_5_72b/predict_qwen1_5_72b.yaml` ï¼š

   ```yaml
   parallel_config:
     data_parallel: 1
     model_parallel: 4
     pipeline_stage: 1
     micro_batch_num: 1
     vocab_emb_dp: True
     gradient_aggregation_group: 4
   model:
     model_config:
     qkv_concat: False
   processor:
     tokenizer:
     vocab_file: "/path/vocab.json"
     merges_file: "/path/merges.txt"
   ```

   *æ³¨*ï¼šå¯é…ç½®`model_config:param_init_type`ä¸º`float32`æé«˜æ¨ç†ç²¾åº¦ï¼Œä½†åŒæ—¶ä¼šå½±å“åœ¨çº¿æ¨ç†æ€§èƒ½ã€‚

å¯åŠ¨å¤šå¡æ¨ç†ï¼š

   ```shell
   # æ¨ç†å‘½ä»¤ä¸­å‚æ•°ä¼šè¦†ç›–yamlæ–‡ä»¶ä¸­çš„ç›¸åŒå‚æ•°
   bash scripts/msrun_launcher.sh "run_mindformer.py \
    --register_path research/qwen1_5 \
    --config research/qwen1_5/qwen1_5_72b/predict_qwen1_5_72b.yaml \
    --load_checkpoint /path/model_dir \
    --run_mode predict \
    --use_parallel True \
    --auto_trans_ckpt True \
    --predict_data å¸®åŠ©æˆ‘åˆ¶å®šä¸€ä»½å»ä¸Šæµ·çš„æ—…æ¸¸æ”»ç•¥" 4

   # å¸®åŠ©æˆ‘åˆ¶å®šä¸€ä»½å»ä¸Šæµ·çš„æ—…æ¸¸æ”»ç•¥ï¼ŒåŒ…æ‹¬æ™¯ç‚¹ã€ç¾é£Ÿã€ä½å®¿ç­‰ä¿¡æ¯...
   ```

### å¤šè½®å¯¹è¯æ¨ç†

`run_qwen1_5_chat.py` åŸºäº`model.generate()`å®ç°ï¼Œæ”¯æŒäº¤äº’å¼å¤šè½®å¯¹è¯ï¼Œæ”¯æŒåŠ è½½loraæƒé‡ã€æƒé‡è½¬æ¢ã€å¤šå¡æ¨ç†ï¼Œæš‚ä¸æ”¯æŒ batch æ¨ç†ã€‚

#### å•å¡æ¨ç†

ä»¥`qwen1_5_7b`å•å¡æ¨ç†ä¸ºä¾‹ï¼Œå³ä½¿ç”¨ `research/qwen1_5/qwen1_5_7b/predict_qwen1_5_7b_chat.yaml`

ä¿®æ”¹é…ç½®æ–‡ä»¶ `research/qwen1_5/qwen1_5_7b/predict_qwen1_5_7b_chat.yaml` ï¼š

```yaml
model:
  model_config:
    qkv_concat: False
processor:
  tokenizer:
    vocab_file: "/path/vocab.json"
    merges_file: "/path/merges.txt"
```

æ‰§è¡Œå¦‚ä¸‹å‘½ä»¤è¿›è¡Œå¤šè½®å¯¹è¯æ¨ç†ã€‚

```shell
cd research/qwen1_5
python run_qwen1_5_chat.py \
 --config qwen1_5_7b/predict_qwen1_5_7b_chat.yaml \
 --load_checkpoint /path/to/qwen1_5_7b_chat.ckpt \
 --enable_history True \
 --use_parallel False \
 --auto_trans_ckpt False \
 --run_demo True \
 --device_id 0

# å‚æ•°è¯´æ˜
# --enable_history: æ˜¯å¦å°†å†å²å¯¹è¯å¸¦å…¥åé¢çš„è¾“å…¥ã€‚åœ¨äº¤äº’å¼æ¨¡å¼ä¸‹ï¼ˆä¸”å¯åŠ¨æ—¶æŒ‡å®šäº†--enable_history=Trueï¼‰ï¼Œå¯ä»¥ç”¨ /clear æ¸…é™¤å‰é¢çš„å¯¹è¯å†å²ï¼Œå¼€å§‹æ–°ä¸€è½®ä¼šè¯;
# --run_demo: å¯åŠ¨æ—¶æ˜¯å¦è‡ªåŠ¨è¿è¡Œé¢„è®¾çš„è‹¥å¹²ä¸ªé—®é¢˜ï¼ˆç”¨äºæ¼”ç¤º/è¯•éªŒç›®çš„ï¼‰;
# --predict_data: æäº¤ç»™æ¨¡å‹è¿›è¡Œæ¨ç†çš„é—®é¢˜ï¼ˆrun_qwen1_5_chat.pyä¼šå°†å†å²å¯¹è¯å’Œé—®é¢˜æŒ‰ç…§chatmlæ ¼å¼ç»„è£…åæäº¤ç»™æ¨¡å‹è¿›è¡Œæ¨ç†ï¼‰ï¼Œå¯ä»¥ç»™å‡ºå¤šä¸ªé—®é¢˜ã€‚ä¸ç»™å‡ºæ­¤å‚æ•°æ—¶ï¼Œ`run_qwen1_5_chat.py`æŒ‰äº¤äº’æ¨¡å¼è¿è¡Œ;
```

#### å¤šå¡æ¨ç†

æ³¨æ„: å¤šå¡è¿è¡Œ`run_qwen1_5_chat.py`æ—¶ï¼Œä¸æ”¯æŒäº¤äº’å¼å¯¹è¯ï¼Œåªèƒ½é€šè¿‡`--predict_data`ä¼ å…¥é¢„å…ˆç»™å‡ºçš„é—®é¢˜ã€‚

ä¿®æ”¹é…ç½®æ–‡ä»¶ `research/qwen1_5/qwen1_5_72b/predict_qwen1_5_72b_chat.yaml` ï¼š

   ```yaml
   parallel_config:
     data_parallel: 1
     model_parallel: 4
     pipeline_stage: 1
     micro_batch_num: 1
     vocab_emb_dp: True
     gradient_aggregation_group: 4
   model:
     model_config:
     qkv_concat: False
   processor:
     tokenizer:
     vocab_file: "/path/vocab.json"
     merges_file: "/path/merges.txt"
   ```

```shell
cd research/qwen1_5
bash ../../scripts/msrun_launcher.sh "run_qwen1_5_chat.py \
 --config qwen1_5_72b/predict_qwen1_5_72b_chat.yaml \
 --use_parallel True \
 --auto_trans_ckpt False \
 --load_checkpoint /path/to/é¢„å…ˆåˆ‡åˆ†å¥½çš„4å¡æƒé‡ \
 --predict_data ã€Šä¸‰ä½“ã€‹è¿™æœ¬å°è¯´çš„ç²¾å½©ä¹‹å¤„åœ¨ä»€ä¹ˆåœ°æ–¹ å†æ¨èå‡ éƒ¨åˆ˜æ…ˆæ¬£çš„ä½œå“å§ å›½å†…è¿™äº›å¹´è¿˜æœ‰å“ªäº›ä¸é”™çš„ç§‘å¹»ä½œå®¶ \
 --enable_history True" 4
tail -f output/msrun_log/*.log  # press Ctrl-C to quit when done
```
