# Llava1.5

> ## ğŸš¨ å¼ƒç”¨è¯´æ˜
>
> æœ¬æ¨¡å‹å·²è¿‡æ—¶ï¼Œä¸å†è¿›è¡Œç»´æŠ¤ï¼Œå¹¶å°†åœ¨ *1.6.0* ä¹‹åçš„ç‰ˆæœ¬ä¸‹æ¶ã€‚å¦‚éœ€ä½¿ç”¨æ­¤æ¨¡å‹ï¼Œå»ºè®®æ ¹æ®å®˜æ–¹æ–‡æ¡£ä¸­çš„ **[æ¨¡å‹åº“](https://www.mindspore.cn/mindformers/docs/zh-CN/r1.5.0/start/models.html)** é€‰æ‹©åˆé€‚çš„ç‰ˆæœ¬è¿›è¡Œä½¿ç”¨ã€‚
>
> å¦‚æœ‰ä»»ä½•é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·é€šè¿‡ **[ç¤¾åŒºIssue](https://gitee.com/mindspore/mindformers/issues/new)** æäº¤åé¦ˆã€‚æ„Ÿè°¢æ‚¨çš„ç†è§£ä¸æ”¯æŒï¼

## æ¨¡å‹æè¿°

LLaVA 1.5æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯è®­ç»ƒçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼Œè¿æ¥è§†è§‰ç¼–ç å™¨å’Œå¤§è¯­è¨€æ¨¡å‹ï¼Œä»¥å®ç°é€šç”¨è§†è§‰å’Œè¯­è¨€ç†è§£ï¼Œé€šè¿‡åœ¨ GPT ç”Ÿæˆçš„å¤šæ¨¡å¼æŒ‡ä»¤è·Ÿè¸ªæ•°æ®ä¸Šå¾®è°ƒ LLaMA/Vicuna è¿›è¡Œè®­ç»ƒã€‚å®ƒæ˜¯ä¸€ç§åŸºäº Transformer æ¶æ„çš„è‡ªå›å½’è¯­è¨€æ¨¡å‹ã€‚

```text
@inproceedings{liu2023llava,
    author      = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
    title       = {Visual Instruction Tuning},
    booktitle   = {NeurIPS},
    year        = {2023}
  }
```

## æ¨¡å‹æ–‡ä»¶

`Llava1.5` åŸºäº `mindformers` å®ç°ï¼Œä¸»è¦æ¶‰åŠçš„æ–‡ä»¶æœ‰ï¼š

1. æ¨¡å‹å…·ä½“å®ç°ï¼š

   ```text
   research/llava/
       â”œâ”€â”€ llava_model.py            # æ¨¡å‹å®ç°
       â””â”€â”€ llava_config.py           # æ¨¡å‹é…ç½®
   ```

2. æ¨¡å‹é…ç½®ï¼š

   ```text
   research/llava/
       â””â”€â”€ llava1_5_7B
                â””â”€â”€ predict_llava1_5_7b.yaml     # 7Bæ¨ç†é…ç½®
   ```

## ç¯å¢ƒåŠæ•°æ®å‡†å¤‡

### å®‰è£…ç¯å¢ƒ

MindFormersè½¯ç¡¬ä»¶é…å¥—å…³ç³»ä»¥åŠå®‰è£…å‚è€ƒ[ç¯å¢ƒå®‰è£…æŒ‡å—](../../README_CN.md#æºç ç¼–è¯‘å®‰è£…)å’Œ[ç‰ˆæœ¬åŒ¹é…å…³ç³»](../../README_CN.md#ç‰ˆæœ¬åŒ¹é…å…³ç³»)ã€‚

|     æ¨¡å‹     |     ç¡¬ä»¶      | æ¨ç† |
| :----------: | :-----------: | :--: |
| Llava-1.5-7b | Atlas 800T A2 | å•å¡ |

### æƒé‡å‡†å¤‡

æ•°æ®é¢„å¤„ç†ä¸­æ‰€ç”¨çš„`tokenizer.model`å¯ä»¥å‚è€ƒ[æ¨¡å‹æƒé‡ä¸‹è½½](#æ¨¡å‹æƒé‡ä¸‹è½½)è¿›è¡Œä¸‹è½½ã€‚

#### æ¨¡å‹æƒé‡ä¸‹è½½

MindFormersæš‚æ—¶æ²¡æœ‰æä¾›æƒé‡ï¼Œç”¨æˆ·å¯ä»¥ä¸‹è½½HuggingFaceå®˜æ–¹æƒé‡ç»è¿‡[æ¨¡å‹æƒé‡è½¬æ¢](#æ¨¡å‹æƒé‡è½¬æ¢)åè¿›è¡Œä½¿ç”¨ã€‚

è¯è¡¨ä¸‹è½½é“¾æ¥ï¼š[tokenizer.model](https://huggingface.co/llava-hf/llava-1.5-7b-hf/blob/main/tokenizer.model)

| æ¨¡å‹åç§°    | MindSporeæƒé‡ |                       HuggingFaceæƒé‡                        |
| :---------- | :-----------: | :----------------------------------------------------------: |
| Llava1.5-7B |       \       | [Link](https://huggingface.co/llava-hf/llava-1.5-7b-hf/tree/main) |

#### æ¨¡å‹æƒé‡è½¬æ¢

ä¸‹è½½å®Œæˆåï¼Œè¿è¡Œ`mindformers/convert_weight.py`è½¬æ¢è„šæœ¬ï¼Œå°†huggingfaceçš„æƒé‡è½¬æ¢ä¸ºå®Œæ•´çš„ckptæƒé‡ã€‚

```shell
python convert_weight.py --model llava --input_path TORCH_CKPT_DIR --output_path {path}/MS_CKPT_NAME

# å‚æ•°è¯´æ˜
model:       æ¨¡å‹åç§°
input_path:  ä¸‹è½½HuggingFaceæƒé‡çš„æ–‡ä»¶å¤¹è·¯å¾„
output_path: è½¬æ¢åçš„MindSporeæƒé‡æ–‡ä»¶ä¿å­˜è·¯å¾„
```

## æ¨ç†

è¿›è¡Œæ¨ç†å‰ï¼Œæ¨¡å‹æƒé‡ä»¥åŠtokenizeræ–‡ä»¶å¯å‚è€ƒ[æ¨¡å‹æƒé‡ä¸‹è½½](#æ¨¡å‹æƒé‡ä¸‹è½½)è¿›è¡Œå‡†å¤‡ï¼Œå¹¶ä¿®æ”¹`predict_llava1_5_7b.yaml`ä¸­ç›¸å…³é…ç½®ï¼Œè¡¥å……è¯è¡¨è·¯å¾„ã€‚

   ```yaml
   processor:
     tokenizer:
       add_bos_token: True
       add_eos_token: False
       vocab_file: "/path/to/tokenizer.model"
       type: LlavaTokenizer
       auto_register: llava_tokenizer.LlavaTokenizer
   ```

### å•å¡æ¨ç†

ä»¥`llava1.5-7b`å•å¡æ¨ç†ä¸ºä¾‹ï¼Œä»¥ä¸‹æä¾›æ¨ç†æ ·ä¾‹ã€‚

- é¦–å…ˆä¸‹è½½ç¤ºä¾‹å›¾ç‰‡[demo.jpeg](https://ascend-repo-modelzoo.obs.cn-east-2.myhuaweicloud.com/MindFormers/qwenvl/demo.jpeg)ï¼Œå¹¶åœ¨å¦‚ä¸‹ç¤ºä¾‹è„šæœ¬ä¸­çš„`--predict_data`å¤„æŒ‡å®šè¯¥å›¾ç‰‡è·¯å¾„ã€‚
- è¿è¡Œå¦‚ä¸‹è„šæœ¬

```shell
python run_mindformer.py \
--config research/llava/llava1_5_7B/predict_llava1_5_7b.yaml \
--register_path research/llava \
--run_mode predict \
--predict_data 'path of demo.jpeg' 'Describe the image in English:' \ # ä¾æ¬¡ä¼ å…¥å›¾ç‰‡è·¯å¾„æˆ–é“¾æ¥ã€æè¯
--modal_type image text \ # å¯¹åº”æ¨¡æ€ä¸ºimageå’Œtext
--load_checkpoint /path/to/ckpt \
--use_parallel False \
--auto_trans_ckpt False
# load_checkpoint: å•å¡æ¨ç†éœ€ä¼ å…¥å®Œæ•´æƒé‡çš„ckptè·¯å¾„
# auto_trans_ckpt: å•å¡æ¨ç†ä¸è¿›è¡Œæƒé‡è½¬æ¢ï¼Œä¼ å…¥False
```

### å¤šå¡æ¨ç†

ä»¥`Llava1.5-7b`2å¡æ¨ç†ä¸ºä¾‹ï¼Œè¿›è¡Œæ¨ç†å‰ï¼Œè¿˜éœ€ä¿®æ”¹å¹¶è¡Œé…ç½®

   ```yaml
    parallel_config:
      data_parallel: 1
      model_parallel: 2 # å¯¹äº2å¡å¹¶è¡Œè®¾ç½®mp=2
      pipeline_stage: 1
      use_seq_parallel: False
      micro_batch_num: 1
      vocab_emb_dp: True
      gradient_aggregation_group: 4
   micro_batch_interleave_num: 1
   ```

æ­¤åè¿è¡Œå¹¶è¡Œè„šæœ¬msrun_launcher.shæ‹‰èµ·å¹¶è¡Œæ¨ç†è¿›ç¨‹

- é¦–å…ˆä¸‹è½½ç¤ºä¾‹å›¾ç‰‡[demo.jpeg](https://ascend-repo-modelzoo.obs.cn-east-2.myhuaweicloud.com/MindFormers/qwenvl/demo.jpeg)ï¼Œå¹¶åœ¨å¦‚ä¸‹ç¤ºä¾‹è„šæœ¬ä¸­çš„`--predict_data`å¤„æŒ‡å®šè¯¥å›¾ç‰‡è·¯å¾„ã€‚
- è¿è¡Œå¦‚ä¸‹è„šæœ¬

```shell
bash scripts/msrun_launcher.sh "run_mindformer.py \
--config research/llava/llava1_5_7B/predict_llava1_5_7b.yaml \
--register_path research/llava \
--run_mode predict \
--predict_data 'path of demo.jpeg' 'Describe the image in English:' \ # ä¾æ¬¡ä¼ å…¥å›¾ç‰‡è·¯å¾„æˆ–é“¾æ¥ã€æè¯
--modal_type image text \ # å¯¹åº”æ¨¡æ€ä¸ºimageå’Œtext
--load_checkpoint /path/to/ckpt \
--use_parallel True \
--auto_trans_ckpt True" 2
# load_checkpoint: å½“ä½¿ç”¨å®Œæ•´æƒé‡æ—¶ä¼ å…¥ckptè·¯å¾„ï¼›å½“ä½¿ç”¨åˆ†å¸ƒå¼æƒé‡æ—¶ä¼ å…¥æƒé‡æ–‡ä»¶å¤¹è·¯å¾„model_dirï¼Œæƒé‡æŒ‰ç…§'model_dir/rank_0/xxx.ckpt'æ ¼å¼å­˜æ”¾
# auto_trans_ckpt: è‡ªåŠ¨æƒé‡è½¬æ¢å¼€å…³ï¼Œå½“ä¼ å…¥å®Œæ•´æƒé‡æ—¶æ‰“å¼€
```
