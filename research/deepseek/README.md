# DeepSeek Coder

> ## ğŸš¨ å¼ƒç”¨è¯´æ˜
>
> æœ¬æ¨¡å‹å·²è¿‡æ—¶ï¼Œä¸å†è¿›è¡Œç»´æŠ¤ï¼Œå¹¶å°†åœ¨ *1.6.0* ç‰ˆæœ¬ä¸‹æ¶ã€‚å¦‚éœ€ä½¿ç”¨æ­¤æ¨¡å‹ï¼Œå»ºè®®æ ¹æ®å®˜æ–¹æ–‡æ¡£ä¸­çš„ **[æ¨¡å‹åº“](https://www.mindspore.cn/mindformers/docs/zh-CN/r1.5.0/start/models.html)** é€‰æ‹©åˆé€‚çš„ç‰ˆæœ¬è¿›è¡Œä½¿ç”¨ã€‚
>
> å¦‚æœ‰ä»»ä½•é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·é€šè¿‡ **[ç¤¾åŒºIssue](https://gitee.com/mindspore/mindformers/issues/new)** æäº¤åé¦ˆã€‚æ„Ÿè°¢æ‚¨çš„ç†è§£ä¸æ”¯æŒï¼

## æ¨¡å‹æè¿°

DeepSeek Coderç”±ä¸€ç³»åˆ—ä»£ç è¯­è¨€æ¨¡å‹ç»„æˆï¼Œæ¯ä¸ªæ¨¡å‹éƒ½åœ¨2T tokenä¸Šä»é›¶å¼€å§‹è®­ç»ƒï¼Œå…¶ä¸­87%çš„ä»£ç å’Œ13%çš„è‡ªç„¶è¯­è¨€ç»„æˆï¼Œè‹±æ–‡å’Œä¸­æ–‡éƒ½æœ‰ã€‚åœ¨ç¼–ç åŠŸèƒ½æ–¹é¢ï¼ŒDeepSeek Coderåœ¨å¤šç§ç¼–ç¨‹è¯­è¨€å’Œå„ç§åŸºå‡†æµ‹è¯•ä¸Šçš„å¼€æºä»£ç æ¨¡å‹ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

## æ¨¡å‹æ€§èƒ½

ä»¥ä¸‹æ¨¡å‹æ€§èƒ½å‡ç”±Atlas 800T A2ç¡¬ä»¶ç¯å¢ƒä¸‹æµ‹è¯•å¾—å‡ºã€‚

| Config                                       |      Task       |  Datasets   | SeqLength |  Phase   |  Performance   |
|:---------------------------------------------|:---------------:|:-----------:|:---------:|:--------:|:--------------:|
| [deepseek-33b](deepseek_33b/predict_deepseek_33b.yaml)  | text_generation |      -      |   16384   | Predict  |  292 tokens/s  |

ä»¥ä¸‹æ¨¡å‹æ€§èƒ½å‡ç”±Atlas 900 A2 PoDcç¡¬ä»¶ç¯å¢ƒä¸‹æµ‹è¯•å¾—å‡ºã€‚

| Config                                       |      Task       |  Datasets   | SeqLength |  Phase   |  Performance   |
|:---------------------------------------------|:---------------:|:-----------:|:---------:|:--------:|:--------------:|
| [deepseek-33b](deepseek_33b/finetune_deepseek_33b.yaml) | text_generation | code_alpaca |   4096    | Finetune | 572 tokens/s/p |

## æ¨¡å‹æ–‡ä»¶

`deepseek_33b` åŸºäº `mindformers` å®ç°ï¼Œä¸»è¦æ¶‰åŠçš„æ–‡ä»¶æœ‰ï¼š

1. æ¨¡å‹é…ç½®ï¼š

    ```text
    research/deepseek/deepseek_33b
        â”œâ”€â”€ finetune_deepseek_33b.yaml     # å…¨å‚å¾®è°ƒå¯åŠ¨é…ç½®
        â”œâ”€â”€ pretrain_deepseek_33b_16k.yaml # é¢„è®­ç»ƒå¯åŠ¨é…ç½®
        â””â”€â”€ predict_deepseek_33b.yaml      # huggingfaceè½¬ckpt

    ```

2. æ•°æ®é¢„å¤„ç†è„šæœ¬ï¼š

   ```text
    research/deepseek
        â”œâ”€â”€ alpaca_converter.py           # code_alpacaæ•°æ®é›†æ ¼å¼è½¬æ¢è„šæœ¬
        â””â”€â”€ deepseek_preprocess.py        # æ•°æ®é›†é¢„å¤„ç†è„šæœ¬
    ```

## ç¯å¢ƒåŠæ•°æ®å‡†å¤‡

### å®‰è£…ç¯å¢ƒ

MindFormersè½¯ç¡¬ä»¶é…å¥—å…³ç³»ä»¥åŠå®‰è£…å‚è€ƒ[ç¯å¢ƒå®‰è£…æŒ‡å—](../../README_CN.md#æºç ç¼–è¯‘å®‰è£…)å’Œ[ç‰ˆæœ¬åŒ¹é…å…³ç³»](../../README_CN.md#ç‰ˆæœ¬åŒ¹é…å…³ç³»)ã€‚

> æ³¨ï¼šAtlas 800T A2èŠ¯ç‰‡æ”¯æŒ33bå•æœº4å¡æ¨ç†ï¼Œå…¨å‚å¾®è°ƒè‡³å°‘éœ€è¦2æœº16å¡ï¼Œé¢„è®­ç»ƒè‡³å°‘éœ€è¦2æœº16å¡ã€‚

### æ•°æ®åŠæƒé‡å‡†å¤‡

#### æ•°æ®é›†ä¸‹è½½

MindFormersæä¾›`Wikitext-103`ä½œä¸º[é¢„è®­ç»ƒ](#é¢„è®­ç»ƒ)æ•°æ®é›†ï¼Œ`code_alpaca`ä½œä¸º[å…¨å‚å¾®è°ƒ](#å…¨å‚å¾®è°ƒ)æ•°æ®é›†ã€‚

| æ•°æ®é›†åç§°        |     é€‚ç”¨æ¨¡å‹     |   é€‚ç”¨é˜¶æ®µ   |                                            ä¸‹è½½é“¾æ¥                                            |
|:-------------|:------------:|:--------:|:------------------------------------------------------------------------------------------:|
| Wikitext-103 | deepseek_33b | Pretrain | [Link](https://dagshub.com/DagsHub/WIkiText-103/src/main/dataset/tokens/wiki.train.tokens) |
| code_alpaca  | deepseek_33b | Finetune |  [Link](https://github.com/sahil280114/codealpaca/blob/master/data/code_alpaca_20k.json)   |

æ•°æ®é¢„å¤„ç†ä¸­æ‰€ç”¨çš„`tokenizer.json`å¯ä»¥é€šè¿‡[é“¾æ¥](https://huggingface.co/deepseek-ai/deepseek-coder-33b-instruct/blob/main/tokenizer.json)è¿›è¡Œä¸‹è½½ã€‚

- **Wikitext-103 æ•°æ®é¢„å¤„ç†**

  ä½¿ç”¨`research/deepseek/deepseek_preprocess.py`å¯¹ä¸‹è½½åçš„æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼Œå¹¶ç”ŸæˆMindrecordæ•°æ®ã€‚

  ```bash
  python deepseek_preprocess.py \
   --dataset_type 'wiki' \
   --input_glob /path/wiki.train.tokens \
   --model_file /path/tokenizer.json \
   --seq_length 16384 \
   --output_file /path/wiki.mindrecord

  # å‚æ•°è¯´æ˜
  dataset_type: é¢„å¤„ç†æ•°æ®ç±»å‹
  input_glob:   è¾“å…¥ä¸‹è½½åwiki.train.tokensçš„æ–‡ä»¶è·¯å¾„
  model_file:   vocab.jsonæ–‡ä»¶è·¯å¾„
  seq_length:   è¾“å‡ºæ•°æ®çš„åºåˆ—é•¿åº¦
  output_file:  è¾“å‡ºæ–‡ä»¶çš„ä¿å­˜è·¯å¾„
  ```

- **code_alpaca æ•°æ®é¢„å¤„ç†**

  æ‰§è¡Œ`research/deepseek/alpaca_converter.py`ï¼Œå°†åŸå§‹æ•°æ®é›†è½¬æ¢ä¸ºæŒ‡å®šæ ¼å¼ã€‚

  ```shell
  python alpaca_converter.py \
   --data_path path/code_alpaca_20k.json \
   --output_path path/alpaca-data-messages.json

  # å‚æ•°è¯´æ˜
  data_path:   è¾“å…¥ä¸‹è½½åcode_alpacaçš„æ–‡ä»¶è·¯å¾„
  output_path: è¾“å‡ºè½¬æ¢åæ–‡ä»¶çš„ä¿å­˜è·¯å¾„
  ```

  æ‰§è¡Œ`research/deepseek/deepseek_preprocess.py`ï¼Œè¿›è¡Œæ•°æ®é¢„å¤„ç†å’ŒMindrecordæ•°æ®ç”Ÿæˆã€‚

  ```shell
  python deepseek_preprocess.py \
   --dataset_type qa \
   --input_glob /path/alpaca-data-messages.json \
   --model_file /path/tokenizer.json \
   --seq_length 4096 \
   --output_file /path/alpaca-messages.mindrecord

  # å‚æ•°è¯´æ˜
  dataset_type: é¢„å¤„ç†æ•°æ®ç±»å‹
  input_glob:   è½¬æ¢åçš„alpacaçš„æ–‡ä»¶è·¯å¾„
  model_file:   tokenizer.jsonæ–‡ä»¶è·¯å¾„
  seq_length:   è¾“å‡ºæ•°æ®çš„åºåˆ—é•¿åº¦
  output_file:  è¾“å‡ºæ–‡ä»¶çš„ä¿å­˜è·¯å¾„
  ```

### æ¨¡å‹æƒé‡ä¸‹è½½

MindFormersæä¾›ä¸‹è½½HuggingFaceå®˜æ–¹æƒé‡çš„ä¸‹è½½é“¾æ¥ï¼Œç”¨æˆ·å¯é€šè¿‡é“¾æ¥ä¸‹è½½æƒé‡å¹¶ç»è¿‡[æ¨¡å‹æƒé‡è½¬æ¢](#æ¨¡å‹æƒé‡è½¬æ¢)åè¿›è¡Œä½¿ç”¨ï¼Œ`tokenizer.json`æ–‡ä»¶ä¹Ÿåœ¨é“¾æ¥ä¸­ä¸‹è½½ã€‚

è¯è¡¨ä¸‹è½½é“¾æ¥ï¼š[tokenizer.json](https://huggingface.co/deepseek-ai/deepseek-coder-33b-instruct/blob/main/tokenizer.json)

| æ¨¡å‹åç§°         | MindSporeæƒé‡ |                             HuggingFaceæƒé‡                              |
|:-------------|:-----------:|:----------------------------------------------------------------------:|
| deepseek-33b |      -      | [Link](https://huggingface.co/deepseek-ai/deepseek-coder-33b-instruct) |

#### æ¨¡å‹æƒé‡è½¬æ¢

æ‰§è¡Œ`convert_weight.py`è½¬æ¢è„šæœ¬ï¼Œå°†HuggingFaceçš„æƒé‡è½¬æ¢ä¸ºå®Œæ•´çš„ckptæƒé‡ã€‚

```shell
python convert_weight.py \
--model deepseek \
--input_path /path/ckpt \
--output_path MS_CKPT_NAME

# å‚æ•°è¯´æ˜
torch_ckpt_path: ä¸‹è½½HuggingFaceæƒé‡æ–‡ä»¶å¤¹è·¯å¾„
mindspore_ckpt_path: è½¬æ¢åçš„MindSporeæƒé‡æ–‡ä»¶ä¿å­˜è·¯å¾„
```

- **æ¨¡å‹æƒé‡åˆ‡åˆ†ä¸åˆå¹¶**

  ä»hugging faceæˆ–å®˜æ–¹githubä»“åº“è½¬æ¢è€Œæ¥çš„æƒé‡é€šå¸¸æ˜¯å•å¡æƒé‡ï¼ŒåŸºäºè¯¥æƒé‡è¿›è¡Œå¤šå¡å¾®è°ƒï¼Œè¯„æµ‹ï¼Œæ¨ç†ï¼Œæ¶‰åŠckptä»å•æœºç­–ç•¥åˆ°åˆ†å¸ƒå¼ç­–ç•¥çš„åˆ‡æ¢ã€‚

  é€šå¸¸è®­ç»ƒé‡‡ç”¨åˆ†å¸ƒå¼è®­ç»ƒï¼ŒåŸºäºè¯¥æƒé‡è¿›è¡Œè¯„æµ‹ï¼Œæ¨ç†å¤šé‡‡ç”¨å•å¡ï¼Œæ¶‰åŠckptä»åˆ†å¸ƒå¼ç­–ç•¥åˆ°å•æœºç­–ç•¥çš„åˆ‡æ¢ã€‚

  ä»¥ä¸Šæ¶‰åŠåˆ°ckptçš„å•å¡ï¼Œå¤šå¡è½¬æ¢ï¼Œè¯¦ç»†æ•™ç¨‹è¯·å‚è€ƒç‰¹æ€§æ–‡æ¡£[åˆ†å¸ƒå¼æƒé‡åˆ‡åˆ†ä¸åˆå¹¶](https://www.mindspore.cn/mindformers/docs/zh-CN/r1.5.0/function/transform_weight.html)

## é¢„è®­ç»ƒ

MindFormersæä¾›`deepseek-33b`å¤šæœºé¢„è®­ç»ƒç¤ºä¾‹ï¼Œï¼Œä½¿ç”¨`Wikitext-103`æ•°æ®é›†å¯¹æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œæ•°æ®é›†å¯ä»¥å‚è€ƒ[æ•°æ®é›†ä¸‹è½½](#æ•°æ®é›†ä¸‹è½½)è·å¾—ã€‚

`deepseek-33b`å¤šæœºé¢„è®­ç»ƒä½¿ç”¨é…ç½®æ–‡ä»¶`pretrain_deepseek_33b_16k.yaml`ï¼Œä¸æ”¯æŒå•æœºè¿›è¡Œé¢„è®­ç»ƒä»»åŠ¡ã€‚

å¤šæœºå¤šå¡è®­ç»ƒéœ€è¦ä¸åŒèŠ‚ç‚¹ä¸Šæ‰§è¡Œå¯åŠ¨å‘½ä»¤ï¼Œå°†å‚æ•°`MASTER_ADDR`è®¾ç½®ä¸ºä¸»èŠ‚ç‚¹çš„ipåœ°å€ï¼Œ æ‰€æœ‰èŠ‚ç‚¹è®¾ç½®çš„ipåœ°å€ç›¸åŒï¼Œä¸åŒèŠ‚ç‚¹ä¹‹é—´ä»…å‚æ•°`NODE_RANK`ä¸åŒï¼Œå…·ä½“å¯å‚è€ƒ[ä½¿ç”¨æŒ‡å—](../../README_CN.md#ä¸‰ä½¿ç”¨æŒ‡å—)ã€‚

æ‰§è¡Œå¦‚ä¸‹å‘½ä»¤å¯åŠ¨2æœº16å¡é¢„è®­ç»ƒä»»åŠ¡ï¼š

```shell
# èŠ‚ç‚¹0ï¼ŒèŠ‚ç‚¹ipä¸º{ip_addr}ï¼Œä½œä¸ºä¸»èŠ‚ç‚¹ï¼Œæ€»å…±16å¡ä¸”æ¯ä¸ªèŠ‚ç‚¹8å¡
bash scripts/msrun_launcher.sh "run_mindformer.py \
 --config research/deepseek/pretrain_deepseek_33b_16k.yaml \
 --train_dataset_dir /path/wiki.mindrecord \
 --use_parallel True \
 --run_mode train" \
 16 8 {ip_addr} 8118 0 output/msrun_log False 300

# èŠ‚ç‚¹1ï¼ŒèŠ‚ç‚¹0ä¸èŠ‚ç‚¹1å¯åŠ¨å‘½ä»¤ä»…å‚æ•°NODE_RANKä¸åŒ
bash scripts/msrun_launcher.sh "run_mindformer.py \
 --config research/deepseek/pretrain_deepseek_33b_16k.yaml \
 --train_dataset_dir /path/wiki.mindrecord \
 --use_parallel True \
 --run_mode train" \
 16 8 {ip_addr} 8118 1 output/msrun_log False 300

# å‚æ•°è¯´æ˜
config:            é…ç½®æ–‡ä»¶è·¯å¾„
run_mode:          è¿è¡Œæ¨¡å¼, é¢„è®­ç»ƒæ—¶è®¾ç½®ä¸ºtrain
train_dataset_dir: è®­ç»ƒæ•°æ®é›†è·¯å¾„
use_parallel:      æ˜¯å¦å¼€å¯å¹¶è¡Œè®­ç»ƒ
```

> æ³¨ï¼šæ­¤æ¨¡å‹æš‚ä¸æ”¯æŒé…ç½®`context_parallel`ï¼Œå› æ­¤æš‚ä¸æ”¯æŒé•¿åºåˆ—ã€‚

## å…¨å‚å¾®è°ƒ

MindFormersæä¾›`deepseek-33b`å¤šæœºå¤šå¡å¾®è°ƒç¤ºä¾‹ï¼Œä½¿ç”¨`code_alpaca`æ•°æ®é›†å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œæ•°æ®é›†å¯ä»¥å‚è€ƒ[æ•°æ®é›†ä¸‹è½½](#æ•°æ®é›†ä¸‹è½½)è·å¾—ã€‚

`deepseek-33b`å¤šæœºå¾®è°ƒä½¿ç”¨é…ç½®æ–‡ä»¶`finetune_deepseek_33b.yaml`ï¼Œä¸æ”¯æŒå•æœºè¿›è¡Œå¾®è°ƒä»»åŠ¡ã€‚

1. ç”Ÿæˆå¤šæœºåˆ†å¸ƒå¼æƒé‡

   å¦‚æœä½¿ç”¨å…±äº«å­˜å‚¨ï¼Œå¯ä»¥å°†æ¨¡å‹å®Œæ•´æƒé‡æ”¾åœ¨å…±äº«å­˜å‚¨å†…ï¼ŒåŒæ—¶è®¾ç½®é…ç½®æ–‡ä»¶æˆ–è„šæœ¬å‚æ•°`auto_trans_ckpt=True`ï¼Œä½¿ç”¨æƒé‡è‡ªåŠ¨è½¬æ¢åŠŸèƒ½ã€‚

   å¦‚æœä¸ä½¿ç”¨å…±äº«å­˜å‚¨ï¼Œå¯ä»¥å‚è€ƒ[å¤šå¡æƒé‡ä¹‹é—´çš„è½¬æ¢](https://www.mindspore.cn/mindformers/docs/zh-CN/r1.5.0/function/transform_weight.html#%E5%9C%BA%E6%99%AF%E4%BA%8C-%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B9%8B%E9%97%B4%E6%97%A0%E5%85%B1%E4%BA%AB%E7%9B%98)å®Œæˆåˆ†å¸ƒå¼æƒé‡è½¬æ¢åæ‹‰èµ·é¢„è®­ç»ƒä»»åŠ¡ã€‚

2. æ‰§è¡Œå‘½ä»¤å¯åŠ¨2æœº16å¡å¾®è°ƒä»»åŠ¡ï¼Œä»¥ä½¿ç”¨å…±äº«å­˜å‚¨ä¸ºä¾‹

   ä¿®æ”¹æ¨¡å‹é…ç½®æ–‡ä»¶`research/deepseek/finetune_deepseek_33b.yaml`ä¸­åˆ†å¸ƒå¼å¹¶è¡Œç­–ç•¥ã€‚

   ```yaml
   parallel_config:
     data_parallel: 1
     model_parallel: 8
     pipeline_stage: 2
     micro_batch_num: 128
     vocab_emb_dp: True
     gradient_aggregation_group: 4
   ```

   å¤šæœºå¤šå¡è®­ç»ƒéœ€è¦ä¸åŒèŠ‚ç‚¹ä¸Šæ‰§è¡Œå¯åŠ¨å‘½ä»¤ï¼Œå°†å‚æ•°`MASTER_ADDR`è®¾ç½®ä¸ºä¸»èŠ‚ç‚¹çš„ipåœ°å€ï¼Œ æ‰€æœ‰èŠ‚ç‚¹è®¾ç½®çš„ipåœ°å€ç›¸åŒï¼Œä¸åŒèŠ‚ç‚¹ä¹‹é—´ä»…å‚æ•°`NODE_RANK`ä¸åŒï¼Œå…·ä½“å¯å‚è€ƒ[ä½¿ç”¨æŒ‡å—](../../README_CN.md#ä¸‰ä½¿ç”¨æŒ‡å—)ã€‚

   ```shell
   # èŠ‚ç‚¹0ï¼ŒèŠ‚ç‚¹ipä¸º{ip_addr}ï¼Œä½œä¸ºä¸»èŠ‚ç‚¹ï¼Œæ€»å…±16å¡ä¸”æ¯ä¸ªèŠ‚ç‚¹8å¡
   bash scripts/msrun_launcher.sh "run_mindformer.py \
    --config research/deepseek/finetune_deepseek_33b.yaml \
    --load_checkpoint /path/deepseek_33b.ckpt \
    --train_dataset_dir /path/alpaca-messages.mindrecord \
    --use_parallel True \
    --auto_trans_ckpt True \
    --run_mode finetune" \
    16 8 {ip_addr} 8118 0 output/msrun_log False 300

   # èŠ‚ç‚¹1ï¼ŒèŠ‚ç‚¹0ä¸èŠ‚ç‚¹1å¯åŠ¨å‘½ä»¤ä»…å‚æ•°NODE_RANKä¸åŒ
   bash scripts/msrun_launcher.sh "run_mindformer.py \
    --config research/deepseek/finetune_deepseek_33b.yaml \
    --load_checkpoint /path/deepseek_33b.ckpt \
    --train_dataset_dir /path/alpaca-messages.mindrecord \
    --use_parallel True \
    --auto_trans_ckpt True \
    --run_mode finetune" \
    16 8 {ip_addr} 8118 1 output/msrun_log False 300

   # å‚æ•°è¯´æ˜
   config:            é…ç½®æ–‡ä»¶è·¯å¾„
   load_checkpoint:   æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„
   train_dataset_dir: è®­ç»ƒæ•°æ®é›†è·¯å¾„
   use_parallel:      æ˜¯å¦å¼€å¯å¹¶è¡Œè®­ç»ƒ
   auto_trans_ckpt:   è‡ªåŠ¨æƒé‡è½¬æ¢å¼€å…³
   run_mode:          è¿è¡Œæ¨¡å¼, å¾®è°ƒæ—¶è®¾ç½®ä¸ºfinetune
   ```

## æ¨ç†

MindFormersæä¾›`deepseek-33b`æ¨ç†ç¤ºä¾‹ï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶`predict_deepseek_33b.yaml`ï¼Œä»…æ”¯æŒå¤šå¡æ¨ç†ã€‚

1. ä¿®æ”¹é…ç½®æ–‡ä»¶`research/deepseek/deepseek_33b/predict_deepseek_33b.yaml`

   ```yaml
   processor:
     tokenizer:
       vocab_file: None
       tokenizer_file: "/path/tokenizer.json"
   ```

2. æ‰§è¡Œ4å¡æ¨ç†å‘½ä»¤ï¼š

   ```shell
   bash scripts/msrun_launcher.sh "run_mindformer.py \
    --config research/deepseek/predict_deepseek_33b.yaml \
    --run_mode predict \
    --predict_data '#write a quick sort algorithm' \
    --predict_length 100 \
    --use_parallel True \
    --use_past True" 4

   # æ¨ç†ç»“æœ
   # #write a quick sort algorithm
   # def quick_sort(arr):
   #     if len(arr) <= 1:
   #         return arr
   #     pivot = arr[len(arr) // 2]
   #     left = [x for x in arr if x < pivot]
   #     middle = [x for x in arr if x == pivot]
   #     right = [x for x in arr if x > pivot]
   #    return quick_sort(left) + middle + quick_sort(right)
   #
   # print(quick_sort([3,6,8,10,1,2,1]))
   # # Prints "[1, 1, 2, 3, 6, 8, 10]"
   ```
