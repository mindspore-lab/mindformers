# KnowLM

> ## ğŸš¨ å¼ƒç”¨è¯´æ˜
>
> æœ¬æ¨¡å‹å·²è¿‡æ—¶ï¼Œä¸å†è¿›è¡Œç»´æŠ¤ï¼Œå¹¶å°†åœ¨ *1.6.0* ä¹‹åçš„ç‰ˆæœ¬ä¸‹æ¶ã€‚å¦‚éœ€ä½¿ç”¨æ­¤æ¨¡å‹ï¼Œå»ºè®®æ ¹æ®å®˜æ–¹æ–‡æ¡£ä¸­çš„ **[æ¨¡å‹åº“](https://www.mindspore.cn/mindformers/docs/zh-CN/r1.5.0/start/models.html)** é€‰æ‹©åˆé€‚çš„ç‰ˆæœ¬è¿›è¡Œä½¿ç”¨ã€‚
>
> å¦‚æœ‰ä»»ä½•é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·é€šè¿‡ **[ç¤¾åŒºIssue](https://gitee.com/mindspore/mindformers/issues/new)** æäº¤åé¦ˆã€‚æ„Ÿè°¢æ‚¨çš„ç†è§£ä¸æ”¯æŒï¼

KnowLMæ˜¯ä¸€ä¸ªçŸ¥è¯†å¢å¼ºçš„å¼€æºè¯­è¨€å¤§æ¨¡å‹æ¡†æ¶ï¼Œæ—¨åœ¨æä¾›çµæ´»ä¸”å¯å®šåˆ¶çš„å·¥å…·é›†å’Œå¹¶å‘å¸ƒç›¸åº”çš„æ¨¡å‹ï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…æ›´å¥½åœ°å¤„ç†å¤§æ¨¡å‹çŸ¥è¯†æ›´æ–°å’ŒçŸ¥è¯†è°¬è¯¯ç­‰é—®é¢˜ï¼Œå…·ä½“åŒ…æ‹¬ï¼š

1.**çŸ¥è¯†æç¤º**ï¼šåŸºäºçŸ¥è¯†æç¤ºæŠ€æœ¯ä»çŸ¥è¯†å›¾è°±ç”Ÿæˆå’Œä¼˜åŒ–æŒ‡ä»¤æ•°æ®ä»¥è§£å†³çŸ¥è¯†æŠ½å–é—®é¢˜

2.**çŸ¥è¯†ç¼–è¾‘**ï¼šåŸºäºçŸ¥è¯†ç¼–è¾‘æŠ€æœ¯å¯¹é½å¤§æ¨¡å‹å†…è¿‡æ—¶åŠä»·å€¼è§‚ä¸æ­£ç¡®çš„çŸ¥è¯†ä»¥è§£å†³çŸ¥è¯†è°¬è¯¯é—®é¢˜

3.**çŸ¥è¯†äº¤äº’**ï¼šåŸºäºçŸ¥è¯†äº¤äº’æŠ€æœ¯å®ç°å·¥å…·ç»„åˆå­¦ä¹ åŠå¤šæ™ºèƒ½ä½“åä½œä»¥è§£å†³è¯­è¨€æ¨¡å‹å…·èº«è®¤çŸ¥é—®é¢˜

ç°é˜¶æ®µKnowLMå·²å‘å¸ƒåŸºäºLLaMA1çš„13BåŸºç¡€æ¨¡å‹ä¸€ä¸ªï¼ˆKnowLM-13B-Baseï¼‰ï¼ŒçŸ¥è¯†æŠ½å–å¤§æ¨¡å‹ä¸€ä¸ªï¼ˆKnowLM-13B-ZhiXiï¼ŒKnowLM-13B-IE2ä¸ªç‰ˆæœ¬ï¼‰ã€‚

é¡¹ç›®ä¸»é¡µï¼š[KnowLM](https://github.com/zjunlp/KnowLM)

## KnowLM-13B-ZhiXi

KnowLM-13B-Baseä»¥ LlaMA-13B ä¸ºåŸºç¡€ï¼Œä½¿ç”¨ä¸­è‹±æ–‡åŒè¯­æ•°æ®è¿›è¡Œäº†äºŒæ¬¡é¢„è®­ç»ƒï¼Œæé«˜äº†æ¨¡å‹å¯¹ä¸­æ–‡çš„ç†è§£èƒ½åŠ›ã€‚KnowLM-13B-ZhiXiåœ¨ Knowlm-13B-Base çš„åŸºç¡€ä¸Šï¼Œåˆ©ç”¨çŸ¥è¯†å›¾è°±è½¬æ¢æŒ‡ä»¤æŠ€æœ¯ç”Ÿæˆæ•°æ®å¯¹è¯¥æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒã€‚è¯¦æƒ…è¯·å‚è€ƒ[KnowLM](https://github.com/zjunlp/KnowLM)é¡¹ç›®

```text
@misc{knowlm,
  author = {Ningyu Zhang and Jintian Zhang and Xiaohan Wang and Honghao Gui and Kangwei Liu and Yinuo Jiang and Xiang Chen and Shengyu Mao and Shuofei Qiao and Yuqi Zhu and Zhen Bi and Jing Chen and Xiaozhuan Liang and Yixin Ou and Runnan Fang and Zekun Xi and Xin Xu and Lei Li and Peng Wang and Mengru Wang and Yunzhi Yao and Bozhong Tian and Yin Fang and Guozhou Zheng and Huajun Chen},
  title = {KnowLM: An Open-sourced Knowledgeable Large Language Model Framework},
  year = {2023},
 url = {http://knowlm.zjukg.cn/},
}

@article{wang2023easyedit,
  title={EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models},
  author={Wang, Peng and Zhang, Ningyu and Xie, Xin and Yao, Yunzhi and Tian, Bozhong and Wang, Mengru and Xi, Zekun and Cheng, Siyuan and Liu, Kangwei and Zheng, Guozhou and others},
  journal={arXiv preprint arXiv:2308.07269},
  year={2023}
}

@article{yao2023editing,
  title={Editing Large Language Models: Problems, Methods, and Opportunities},
  author={Yao, Yunzhi and Wang, Peng and Tian, Bozhong and Cheng, Siyuan and Li, Zhoubo and Deng, Shumin and Chen, Huajun and Zhang, Ningyu},
  journal={arXiv preprint arXiv:2305.13172},
  year={2023}
}
```

## å¿«é€Ÿä½¿ç”¨

### KnowLM-13B-ZhiXi é¢„è®­ç»ƒæƒé‡è½¬æ¢

ä»huggingfaceä¸‹è½½[KnowLM-13B-ZhiXi](https://huggingface.co/zjunlp/knowlm-13b-zhixi/tree/main);æŠŠæ–‡ä»¶å…¨éƒ¨ä¸‹è½½ä¸‹æ¥

æ‰§è¡Œæƒé‡è½¬æ¢è„šæœ¬

```shell
python research/knowlm/convert_weight.py --torch_bin_path TORCH_BIN_PATH --mindspore_ckpt_path MS_CKPT_PATH
```

```text
# å‚æ•°è¯´æ˜
TORCH_BIN_PATH: huggingfaceæƒé‡ä¿å­˜ç›®å½•ä¸‹ä»»æ„æƒé‡binæ–‡ä»¶ï¼Œæ ¹æ®è¯¥æ–‡ä»¶è·¯å¾„è¯»å–ç›®å½•ä¸‹æ‰€æœ‰æƒé‡
MS_CKPT_PATH: mindsporeæƒé‡ä¿å­˜æ–‡ä»¶è·¯å¾„
```

### APIæ–¹å¼è°ƒç”¨

> éœ€å¼€å‘è€…æå‰pipå®‰è£…ã€‚å…·ä½“æ¥å£è¯´æ˜è¯·å‚è€ƒ[APIæ¥å£](https://gitee.com/mindspore/transformer/wikis/API/)
> éµä»Knowlm-13B-zhixiçš„licenseï¼Œæœ¬æ¨¡å‹éœ€è¦ç”¨æˆ·è‡ªè¡Œä¸‹è½½æƒé‡è¿›è¡Œå¤„ç†

- pipelineæ¥å£å¼€å¯å¿«é€Ÿæ¨ç†

```python
from mindspore import context
from mindformers.pipeline import pipeline
from mindformers import LlamaForCausalLM, LlamaConfig, LlamaTokenizer

context.set_context(device_target="Ascend")
# init knowlm-13b-zhixi model
knowlm_model_path = "/path/to/your/weight.ckpt" # knowlm-13B-zhixi ckpt path
knowlm_config = LlamaConfig(
    seq_length=2048,
    vocab_size=32000,
    pad_token_id=0,
    checkpoint_name_or_path=knowlm_model_path,
    hidden_size=5120,
    num_layers=40,
    num_heads=40,
    rms_norm_eps=1e-6
)
knowlm_model = LlamaForCausalLM(
    config=knowlm_config
)
# init knowlm-13b-zhixi tokenizer
tokenizer_path = "/path/to/your/tokenizer" # knowlm-13B-zhixi tokenizer.model path
tokenizer = LlamaTokenizer(
    vocab_file=tokenizer_path
)
pipeline_task = pipeline("text_generation", model=knowlm_model, tokenizer=tokenizer, max_length=32)
peline_result = pipeline_task("ä½ éå¸¸äº†è§£ä¸€äº›å¥åº·ç”Ÿæ´»çš„ä¹ æƒ¯ï¼Œè¯·åˆ—ä¸¾å‡ ä¸ªå¥åº·ç”Ÿæ´»çš„å»ºè®®", top_k=3, do_sample=True, top_p=0.95, repetition_penalty=1.3, max_length=256)

print(peline_result)
#ä½ éå¸¸äº†è§£ä¸€äº›å¥åº·ç”Ÿæ´»çš„ä¹ æƒ¯ï¼Œè¯·åˆ—ä¸¾å‡ ä¸ªå¥åº·ç”Ÿæ´»çš„å»ºè®®ï¼š1.æ¯å¤©åšæŒé”»ç‚¼30åˆ†é’Ÿä»¥ä¸Šã€‚ 2.ä¸å¸çƒŸï¼Œä¸é…—é…’ã€‚ 3.å°‘åƒé«˜è„‚è‚ªé£Ÿç‰©ã€‚ 4.å¤šåƒè”¬èœå’Œæ°´æœã€‚ 5.ä¿è¯å……è¶³çš„ç¡çœ ã€‚ 6.ä¿æŒè‰¯å¥½çš„å¿ƒæƒ…ã€‚ 7.å®šæœŸä½“æ£€ã€‚ 8.å…»æˆè‰¯å¥½çš„å«ç”Ÿä¹ æƒ¯
```

### KnowLM-13B-ZhiXi Loraå¾®è°ƒè®­ç»ƒ

#### å‰æœŸå‡†å¤‡

ç¯å¢ƒè¦æ±‚å’Œå¾®è°ƒå‡†å¤‡å‚è€ƒ llama-7b-loraçš„å‰æœŸå‡†å¤‡

#### æ•°æ®é›†å‡†å¤‡

å¾®è°ƒè®­ç»ƒé‡‡ç”¨çš„æ•°æ®é›†ä¸ºalpacaæ•°æ®é›†ï¼Œæ•°æ®å¤„ç†éƒ¨åˆ†å¯ä»¥å‚è€ƒ llama-7bçš„æ•°æ®å¤„ç†è¿‡ç¨‹

ç»™å‡ºäº†knowlm-13b-zhixié€‚é…çš„loraé…ç½®æ–‡ä»¶-run_knowlm_13b.yaml

#### è„šæœ¬å¯åŠ¨

```sh
cd scripts
# å•å¡å¯åŠ¨
bash run_standalone.sh run_knowlm_13b.yaml [DEVICE_ID] finetune
# å¤šå¡å¯åŠ¨ï¼ˆä»¥å•æœºå…«å¡ä¸ºä¾‹ï¼‰
bash run_distribute.sh [RANK_TABLE_FILE] run_knowlm_13b.yaml [0,8] finetune
```

### è®­ç»ƒé€Ÿåº¦å’Œç²¾åº¦

æˆ‘ä»¬åœ¨åä¸ºæ˜‡è…¾NPUAscend 910 32GBæ˜¾å­˜ä¸Šè¿›è¡Œè®­ç»ƒï¼Œé‡‡ç”¨äº†fp32ï¼ˆå•ç²¾åº¦æµ®ç‚¹æ•°ï¼‰çš„æ•°æ®æ ¼å¼è¿›è¡Œè®¡ç®—ã€‚åœ¨æ¯ä¸ªstepä¸­ï¼ŒLoraæ‰€éœ€çš„æ—¶é—´çº¦ä¸º2480msï¼ŒåŒæ—¶ï¼Œæ¯ç§’å¤„ç†çš„æ ·æœ¬æ•°æ˜¯0.81samples s/p

åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„ç²¾åº¦å¦‚ä¸‹
|f1|A800-3epoch|v100-1.6epoch|Ascend-3epoch|
|-|-|-|-|
|GIDS|68.64|74.04|76.23|
|NYT11|72.43|75.51|75.14|
|SciERC|25.15|37.28|36.49|
|kbp37|93.44|95.48|95.73|

