# Llama 3.1

> ## ğŸš¨ å¼ƒç”¨è¯´æ˜
>
> æœ¬æ–‡æ¡£ä¸­çš„æ¨¡å‹å·²è¿‡æ—¶ï¼Œä¸å†è¿›è¡Œç»´æŠ¤ï¼Œå¹¶å°†åœ¨ *1.8.0* ç‰ˆæœ¬ä¸‹æ¶ã€‚å¦‚éœ€ä½¿ç”¨æ­¤æ¨¡å‹ï¼Œå»ºè®®æ ¹æ®å®˜æ–¹æ–‡æ¡£ä¸­çš„ **[æ¨¡å‹åº“](https://www.mindspore.cn/mindformers/docs/zh-CN/master/introduction/models.html)** é€‰æ‹©åˆé€‚çš„ç‰ˆæœ¬è¿›è¡Œä½¿ç”¨ã€‚
>
> å¦‚æœ‰ä»»ä½•é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·é€šè¿‡ **[ç¤¾åŒºIssue](https://gitee.com/mindspore/mindformers/issues/new)** æäº¤åé¦ˆã€‚æ„Ÿè°¢æ‚¨çš„ç†è§£ä¸æ”¯æŒï¼

## æ¨¡å‹æè¿°

Llama 3.1ï¼Œæ˜¯å¼€æºLlamaç³»åˆ—çš„æœ€æ–°äº§å“ï¼Œç›®å‰æœ‰ä¸‰ä¸ªç‰ˆæœ¬ï¼šLlama 3.1-8Bï¼ŒLlama 3.1-70Bï¼ŒLlama 3.1-405Bã€‚
Llama 3.1åœ¨æ¥è‡ªå…¬å¼€å¯ç”¨æ¥æºçš„è¶…è¿‡15Tçš„æ•°æ®ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒã€‚å¾®è°ƒæ•°æ®åŒ…æ‹¬å…¬å¼€å¯ç”¨çš„æŒ‡ä»¤æ•°æ®é›†ï¼Œä»¥åŠè¶…è¿‡1000ä¸‡ä¸ªäººå·¥æ ‡æ³¨çš„ç¤ºä¾‹ã€‚
æ¨¡å‹æ”¯æŒä¸Šä¸‹æ–‡çª—å£é•¿åº¦128Kï¼Œå¹¶ä½¿ç”¨äº†æ–°çš„åˆ†è¯å™¨ï¼Œè¯æ±‡è¡¨å¤§å°è¾¾åˆ°128256ä¸ªï¼Œé‡‡ç”¨äº†åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›æœºåˆ¶(GQA)ã€‚
Llama 3.1æ¨¡å‹æ˜¯ç±»GPTæ¨¡å‹ï¼Œæ˜¯ä¸€ä¸ªç”Ÿæˆå¼çš„è¯­è¨€æ¨¡å‹ï¼Œä¸»è¦æ˜¯ç”¨äºé¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ã€‚
ç›®å‰Mindformersæ”¯æŒLlama 3.1-8Bï¼ŒLlama 3.1-70Bï¼Œæ•¬è¯·æœŸå¾…Llama 3.1-405Bã€‚

## æ¨¡å‹æ€§èƒ½

ä»¥ä¸‹æ¨¡å‹æ€§èƒ½å‡ç”±Atlas 800T A2ç¡¬ä»¶ç¯å¢ƒä¸‹æµ‹è¯•å¾—å‡ºã€‚

| Config                                                 |      Task       | Datasets | SeqLength | Performance  |  Phase  |
|:-------------------------------------------------------|:---------------:|:--------:|:---------:|:------------:|:-------:|
| [llama3_1_8b](llama3_1_8b/predict_llama3_1_8b.yaml)    | text_generation |    -     |   2048    | 591 tokens/s | Predict |
| [llama3_1_70b](llama3_1_70b/predict_llama3_1_70b.yaml) | text_generation |    -     |   4096    | 509 tokens/s | Predict |

ä»¥ä¸‹æ¨¡å‹æ€§èƒ½å‡ç”±Atlas 900 A2 PoDcç¡¬ä»¶ç¯å¢ƒä¸‹æµ‹è¯•å¾—å‡ºã€‚

| Config                                                  |      Task       | Datasets | SeqLength |   Performance   |  Phase   |
|:--------------------------------------------------------|:---------------:|:--------:|:---------:|:---------------:|:--------:|
| [llama3_1_8b](llama3_1_8b/finetune_llama3_1_8b.yaml)    | text_generation |  alpaca  |   8192    | 2703 tokens/s/p | Finetune |
| [llama3_1_70b](llama3_1_70b/finetune_llama3_1_70b.yaml) | text_generation |  alpaca  |   8192    | 337 tokens/s/p  | Finetune |

## æ¨¡å‹æ–‡ä»¶

`Llama 3.1` åŸºäº `mindformers` å®ç°ï¼Œä¸»è¦æ¶‰åŠçš„æ–‡ä»¶æœ‰ï¼š

1. æ¨¡å‹å…·ä½“å®ç°ï¼š

   ```text
   mindformers/models/llama
       â”œâ”€â”€ __init__.py
       â”œâ”€â”€ llama.py                  # æ¨¡å‹å®ç°
       â”œâ”€â”€ llama_config.py           # æ¨¡å‹é…ç½®é¡¹
       â”œâ”€â”€ llama_layer.py            # llamaç½‘ç»œå±‚å®šä¹‰
       â”œâ”€â”€ llama_processor.py        # llamaé¢„å¤„ç†
       â””â”€â”€ llama_transformer.py      # transformerå±‚å®ç°
   ```

2. æ¨¡å‹é…ç½®ï¼š

   ```text
   research/llama3_1
       â”œâ”€â”€llama3_1_8b
       â”‚    â”œâ”€â”€ predict_llama3_1_8b.yaml     # 8Bæ¨ç†é…ç½®
       â”‚    â””â”€â”€ finetune_llama3_1_8b.yaml    # 8Bå…¨é‡å¾®è°ƒå¯åŠ¨é…ç½®
       â””â”€â”€llama3_1_70b
            â”œâ”€â”€ predict_llama3_1_70b.yaml    # 70Bæ¨ç†é…ç½®
            â””â”€â”€ finetune_llama3_1_70b.yaml   # 70Bå…¨é‡å¾®è°ƒå¯åŠ¨é…ç½®
   ```

3. æ•°æ®é¢„å¤„ç†è„šæœ¬å’Œä»»åŠ¡å¯åŠ¨è„šæœ¬ï¼š

   ```text
   research/llama3_1
       â”œâ”€â”€ llama3_1_tokenizer.py      # llama3_1 tokenizerå¤„ç†è„šæœ¬
       â”œâ”€â”€ llama3_1_conversation.py   # å¾®è°ƒæ•°æ®é›†å¤„ç†ï¼Œå°†åŸå§‹alpacaè½¬æ¢ä¸ºå¯¹è¯å½¢å¼alpaca
       â””â”€â”€ llama3_1_preprocess.py     # llamaæ¨¡å‹çš„mindrecordæ•°æ®å¤„ç†è„šæœ¬
   ```

## ç¯å¢ƒåŠæ•°æ®å‡†å¤‡

### å®‰è£…ç¯å¢ƒ

MindFormersè½¯ç¡¬ä»¶é…å¥—å…³ç³»ä»¥åŠå®‰è£…å‚è€ƒ[ç¯å¢ƒå®‰è£…æŒ‡å—](../../README_CN.md#æºç ç¼–è¯‘å®‰è£…)
å’Œ[ç‰ˆæœ¬åŒ¹é…å…³ç³»](../../README_CN.md#ç‰ˆæœ¬åŒ¹é…å…³ç³»)ã€‚

### æ•°æ®é›†åŠæƒé‡å‡†å¤‡

#### æ•°æ®é›†ä¸‹è½½

MindFormersæä¾›**alpaca**ä½œä¸º[å¾®è°ƒ](#å¾®è°ƒ)æ•°æ®é›†ã€‚

| æ•°æ®é›†åç§°   |              é€‚ç”¨æ¨¡å‹              |   é€‚ç”¨é˜¶æ®µ   |                                      ä¸‹è½½é“¾æ¥                                       |
|:--------|:------------------------------:|:--------:|:-------------------------------------------------------------------------------:|
| alpaca  | llama3_1-8b <br/> llama3_1-70b | Finetune | [Link](https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json) |

æ•°æ®é¢„å¤„ç†ä¸­æ‰€ç”¨çš„`tokenizer.model`å¯ä»¥å‚è€ƒ[æ¨¡å‹æƒé‡ä¸‹è½½](#æ¨¡å‹æƒé‡ä¸‹è½½)è¿›è¡Œä¸‹è½½ã€‚

- **alpaca æ•°æ®é¢„å¤„ç†**

    1. æ‰§è¡Œ`mindformers/tools/dataset_preprocess/llama/alpaca_converter.py`ï¼Œä½¿ç”¨fastchatå·¥å…·æ·»åŠ promptsæ¨¡æ¿ï¼Œå°†åŸå§‹æ•°æ®é›†è½¬æ¢ä¸ºå¤šè½®å¯¹è¯æ ¼å¼ã€‚

       ```shell
       python alpaca_converter.py \
         --data_path /{path}/alpaca_data.json \
         --output_path /{path}/alpaca-data-conversation.json

       # å‚æ•°è¯´æ˜
       data_path:   è¾“å…¥ä¸‹è½½çš„æ–‡ä»¶è·¯å¾„
       output_path: è¾“å‡ºæ–‡ä»¶çš„ä¿å­˜è·¯å¾„
       ```

    2. æ‰§è¡Œ`research/llama3_1/llama3_1_preprocess.py`ï¼Œç”ŸæˆMindrecordæ•°æ®ï¼Œå°†å¸¦æœ‰promptæ¨¡æ¿çš„æ•°æ®è½¬æ¢ä¸ºmindrecordæ ¼å¼ã€‚

       ```shell
       # æ­¤å·¥å…·ä¾èµ–fschatå·¥å…·åŒ…è§£æpromptæ¨¡æ¿, è¯·æå‰å®‰è£…fschat >= 0.2.13 python = 3.9
       python llama3_1_preprocess.py \
         --dataset_type qa \
         --input_glob /{path}/alpaca-data-conversation.json \
         --model_file /{path}/tokenizer.model \
         --seq_length 8192 \
         --output_file /{path}/alpaca-fastchat8192.mindrecord

       # å‚æ•°è¯´æ˜
       dataset_type: é¢„å¤„ç†æ•°æ®ç±»å‹
       input_glob:   è½¬æ¢åçš„alpacaçš„æ–‡ä»¶è·¯å¾„
       model_file:   æ¨¡å‹tokenizer.modelæ–‡ä»¶è·¯å¾„
       seq_length:   è¾“å‡ºæ•°æ®çš„åºåˆ—é•¿åº¦
       output_file:  è¾“å‡ºæ–‡ä»¶çš„ä¿å­˜è·¯å¾„
       ```

> æ•°æ®å¤„ç†æ—¶å€™æ³¨æ„bosï¼Œeosï¼Œpadç­‰ç‰¹æ®Š`ids`è¦å’Œé…ç½®æ–‡ä»¶ä¸­`model_config`é‡Œä¿æŒä¸€è‡´ã€‚

#### æ¨¡å‹æƒé‡ä¸‹è½½

MindFormersæš‚æ—¶æ²¡æœ‰æä¾›æƒé‡ï¼Œç”¨æˆ·å¯ä»¥ä¸‹è½½HuggingFaceå®˜æ–¹æƒé‡ç»è¿‡[æ¨¡å‹æƒé‡è½¬æ¢](#æ¨¡å‹æƒé‡è½¬æ¢)åè¿›è¡Œä½¿ç”¨ã€‚

è¯è¡¨ä¸‹è½½é“¾æ¥ï¼š[tokenizer.model](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B)

| æ¨¡å‹åç§°         | MindSporeæƒé‡ |                        HuggingFaceæƒé‡                         |
|:-------------|:-----------:|:------------------------------------------------------------:|
| Llama3_1-8B  |      -      | [Link](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B)  |
| Llama3_1-70B |      -      | [Link](https://huggingface.co/meta-llama/Meta-Llama-3.1-70B) |

> æ³¨: è¯·è‡ªè¡Œç”³è¯·huggingfaceä¸Šllama3_1ä½¿ç”¨æƒé™ï¼Œå¹¶å®‰è£…transformers=4.40ç‰ˆæœ¬

#### æ¨¡å‹æƒé‡è½¬æ¢

ä¸‹è½½å®Œæˆåï¼Œè¿è¡Œ`mindformers/convert_weight.py`è½¬æ¢è„šæœ¬ï¼Œå°†huggingfaceçš„æƒé‡è½¬æ¢ä¸ºå®Œæ•´çš„ckptæƒé‡ã€‚

```shell
python convert_weight.py --model llama --input_path TORCH_CKPT_DIR --output_path {path}/MS_CKPT_NAME --dtype bf16

# å‚æ•°è¯´æ˜
model:       æ¨¡å‹åç§°
input_path:  ä¸‹è½½HuggingFaceæƒé‡çš„æ–‡ä»¶å¤¹è·¯å¾„
output_path: è½¬æ¢åçš„MindSporeæƒé‡æ–‡ä»¶ä¿å­˜è·¯å¾„
dtype:       è½¬æ¢æƒé‡çš„ç²¾åº¦
```

## å¾®è°ƒ

### å…¨å‚å¾®è°ƒ

MindSpore Transformersæä¾› `Llama3.1-8B` å•æœºå¤šå¡ä»¥åŠ `Llama3.1-70B` å¤šæœºå¤šå¡çš„å¾®è°ƒç¤ºä¾‹ï¼Œè¿‡ç¨‹ä¸­ä½¿ç”¨ `alpaca`
æ•°æ®é›†å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œæ•°æ®é›†å¯ä»¥å‚è€ƒ[æ•°æ®é›†ä¸‹è½½](#æ•°æ®é›†ä¸‹è½½)è·å¾—ã€‚

#### å•æœºè®­ç»ƒ

ä»¥Llama3_1-8bä¸ºä¾‹ï¼ŒLlama3_1-8Båœ¨Atlas 800T A2ä¸Šè®­ç»ƒï¼Œæ”¯æŒ**å•æœº/å¤šæœºè®­ç»ƒ**ã€‚

ä½¿ç”¨`finetune_llama3_1_8b.yaml`è¿›è¡Œè®­ç»ƒï¼Œæˆ–ä¿®æ”¹é»˜è®¤é…ç½®æ–‡ä»¶ä¸­çš„`model_config.seq_length`
ï¼Œä½¿è®­ç»ƒé…ç½®ä¸æ•°æ®é›†çš„`seq_length`ä¿æŒä¸€è‡´ã€‚

æ‰§è¡Œå‘½ä»¤å¯åŠ¨å¾®è°ƒä»»åŠ¡ï¼Œåœ¨å•æœºä¸Šæ‹‰èµ·ä»»åŠ¡ã€‚

```shell
# å•æœº8å¡é»˜è®¤å¿«é€Ÿå¯åŠ¨
bash scripts/msrun_launcher.sh "run_mindformer.py \
 --register_path research/llama3_1 \
 --config research/llama3_1/llama3_1_8b/finetune_llama3_1_8b.yaml \
 --load_checkpoint model_dir/xxx.ckpt \
 --auto_trans_ckpt True \
 --use_parallel True \
 --run_mode finetune \
 --train_data dataset_dir"

# å‚æ•°è¯´æ˜
config:          é…ç½®æ–‡ä»¶è·¯å¾„
load_checkpoint: æƒé‡æ–‡ä»¶è·¯å¾„
auto_trans_ckpt: è‡ªåŠ¨æƒé‡è½¬æ¢å¼€å…³
run_mode:        è¿è¡Œæ¨¡å¼, å¾®è°ƒæ—¶è®¾ç½®ä¸ºfinetune
train_data:      è®­ç»ƒæ•°æ®é›†è·¯å¾„
```

#### å¤šæœºè®­ç»ƒ

ä»¥llama3_1-70bä¸ºä¾‹ï¼Œä½¿ç”¨`finetune_llama3_1_70b.yaml`é…ç½®æ–‡ä»¶ï¼Œæ‰§è¡Œ8æœº64å¡å¾®è°ƒã€‚éœ€è¦å…ˆå¯¹æƒé‡è¿›è¡Œåˆ‡åˆ†ï¼Œåˆ‡åˆ†æƒé‡å¯ä»¥å‚è§[æƒé‡åˆ‡åˆ†ä¸åˆå¹¶](https://www.mindspore.cn/mindformers/docs/zh-CN/master/feature/ckpt.html#%E6%9D%83%E9%87%8D%E5%88%87%E5%88%86%E4%B8%8E%E5%90%88%E5%B9%B6)ï¼ˆå¦‚æœæ˜¯å…±äº«ç›˜ä¹Ÿå¯ä»¥å¼€å¯è‡ªåŠ¨æƒé‡è½¬æ¢ï¼Œä½¿ç”¨å®Œæ•´æƒé‡ï¼‰ã€‚

å¤šæœºå¤šå¡æ‰§è¡Œè„šæœ¬è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒéœ€è¦åˆ†åˆ«åœ¨ä¸åŒèŠ‚ç‚¹è¿è¡Œè„šæœ¬ï¼Œå¹¶å°†å‚æ•°MASTER_ADDRè®¾ç½®ä¸ºä¸»èŠ‚ç‚¹çš„ipåœ°å€ï¼Œæ‰€æœ‰èŠ‚ç‚¹è®¾ç½®çš„ipåœ°å€ç›¸åŒï¼Œä¸åŒèŠ‚ç‚¹ä¹‹é—´ä»…å‚æ•°NODE_RANKä¸åŒï¼Œå„ä¸ªå‚æ•°ä½ç½®å«ä¹‰å‚è§[ä½¿ç”¨æŒ‡å—](../../README_CN.md#ä¸‰ä½¿ç”¨æŒ‡å—)ã€‚

åœ¨æ¯å°æœºå™¨ä¸Šè¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œå¤šæœºè¿è¡Œå‘½ä»¤åœ¨æ¯å°æœºå™¨ä¸Šä»…`node_num` ä¸åŒï¼Œä»0å¼€å§‹è®¡æ•°ï¼Œå‘½ä»¤ä¸­ä¸»èŠ‚ç‚¹ipä¸ºç¬¬0ä¸ªèŠ‚ç‚¹ipã€‚

```shell
# èŠ‚ç‚¹0ï¼Œè®¾0èŠ‚ç‚¹ipä¸º192.168.1.1ï¼Œä½œä¸ºä¸»èŠ‚ç‚¹ipï¼Œæ€»å…±64å¡ä¸”æ¯ä¸ªèŠ‚ç‚¹8å¡
# èŠ‚ç‚¹0ã€èŠ‚ç‚¹1ã€...èŠ‚ç‚¹7 ä¾æ­¤ä¿®æ”¹node_numï¼Œæ¯”å¦‚8æœºï¼Œnode_numä¸º0~7ã€‚
bash scripts/msrun_launcher.sh "run_mindformer.py \
 --register_path research/llama3_1 \
 --config research/llama3_1/llama3_1_70b/finetune_llama3_1_70b.yaml \
 --load_checkpoint model_dir/xxx.ckpt \
 --train_data dataset_dir \
 --auto_trans_ckpt False \
 --use_parallel True \
 --run_mode finetune" \
 64 8 {ä¸»èŠ‚ç‚¹ip} 8118 {node_num} output/msrun_log False 300
```

## æ¨ç†

MindFormersæä¾›`Llama3_1-8b`çš„å¿«é€Ÿæ¨ç†è„šæœ¬ï¼Œè„šæœ¬ä¸»è¦é€šè¿‡generateé«˜é˜¶æ¥å£å®ç°ï¼Œæ”¯æŒå•å¡æ¨ç†ã€‚æ¨ç†è¾“å…¥é»˜è®¤ä¸æ·»åŠ boså­—ç¬¦ï¼Œå¦‚æœéœ€è¦æ·»åŠ å¯åœ¨configä¸­å¢åŠ add_bos_tokené€‰é¡¹ã€‚

```shell
# è„šæœ¬ä½¿ç”¨
bash scripts/examples/llama3/run_llama3_predict.sh PARALLEL CONFIG_PATH CKPT_PATH DEVICE_NUM

# å‚æ•°è¯´æ˜
PARALLEL:    æ˜¯å¦ä½¿ç”¨å¤šå¡æ¨ç†, 'single'è¡¨ç¤ºå•å¡æ¨ç†, 'parallel'è¡¨ç¤ºå¤šå¡æ¨ç†
CONFIG_PATH: æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„
CKPT_PATH:   æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„
VOCAB_FILE:  è¯è¡¨è·¯å¾„
DEVICE_NUM:  ä½¿ç”¨å¡æ•°, ä»…å¼€å¯å¤šå¡æ¨ç†æ—¶ç”Ÿæ•ˆ
```

### å•å¡æ¨ç†

ä»¥`Llama3_1-8b`å•å¡æ¨ç†ä¸ºä¾‹ã€‚

```shell
bash scripts/examples/llama3/run_llama3_predict.sh single \
 research/llama3_1/llama3_1_8b/predict_llama3_1_8b.yaml \
 path/to/llama3_1_8b.ckpt \
 path/to/tokenizer.model
```

### å¤šå¡æ¨ç†

ä»¥`Llama3_1-70b`4å¡æ¨ç†ä¸ºä¾‹ã€‚Llama3_1-70bæƒé‡è¾ƒå¤§ï¼Œå»ºè®®å…ˆè¿›è¡Œæƒé‡åˆ‡åˆ†ï¼Œå‚è§[æƒé‡åˆ‡åˆ†ä¸åˆå¹¶](https://www.mindspore.cn/mindformers/docs/zh-CN/master/feature/ckpt.html#%E6%9D%83%E9%87%8D%E5%88%87%E5%88%86%E4%B8%8E%E5%90%88%E5%B9%B6)ã€‚

```shell
bash scripts/examples/llama3/run_llama3_predict.sh parallel \
 research/llama3_1/llama3_1_70b/predict_llama3_1_70b.yaml \
 path/to/model_dir \
 path/to/tokenizer.model 4
```

## åŸºäºMindIEçš„æœåŠ¡åŒ–æ¨ç†

MindIEï¼Œå…¨ç§°Mind Inference Engineï¼Œæ˜¯åä¸ºæ˜‡è…¾é’ˆå¯¹AIå…¨åœºæ™¯ä¸šåŠ¡çš„æ¨ç†åŠ é€Ÿå¥—ä»¶ã€‚

MindFormersæ‰¿è½½åœ¨æ¨¡å‹åº”ç”¨å±‚MindIE-LLMä¸­ï¼ŒMindIE-LLMæ˜¯å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ¡†æ¶ï¼Œæä¾›APIæ”¯æŒå¤§æ¨¡å‹æ¨ç†èƒ½åŠ›ã€‚

MindIEå®‰è£…æµç¨‹è¯·å‚è€ƒ[MindIEæœåŠ¡åŒ–éƒ¨ç½²æ–‡æ¡£](https://www.mindspore.cn/mindformers/docs/zh-CN/master/guide/deployment.html)ã€‚

ä»¥ä¸‹ä¾‹å­é»˜è®¤å·²å®ŒæˆMindIEå®‰è£…éƒ¨ç½²ä¸”ä»…é€‚ç”¨äº**MindIE RC3ç‰ˆæœ¬**ï¼Œä¸”å®‰è£…è·¯å¾„å‡ä¸ºé»˜è®¤è·¯å¾„`/usr/local/Ascend/`ã€‚

### å•å¡æ¨ç†

æ­¤ä¾‹å­ä½¿ç”¨llama3_1-8Bæ¨¡å‹æ¼”ç¤ºã€‚

#### ä¿®æ”¹MindIEå¯åŠ¨é…ç½®

æ‰“å¼€mindie-serviceä¸­çš„config.jsonæ–‡ä»¶ï¼Œä¿®æ”¹serverç›¸å…³é…ç½®ã€‚

```bash
vim /usr/local/Ascend/mindie/1.0.RC3/mindie-service/conf/config.json
```

éœ€è¦å…³æ³¨ä»¥ä¸‹å­—æ®µçš„é…ç½®

1. `ModelDeployConfig.ModelConfig.backendType`

   è¯¥é…ç½®ä¸ºå¯¹åº”çš„åç«¯ç±»å‹ï¼Œå¿…å¡«"ms"ã€‚

   ```json
   "backendType": "ms"
   ```

   2. `ModelDeployConfig.ModelConfig.modelWeightPath`

      è¯¥é…ç½®ä¸ºæ¨¡å‹é…ç½®æ–‡ä»¶ç›®å½•ï¼Œæ”¾ç½®æ¨¡å‹å’Œtokenizerç­‰ç›¸å…³æ–‡ä»¶ã€‚

      ä»¥llama3_1-8Bä¸ºä¾‹ï¼Œ`modelWeightPath`çš„ç»„ç»‡ç»“æ„å¦‚ä¸‹ï¼š

      ```reStructuredText
      mf_model
       â””â”€â”€ llama3_1_8b
              â”œâ”€â”€ config.json                             # æ¨¡å‹jsoné…ç½®æ–‡ä»¶
              â”œâ”€â”€ tokenizer.model                         # æ¨¡å‹vocabæ–‡ä»¶ï¼Œhfä¸Šå¯¹åº”æ¨¡å‹ä¸‹è½½
              â”œâ”€â”€ predict_llama3_1_8b.yaml                # æ¨¡å‹yamlé…ç½®æ–‡ä»¶
              â”œâ”€â”€ llama3_1_tokenizer.py                   # æ¨¡å‹tokenizeræ–‡ä»¶,ä»mindformersä»“ä¸­researchç›®å½•ä¸‹æ‰¾åˆ°å¯¹åº”æ¨¡å‹å¤åˆ¶
              â””â”€â”€ llama3_1_8b.ckpt                        # å•å¡æ¨¡å‹æƒé‡æ–‡ä»¶
      ```

      predict_llama3_1_8b.yamléœ€è¦å…³æ³¨ä»¥ä¸‹é…ç½®ï¼š

      ```yaml
      load_checkpoint: '/mf_model/llama3_1_8b/llama3_1_8b.ckpt' # ä¸ºå­˜æ”¾æ¨¡å‹å•å¡æƒé‡æ–‡ä»¶è·¯å¾„
      use_parallel: False
      model:
        model_config:
          type: LlamaConfig
          auto_map:
            AutoTokenizer: [llama3_1_tokenizer.Llama3Tokenizer, null]
      processor:
        tokenizer:
          vocab_file: "/mf_model/llama3_1_8b/tokenizer.model"  #vocabæ–‡ä»¶è·¯å¾„
      ```

      æ¨¡å‹çš„config.jsonæ–‡ä»¶å¯ä»¥ä½¿ç”¨`save_pretrained`æ¥å£ç”Ÿæˆï¼Œç¤ºä¾‹å¦‚ä¸‹ï¼š

      ```python
      from mindformers import AutoConfig

      model_config = AutoConfig.from_pretrained("/mf_model/llama3_1_8b/predict_llama3_1_8b.yaml ")
      model_config.save_pretrained(save_directory="/mf_model/llama3_1_8b", save_json=True)
      ```

      æ¨¡å‹æƒé‡ä¸‹è½½å’Œè½¬æ¢å¯å‚è€ƒ [æƒé‡æ ¼å¼è½¬æ¢](https://www.mindspore.cn/docs/zh-CN/master/api_python/mindspore/mindspore.ckpt_to_safetensors.html)ã€‚

      å‡†å¤‡å¥½æ¨¡å‹é…ç½®ç›®å½•åï¼Œè®¾ç½®å‚æ•°`modelWeightPath`ä¸ºè¯¥ç›®å½•è·¯å¾„ã€‚

```json
   "modelWeightPath": "/mf_model/llama3_1_8b"
```

æœ€ç»ˆä¿®æ”¹å®Œåçš„config.jsonå¦‚ä¸‹ï¼š

```json
{
    "Version": "1.0.0",
    "LogConfig" :
    {
        "logLevel" : "Info",
        "logFileSize" : 20,
        "logFileNum" : 20,
        "logPath" : "logs/mindservice.log"
    },

    "ServerConfig" :
    {
        "ipAddress" : "127.0.0.1",
        "managementIpAddress": "127.0.0.2",
        "port" : 1025,
        "managementPort" : 1026,
        "metricsPort" : 1027,
        "maxLinkNum" : 1000,
        "httpsEnabled" : false,
        "fullTextEnabled" : false,
        "tlsCaPath" : "security/ca/",
        "tlsCaFile" : ["ca.pem"],
        "tlsCert" : "security/certs/server.pem",
        "tlsPk" : "security/keys/server.key.pem",
        "tlsPkPwd" : "security/pass/key_pwd.txt",
        "tlsCrl" : "security/certs/server_crl.pem",
        "managementTlsCaFile" : ["management_ca.pem"],
        "managementTlsCert" : "security/certs/management/server.pem",
        "managementTlsPk" : "security/keys/management/server.key.pem",
        "managementTlsPkPwd" : "security/pass/management/key_pwd.txt",
        "managementTlsCrl" : "security/certs/management/server_crl.pem",
        "kmcKsfMaster" : "tools/pmt/master/ksfa",
        "kmcKsfStandby" : "tools/pmt/standby/ksfb",
        "inferMode" : "standard",
        "pdInterNodeTLSEnabled": false,
        "pdCommunicationPort": 1121,
        "interNodeTlsCaFile" : "security/grpc/ca/ca.pem",
        "interNodeTlsCert" : "security/grpc/certs/server.pem",
        "interNodeTlsPk" : "security/grpc/keys/server.key.pem",
        "interNodeTlsPkPwd" : "security/grpc/pass/key_pwd.txt",
        "interCommTlsCrl" : "security/certs/server_crl.pem",
        "interNodeKmcKsfMaster": "tools/pmt/master/ksfa",
        "interNodeKmcKsfStandby": "tools/pmt/standby/ksfb"
    },

    "BackendConfig": {
        "backendName" : "mindieservice_llm_engine",
        "modelInstanceNumber" : 1,
        "npuDeviceIds" : [[0]],
        "tokenizerProcessNumber" : 8,
        "multiNodesInferEnabled": false,
        "multiNodesInferPort": 1120,
        "interNodeTLSEnabled": true,
        "interNodeTlsCaFile": "security/grpc/ca/ca.pem",
        "interNodeTlsCert": "security/grpc/certs/server.pem",
        "interNodeTlsPk": "security/grpc/keys/server.key.pem",
        "interNodeTlsPkPwd": "security/grpc/pass/mindie_server_key_pwd.txt",
        "interNodeTlsCrl" : "security/grpc/certs/server_crl.pem",
        "interNodeKmcKsfMaster": "tools/pmt/master/ksfa",
        "interNodeKmcKsfStandby": "tools/pmt/standby/ksfb",
        "ModelDeployConfig":
        {
            "maxSeqLen" : 2560,
            "maxInputTokenLen" : 2048,
            "truncation" : false,
            "ModelConfig" : [
                {
                    "modelInstanceType": "Standard",
                    "modelName" : "llama3_1_8b",
                    "modelWeightPath" : "/mf_model/llama3_1_8b",
                    "worldSize" : 1,
                    "cpuMemSize" : 16,
                    "npuMemSize" : 16,
                    "backendType": "ms"
                }
            ]
        },

        "ScheduleConfig":
        {
            "templateType": "Standard",
            "templateName" : "Standard_LLM",
            "cacheBlockSize" : 128,

            "maxPrefillBatchSize" : 50,
            "maxPrefillTokens" : 8192,
            "prefillTimeMsPerReq" : 150,
            "prefillPolicyType" : 0,

            "decodeTimeMsPerReq" : 50,
            "decodePolicyType" : 0,

            "maxBatchSize" : 200,
            "maxIterTimes" : 512,
            "maxPreemptCount" : 0,
            "supportSelectBatch" : false,
            "maxQueueDelayMicroseconds" : 5000
        }
    }
}
```

> æ³¨ï¼šä¸ºä¾¿äºæµ‹è¯•ï¼Œ`httpsEnabled`å‚æ•°è®¾ç½®ä¸º`false`ï¼Œå¿½ç•¥åç»­httpsé€šä¿¡ç›¸å…³å‚æ•°ã€‚

#### å¯åŠ¨æœåŠ¡

```bash
cd /usr/local/Ascend/mindie/1.0.RC3/mindie-service
nohup ./bin/mindieservice_daemon > output.log 2>&1 &
tail -f output.log
```

æ‰“å°å¦‚ä¸‹ä¿¡æ¯ï¼Œå¯åŠ¨æˆåŠŸã€‚

```json
Daemon start success!
```

#### è¯·æ±‚æµ‹è¯•

æœåŠ¡å¯åŠ¨æˆåŠŸåï¼Œå¯ä½¿ç”¨curlå‘½ä»¤å‘é€è¯·æ±‚éªŒè¯ï¼Œæ ·ä¾‹å¦‚ä¸‹ï¼š

```bash
curl -w "\ntime_total=%{time_total}\n" -H "Accept: application/json" -H "Content-type: application/json" -X POST -d '{"inputs": "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±<|im_end|>\n<|im_start|>assistant\n","stream": false}' http://127.0.0.1:1035/generate
```

è¿”å›æ¨ç†ç»“æœéªŒè¯æˆåŠŸï¼š

```json
{"generated_text":"æˆ‘å«å°åŠ©æ‰‹ï¼Œä¸“é—¨ä¸ºæ‚¨æœåŠ¡çš„ã€‚<|im_end|>\n<"}
```
