# Yiå¤§æ¨¡å‹

> ## ğŸš¨ å¼ƒç”¨è¯´æ˜
>
> æœ¬æ¨¡å‹å·²è¿‡æ—¶ï¼Œä¸å†è¿›è¡Œç»´æŠ¤ï¼Œå¹¶å°†åœ¨ *1.6.0* ä¹‹åçš„ç‰ˆæœ¬ä¸‹æ¶ã€‚å¦‚éœ€ä½¿ç”¨æ­¤æ¨¡å‹ï¼Œå»ºè®®æ ¹æ®å®˜æ–¹æ–‡æ¡£ä¸­çš„ **[æ¨¡å‹åº“](https://www.mindspore.cn/mindformers/docs/zh-CN/r1.5.0/start/models.html)** é€‰æ‹©åˆé€‚çš„ç‰ˆæœ¬è¿›è¡Œä½¿ç”¨ã€‚
>
> å¦‚æœ‰ä»»ä½•é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·é€šè¿‡ **[ç¤¾åŒºIssue](https://gitee.com/mindspore/mindformers/issues/new)** æäº¤åé¦ˆã€‚æ„Ÿè°¢æ‚¨çš„ç†è§£ä¸æ”¯æŒï¼

Yiç³»åˆ—æ˜¯ç”±é›¶ä¸€ä¸‡ç‰©ç ”ç©¶çš„å¤§è§„æ¨¡è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹ï¼Œç›®å‰å¼€æºçš„æœ‰Yi-6B/34B-Base/Chatï¼ŒYi-VL-6B/34Bï¼ŒMindFormerså·²æ”¯æŒYi-6B-Base,Yi-34B-Base/Chatã€‚å½“å‰è®­ç»ƒä½¿ç”¨Baseæƒé‡ï¼Œæ¨ç†ä½¿ç”¨Base/Chatæƒé‡

[Yi: Open Foundation Models by 01.AI](https://arxiv.org/abs/2403.04652v1)

``` text
@article{ai2024yiopenfoundationmodels,
      title={Yi: Open Foundation Models by 01.AI},
      author={01. AI and : and Alex Young and Bei Chen and Chao Li and Chengen Huang and Ge Zhang and Guanwei Zhang and Heng Li and Jiangcheng Zhu and Jianqun Chen and Jing Chang and Kaidong Yu and Peng Liu and Qiang Liu and Shawn Yue and Senbin Yang and Shiming Yang and Tao Yu and Wen Xie and Wenhao Huang and Xiaohui Hu and Xiaoyi Ren and Xinyao Niu and Pengcheng Nie and Yuchi Xu and Yudong Liu and Yue Wang and Yuxuan Cai and Zhenyu Gu and Zhiyuan Liu and Zonghong Dai},
      year={2024},
      eprint={2403.04652},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.04652},
}
```

## æ¨¡å‹æ€§èƒ½

ä»¥ä¸‹æ¨¡å‹æ€§èƒ½å‡ç”±Atlas 800T A2ç¡¬ä»¶ç¯å¢ƒä¸‹æµ‹è¯•å¾—å‡ºã€‚

| Config                               |      Task       |      Datasets       | SeqLength |   Performance   |  Phase   |
|:-------------------------------------|:---------------:|:-------------------:|:---------:|:---------------:|:--------:|
| [yi_6b](yi_6b/finetune_yi_6b.yaml)       | text_generation | alpaca_gpt4_data_zh |   2048    | 3324 tokens/s/p | Finetune |
| [yi_34b](yi_34b/finetune_yi_34b.yaml)     | text_generation |       alpaca        |   4096    | 660 tokens/s/p  | Finetune |
| [yi_6b](yi_6b/predict_yi_6b.yaml)        | text_generation |          -          |    512    |   31 tokens/s   | Predict  |
| [yi_34b](yi_34b/predict_yi_34b_chat.yaml) | text_generation |          -          |   16384   |   41 tokens/s   | Predict  |

## æ¨¡å‹æ–‡ä»¶

1. æ¨¡å‹å®ç°ï¼š

   ```text
    research/yi
     â””â”€â”€ yi_model.py           # æ¨¡å‹å®ä¾‹
   ```

2. æ¨¡å‹é…ç½®ï¼š

   ```text
    research/yi
     â”œâ”€â”€ yi_6b
     |    â”œâ”€â”€ finetune_yi_6b.yaml                   # 6B å…¨å‚å¾®è°ƒå¯åŠ¨é…ç½®
     |    â””â”€â”€ predict_yi_6b.yaml                    # 6B baseåœ¨çº¿æ¨ç†å¯åŠ¨é…ç½®  
     â””â”€â”€ yi_34b
          â”œâ”€â”€ pretrain_yi_34b.yaml                  # 34B é¢„è®­ç»ƒå¯åŠ¨é…ç½®
          â”œâ”€â”€ finetune_yi_34b.yaml                  # 34B å…¨å‚å¾®è°ƒå¯åŠ¨é…ç½®
          â”œâ”€â”€ predict_yi_34b.yaml                   # 34B baseåœ¨çº¿æ¨ç†å¯åŠ¨é…ç½®
          â””â”€â”€ predict_yi_34b_chat.yaml              # 34B chatåœ¨çº¿æ¨ç†å¯åŠ¨é…ç½®
   ```

3. æ¨¡å‹ç›¸å…³è„šæœ¬ï¼š

   ```text
    research/yi
     â”œâ”€â”€ alpaca_converter.py           # alpacaæ•°æ®é›†æ ¼å¼è½¬æ¢è„šæœ¬
     â”œâ”€â”€ conversation.py               # å¾®è°ƒæ•°æ®é›†å¤„ç†ï¼Œå°†åŸå§‹alpacaè½¬æ¢ä¸ºå¯¹è¯å½¢å¼alpaca
     â”œâ”€â”€ yi_preprocess.py              # æ•°æ®é›†é¢„å¤„ç†è„šæœ¬
     â”œâ”€â”€ convert_ckpt_bf16.py          # æƒé‡è½¬æ¢è„šæœ¬
     â”œâ”€â”€ convert_reversed.py           # æƒé‡è½¬æ¢è„šæœ¬
     â””â”€â”€ convert_weight.py             # æƒé‡è½¬æ¢è„šæœ¬
   ```

## ç¯å¢ƒåŠæ•°æ®å‡†å¤‡

### å®‰è£…ç¯å¢ƒ

MindFormersè½¯ç¡¬ä»¶é…å¥—å…³ç³»ä»¥åŠå®‰è£…å‚è€ƒ[ç¯å¢ƒå®‰è£…æŒ‡å—](../../README_CN.md#æºç ç¼–è¯‘å®‰è£…)å’Œ[ç‰ˆæœ¬åŒ¹é…å…³ç³»](../../README_CN.md#ç‰ˆæœ¬åŒ¹é…å…³ç³»)ã€‚

> æ³¨ï¼šAtlas 800T A2èŠ¯ç‰‡æ”¯æŒ6bå•å¡æ¨ç†ï¼Œå…¨å‚å¾®è°ƒè‡³å°‘éœ€è¦4å¡ï¼Œå»ºè®®8å¡ï¼›34bæ¨ç†éœ€è¦4å¡ï¼Œå…¨å‚å¾®è°ƒéœ€è¦åŒæœº32å¡ã€‚

### æ•°æ®åŠæƒé‡å‡†å¤‡

#### æ•°æ®é›†ä¸‹è½½

| æ•°æ®é›†åç§°               |        é€‚ç”¨æ¨¡å‹        |   é€‚ç”¨é˜¶æ®µ   |                                                         ä¸‹è½½é“¾æ¥                                                          |
|:--------------------|:------------------:|:--------:|:---------------------------------------------------------------------------------------------------------------------:|
| Wikitext2           |       yi-34b       | Pretrain | [Link](https://www.mindspore.cn/mindformers/docs/zh-CN/r1.5.0/faq/func_related.html) |
| alpaca              | yi-6b <br/> yi-34b | Finetune |                    [Link](https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json)                    |
| alpaca_gpt4_data_zh | yi-6b <br/> yi-34b | Finetune |       [Link](https://huggingface.co/datasets/llamafactory/alpaca_gpt4_zh/resolve/main/alpaca_gpt4_data_zh.json?download=true)       |

æ•°æ®é›†å¤„ç†è¿‡ç¨‹ä¸­ä½¿ç”¨çš„`tokenizer.model`å¯ä»¥é€šè¿‡[é“¾æ¥](https://huggingface.co/01-ai/Yi-6B/blob/main/tokenizer.model)ä¸‹è½½ã€‚

- **Wikitext2 æ•°æ®é¢„å¤„ç†**

  ä½¿ç”¨`mindformers/tools/dataset_preprocess/llama/llama_preprocess.py`å¯¹ä¸‹è½½åçš„æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼Œå¹¶ç”ŸæˆMindrecordæ•°æ®ã€‚

  ```shell
  python llama_preprocess.py \
    --dataset_type wiki \
    --input_glob /{path}/wiki.train.tokens \
    --model_file /{path}/tokenizer.model \
    --seq_length 4096 \
    --output_file /{path}/wiki4096.mindrecord

  # å‚æ•°è¯´æ˜
  dataset_type: é¢„å¤„ç†æ•°æ®ç±»å‹
  input_glob:   è¾“å…¥ä¸‹è½½åwiki.train.tokensçš„æ–‡ä»¶è·¯å¾„
  model_file:   æ¨¡å‹tokenizer.modelæ–‡ä»¶è·¯å¾„
  seq_length:   è¾“å‡ºæ•°æ®çš„åºåˆ—é•¿åº¦
  output_file:  è¾“å‡ºæ–‡ä»¶çš„ä¿å­˜è·¯å¾„
  ```

- **alpaca_gpt4_data_zh æ•°æ®é¢„å¤„ç†**

  1. æ‰§è¡Œ`research/yi/alpaca_converter.py`ï¼Œå°†åŸå§‹æ•°æ®é›†è½¬æ¢ä¸ºå¯¹è¯æ ¼å¼ã€‚

     ```shell
     python research/yi/alpaca_converter.py \
      --data_path /{path}/alpaca_gpt4_data_zh.json \
      --output_path /{path}/alpaca_gpt4_data_zh-conversation.json

     # å‚æ•°è¯´æ˜
     data_path:   è¾“å…¥ä¸‹è½½çš„æ•°æ®é›†è·¯å¾„
     output_path: è¾“å‡ºè½¬æ¢åæ•°æ®é›†ä¿å­˜è·¯å¾„
     ```

  2. æ‰§è¡Œ`research/yi/yi_preprocess.py`ï¼Œè¿›è¡Œæ•°æ®é¢„å¤„ç†ã€Mindrecordæ•°æ®ç”Ÿæˆï¼Œå°†å¸¦æœ‰promptæ¨¡æ¿çš„æ•°æ®è½¬æ¢ä¸ºmindrecordæ ¼å¼ã€‚æ‰§è¡Œæ­¤è„šæœ¬éœ€è¦æ·»åŠ PYTHONPATHæŒ‡å®šåˆ°research/yiç›®å½•ã€‚

     ```shell
     python research/yi/yi_preprocess.py \
      --dataset_type qa \
      --input_glob /{path}/alpaca_gpt4_data_zh-conversation.json \
      --model_file /{path}/tokenizer.model \
      --seq_length 2048 \
      --output_file /{path}/alpaca_gpt4_data_zh.mindrecord

     # å‚æ•°è¯´æ˜
     input_file_path: è¾“å…¥æ•°æ®é›†æ–‡ä»¶è·¯å¾„
     output_file:     è¾“å‡ºæ–‡ä»¶çš„ä¿å­˜è·¯å¾„
     dataset_type:    æ•°æ®é›†ç±»å‹, ç›®å‰ä»…æ”¯æŒ'text'å’Œ'qa'
     model_file:      æ¨¡å‹è¯è¡¨æ–‡ä»¶è·¯å¾„
     seq_length:      æ•°æ®åºåˆ—é•¿åº¦
     ```

#### æ¨¡å‹æƒé‡ä¸‹è½½

MindFormersæä¾›ä¸‹è½½HuggingFaceå®˜æ–¹æƒé‡çš„ä¸‹è½½é“¾æ¥ï¼Œç”¨æˆ·å¯é€šè¿‡é“¾æ¥ä¸‹è½½æƒé‡å¹¶ç»è¿‡[æ¨¡å‹æƒé‡è½¬æ¢](#æ¨¡å‹æƒé‡è½¬æ¢)åè¿›è¡Œä½¿ç”¨ã€‚

è¯è¡¨ä¸‹è½½é“¾æ¥ï¼š[tokenizer.model](https://huggingface.co/01-ai/Yi-6B/blob/main/tokenizer.model)

| æ¨¡å‹åç§°        | MindSporeæƒé‡ |                  HuggingFaceæƒé‡                   |
|:------------|:-----------:|:------------------------------------------------:|
| Yi-6B-Base  |      -      |    [Link](https://huggingface.co/01-ai/Yi-6B)    |
| Yi-34B-Base |      -      |   [Link](https://huggingface.co/01-ai/Yi-34B)    |
| Yi-34B-Chat |      -      | [Link](https://huggingface.co/01-ai/Yi-34B-Chat) |

#### æ¨¡å‹æƒé‡è½¬æ¢

æ‰§è¡Œ`mindformers/convert_weight.py`è½¬æ¢è„šæœ¬ï¼Œå°†HuggingFaceçš„æƒé‡è½¬æ¢ä¸ºå®Œæ•´çš„ckptæƒé‡ã€‚

```shell
python convert_weight.py --model yi --input_path TORCH_CKPT_DIR --output_path {path}/MS_CKPT_NAME

# å‚æ•°è¯´æ˜
model:       æ¨¡å‹åç§°
input_path:  ä¸‹è½½HuggingFaceæƒé‡çš„æ–‡ä»¶å¤¹è·¯å¾„
output_path: è½¬æ¢åçš„MindSporeæƒé‡æ–‡ä»¶ä¿å­˜è·¯å¾„
```

## é¢„è®­ç»ƒ

MindFormersæä¾›`Yi-34b`å¤šæœºå¤šå¡é¢„è®­ç»ƒç¤ºä¾‹ï¼Œç›®å‰`Yi-34b`æ¨¡å‹ä¸æ”¯æŒè¿›è¡Œå•æœºé¢„è®­ç»ƒä»»åŠ¡ï¼Œé¢„è®­ç»ƒæ•°æ®é›†å¯é€šè¿‡[æ•°æ®é›†ä¸‹è½½](#æ•°æ®é›†ä¸‹è½½)è·å¾—ã€‚

å¤šæœºå¤šå¡æ‹‰èµ·ä»»åŠ¡éœ€è¦å¤šæœºåŒæ—¶æ‰§è¡Œå‘½ä»¤ï¼Œå°†å‚æ•°`MASTER_ADDR`è®¾ç½®ä¸ºä¸»èŠ‚ç‚¹çš„ipåœ°å€ï¼Œ æ‰€æœ‰èŠ‚ç‚¹è®¾ç½®çš„ipåœ°å€ç›¸åŒï¼Œä¸åŒèŠ‚ç‚¹ä¹‹é—´ä»…å‚æ•°`NODE_RANK`ä¸åŒï¼Œå…·ä½“å¯å‚è€ƒ[ä½¿ç”¨æŒ‡å—](../../README_CN.md#ä¸‰ä½¿ç”¨æŒ‡å—)ã€‚

ä»¥ä¸‹ä¸º`Yi-34b`2æœº16å¡æ‰§è¡Œå‘½ä»¤ï¼š

```shell
# èŠ‚ç‚¹0ï¼ŒèŠ‚ç‚¹ipä¸º{ip_addr}ï¼Œä½œä¸ºä¸»èŠ‚ç‚¹ï¼Œæ€»å…±16å¡ä¸”æ¯ä¸ªèŠ‚ç‚¹8å¡
bash scripts/msrun_launcher.sh "run_mindformer.py \
 --register_path research/yi \
 --config research/yi/yi_34b/pretrain_yi_34b.yaml \
 --use_parallel True \
 --run_mode train \
 --auto_trans_ckpt False \
 --train_dataset /{path}/wiki4096.mindrecord" \
 16 8 {ip_addr} 8118 0 output/msrun_log False 300

# èŠ‚ç‚¹1ï¼ŒèŠ‚ç‚¹0ä¸èŠ‚ç‚¹1å¯åŠ¨å‘½ä»¤ä»…å‚æ•°NODE_RANKä¸åŒ
bash scripts/msrun_launcher.sh "run_mindformer.py \
 --register_path research/yi \
 --config research/yi/yi_34b/pretrain_yi_34b.yaml \
 --use_parallel True \
 --run_mode train \
 --auto_trans_ckpt False \
 --train_dataset /{path}/wiki4096.mindrecord" \
 16 8 {ip_addr} 8118 1 output/msrun_log False 300

# å‚æ•°è¯´æ˜
register_path:   æ³¨å†Œè·¯å¾„
config:          é…ç½®æ–‡ä»¶è·¯å¾„
use_parallel:    æ˜¯å¦å¼€å¯å¹¶è¡Œè®­ç»ƒ
run_mode:        è¿è¡Œæ¨¡å¼, é¢„è®­ç»ƒæ—¶è®¾ç½®ä¸ºtrain
auto_trans_ckpt: æ˜¯å¦å¼€å¯è‡ªåŠ¨æƒé‡è½¬æ¢
train_dataset:   è®­ç»ƒæ•°æ®é›†è·¯å¾„
```

## å¾®è°ƒ

### å…¨å‚å¾®è°ƒ

MindFormersæä¾›`Yi-6b`å•æœºå¾®è°ƒä»¥åŠ`Yi-34b`å¤šæœºå¾®è°ƒç¤ºä¾‹ï¼Œç›®å‰`Yi-34b`æ¨¡å‹ä¸æ”¯æŒè¿›è¡Œå•æœºå¾®è°ƒä»»åŠ¡ï¼Œå¾®è°ƒæ•°æ®é›†å¯é€šè¿‡[æ•°æ®é›†ä¸‹è½½](#æ•°æ®é›†ä¸‹è½½)è·å¾—ã€‚

#### å•æœºè®­ç»ƒ

ä»¥`Yi-6b`å…¨å‚å¾®è°ƒä¸ºä¾‹ï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶`research/yi/yi_6b/finetune_yi_6b.yaml`ï¼Œæ‰§è¡Œå¦‚ä¸‹å‘½ä»¤æ‹‰èµ·å•æœº8å¡å¾®è°ƒä»»åŠ¡ã€‚

```shell
bash scripts/msrun_launcher.sh "run_mindformer.py \
 --register_path research/yi \
 --config research/yi/yi_6b/finetune_yi_6b.yaml \
 --run_mode finetune \
 --load_checkpoint /{path}/yi_6b.ckpt \
 --train_dataset /{path}/alpaca_gpt4_data_zh.mindrecord \
 --auto_trans_ckpt False \
 --use_parallel True" 8


# å‚æ•°è¯´æ˜
register_path:      æ³¨å†Œè·¯å¾„
config:             é…ç½®æ–‡ä»¶è·¯å¾„
run_mode:           è¿è¡Œæ¨¡å¼, å¾®è°ƒæ—¶è®¾ç½®ä¸ºfinetune
load_checkpoint:    é¢„è®­ç»ƒæƒé‡è·¯å¾„
train_dataset:      è®­ç»ƒæ•°æ®é›†è·¯å¾„
auto_trans_ckpt:    æ˜¯å¦å¼€å¯è‡ªåŠ¨æƒé‡è½¬æ¢
use_parallel:       æ˜¯å¦å¼€å¯å¹¶è¡Œè®­ç»ƒ
```

#### å¤šæœºè®­ç»ƒ

ä»¥`Yi-34b`å…¨å‚å¾®è°ƒä¸ºä¾‹ï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶`research/yi/yi_34b/finetune_yi_34b.yaml`ï¼Œæ‰§è¡Œå¦‚ä¸‹å‘½ä»¤æ‹‰èµ·2æœº16å¡å¾®è°ƒä»»åŠ¡ã€‚

å¤šæœºå¤šå¡æ‹‰èµ·ä»»åŠ¡éœ€è¦å¤šæœºåŒæ—¶æ‰§è¡Œå‘½ä»¤ï¼Œå°†å‚æ•°`MASTER_ADDR`è®¾ç½®ä¸ºä¸»èŠ‚ç‚¹çš„ipåœ°å€ï¼Œ æ‰€æœ‰èŠ‚ç‚¹è®¾ç½®çš„ipåœ°å€ç›¸åŒï¼Œä¸åŒèŠ‚ç‚¹ä¹‹é—´ä»…å‚æ•°`NODE_RANK`ä¸åŒï¼Œå…·ä½“å¯å‚è€ƒ[ä½¿ç”¨æŒ‡å—](../../README_CN.md#ä¸‰ä½¿ç”¨æŒ‡å—)ã€‚

```shell
# èŠ‚ç‚¹0ï¼ŒèŠ‚ç‚¹ipä¸º{ip_addr}ï¼Œä½œä¸ºä¸»èŠ‚ç‚¹ï¼Œæ€»å…±16å¡ä¸”æ¯ä¸ªèŠ‚ç‚¹8å¡
bash scripts/msrun_launcher.sh "run_mindformer.py \
 --register_path research/yi \
 --config research/yi/yi_34b/finetune_yi_34b.yaml \
 --load_checkpoint /path/ckpt_dir \
 --use_parallel True \
 --run_mode finetune \
 --auto_trans_ckpt False \
 --train_dataset /{path}/alpaca.mindrecord" \
 16 8 {ip_addr} 8118 0 output/msrun_log False 300

# èŠ‚ç‚¹1ï¼ŒèŠ‚ç‚¹0ä¸èŠ‚ç‚¹1å¯åŠ¨å‘½ä»¤ä»…å‚æ•°NODE_RANKä¸åŒ
bash scripts/msrun_launcher.sh "run_mindformer.py \
 --register_path research/yi \
 --config research/yi/yi_34b/finetune_yi_34b.yaml \
 --load_checkpoint /path/ckpt_dir \
 --use_parallel True \
 --run_mode finetune \
 --auto_trans_ckpt False \
 --train_dataset /{path}/alpaca.mindrecord" \
 16 8 {ip_addr} 8118 1 output/msrun_log False 300

# å‚æ•°è¯´æ˜
register_path:      æ³¨å†Œè·¯å¾„
config:             é…ç½®æ–‡ä»¶è·¯å¾„
load_checkpoint:    æƒé‡æ–‡ä»¶å¤¹è·¯å¾„, æƒé‡æŒ‰ç…§'model_dir/rank_0/xxx.ckpt'æ ¼å¼å­˜æ”¾
auto_trans_ckpt:    è‡ªåŠ¨æƒé‡è½¬æ¢å¼€å…³
run_mode:           è¿è¡Œæ¨¡å¼, å¾®è°ƒæ—¶è®¾ç½®ä¸ºfinetune
train_dataset:      è®­ç»ƒæ•°æ®é›†è·¯å¾„
```

### åˆ†å¸ƒå¼è®­ç»ƒæƒé‡åˆå¹¶

åˆ†å¸ƒå¼è®­ç»ƒï¼ˆå¾®è°ƒï¼‰åæ‰€å¾—åˆ°çš„æƒé‡æ–‡ä»¶ä¸ºæ ¹æ®ç­–ç•¥åˆ‡åˆ†åçš„æƒé‡ï¼Œå¯ä»¥æ‰‹åŠ¨å°†åˆ‡åˆ†æƒé‡åˆä¸€ï¼Œä»¥ç”¨äºè¯„ä¼°å’Œæ¨ç†ã€‚

MindFormersæä¾›è‡ªåŠ¨æƒé‡è½¬æ¢å’Œç¦»çº¿æƒé‡è½¬æ¢åŠŸèƒ½ï¼Œå¯å‚è€ƒ[è‡ªåŠ¨è½¬æ¢æ¡ˆä¾‹](../../../../docs/feature_cards/Transform_Ckpt.md#è‡ªåŠ¨è½¬æ¢æ¡ˆä¾‹)å’Œ[ç¦»çº¿æƒé‡è½¬æ¢](../../../../docs/feature_cards/Transform_Ckpt.md#ç¦»çº¿æƒé‡è½¬æ¢)è¿›è¡Œåˆ†å¸ƒå¼æ¨¡å‹æƒé‡è½¬æ¢ã€‚

## æ¨ç†

`Yi-6b-Base`æ”¯æŒå•å¡æ¨ç†ï¼Œ`Yi-34b`æ¨¡å‹è§„æ¨¡è¾ƒå¤§ï¼Œä»…æ”¯æŒå¤šå¡æ¨ç†ã€‚

### å•å¡æ¨ç†

ä»¥`Yi-6b`å•å¡æ¨ç†ä¸ºä¾‹ï¼Œæ‰§è¡Œå¦‚ä¸‹å‘½ä»¤è¿›è¡Œæ¨ç†ã€‚

1. ä¿®æ”¹æ¨¡å‹é…ç½®æ–‡ä»¶ `research/yi/yi_6b/predict_yi_6b.yaml`

```yaml
 processor:
     tokenizer:
       vocab_file: "/{path}/tokenizer.model"  # æŒ‡å®štokenizeræ–‡ä»¶è·¯å¾„
```

2. å¯åŠ¨æ¨ç†è„šæœ¬

```shell
python run_mindformer.py \
 --register_path research/yi \
 --config research/yi/yi_6b/predict_yi_6b.yaml \
 --run_mode predict \
 --load_checkpoint /path/ckpt_dir \
 --predict_data 'ä»¥é›·éœ†ä¹‹åŠ›'

# æ¨ç†ç»“æœ
# ä»¥é›·éœ†ä¹‹åŠ›ï¼Œå°†è¿™è‚¡åŠ›é‡åŒ–ä¸ºä¸€é“é“å‰‘æ°”ã€‚â€œå™—ï¼â€ä¸€æŸ„é•¿æªè¢«æ–©æ–­æˆä¸¤æˆªåï¼Œ...
```

### å¤šå¡æ¨ç†

ä»¥`Yi-34b-Chat`4å¡æ¨ç†ä¸ºä¾‹ï¼Œæ‰§è¡Œå¦‚ä¸‹å‘½ä»¤è¿›è¡Œæ¨ç†ã€‚

1. ä¿®æ”¹æ¨¡å‹é…ç½®æ–‡ä»¶ `research/yi/yi_34b/predict_yi_34b_chat.yaml`

```yaml
processor:
  tokenizer:
    vocab_file: "/{path}/tokenizer.model"  # æŒ‡å®štokenizeræ–‡ä»¶è·¯å¾„
```

2. å¯åŠ¨æ¨ç†è„šæœ¬

```shell
bash scripts/msrun_launcher.sh "run_mindformer.py \
 --register_path research/yi \
 --config research/yi/yi_34b/predict_yi_34b_chat.yaml \
 --load_checkpoint /path/ckpt_dir \
 --predict_data 'ä»¥é›·éœ†ä¹‹åŠ›' \
 --use_parallel True \
 --run_mode predict \
 --auto_trans_ckpt True" 4

# æ¨ç†ç»“æœ
# ä»¥é›·éœ†ä¹‹åŠ›ï¼Œå°†è¿™è‚¡åŠ›é‡åŒ–ä¸ºä¸€é“é“å‰‘æ°”ã€‚â€œå™—ï¼â€ä¸€æŸ„é•¿æªè¢«æ–©æ–­æˆä¸¤æˆªåï¼Œ...
```
