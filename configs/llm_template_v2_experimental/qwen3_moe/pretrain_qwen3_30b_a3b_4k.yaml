# 配置说明和更多配置功能请查阅: llm_config_whitelist_introduce.md
run_mode: train
output_dir: ./output
train_precision_sync: false

pretrained_model_dir: ''

checkpoint_config:
  load_checkpoint: ''
  prefix: llm_model
  save_checkpoint_steps: 100
  keep_checkpoint_max: 1

training_args:
  epochs: 1
  micro_batch_size: 1
  training_seed: 1234
  dataset_seed: 1234
  resume_training: false
  global_batch_size: 16
  stop_step: null

optimizer:
  type: AdamW
  betas:
  - 0.9
  - 0.95
  eps: 1.0e-08
  weight_decay: 0.0

lr_schedule:
  type: ConstantWarmUpLR
  learning_rate: 1.0e-05
  warmup_steps: 0
  total_steps: -1

use_parallel: true
distribute_parallel_config:
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 4
  context_parallel_size: 1
  cp_comm_type: all_gather
  expert_model_parallel_size: 4
  sequence_parallel: true
  micro_batch_interleave_num: 1
  pipeline_parallel_config:
    pipeline_interleave: false
    pipeline_scheduler: 1f1b
    virtual_pipeline_model_parallel_size: 1
    pipeline_stage_offset: 0
  optimizer_parallel_config:
    enable_parallel_optimizer: false
    optimizer_level: level1
    parallel_optimizer_threshold: 8

recompute_config:
  recompute: true

train_dataset:
  data_loader:
    type: BlendedMegatronDatasetDataLoader
    datasets_type: GPTDataset
    sizes:
    - 8000
    - 0
    - 0
    config:
      split: 1, 0, 0
      seq_length: 4096
      eod_mask_loss: false
      reset_position_ids: false
      create_attention_mask: true
      reset_attention_mask: false
      create_compressed_eod_mask: false
      eod_pad_length: 128
      eod: 1
      pad: -1
      data_path:
      - '1'
      - /path/to/wiki103-megatron_text_document
  num_parallel_workers: 8
  python_multiprocessing: false
  drop_remainder: true

model_config:
  model_type: qwen3_moe
  architectures: Qwen3MoeForCausalLM
  vocab_size: 151936
  hidden_size: 2048
  intermediate_size: 6144
  num_hidden_layers: 48
  max_position_embeddings: 40960
  hidden_act: silu
  num_attention_heads: 32
  num_key_value_heads: 4
  head_dim: 128
  rms_norm_eps: 1.0e-06
  add_bias_linear: false
  tie_word_embeddings: false
  use_flash_attention: true
  use_contiguous_weight_layout_attention: false
  input_sliced_sig: true
  hidden_dropout: 0.0
  attention_dropout: 0.0
  position_embedding_type: rope
  rope_theta: 1000000
  params_dtype: float32
  compute_dtype: bfloat16
  layernorm_compute_dtype: float32
  softmax_compute_dtype: float32
  rotary_dtype: float32
  initializer_range: 0.02
  moe_grouped_gemm: true
  num_experts: 128
  num_experts_per_tok: 8
  moe_intermediate_size: 768
  router_dense_type: float32
  gated_linear_unit: true
  norm_topk_prob: false
  moe_router_pre_softmax: true
  moe_token_drop_policy: probs
  routed_scaling_factor: 1.5
  router_aux_loss_coef: 0.001
  moe_aux_loss_coeff: 0.001
  moe_router_load_balancing_type: gbs_aux_loss

parallel:
  parallel_mode: 1
  enable_alltoall: true

context:
  max_device_memory: 58GB
  memory_optimize_level: O0
  jit_config:
    jit_level: O0
  ascend_config:
    precision_mode: must_keep_origin_dtype
    parallel_speed_up_json_path: configs/qwen3_moe/parallel_speed_up.json
  mempool_block_size: 1GB
