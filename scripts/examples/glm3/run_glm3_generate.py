# Copyright 2024 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""glm3 predict example."""
import os
import argparse
from copy import deepcopy

import mindspore as ms
from mindspore import Tensor, Model
from mindspore.common import initializer as init

from mindformers import MindFormerConfig
from mindformers.tools.logger import logger
from mindformers.core.context import build_context
from mindformers.core.parallel_config import build_parallel_config
from mindformers.trainer.utils import transform_and_load_checkpoint
from mindformers.models.glm2 import ChatGLM2Config, ChatGLM2ForConditionalGeneration, ChatGLM3Tokenizer


def get_model(config_path, load_checkpoint):
    """build model for prediction."""
    # init config with yaml
    config = MindFormerConfig(config_path)
    config.use_parallel = False
    config.load_checkpoint = load_checkpoint

    # init context
    build_context(config)
    build_parallel_config(config)

    config.model.model_config.parallel_config = config.parallel_config
    model_config = ChatGLM2Config(**config.model.model_config)
    model_config.seq_length = 1024
    model_config.checkpoint_name_or_path = None
    model_name = config.trainer.model_name

    # build tokenizer
    tokenizer = ChatGLM3Tokenizer.from_pretrained(model_name)

    # build model
    network = ChatGLM2ForConditionalGeneration(model_config)
    model = Model(network)

    # load checkpoint
    if config.load_checkpoint:
        logger.info("----------------Transform and load checkpoint----------------")
        seq_length = config.model.model_config.seq_length
        # set auto transform ckpt
        if os.path.isdir(config.load_checkpoint):
            config.auto_trans_ckpt = True
        else:
            config.auto_trans_ckpt = False
        input_ids = Tensor(shape=(1, seq_length), dtype=ms.int32, init=init.One())
        infer_data = network.prepare_inputs_for_predict_layout(input_ids)
        transform_and_load_checkpoint(config, model, network, infer_data, do_predict=True)

    return network, model_config, tokenizer


def main(config_path, load_checkpoint):
    model, model_config, tokenizer = get_model(config_path, load_checkpoint)

    queries = ["ä½ å¥½",
               "è¯·ä»‹ç»ä¸€ä¸‹åä¸º"]
    for query in queries:
        input_ids = tokenizer.build_chat_input(query, history=[], role='user')["input_ids"]
        outputs = model.generate(input_ids,
                                 max_length=model_config.seq_length,
                                 do_sample=False,
                                 top_k=1)
        for i, output in enumerate(outputs):
            output = output[len(input_ids[i]):]
            response = tokenizer.decode(output)
            print(response)

    # answer 1:
    # ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM3-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚
    # answer 2:
    # åä¸ºæ˜¯ä¸€å®¶æ€»éƒ¨ä½äºä¸­å›½æ·±åœ³çš„å¤šå…ƒåŒ–ç§‘æŠ€å…¬å¸,æˆç«‹äº1987å¹´,æ˜¯å…¨çƒæœ€å¤§çš„ç”µä¿¡è®¾å¤‡åˆ¶é€ å•†ä¹‹ä¸€ã€‚è¯¥å…¬å¸ä¹Ÿåœ¨æ™ºèƒ½æ‰‹æœºã€ç”µè„‘ã€å¹³æ¿ç”µè„‘ã€
    # äº‘è®¡ç®—ç­‰é¢†åŸŸå¼€å±•ä¸šåŠ¡,å…¶äº§å“å’ŒæœåŠ¡è¦†ç›–å…¨çƒ170å¤šä¸ªå›½å®¶å’Œåœ°åŒºã€‚
    # åä¸ºçš„ä¸»è¦ä¸šåŠ¡åŒ…æ‹¬ç”µä¿¡ç½‘ç»œè®¾å¤‡ã€æ™ºèƒ½æ‰‹æœºã€ç”µè„‘å’Œæ¶ˆè´¹ç”µå­äº§å“ã€‚å…¬å¸åœ¨å…¨çƒèŒƒå›´å†…æœ‰è¶…è¿‡190,000åå‘˜å·¥,
    # å…¶ä¸­çº¦ä¸€åŠä»¥ä¸Šä»äº‹ç ”å‘å·¥ä½œã€‚åä¸ºä»¥å…¶é«˜å“è´¨çš„äº§å“å’ŒæœåŠ¡èµ¢å¾—äº†å…¨çƒå®¢æˆ·çš„ä¿¡ä»»å’Œå¥½è¯„,ä¹Ÿæ›¾å› å…¶é¢†å…ˆæŠ€æœ¯å’Œåˆ›æ–°ç²¾ç¥è€Œè·å¾—å¤šé¡¹å›½é™…å¥–é¡¹å’Œè®¤å¯ã€‚
    # ç„¶è€Œ,åä¸ºä¹Ÿé¢ä¸´ç€æ¥è‡ªä¸€äº›å›½å®¶æ”¿åºœçš„å®‰å…¨é—®é¢˜å’Œæ”¿æ²»å‹åŠ›,å…¶ä¸­åŒ…æ‹¬ç¾å›½æ”¿åºœå¯¹å…¶äº§å“çš„ç¦ä»¤å’Œé™åˆ¶ã€‚
    # åä¸ºä¸€ç›´åšç§°è‡ªå·±çš„äº§å“æ˜¯å®‰å…¨çš„,å¹¶é‡‡å–äº†ä¸€ç³»åˆ—æªæ–½æ¥ç¡®ä¿å…¶äº§å“çš„å®‰å…¨æ€§å’Œé€æ˜åº¦ã€‚


def process_response(output, history):
    """process predict results."""
    content_dict = None
    history = deepcopy(history)
    for response in output.split("<|assistant|>"):
        metadata, content = response.split("\n", maxsplit=1)
        if not metadata.strip():
            content = content.strip()
            history.append({"role": "assistant", "metadata": metadata, "content": content})
            content_dict = content.replace("[[è®­ç»ƒæ—¶é—´]]", "2023å¹´")
        else:
            history.append({"role": "assistant", "metadata": metadata, "content": content})
            if history[0]["role"] == "system" and "tools" in history[0]:
                content = "\n".join(content.split("\n")[1:-1])
                # pylint: disable=eval-used
                parameters = eval(content)
                content_dict = {"name": metadata.strip(), "parameters": parameters}
            else:
                content_dict = {"name": metadata.strip(), "content": content}
    return content_dict, history


def multi_role_predict(config_path, load_checkpoint):
    """multi-role predict process."""
    model, model_config, tokenizer = get_model(config_path, load_checkpoint)

    generate_config = {
        "max_length": model_config.seq_length,
        "num_beams": 1,
        "do_sample": False,
        "top_p": 1,
        "top_k": 1,
        "temperature": 1
    }

    # first input
    role = "system"
    text = "å‡è®¾ä½ ç°åœ¨æ˜¯ä¸€ä¸ªå¯¼æ¸¸ï¼Œè¯·å°½å¯èƒ½è´´è¿‘è¿™ä¸ªè§’è‰²å›ç­”é—®é¢˜ã€‚"
    history = []
    inputs = tokenizer.build_chat_input(text, history=history, role=role)['input_ids']
    history.append({'role': role, 'content': text})

    outputs = model.generate(inputs, **generate_config)
    outputs = outputs[0][len(inputs[0]):-1]
    response = tokenizer.decode(outputs)
    print(response)
    # æ‚¨å¥½ï¼Œæˆ‘æ˜¯æ‚¨çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œä¹Ÿå¯ä»¥æ˜¯ä½ çš„å¯¼æ¸¸ã€‚è¯·é—®æœ‰ä»€ä¹ˆé—®é¢˜æˆ‘å¯ä»¥å¸®æ‚¨è§£ç­”å‘¢ï¼Ÿ
    response, history = process_response(response, history)
    print(f'history: {history}')

    # second input
    role = "user"
    text = "æˆ‘æ‰“ç®—1æœˆä»½å»æµ·å—ç©ï¼Œå¯ä»¥ä»‹ç»ä¸€ä¸‹æµ·å—æœ‰å“ªäº›å¥½ç©çš„ï¼Œå¥½åƒçš„ä¹ˆï¼Ÿ"
    inputs = tokenizer.build_chat_input(text, history=history, role=role)['input_ids']
    history.append({'role': role, 'content': text})
    outputs = model.generate(inputs, **generate_config)
    outputs = outputs[0][len(inputs[0]):-1]
    response = tokenizer.decode(outputs)
    print(response)
    # å½“ç„¶å¯ä»¥ï¼æµ·å—æ˜¯ä¸€ä¸ªé£æ™¯ä¼˜ç¾ã€æ°”å€™å®œäººçš„çƒ­å¸¦æµ·æ´‹çœä»½ï¼Œæ‹¥æœ‰ä¸°å¯Œçš„æ—…æ¸¸èµ„æºå’Œç¾é£Ÿã€‚ä»¥ä¸‹æ˜¯ä¸€äº›æ‚¨å¯èƒ½ä¼šæ„Ÿå…´è¶£çš„æ™¯ç‚¹å’Œç¾é£Ÿï¼š
    # 1.
    # æ™¯ç‚¹ï¼š
    # - æµ·å—å²›ï¼šè¿™æ˜¯æµ·å—æœ€è‘—åçš„æ™¯ç‚¹ä¹‹ä¸€ï¼Œæ‹¥æœ‰ç¾ä¸½çš„æ²™æ»©å’Œçƒ­å¸¦é›¨æ—ã€‚
    # - äºšé¾™æ¹¾ï¼šè¿™æ˜¯æµ·å—æœ€è‘—åçš„æµ·æ»©ä¹‹ä¸€ï¼Œæ‹¥æœ‰æŸ”è½¯çš„æ²™æ»©å’Œæ¸…æ¾ˆçš„æµ·æ°´ã€‚
    # - å—å±±å¯ºï¼šè¿™æ˜¯æµ·å—æœ€è‘—åçš„ä½›æ•™å¯ºåº™ä¹‹ä¸€ï¼Œæ‹¥æœ‰ç²¾ç¾çš„å»ºç­‘å’Œæ‚ ä¹…çš„å†å²ã€‚
    # - åšé³Œäºšæ´²è®ºå›æ°¸ä¹…ä¼šå€ï¼šè¿™æ˜¯ä¸­å›½æœ€è‘—åçš„å›½é™…ä¼šè®®ä¸­å¿ƒï¼Œä¹Ÿæ˜¯äºšæ´²åœ°åŒºæœ€é‡è¦çš„æ”¿æ²»ã€ç»æµã€æ–‡åŒ–è®ºå›ä¹‹ä¸€ã€‚
    # 2.
    # ç¾é£Ÿï¼š
    # - æµ·å—é¸¡é¥­ï¼šè¿™æ˜¯æµ·å—æœ€è‘—åçš„ç¾é£Ÿä¹‹ä¸€ï¼Œä»¥é¸¡è‚‰ã€ç±³é¥­å’Œæ¤°æ±ä¸ºä¸»è¦ææ–™ï¼Œå‘³é“é²œç¾ã€‚
    # - æµ·é²œï¼šæµ·å—çš„æµ·é²œéå¸¸æ–°é²œï¼Œæ‚¨å¯ä»¥åœ¨å½“åœ°çš„æµ·é²œå¸‚åœºæˆ–é¤å…å“å°åˆ°å„ç§æµ·é²œç¾é£Ÿï¼Œå¦‚æ¸…è’¸æµ·é²œã€çº¢çƒ§æµ·é²œç­‰ã€‚
    # - æ¤°å­é¥­ï¼šè¿™æ˜¯æµ·å—æœ€è‘—åçš„ä¼ ç»Ÿç¾é£Ÿä¹‹ä¸€ï¼Œä»¥æ¤°å­è‚‰ã€ç³¯ç±³å’Œæ¤°å­æ±ä¸ºä¸»è¦ææ–™ï¼Œå‘³é“é¦™ç”œã€‚
    # - æµ·å—ç²‰ï¼šè¿™æ˜¯æµ·å—æœ€è‘—åçš„ä¼ ç»Ÿå°åƒä¹‹ä¸€ï¼Œä»¥ç±³ç²‰ã€çŒªè‚‰ã€èŠ±ç”Ÿã€è”¬èœç­‰ä¸ºä¸»è¦ææ–™ï¼Œå‘³é“é²œç¾ã€‚
    # å¸Œæœ›è¿™äº›ä¿¡æ¯å¯¹æ‚¨æœ‰æ‰€å¸®åŠ©ï¼Œå¦‚æœæ‚¨è¿˜æœ‰å…¶ä»–é—®é¢˜ï¼Œè¯·éšæ—¶é—®æˆ‘ã€‚

    # third input
    response, history = process_response(response, history)
    role = "user"
    text = "å“ªé‡Œé€‚åˆå†²æµªå’Œæ½œæ°´å‘¢ï¼Ÿ"
    inputs = tokenizer.build_chat_input(text, history=history, role=role)['input_ids']
    history.append({'role': role, 'content': text})
    outputs = model.generate(inputs, **generate_config)
    outputs = outputs[0][len(inputs[0]):-1]
    response = tokenizer.decode(outputs)
    print(response)
    # åœ¨æµ·å—ï¼Œå†²æµªå’Œæ½œæ°´çš„å¥½å»å¤„æœ‰å¾ˆå¤šã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å»ºè®®ï¼š
    # 1.
    # å†²æµªï¼š
    # - èºæ­Œæµ·ï¼šä½äºæµ·å—å²›è¥¿æµ·å²¸ï¼Œæ˜¯å†²æµªçˆ±å¥½è€…çš„å¤©å ‚ã€‚è¿™é‡Œçš„æµ·æµªé€‚ä¸­ï¼Œæ²™æ»©æ¼‚äº®ï¼Œéå¸¸é€‚åˆå†²æµªã€‚
    # - ä¸‰äºšï¼šä½äºæµ·å—å²›å—ç«¯ï¼Œæ˜¯æµ·å—æœ€è‘—åçš„å†²æµªèƒœåœ°ä¹‹ä¸€ã€‚è¿™é‡Œçš„æ²™æ»©ç»†è…»ï¼Œæµ·æµªè¾ƒå¤§ï¼Œéå¸¸é€‚åˆå†²æµªã€‚
    # 2.
    # æ½œæ°´ï¼š
    # - èœˆæ”¯æ´²å²›ï¼šä½äºæµ·å—å²›ä¸œæµ·å²¸ï¼Œæ˜¯æµ·å—æœ€è‘—åçš„æ½œæ°´èƒœåœ°ä¹‹ä¸€ã€‚è¿™é‡Œçš„æ½œæ°´æ¡ä»¶è¾ƒå¥½ï¼Œèƒ½è§åº¦è¾ƒé«˜ï¼Œæ°´ä¸‹ç”Ÿç‰©ä¸°å¯Œï¼Œéå¸¸é€‚åˆæ½œæ°´ã€‚
    # - è¥¿æ²™ç¾¤å²›ï¼šä½äºæµ·å—å²›ä¸œå—æ–¹å‘ï¼Œæ˜¯æµ·å—å¦ä¸€ä¸ªè‘—åçš„æ½œæ°´èƒœåœ°ã€‚è¿™é‡Œçš„æ½œæ°´æ¡ä»¶éå¸¸å¥½ï¼Œæ°´ä¸‹ä¸–ç•Œä¸°å¯Œå¤šå½©ï¼Œéå¸¸é€‚åˆæ½œæ°´çˆ±å¥½è€…ã€‚
    # å½“ç„¶ï¼Œå†²æµªå’Œæ½œæ°´éƒ½éœ€è¦ä¸€å®šçš„æŠ€èƒ½å’Œç»éªŒï¼Œå¦‚æœæ‚¨æ˜¯åˆå­¦è€…ï¼Œå»ºè®®åœ¨ä¸“ä¸šäººå£«çš„æŒ‡å¯¼ä¸‹è¿›è¡Œã€‚å¸Œæœ›è¿™äº›ä¿¡æ¯å¯¹æ‚¨æœ‰æ‰€å¸®åŠ©ï¼Œå¦‚æœæ‚¨è¿˜æœ‰å…¶ä»–é—®é¢˜ï¼Œ
    # è¯·éšæ—¶é—®æˆ‘ã€‚

    # forth input
    role = "user"
    text = "å¯ä»¥å¸®æˆ‘åšä¸€ä»½æ—…æ¸¸æ”»ç•¥å—ï¼Ÿ"
    inputs = tokenizer.build_chat_input(text, history=history, role=role)['input_ids']
    history.append({'role': role, 'content': text})
    outputs = model.generate(inputs, **generate_config)
    outputs = outputs[0][len(inputs[0]):-1]
    response = tokenizer.decode(outputs)
    print(response)
    # å½“ç„¶å¯ä»¥ï¼ä»¥ä¸‹æ˜¯ä¸€ä»½é’ˆå¯¹æµ·å—å†²æµªå’Œæ½œæ°´æ™¯ç‚¹çš„æ—…æ¸¸æ”»ç•¥ï¼š
    # ç¬¬ä¸€å¤©ï¼š
    # ä¸Šåˆï¼šæŠµè¾¾ä¸‰äºšï¼Œå‰å¾€äºšé¾™æ¹¾ã€‚åœ¨è¿™é‡Œï¼Œæ‚¨å¯ä»¥å°½æƒ…äº«å—é˜³å…‰ã€æ²™æ»©å’Œæµ·æ°´ã€‚äºšé¾™æ¹¾æ˜¯æµ·å—æœ€è‘—åçš„æµ·æ»©ä¹‹ä¸€ï¼Œæ‹¥æœ‰æŸ”è½¯çš„æ²™æ»©å’Œæ¸…æ¾ˆçš„æµ·æ°´ï¼Œ
    # éå¸¸é€‚åˆå†²æµªå’Œæ½œæ°´ã€‚
    # ä¸­åˆï¼šåœ¨äºšé¾™æ¹¾æµ·æ»©é™„è¿‘äº«ç”¨åˆé¤ï¼Œå“å°å½“åœ°ç¾é£Ÿã€‚
    # ä¸‹åˆï¼šå‰å¾€å—å±±å¯ºï¼Œè¿™æ˜¯æµ·å—æœ€è‘—åçš„ä½›æ•™å¯ºåº™ä¹‹ä¸€ï¼Œæ‹¥æœ‰ç²¾ç¾çš„å»ºç­‘å’Œæ‚ ä¹…çš„å†å²ã€‚åœ¨è¿™é‡Œï¼Œæ‚¨å¯ä»¥é¢†ç•¥åˆ°ä¸­å›½ä¼ ç»Ÿæ–‡åŒ–çš„é­…åŠ›ã€‚
    # æ™šä¸Šï¼šè¿”å›ä¸‰äºšå¸‚åŒºï¼Œåœ¨ä¸‰äºšæ¹¾æµ·æ»©é™„è¿‘äº«ç”¨æ™šé¤ï¼Œå¹¶æ¬£èµä¸‰äºšå¤œæ™¯ã€‚
    # ç¬¬äºŒå¤©ï¼š
    # ä¸Šåˆï¼šå‰å¾€èœˆæ”¯æ´²å²›ï¼Œè¿™æ˜¯æµ·å—æœ€è‘—åçš„æ½œæ°´èƒœåœ°ä¹‹ä¸€ã€‚åœ¨è¿™é‡Œï¼Œæ‚¨å¯ä»¥å°½æƒ…äº«å—æ½œæ°´çš„ä¹è¶£ã€‚
    # ä¸­åˆï¼šåœ¨èœˆæ”¯æ´²å²›é™„è¿‘äº«ç”¨åˆé¤ï¼Œå“å°å½“åœ°ç¾é£Ÿã€‚
    # ä¸‹åˆï¼šåœ¨èœˆæ”¯æ´²å²›æ½œæ°´ï¼Œæ¬£èµç¾ä¸½çš„æµ·åº•ä¸–ç•Œã€‚
    # æ™šä¸Šï¼šè¿”å›ä¸‰äºšå¸‚åŒºï¼Œåœ¨ä¸‰äºšæ¹¾æµ·æ»©é™„è¿‘äº«ç”¨æ™šé¤ï¼Œå¹¶æ¬£èµä¸‰äºšå¤œæ™¯ã€‚
    # ç¬¬ä¸‰å¤©ï¼š
    # ä¸Šåˆï¼šå‰å¾€åšé³Œäºšæ´²è®ºå›æ°¸ä¹…ä¼šå€ï¼Œè¿™æ˜¯ä¸­å›½æœ€è‘—åçš„å›½é™…ä¼šè®®ä¸­å¿ƒï¼Œä¹Ÿæ˜¯äºšæ´²åœ°åŒºæœ€é‡è¦çš„æ”¿æ²»ã€ç»æµã€æ–‡åŒ–è®ºå›ä¹‹ä¸€ã€‚
    # ä¸­åˆï¼šåœ¨åšé³Œäºšæ´²è®ºå›æ°¸ä¹…ä¼šå€é™„è¿‘äº«ç”¨åˆé¤ï¼Œå“å°å½“åœ°ç¾é£Ÿã€‚
    # ä¸‹åˆï¼šåœ¨åšé³Œäºšæ´²è®ºå›æ°¸ä¹…ä¼šå€é™„è¿‘å‚è§‚æ¸¸è§ˆã€‚
    # æ™šä¸Šï¼šè¿”å›ä¸‰äºšå¸‚åŒºï¼Œåœ¨ä¸‰äºšæ¹¾æµ·æ»©é™„è¿‘äº«ç”¨æ™šé¤ï¼Œå¹¶æ¬£èµä¸‰äºšå¤œæ™¯ã€‚
    # ç¬¬å››å¤©ï¼š
    # ä¸Šåˆï¼šå‰å¾€å—å±±å¯ºï¼Œè¿™æ˜¯æµ·å—æœ€è‘—åçš„ä½›æ•™å¯ºåº™ä¹‹ä¸€ï¼Œæ‹¥æœ‰ç²¾ç¾çš„å»ºç­‘å’Œæ‚ ä¹…çš„å†å²ã€‚åœ¨è¿™é‡Œï¼Œæ‚¨å¯ä»¥é¢†ç•¥åˆ°ä¸­å›½ä¼ ç»Ÿæ–‡åŒ–çš„é­…åŠ›ã€‚
    # ä¸­åˆï¼šåœ¨å—å±±å¯ºé™„è¿‘äº«ç”¨åˆé¤ï¼Œå“å°å½“åœ°ç¾é£Ÿã€‚
    # ä¸‹åˆï¼šå‰å¾€æµ·å—å²›ï¼Œè¿™æ˜¯æµ·å—æœ€è‘—åçš„æ™¯ç‚¹ä¹‹ä¸€ï¼Œæ‹¥æœ‰ç¾ä¸½çš„æ²™æ»©å’Œçƒ­å¸¦é›¨æ—ã€‚
    # æ™šä¸Šï¼šè¿”å›ä¸‰äºšå¸‚åŒºï¼Œåœ¨ä¸‰äºšæ¹¾æµ·æ»©é™„è¿‘äº«ç”¨æ™šé¤ï¼Œå¹¶æ¬£èµä¸‰äºšå¤œæ™¯ã€‚
    # ç¬¬äº”å¤©ï¼š
    # ä¸Šåˆï¼šå‰å¾€äºšé¾™æ¹¾ï¼Œåœ¨è¿™é‡Œï¼Œæ‚¨å¯ä»¥å°½æƒ…äº«å—é˜³å…‰ã€æ²™æ»©å’Œæµ·æ°´ã€‚äºšé¾™æ¹¾æ˜¯æµ·å—æœ€è‘—åçš„æµ·æ»©ä¹‹ä¸€ï¼Œæ‹¥æœ‰æŸ”è½¯çš„æ²™æ»©å’Œæ¸…æ¾ˆçš„æµ·æ°´ï¼Œéå¸¸é€‚åˆå†²æµªå’Œæ½œæ°´ã€‚
    # ä¸­åˆï¼šåœ¨äºšé¾™æ¹¾æµ·æ»©é™„è¿‘äº«ç”¨åˆé¤ï¼Œå“å°å½“åœ°ç¾é£Ÿã€‚
    # ä¸‹åˆï¼šå‰å¾€èœˆæ”¯æ´²å²›ï¼Œè¿™æ˜¯æµ·å—æœ€è‘—åçš„æ½œæ°´èƒœåœ°ä¹‹ä¸€ã€‚åœ¨è¿™é‡Œï¼Œæ‚¨å¯ä»¥å°½æƒ…äº«å—æ½œæ°´çš„ä¹è¶£ã€‚
    # æ™šä¸Šï¼šè¿”å›ä¸‰äºšå¸‚åŒºï¼Œåœ¨ä¸‰äºšæ¹¾æµ·æ»©é™„è¿‘äº«ç”¨æ™šé¤ï¼Œå¹¶æ¬£èµä¸‰äºšå¤œæ™¯ã€‚
    # å¸Œæœ›è¿™ä»½æ—…æ¸¸æ”»ç•¥å¯¹æ‚¨æœ‰æ‰€å¸®åŠ©ï¼Œç¥æ‚¨æ—…é€”æ„‰å¿«ï¼


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--config_path', default='predict_glm3_6b.yaml', type=str,
                        help='model config file path.')
    parser.add_argument('--load_checkpoint', type=str,
                        help='load model checkpoint path or directory.')
    parser.add_argument('--multi_role', action='store_true',
                        help='if run model prediction in multi_role mode.')

    args = parser.parse_args()
    if args.multi_role:
        multi_role_predict(
            args.config_path,
            args.load_checkpoint
        )
    else:
        main(
            args.config_path,
            args.load_checkpoint
        )
