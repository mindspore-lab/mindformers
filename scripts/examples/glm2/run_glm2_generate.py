# Copyright 2024 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""glm2 predict example."""
import os
import argparse

import mindspore as ms
from mindspore import Tensor, Model
from mindspore.common import initializer as init

from mindformers import MindFormerConfig
from mindformers.tools.logger import logger
from mindformers.core.context import build_context
from mindformers.core.parallel_config import build_parallel_config
from mindformers.trainer.utils import transform_and_load_checkpoint
from mindformers.models.glm2 import ChatGLM2Config, ChatGLM2Tokenizer, ChatGLM2ForConditionalGeneration


def main(config_path, load_checkpoint):
    # init config with yaml
    config = MindFormerConfig(config_path)
    config.use_parallel = False
    config.load_checkpoint = load_checkpoint

    # init context
    build_context(config)
    build_parallel_config(config)

    config.model.model_config.parallel_config = config.parallel_config
    model_config = ChatGLM2Config(**config.model.model_config)
    model_config.seq_length = 1024
    model_config.checkpoint_name_or_path = None
    model_name = config.trainer.model_name

    # build tokenizer
    tokenizer = ChatGLM2Tokenizer.from_pretrained(model_name)

    # build model
    network = ChatGLM2ForConditionalGeneration(model_config)
    model = Model(network)

    # load checkpoint
    if config.load_checkpoint:
        logger.info("----------------Transform and load checkpoint----------------")
        seq_length = config.model.model_config.seq_length
        # set auto transform ckpt
        if os.path.isdir(config.load_checkpoint):
            config.auto_trans_ckpt = True
        else:
            config.auto_trans_ckpt = False
        input_ids = Tensor(shape=(1, seq_length), dtype=ms.int32, init=init.One())
        infer_data = network.prepare_inputs_for_predict_layout(input_ids)
        transform_and_load_checkpoint(config, model, network, infer_data, do_predict=True)

    queries = ["ä½ å¥½",
               "è¯·ä»‹ç»ä¸€ä¸‹æ­å·",
               "é‚£é‡Œæœ‰ä»€ä¹ˆå¥½åƒçš„å—"]
    history = []
    for query in queries:
        prompt = tokenizer.build_prompt(query, history=history)
        input_ids = tokenizer(prompt)["input_ids"]

        output = network.generate([input_ids],
                                  max_length=model_config.seq_length,
                                  do_sample=False,
                                  top_p=3,
                                  top_k=0.7,
                                  temperature=1)

        output = output[0][len(input_ids):]
        response = tokenizer.decode(output)
        print(response)
        history += [(query, response)]


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--config_path', default='predict_glm2_6b.yaml', type=str,
                        help='model config file path.')
    parser.add_argument('--load_checkpoint', type=str,
                        help='load model checkpoint path or directory.')

    args = parser.parse_args()
    main(
        args.config_path,
        args.load_checkpoint
    )

# æ¨ç†ç»“æœï¼š
# response1:
# ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚
#
# response2:
# æ­å·æ˜¯ä¸­å›½æµ™æ±Ÿçœçœä¼šï¼Œä½äºæµ™æ±Ÿçœä¸œå—éƒ¨ï¼Œåœ°å¤„æµ™æ±ŸçœåŒ—éƒ¨ï¼Œä¸œä¸´ä¸œæµ·ï¼Œå—æ¥ç¦å»ºçœï¼ŒåŒ—ä¸æ±Ÿè‹çœæ¯—é‚»ï¼Œæ˜¯ä¸­å›½è‘—åçš„æ—…æ¸¸åŸå¸‚ä¹‹ä¸€ã€‚
#
# æ­å·æœ‰ç€æ‚ ä¹…çš„å†å²å’Œæ–‡åŒ–ï¼Œè¢«èª‰ä¸ºâ€œäººé—´å¤©å ‚â€ï¼Œè¢«èª‰ä¸ºâ€œå—å®‹éƒ½åŸâ€ï¼Œæ˜¯ä¸­å›½å—æ–¹è‘—åçš„å†å²æ–‡åŒ–ååŸä¹‹ä¸€ã€‚æ­å·è¿˜è¢«èª‰ä¸ºâ€œå…¨å›½æœ€å…·å¹¸ç¦æ„ŸåŸå¸‚â€ï¼Œå…·æœ‰ä¸°å¯Œçš„å†å²é—å­˜ã€ä¼˜ç¾çš„è‡ªç„¶é£å…‰å’Œæµ“éƒçš„æ–‡åŒ–æ°›å›´ã€‚
#
# æ­å·çš„ç»æµä»¥æœåŠ¡ä¸šä¸ºä¸»å¯¼äº§ä¸šï¼Œç‰¹åˆ«æ˜¯äº¤é€šè¿è¾“ã€ä»“å‚¨å’Œé‚®æ”¿ä¸šã€‚åŒæ—¶ï¼Œæ­å·ä¹Ÿæ˜¯ä¸­å›½é‡è¦çš„ç”µå­å•†åŠ¡å’Œäº’è”ç½‘äº§ä¸šåŸºåœ°ä¹‹ä¸€ï¼Œè¢«èª‰ä¸ºâ€œä¸­å›½ç”µå­å•†åŠ¡ä¹‹éƒ½â€ã€‚
#
# æ­å·çš„è‘—åæ™¯ç‚¹åŒ…æ‹¬è¥¿æ¹–ã€çµéšå¯ºã€åƒå²›æ¹–ã€é’±å¡˜æ±Ÿç­‰ã€‚è¥¿æ¹–æ˜¯ä¸­å›½è‘—åçš„é£æ™¯åèƒœåŒºä¹‹ä¸€ï¼Œè¢«èª‰ä¸ºâ€œäººé—´å¤©å ‚â€ï¼Œçµéšå¯ºæ˜¯ä¸­å›½è‘—åçš„ä½›æ•™å¯ºåº™ä¹‹ä¸€ï¼Œåƒå²›æ¹–å’Œé’±å¡˜æ±Ÿæ˜¯ä¸­å›½è‘—åçš„è‡ªç„¶é£æ™¯åŒºä¹‹ä¸€ã€‚
#
# æ­å·è¿˜æ‹¥æœ‰ä¸°å¯Œçš„äººæ–‡èµ„æºï¼Œè¢«èª‰ä¸ºâ€œäººé—´å¤©å ‚â€çš„æ­å·è¥¿æ¹–ã€çµéšå¯ºã€åƒå²›æ¹–ã€é’±å¡˜æ±Ÿç­‰æ™¯ç‚¹ï¼Œä»¥åŠå®‹åŸã€å—å®‹å¾¡è¡—ç­‰å†å²æ–‡åŒ–æ™¯ç‚¹ï¼Œéƒ½æ˜¯æ¸¸å®¢å‰æ¥æ­å·æ—…æ¸¸çš„çƒ­é—¨æ™¯ç‚¹ã€‚
#
# response3:
# æ­å·æ˜¯ä¸­å›½è‘—åçš„ç¾é£ŸåŸå¸‚ä¹‹ä¸€ï¼Œæœ‰è®¸å¤šç‰¹è‰²ç¾é£Ÿå’Œä¼ ç»Ÿèœè‚´ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›æ­å·çš„è‘—åç¾é£Ÿ:
#
# 1. è¥¿æ¹–é†‹é±¼ï¼šè¿™æ˜¯æ­å·æœ€è‘—åçš„èœè‚´ä¹‹ä¸€ï¼Œé±¼è‚‰é²œç¾ï¼Œå…¥å£å³åŒ–ï¼Œä½ä»¥é¦™é†‹ã€ç³–ã€å§œä¸ç­‰è°ƒæ–™ï¼Œå£æ„Ÿé…¸ç”œé€‚ä¸­ã€‚
#
# 2. é¾™äº•è™¾ä»ï¼šä»¥å½“åœ°ç‰¹äº§çš„é¾™äº•èŒ¶ä¸ºä½æ–™ï¼Œå°†é²œå«©çš„è™¾ä»ç‚’åˆ¶è€Œæˆï¼Œå£æ„Ÿæ¸…é¦™å¯å£ã€‚
#
# 3. çŒæ±¤åŒ…ï¼šåˆç§°å°ç¬¼åŒ…ï¼Œæ˜¯æ­å·çš„ä¼ ç»Ÿç‚¹å¿ƒä¹‹ä¸€ã€‚åŒ…å­çš„çš®è½¯é¦…é²œï¼Œæ±¤æ±é²œç¾ï¼Œéå¸¸å—æ¬¢è¿ã€‚
#
# 4. å§œæ¯é¸­ï¼šè¿™æ˜¯ä¸€é“æ­å¸®èœï¼Œä»¥é¸­è‚‰ã€å§œæ¯ã€è‘±ç­‰è°ƒæ–™çƒ¹åˆ¶è€Œæˆï¼Œå£æ„Ÿé²œç¾ã€‚
#
# 5. è€å­—å·å°åƒï¼šæ­å·è¿˜æœ‰å¾ˆå¤šè€å­—å·å°åƒåº—ï¼Œå¦‚èƒ¡åŒå£çƒ¤è‚‰ä¸²ã€å­”åºœå®¶å®´ã€å®‹å«‚é±¼ç¾¹ç­‰ï¼Œæ˜¯å½“åœ°å±…æ°‘å’Œæ¸¸å®¢çš„ç¾é£Ÿé€‰æ‹©ã€‚
#
# æ­¤å¤–ï¼Œæ­å·è¿˜æœ‰è®¸å¤šç‰¹è‰²å°åƒï¼Œå¦‚ç²½å­ã€è‡­è±†è…ã€ç³¯ç±³é¸¡ã€è‚‰å¤¹é¦ã€é¸­è¡€ç²‰ä¸æ±¤ç­‰ï¼Œè®©äººå‚æ¶æ¬²æ»´ã€‚
